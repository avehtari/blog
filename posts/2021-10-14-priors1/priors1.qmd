---
title: "Priors: Night work (Track 1)"
description: |
  Priors? Defined. Questions? Outlined. Purpose? Declared.
date: 10-15-2021
image: perm.JPG
categories: [Prior distributions, Fundamentals]
twitter-card:
  title:  "Priors: Night work (Track 1)"
  creator: "@dan_p_simpson"
citation: 
  url: https://dansblog.netlify.app/priors1

---

I have feelings. Too many feelings. And ninety six point seven three percent of them are about prior distributions^[The rest are about the night I saw Patti LuPone trying to get through the big final scene in War Paint as part of her costume loudly disintegrated.]. So I am going to write a few blog posts about prior distributions. 

To be very honest, this is mostly a writing exercise^[I'm told it's useful to warm up sometimes because this pandemic has me ice cold.] to get me out of a slump. 

So let's do this.

## No love, deep web

As far as I am concerned it's really fucking stupid to try to write about priors on their own. They are meaningless outside of their context. But, you know, this is a blog. So I get to be stupid.

So what is a prior distribution? It is whatever you want it to be. It is a probability distribution^[Sometimes.] that ... I don't know. Exists^[Except, and I cannot stress this enough, when it doesn't.].

Ok. This is not going well. Let's try again.

A prior distribution is, most of the time, a probability distribution on the parameters of a statistical model. For all practical purposes, we tend to work with its density, so if the parameter $\theta$, which could be a scalar but, in any interesting case, isn't, has prior $p(\theta)$.

## Captain fantastic and the brown dirt cowboy

But what does it all meeeeeeeean?

We have a prior distribution specified, gloriously, by it's density. And unlike destiny, density is meaningless. It only makes sense when we integrate it up to get a probability 
$$
\Pr(A) = \int_A p(\theta)\,d\theta.
$$

So what does the prior probabilty $\Pr(A)$ of a set $A$ actually mean in real life? The answer may shock you: it means something between nothing and everything.

**Scenario 1:** Let's imagine that we were trying to estimate the probability that someone in some relative homogeneous subgroup of customers completed a purchase on our website. It's a binary process, so the parameter of interest can probably just be the probability that a sale is made. While we don't know what the probability of a sale is for the subgroup of interest, we know a lot sales on our website in general (in particular, we know that about 3% of visits result in sales). So if I also believe that it would be wildly unlikely for 20% of visits to result in a sale, I could posit a prior like a $\text{Beta}(0.4,5)$ prior that captures (a version of) these two pieces of information.

```{r, code_folding=TRUE, eval = FALSE}
  ## Step 1: 
  
fn <- \(x) (qbeta(0.5,x[1], x[2]) - 0.02)^2 + 
  (qbeta(0.9, x[1], x[2]) - 0.2)^2

best <- optim(c(1/2,1/2), fn)

## Step 3: Profit.
## (AKA round and check)
qbeta(0.9, 0.4, 5)
qbeta(0.5, 0.4, 5)
```

**Scenario 2:** Let's imagine I want to do variable selection. I don't know why. I was just told I want to do variable selection. So I fire up the [Bayesian Lasso](https://statmodeling.stat.columbia.edu/2017/11/02/king-must-die/)^[Please do not do this!] and then threshold in some way. In this case, the prior encode a hoped-for property of my posterior. (To paraphrase Lana, hope is a dangerous thing for a woman like you to have because the Bayeisan Lasso does not work to the point that the original paper doesn't even suggest using it for variable selection^[Except for once in the abstract in a sentence that is in no way shape or formed backed up in the text. [Park and Casella (2008)](https://people.eecs.berkeley.edu/~jordan/courses/260-spring09/other-readings/park-casella.pdf)] it just, idk, liked the name. Statistics is wild.)

**Scenario 3:** I'm doing a regression with just one variable (because why not) and I think that the relationship between the response $y$ and the covariate $x$ is non-linear. That is, I think there is some unknown to me function $f(x)$ such that $\mathbb{E}(y_i) = f(x_i)$. So I ask a friend and they tell me to use a Gaussian Process prior for $f(\cdot)$ with an exponential covariance function. 

While I can write down the density for the joint prior of $(f(x_1), f(x_2,), \ldots, f(x_n))$, I do not know^[I do know. I know a very large amount about Gaussian processes. But lord in heaven I have seen the greatest minds of my generation subtly fuck up the interpretation of GP priors. Because it's increadibly hard. Maybe I'll blog about it one day. Because this is in markdown so I can haz equations.] what this prior means in any substantive sense. But I can tell you, you're gonna need that maths degree to even try.

And should you look deeper, you will find more and more scenarios where priors are doing different things for different reasons^[Some reasons are excellent. Some, like the poor Bayesian Lasso, are simply misguided.]. For each of these priors in each of these scenarios, we will be able to compute the posterior (or a reasonable computational approximation to it) and then work with that posterior to answer our questions. 

Different people^[or the same person in different contexts] will use priors different ways even for very similar problems^[Are any two statistical problems ever _the same_?]. This remains true even though they are nominally working under the same inferential framework.

_Bayesians are chaotic._

## Mapping out a sky / What you feel like, planning a sky

[Sondheim's ode to pointillism](https://www.youtube.com/watch?v=ducG55pfCMQ) feels relevant here. The reality of the prior distribution---and the whole reason the concept is so slippery and chaotic---is that you are, dot by dot, constructing the world of your inference. This act of construction is fundamental to understanding how Bayesian methods work, how to justify your choices, and how to use a [Bayesian workflow](https://arxiv.org/abs/2011.01808) to solve complex problems.

To torture the metaphor, our prior distribution is just our paint, unmixed, slowly congealing, possibly made of [ground up mummys](https://en.wikipedia.org/wiki/Mummy_brown). It is nothing without a painter and a brush.

The painter is the likelihood or, more generally, the generative link between the parameter values and the actual data, $p(y \mid \theta)$. The brush is the computational engine you use to actually produce the posterior painting^[Yes. I have a lot of feelings about this too, but meh. A good artist can make great art with minimal equipment (see When Doves Cry), but most people are not the genius Prince was so just use good tools and stress less!].

This then speaks to the core challenge with writing about priors: it depends on how you use them. It is a fallacy, or perhaps a foolishness, or perhaps a heresy^[I have written extensively about priors in the context of the Arianist heresy because of course I fucking have. [Part 1](https://statmodeling.stat.columbia.edu/2018/08/21/against-arianism/), [Part 2](https://statmodeling.stat.columbia.edu/2018/09/12/against-arianism-2-arianism-grande/), [Part 3](https://statmodeling.stat.columbia.edu/2019/05/24/against-arianism-3-consider-the-cognitive-models-of-the-field/). Apologies for mathematics eaten by a recent formatting change!]. Hell, when trying to understand a single inference [The Prior Can Only Be Understood In The Context Of The Likelihood](https://www.mdpi.com/1099-4300/19/10/555)^[Editors forced the word often into the published title and, like, who's going to fight?]. In the context of an entire _workflow_, [The Experiment is just as Important as the Likelihood in Understanding the Prior](https://link.springer.com/article/10.1007/s42113-019-00051-0).

For instance, using independent Cauchy priors for the coefficients in a linear regression model will result in a perfectly ok posterior. Whereas the same priors used in a logistic regression, [you may end up with posteriors with such heavy tails that they don't have a mean](https://projecteuclid.org/journals/bayesian-analysis/volume-13/issue-2/On-the-Use-of-Cauchy-Prior-Distributions-for-Bayesian-Logistic/10.1214/17-BA1051.full)! (Do we care? Well, yes. If we want reasonable uncertainty intervals we probably want 2 or so moments otherwise those large deviations are gonna getcha!)

## So what?

All of this is fascinating. And it is a lot less chaotic than it initially sounds. 

The reality is that while two Bayesians may use different priors and, hence, produce different posteriors for the same data set.This can be extreme. For example, if I am trying to estimate the mean of data generated by $y_i \sim N(\mu, 1)$, then I can choose a prior^[$N(2-\bar{y},n^{-1})$] (that depends on the data) so that the posterior mean $\mathbb{E}(\mu \mid y) =1$. Or, to put it differently, I can get any answer I want if I choose an prior carefully (and in a data-dependent manner).

But this isn't necessarily a problem. This is because the posteriors produced by two _sensible_ priors for the same problem will produce fairly similar results^[What does this even mean? Depends on your context really. But a working definition is that the big picture features of the posterior are similar enough that if you were to use it to make a decision, that decision doesn't change very much.]. The prior I used to cheat in the previous example would not be considered sensible by anyone looking at it^[But omg subtle high dimensional stuff and I guess I'll talk about that later maybe too?].


But what is a sensible prior? Can you tell if a prior is sensible or not in its particular context? Well honey, how long have you got. The thing about starting a (potential) series of blog posts is that I don't really know how far I'm going to get, but I would really like to talk a lot about that over the next little while.
