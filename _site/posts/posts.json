[
  {
    "path": "posts/2022-06-03-that-psis-proof/",
    "title": "Tail stabilization of importance sampling etimators: A bit of theory",
    "description": "Look. I had to do it so I wrote it out in detail. This is some of the convergence theory for truncated and winzorised importance sampling estimators",
    "author": [
      {
        "name": "Dan Simpson",
        "url": "https://dpsimpson.github.io"
      }
    ],
    "date": "2022-06-15",
    "categories": [],
    "contents": "\nImagine you have a target probability distribution \\(p(\\theta)\\) and you want to estimate the\nexpectation \\(I_h = \\int h(\\theta)\np(\\theta)\\,d(\\theta)\\). That’s lovely and everything, but if it\nwas easy none of us would have jobs. High-dimensional quadrature is a\npain in the arse.\nA very simple way to get an decent estimate of \\(I_h\\) is to use importance\nsampling, that is taking draws \\(\\theta_s\\), \\(s =\n1,\\ldots, S\\) from some proposal distribution \\(\\theta_s \\sim g(\\theta)\\). Then, noting\nthat \\[\nI_h = \\int h(\\theta) p (\\theta)\\,d\\theta = \\int h(\\theta)\n\\underbrace{\\frac{p(\\theta)}{g(\\theta)}}_{r(\\theta)}g(\\theta)\\,d\\theta,\n\\] we can use Monte Carlo to estimate the second integral. This\nleads to the importance sampling estimator \\[\nI_h^S = \\sum_{s=1}^S h(\\theta_s) r(\\theta_s).\n\\]\nThis all seems marvellous, but there is a problem. Even though \\(h\\) is probably a very pleasant function\nand \\(g\\) is a nice friendly\ndistribution, \\(r(\\theta)\\) can be an\nabsolute beast. Why? Well it’s1 the ratio of two\ndensities and there is no guarantee that the ratio of two nice functions\nis itself a nice function. In particular, if the bulk of the\ndistributions \\(p\\) and \\(g\\) are in different places, you’ll end up\nwith the situation where for most draws \\(r(\\theta_s)\\) is very small2 and\na few will be HUGE3.\nThis will lead to an extremely unstable estimator.\nIt is pretty well known that the raw importance sampler \\(I_h^S\\) will behave nicely (that is will be\nunbiased with finite variance) precisely when the distribution of \\(r_s = r(\\theta_s)\\) has finite\nvariance.\nElementary treatments stop there, but they miss two very big\nproblems. The most obvious one is that it’s basically impossible to\ncheck if the variance of \\(r_s\\) is\nfinite. A second, much larger but much more subtle problem, is that the\nvariance can be finite but massive. This is probably the most\ncommon case in high dimensions. McKay has an excellent example where the\nimportance ratios are bounded, but that bound is so large that\nit is infinite for all intents and purposes.\nAll of which is to say that importance sampling doesn’t work unless\nyou work on it.\nTruncated importance\nsampling\nIf the problem is the fucking ratios then by gum we will fix the\nfucking ratios. Or so the saying goes.\nThe trick turns out to be modifying the largest ratios enough that we\nstabilise the variance, but not so much as to overly bias the\nestimate.\nThe first version of this was truncated importance\nsampling (TIS), which selects a threshold \\(T\\) and estimates the expectation as \\[\nI_\\text{TIS}^S = \\frac{1}{S}\\sum_{s= 1}^S h(\\theta_s) \\max\\{r(\\theta_s),\nT\\}.\n\\] It’s pretty obvious that \\(I^S_\\text{TIS}\\) has finite variance for\nany fixed \\(T\\), but we should be\npretty worried about the bias. Unsurprisingly, there is going to be a\ntrade-off between the variance and the bias. So let’s explore that.\nThe bias of TIS\nTo get an expression for the bias, first let us write \\(r_s = r(\\theta_s)\\) and \\(h_s = h(\\theta_s)\\) for \\(\\theta_s \\sim g\\). Occasionally we will\ntalk about the joint distribution or \\((r_s,h_s) \\sim (R,H)\\). Sometimes we will\nalso need to use the indicator variables \\(z_i\n= 1_{r_i < T}\\).\nThen, we can write4 \\[\nI = \\mathbb{E}(HR \\mid R \\leq T) \\Pr(R \\leq T) + \\mathbb{E}(HR \\mid R\n> T) \\Pr(R > T).\n\\]\nHow does this related to TIS? Well. Let \\(M\n= \\sum_{s=1}^S z_i\\) be the random variable denoting the number\nof times \\(r_i > T\\). Then, \\[\\begin{align*}\n\\mathbb{E}(I_\\text{TIC}^S) &= \\mathbb{E}\\left(\n\\frac{1}{S}\\sum_{s=1}^Sz_ih_ir_i\\right)  + \\mathbb{E}\\left(\n\\frac{T}{S}\\sum_{s=1}^S(1-z_i)h_i\\right) \\\\\n&=\\mathbb{E}_M\\left[\\frac{S-M}{S}\\mathbb{E}(HR \\mid R < T) +\n\\frac{MT}{S}\\mathbb{E}(H \\mid R > T)\\right] \\\\\n&=\\mathbb{E}(HR \\mid R \\leq T) \\Pr(R \\leq T) + T\\mathbb{E}(H \\mid R\n> T) \\Pr(R > T).\n\\end{align*}\\]\nHence the bias in TIS is \\[\nI - \\mathbb{E}(I_\\text{TIS}^S) = \\mathbb{E}(H(R-T) \\mid R > T) \\Pr(R\n> T).\n\\]\nTo be honest, this doesn’t look phenomenally interesting for fixed\n\\(T\\), however if we let \\(T = T_S\\) depend on the sample size then as\nlong as \\(T_S \\rightarrow \\infty\\) we\nget vanishing bias.\nWe can get more specific if we make the assumption about the tail of\nthe importance ratios. In particular, we will assume that5\n\\(1-R(r) = \\Pr(R > r) =\ncr^{-1/k}(1+o(1))\\) for some6 \\(k<1\\).\nWhile it seems like this will only be useful for estimating \\(\\Pr(R>T)\\), it turns out that under some\nmild7 technical conditions, the\nconditional excess distribution function8\n\\[\nR_T(y) = \\Pr(R - T \\leq y \\mid R > T) = \\frac{R(T + y) -\nR(T)}{1-R(T)},\n\\] is well approximated by a Generalised Pareto Distribution as\n\\(T\\rightarrow \\infty\\). Or, in maths,\nas \\(T\\rightarrow \\infty\\), \\[\nR_T(y) \\rightarrow \\begin{cases} 1- \\left(1 +\n\\frac{ky}{\\sigma}\\right)^{-1/k}, \\quad & k \\neq 0 \\\\\n1- \\mathrm{e}^{-y/\\sigma}, \\quad &k = 0,\n\\end{cases}\n\\] for some \\(\\sigma > 0\\)\nand \\(k \\in \\mathbb{R}\\). The shape9 parameter \\(k\\) is very important for us, as it tells\nus how many moments the distribution has. In particular, if a\ndistribution \\(X\\) has shape parameter\n\\(k\\), then \\[\n\\mathbb{E}|X|^\\alpha < \\infty, \\quad \\forall \\alpha < \\frac{1}{k}.\n\\] We will focus exclusively on the case where \\(k < 1\\). When \\(k < 1/2\\), the distribution has finite\nvariance.\nIf \\(1- R(r) = cr^{-1/k}(1+ o(1))\\),\nthen the conditional exceedence function is \\[\\begin{align*}\nR_T(y) &=  \\frac{cT^{-1/k}(1+  o(1)) -\nc(T+y)^{-1/k}(1+  o(1))}{cT^{-1/k}(1+  o(1)))} \\\\\n&= \\left[1 - \\left(1 + \\frac{y}{T}\\right)^{-1/k}\\right](1 + o(1)),\n\\end{align*}\\] which suggests that as \\(T\\rightarrow \\infty\\), \\(R_T\\) converges to a generalised Pareto\ndistribution with shape parameter \\(k\\)\nand scale parameter \\(\\mathcal{O}(T)\\).\nAll of this work lets us approximate the distribution of \\((R-T \\mid R>T )\\) and use the formula\nfor the mean of a generalised Pareto distribution. This gives us the\nestimate \\[\n\\mathbb{E}(R- T \\mid R>T) \\approx \\frac{T}{1-k},\n\\] which estimates the bias when \\(h(\\theta)\\) is constant10\nas \\[\nI - \\mathbb{E}(I_\\text{TIS}^S) \\approx\n\\mathcal{O}\\left(T^{1-1/k}\\right).\n\\]\nFor what it’s worth, Ionides got the same result more directly in the\nTIS paper, but he wasn’t trying to do what I’m trying to do.\nThe variance in TIS\nThe variance is a little bit more annoying. We want it to go to\nzero.\nAs before, we condition on \\(z_s\\)\n(or, equivalently, \\(M\\)) and then use\nthe law of total variance. We know from the bias calculation that \\[\n\\mathbb{E}(I_\\text{TIS}^S \\mid M) =\\frac{S-M}{S}\\mathbb{E}(HR \\mid\nR>T) + \\frac{TM}{S}\\mathbb{E}(H \\mid R>T).\n\\]\nA similarly quick calculation tells us that \\[\n\\mathbb{V}(I_\\text{TIS}^S \\mid M) = \\frac{S-M}{S^2}\\mathbb{V}(HR \\mid R\n\\leq T) +\\frac{MT^2}{S^2}\\mathbb{V}(H \\mid R>T).\n\\] To close it out, we recall that \\(M\\) is the sum of Bernoulli random\nvariables so \\[\nM \\sim \\text{Binomial}(S, \\Pr(R > T)).\n\\]\nWith this, we can get an expression for the unconditional variance.\nTo simplify the expression, let’s write \\(p_T\n= \\Pr(R > T)\\). Then, \\[\\begin{align*}\n\\mathbb{V}(I_\\text{TIS}^S) &=\\mathbb{E}_M\\mathbb{V}(I_\\text{TIS}^S\n\\mid M) + \\mathbb{V}_M\\mathbb{E}(I_\\text{TIS}^S \\mid M) \\\\\n&= S^{-1}(1-p_T)\\mathbb{V}(HR \\mid R \\leq T)\n+S^{-1}T^2p_T\\mathbb{V}(H \\mid R>T)\\\\\n&\\quad + S^{-1}p_T(1-p_T)\\mathbb{E}(HR \\mid R>T)^2 +\nS^{-1}Tp_T(1-p_T)\\mathbb{E}(H \\mid R>T)^2.\n\\end{align*}\\]\nThere are four terms in the variance. The first and third terms are\nclearly harmless: they go to zero no matter how we choose \\(T_S\\). Our problem terms are the second and\nfourth. We can tame the fourth term if we choose \\(T_S = o(S)\\). But that doesn’t seem to help\nwith the second term. But it turns out it is enough. To see this, we\nnote that \\[\\begin{align*}\nTp_T\\mathbb{V}(H\\mid R>T) &=\\leq Tp_T\\mathbb{E}(H^2 \\mid\nR>T)\\\\\n&\\leq p_T\\mathbb{E}(H^2 R\\mid R>T) \\\\\n&\\leq \\mathbb{E}(H^2 R)\\\\\n&= \\int h(\\theta)^2 p(\\theta)\\,d\\theta < \\infty.\n\\end{align*}\\] where the second inequality uses the fact that\n\\(R>T\\) and the third comes from the\nlaw of total probability.\nSo the TIS estimator has vanishing bias and variance as long as the\ntruncation \\(T_S \\rightarrow \\infty\\)\nand \\(T_S = o(S)\\). Once again, this is\nin the TIS paper, where it is proved in a much more compact way.\nAsymptotic properties\nIt can also be useful to have an understanding of how wild the\nfluctuations \\(I - I_\\text{TIS}^S\\)\nare. For traditional importance sampling, we know that if \\(\\mathbb{E}(R^2)\\) is finite, then then the\nfluctuations are, asymptotically, normally distributed with mean zero.\nNon-asymptotic results were given by Chatterjee and Diaconis that\nalso hold even when the estimator has infinite variance.\nFor TIS, it’s pretty obvious that for fixed \\(T\\) and \\(h \\geq\n0\\), \\(I_\\text{TIS}^S\\) will be\nasymptotically normal (it is, after all, the sum of bounded random\nvariables). For growing sequences \\(T_S\\) it’s a tiny bit more involved: it is\nnow a triangular array11 rather than a sequence of random\nvariables. But in the end very classical results tell us that for\nbounded12 \\(h\\), the fluctuations of the TIS estimator\nare asymptotically normal.\nIt’s worth saying that when \\(h(\\theta)\\) is unbounded, it might\nbe necessary to truncate the product \\(h_ir_i\\) rather than just \\(r_i\\). This is especially relevant if \\(\\mathbb{E}(H \\mid R=r)\\) grows rapidly with\n\\(r\\). Personally, I can’t think of a\ncase where this happens: \\(r(\\theta)\\)\nusually grows (super-)exponentially in \\(\\theta\\) while \\(h(\\theta)\\) usually grows polynomially,\nwhich implies \\(\\mathbb{E}(H \\mid\nR=r)\\) grows (poly-)logarithmically.\nThe other important edge case is that when \\(h(\\theta)\\) can be both positive and\nnegative, it might be necessary to truncate \\(h_ir_i\\) both above and below.\nWinsorised importance\nsampling\nTIS has lovely theoretical properties, but it’s a bit challenging to\nuse in practice. The problem is, there’s really no practical guidance on\nhow to choose the truncation sequence.\nSo let’s do this differently. What if instead of specifying a\nthreshold directly, we instead decided that the largest \\(M\\) values are potentially problematic and\nshould be modified? Recall that for TIS, the number of samples that\nexceeded the threshold, \\(M\\), was\nrandom while the threshold was fixed. This is the opposite situation:\nthe number of exceedences is fixed but the threshold is random.\nThe threshold is now the \\(M\\)th\nlargest value of \\(r_s\\). We denote\nthis using order statistics notation: we re-order the sample so that\n\\[\nr_{1:S} \\leq r_{2:S}\\leq \\ldots r_{S:S}.\n\\] With this notation, the threshold is \\(T = r_{S-M+1:S}\\) and the Winsorized\nimportance sampler (WIS) is \\[\nI^S_\\text{WIS} = \\frac{1}{S}\\sum_{s = 1}^{S-M} h_{s:S}r_{s:S} +\n\\frac{r_{S-M+1:S}}{S}\\sum_{s=S-M+1}^S h_{s:S},\n\\] where \\((r_{s:S}, h_{s:S})\\)\nare the \\((r_s, h_s)\\) pairs\nordered so that \\(r_{1:S} \\leq\nr_{2:S}\\leq \\cdots \\leq r_{S:S}\\). Note that \\(h_{s:S}\\) are not necessarily in increasing\norder: they are known as concomitants of \\(r_{s:S}\\), which is just a fancy way to say\nthat they’re along for the ride. It’s very important that we\nreorder the \\(h_s\\) when we reorder the\n\\(r_s\\), otherwise we won’t preserve\nthe joint distribution and we’ll end up with absolute rubbish.\nWe can already see that this is both much nicer and much wilder than\nthe TIS distribution. It is convenient that \\(M\\) is no longer random! But what the hell\nare we going to do about those order statistics? Well, the answer is\nvery much the same thing as before: condition on them and hope for the\nbest.\nConditioned on the event13 \\(\\{r_{S-M+1:S} = T\\}\\), we get \\[\n\\mathbb{E}\\left(I_\\text{WIS}^S \\mid r_{S-M+1:S} = T\\right) = \\left(1 -\n\\frac{M}{S}\\right)\\mathbb{E}(RH \\mid R < T) + \\frac{MT}{S}\n\\mathbb{E}(H \\mid R \\geq T).\n\\] From this, we get that the bias, conditional on \\(r_{S-M+1:S} = T\\) is \\[\\begin{multline*}\n\\left|I - \\mathbb{E}\\left(I_\\text{WIS}^S \\mid r_{S-M+1:S} =\nT\\right)\\right| =\\left|\\left[\\Pr(R < T) - \\left(1 -\n\\frac{M}{S}\\right)\\right]\\mathbb{E}(RH \\mid R < T) \\right.\\\\\n\\left.+ \\left[\\Pr(R \\geq T) - \\frac{M}{S}\\right] \\mathbb{E}(H(R - T)\n\\mid R \\geq T)\\right|.\n\\end{multline*}\\]\nYou should immediately notice that we are in quite a different\nsituation from TIS, where only the tail contributed to the bias. By\nfixing \\(M\\) and randomising the\nthreshold, we have bias contributions from both the bulk (due,\nessentially, to a weighting error) and from the tail (due to both the\nweighting error and the truncation). This is going to require us to be a\nbit creative.\nWe could probably do something more subtle and clever here, but that\nis not my way. Instead, let’s use the triangle inequality to say \\[\n\\left|\\mathbb{E}(RH \\mid R > T)\\right| \\leq \\frac{\\mathbb{E}(R |H|\n1(R<T))}{\\Pr(R <T)} \\leq \\frac{\\|h\\|_{L^1(p)}}{\\Pr(R  <T)}\n\\] and so the first term in the bias can be bounded if we can\nbound the relative error \\[\n\\mathbb{E}\\left|1 - \\frac{1- M/S}{\\Pr(R < r_{S-M+1:S})}\\right|.\n\\]\nNow the more sensible among you will say Daniel, No!\nThat’s a ratio! That’s going to be hard to bound. And, of course,\nyou are right. But here’s the thing: if \\(M\\) is small relative to \\(S\\), it is tremendously unlikely\nthat \\(r_{S-M+1:S}\\) is anywhere near\nzero. This is intuitively true, but also mathematically true.\nTo attack this expectation, we are going to look at a slightly\ndifferent quantity that has the good grace of being non-negative.\nLemma Let \\(X_s\\),\n\\(s= 1, \\ldots S\\) be an iid sample\nfrom \\(F_X\\), let \\(0\\leq k\\leq S\\) be an integer. Then \\[\n\\frac{p}{F_X(x_{k:S})} -p \\stackrel{d}{=} \\frac{p(S-k+1)}{k}\n\\mathcal{F},\n\\] and \\[\n\\frac{1-p}{1- F_x/(x_{k:S})} - (1-p) \\stackrel{d}{=}\n\\frac{k(1-p)}{S-k+1}\\mathcal{F}^{-1}\n\\] where \\(\\mathcal{F}\\) is an\nF-distributed random variable with parameters \\((2(S-k+1), 2k)\\).\nProof\nFor any \\(t\\geq 0\\), \\[\\begin{align*}\n\\Pr\\left(\\frac{p}{F_X(x_{k:S})} - p \\leq t\\right) &=\\Pr\\left(p -\npF_X(x_{k:S}) \\leq tF_X(x_{k:S})\\right) \\\\\n&= \\Pr\\left(p  \\leq (t+p)F_X(x_{k:S})\\right) \\\\\n&=\\Pr\\left(F_X(x_{k:S}) \\geq \\frac{p}{p+t}\\right)\\\\\n&= \\Pr\\left(x_{k:S} \\geq F_X^{-1}\\left(\\frac{p}{p+t}\\right)\\right)\\\\\n&= 1- I_{\\frac{p}{p+t}}(k, S-k+1) \\\\\n&= I_{\\frac{t}{p+t}}(S-k+1, k),\n\\end{align*}\\] where \\(I_p(a,b)\\) is the incomplete Beta\nfunction.\nYou could, quite reasonably, ask where the hell that incomplete Beta\nfunction came from. And if I had thought to look this up, I would say\nthat it came from Equation 2.1.5 in David and Nagaraja’s book on order\nstatistics. Unfortunately, I did not look this up. I derived it, which\nis honestly not very difficult. The trick is to basically note that the\nevent \\(\\{x_{k:S} \\leq \\tau\\}\\) is the\nsame as the event that at least \\(k\\)\nof the samples \\(x_s\\) are less than or\nequal to \\(\\tau\\). Because the \\(x_s\\) are independent, this is the\nprobability of observing at least \\(k\\)\nheads from a coin with the probability of a head \\(\\Pr(x \\leq \\tau) = F_X(\\tau)\\). If you look\nthis up on Wikipedia14 you see15\nthat it is \\(I_{1-F_X(\\tau)}(k,S-k+1)\\). The rest just\ncome from noting that \\(\\tau =\nF_X^{-1}(t/(p+t))\\) and using the symmetry \\(1-I_p(a,b) = I_{1-p}(b,a)\\).\nTo finish this off, we note that \\[\n\\Pr(\\mathcal{F} \\leq x) = I_{\\frac{S-k+1}{(S-k+1)x+ k}}(S-k+1,k).\n\\] From which, we see that \\[\\begin{align*}\n\\Pr\\left(\\frac{p}{F_X(x_{k:S})} - p \\leq t\\right)\n&=\\Pr\\left(\\mathcal{F} \\leq \\frac{k}{p(S-k+1)}t\\right) \\\\\n&= \\Pr\\left(\\frac{p(S-k+1)}{k}\\mathcal{F} \\leq t\\right).\n\\end{align*}\\]\nThe second result follows the same way and by noting that \\(\\mathcal{F}^{-1}\\) is also F-distributed\nwith parameters \\((k, S-k+1)\\).\nThe proof has ended\nNow, obviously, in this house we do not trust mathematics. Which is\nto say that I made a stupid mistake the first time I did this and forgot\nthat when \\(Z\\) is binomial, \\(\\Pr(Z \\geq k) = 1 - \\Pr(Z \\leq k-1)\\) and\nhad a persistent off-by-one error in my derivation. But we test out our\nresults so we don’t end up doing the dumb thing.\nSo let’s do that. For this example, we will use generalised\nPareto-distributed \\(X\\).\n\n\nlibrary(tidyverse)\nxi <- 0.7\ns <- 2\nu <- 4\n\nsamp <- function(S, k, p, \n                 Q = \\(x) u + s*((1-x)^(-xi)-1)/xi, \n                 F = \\(x) 1 - (1 + xi*(x - u)/s)^(-1/xi)) {\n  # Use theory to draw x_{k:S}\n  xk <- Q(rbeta(1, k, S - k + 1))\n  c(1 - p / F(xk), 1-(1-p)/(1-F(xk)))\n}\n\nS <- 1000\nM <- 50\nk <- S - M + 1\np <- 1-M/S\nN <- 100000\n\nfs <- rf(N, 2 * (S - k + 1), 2 * k )\ntibble(theoretical = 1-p - p * fs * (S - k + 1)/k,\n       xks = map_dbl(1:N, \\(x) samp(S, k, p)[1])) %>%\n  ggplot() + stat_ecdf(aes(x = xks), colour = \"black\") + \n  stat_ecdf(aes(x = theoretical), colour = \"red\", linetype = \"dashed\") +\n  ggtitle(expression(1 - frac(1-M/S , R(r[S-M+1:S]))))\n\n\n\ntibble(theoretical = p - (1-p) * k/(fs * (S - k + 1)),\n       xks = map_dbl(1:N, \\(x) samp(S, k, p)[2])) %>%\n  ggplot() + stat_ecdf(aes(x = xks), colour = \"black\") + \n  stat_ecdf(aes(x = theoretical), colour = \"red\", linetype = \"dashed\") +\n  ggtitle(expression(1 - frac(M/S , 1-R(r[S-M+1:S]))))\n\n\n\n\nFabulous. It follow then that \\[\n\\left|1 - \\frac{1-M/S}{R(r_{S-M+1})} \\right| \\stackrel{d}=\n\\left|\\frac{M}{S} -  \\frac{M(S-M)}{S(S-M-1)}\\mathcal{F}\\right| \\leq\n\\frac{M}{S} +  \\frac{M(S-M)}{S(S-M-1)} \\mathcal{F},\n\\] where \\(\\mathcal{F}\\) has an\nF-distribution with \\((M, S-M+1)\\)\ndegrees of freedom. As \\(\\mathbb{E}(\\mathcal{F}) = 1 + 1/(S-M-1)\\),\nit follows that this term goes to zero as long as \\(M = o(S)\\). This shows that the first term\nin the bias goes to zero.\nIt’s worth noting here that we’ve also calculated that the bias is\nat most \\(\\mathcal{O}(M/S)\\),\nhowever, this rate is extremely sloppy. That upper bound we just\ncomputed is unlikely to be tight. A better person than me would\nprobably check, but honestly I just don’t give a shit16\nThe second term in the bias is \\[\n\\left[\\Pr(R \\geq T) - \\frac{M}{S}\\right] \\mathbb{E}(H(R - T) \\mid R \\geq\nT).\n\\] As before, we can write this as \\[\n\\left(1 - \\frac{M/S}{1-R(T)}\\right)|\\mathbb{E}(H(R - T) 1_{R \\geq T})|\n\\leq \\left|1 - \\frac{M/S}{1-R(T)}\\right|\\|h\\|_{L^1(p)}.\n\\] By our lemma, we know that the distribution of the term in the\nabsolute value when \\(T = r_{S-M+1}\\)\nis the same as \\[\n1-\\frac{M}{S} -\\left(1 - \\frac{M}{S} + \\frac{1}{S}\\right)\\mathcal{F} =\n(\\mu_F-\\mathcal{F})  +\\frac{M}{S}(\\mathcal{F}-\\mu_F) -\n\\frac{1}{S}\\mathcal{F} +  \\frac{1}{M-1}\\left(\\frac{M}{S} - 1\\right),\n\\] where \\(\\mathcal{F} \\sim\n\\text{F}_{2(S-M+1), 2M}\\), which has mean \\(\\mu_F = 1+(M-1)^{-1}\\) and variance \\[\n\\sigma^2_F = \\frac{M^2S}{(S-M+1)(M-1)^2(M-2)} = \\frac{1}{M}(1 +\n\\mathcal{O}(M^{-1} + MS^{-1}).\n\\] From Jensen’s inequality, we get \\[\n\\mathbb{E}(|\\mathcal{F} - \\mu_F|) \\leq \\sigma_F = M^{-1/2}(1 + o(1)).\n\\] If follows that \\[\n\\mathbb{E}\\left|1 - \\frac{M/S}{1-R(r_{S-M+1:S})}\\right| \\leq\nM^{-1/2}(1+o(1))M^{1/2}S^{-1}(1 + o(1)) + S^{-1}(1+ o(1)) +\n(M-1)^{-1}(1+o(1)),\n\\] and so we get vanishing bias as long as \\(M\\rightarrow \\infty\\) and \\(M/S \\rightarrow 0\\).\nOnce again, I make no claims of tightness17.\nJust because it’s a bit sloppy at this point doesn’t mean the job isn’t\ndone.\nTheorem Let \\(\\theta_s\\), \\(s =\n1,\\ldots, S\\) be an iid sample from \\(G\\) and let \\(r_s\n= r(\\theta_s) \\sim R\\). Assume that\n\\(R\\) is absolutely\ncontinuous\n\\(M \\rightarrow \\infty\\) and\n\\(S^{-1}M \\rightarrow 0\\)\n\\(h \\in L^1(p)\\)\nThen Winsorized importance sampling converges in \\(L^1\\) and is asymptotically unbiased.\nEnd of Theorem\nOk so that’s nice. But you’ll notice that I did not mention our\npiss-poor rate. That’s because there is absolutely no way in hell that\nthe bias is \\(\\mathcal{O}(M^{-1/2})\\)!\nThat rate is an artefact of a very sloppy bound on \\(\\mathbb{E}|1-\\mathcal{F}|\\).\nUnfortunately, Mathematica couldn’t help me out. Its asymptotic\nabilities shit the bed at the sight of \\({}_2F_1(a,b;c;z))\\), which is everywhere in\nthe exact expression (which I’ve put below in the fold.\n\nMathematica expression for \\(\\mathbb{E}|1-\\mathcal{F}|\\).\n-(((M/(1 + S))^(-(1/2) - S/2)*Gamma[(1 + S)/2]*\n     (6*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) - \n        5*M*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) + \n        M^2*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) + \n        8*S*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) - \n        6*M*S*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) + \n        M^2*S*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) + \n        2*S^2*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) - \n        M*S^2*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) - \n         6*Sqrt[-(M/(-1 + M - S))]*Sqrt[(-1 - S)/(-1 + M - S)]*\n        (M/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[1, (1/2)*(-1 + M - S), \n                                                      M/2, M/(-1 + M - S)] + 8*M*Sqrt[-(M/(-1 + M - S))]*\n        Sqrt[(-1 - S)/(-1 + M - S)]*(M/(1 - M + S))^(M/2 + S/2)*\n        Hypergeometric2F1[1, (1/2)*(-1 + M - S), M/2, M/(-1 + M - S)] - \n        2*M^2*Sqrt[-(M/(-1 + M - S))]*Sqrt[(-1 - S)/(-1 + M - S)]*\n        (M/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[1, (1/2)*(-1 + M - S), \n                                                      M/2, M/(-1 + M - S)] - 8*Sqrt[-(M/(-1 + M - S))]*\n        Sqrt[(-1 - S)/(-1 + M - S)]*S*(M/(1 - M + S))^(M/2 + S/2)*\n        Hypergeometric2F1[1, (1/2)*(-1 + M - S), M/2, M/(-1 + M - S)] + \n        4*M*Sqrt[-(M/(-1 + M - S))]*Sqrt[(-1 - S)/(-1 + M - S)]*S*\n        (M/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[1, (1/2)*(-1 + M - S), \n                                                      M/2, M/(-1 + M - S)] - 2*Sqrt[-(M/(-1 + M - S))]*\n        Sqrt[(-1 - S)/(-1 + M - S)]*S^2*(M/(1 - M + S))^(M/2 + S/2)*\n        Hypergeometric2F1[1, (1/2)*(-1 + M - S), M/2, M/(-1 + M - S)] + \n        6*M*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^(M/2 + S/2)*\n        Hypergeometric2F1[(1 + S)/2, (1/2)*(1 - M + S), (1/2)*(3 - M + S), \n                          (-1 + M - S)/M] - 5*M^2*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^\n        (M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, (1/2)*(1 - M + S), \n                                      (1/2)*(3 - M + S), (-1 + M - S)/M] + M^3*(M/(1 + S))^(M/2)*\n        ((1 + S)/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, \n                                                            (1/2)*(1 - M + S), (1/2)*(3 - M + S), (-1 + M - S)/M] + \n        2*M*S*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^(M/2 + S/2)*\n        Hypergeometric2F1[(1 + S)/2, (1/2)*(1 - M + S), (1/2)*(3 - M + S), \n                          (-1 + M - S)/M] - M^2*S*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^\n        (M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, (1/2)*(1 - M + S), \n                                      (1/2)*(3 - M + S), (-1 + M - S)/M] - 2*M*(M/(1 + S))^(M/2)*\n        ((1 + S)/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, \n                                                            (1/2)*(3 - M + S), (1/2)*(5 - M + S), (-1 + M - S)/M] + \n        3*M^2*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^(M/2 + S/2)*\n        Hypergeometric2F1[(1 + S)/2, (1/2)*(3 - M + S), (1/2)*(5 - M + S), \n                          (-1 + M - S)/M] - M^3*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^\n        (M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, (1/2)*(3 - M + S), \n                                      (1/2)*(5 - M + S), (-1 + M - S)/M] - 2*M*S*(M/(1 + S))^(M/2)*\n        ((1 + S)/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, \n                                                            (1/2)*(3 - M + S), (1/2)*(5 - M + S), (-1 + M - S)/M] + \n        M^2*S*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^(M/2 + S/2)*\n        Hypergeometric2F1[(1 + S)/2, (1/2)*(3 - M + S), (1/2)*(5 - M + S), \n                          (-1 + M - S)/M]))/(((1 + S)/(1 - M + S))^S*\n                                               (2*(-2 + M)*M*Sqrt[(-1 - S)/(-1 + M - S)]*Gamma[M/2]*\n                                                  Gamma[(1/2)*(5 - M + S)])))\nBut do not fear: we can recover. At the cost of an assumption about\nthe tails of \\(R\\). (We’re also going\nto assume that \\(h\\) is bounded because\nit makes things ever so slightly easier, although unbounded \\(h\\) is ok18\nas long as it doesn’t grow too quickly relative to \\(r\\).)\nWe are going to make the assumption that \\(R - T \\mid R\\geq T\\) is in the domain of\nattraction of a generalized Pareto distribution with shape parameter\n\\(k\\). A sufficient condition, due to\nvon Mises, is that \\[\n\\lim_{r\\rightarrow \\infty} \\frac{r R'(r)}{1-R(r)} = \\frac{1}{k}.\n\\]\nThis seems like a weird condition, but it’s basically just a\nregularity condition at infinity. For example if \\(1-R(r)\\) is regularly varying at infinity19 and \\(R'(r)\\) is, eventually, monotone20 decreasing, then this condition\nholds.\nThe von Mises condition is very natural for us as Falk\nand Marohn (1993) show that the relative error we get when\napproximating the tail of \\(R\\) by a\ngeneralised Pareto density is the same as the relative error in the von\nMises condition. That is if \\[\n\\frac{rR'(r)}{1-R(r)} = \\frac{1}{k}(1 + \\mathcal{O}(r^{-\\alpha}))\n\\] then \\[\nR'(r) = c w(cr - d)(1 + \\mathcal{O}(r^{-\\alpha})),\n\\] where \\(c,d\\) are constants\nand \\(w\\) is the density of a\ngeneralised Pareto distribution.\nAnyway, under those two assumptions, we can swap out the density of\n\\((R-T)\\mid R>T\\) with its\nasymptotic approximation and get that, conditional on \\(T= r_{S-M+1:S}\\), \\[\n\\mathbb{E}(H(R-T) \\mid R>T) = (k-1)^{-1}T.\n\\]\nHence, the second term in the bias goes to zero if \\[\n\\mathbb{E}\\left(r_{S-M+1:S}\\left(1 - R(r_{s-M+1:S}) -\n\\frac{M}{S}\\right)\\right)\n\\] goes to zero.\nNow this is not particularly pleasant, but it helps to recognise that\neven if a distribution doesn’t have finite moments, away from the\nextremes, its order statistics always do. This means that we can use\nCauchy-Schwartz to get \\[\n\\left|\\mathbb{E}\\left(r_{S-M+1:S}\\left(1 - R(r_{s-M+1:S}) -\n\\frac{M}{S}\\right)\\right)\\right|\n\\leq\\mathbb{E}\\left(r_{S-M+1:S}^2\\right)^{1/2}\\mathbb{E}\\left[\\left(1 -\nR(r_{s-M+1:S}) - \\frac{M}{S}\\right)^2\\right]^{1/2}.\n\\]\nArguably, the most alarming term is the first one, but that can21 be tamed. To do this, we lean into\na result from Bickel\n(1967) who, if you examine the proof and translate some\nobscurely-stated conditions and fix a typo22,\nyou get that \\[\n\\mathbb{E}(r_{k:M}^2) \\leq C k\\begin{pmatrix} S \\\\ k\\end{pmatrix}\n\\int_0^1 t^{k-2-1}(1-t)^{S-k-2}\\,dt.\n\\] You might worry that this is going to grow too quickly. But it\ndoesn’t. Noting that \\(B(n,m) =\n\\Gamma(n)\\Gamma(m)/\\Gamma(n+m)\\), we can rewrite the upper bound\nin terms of the Beta function to get \\[\n\\mathbb{E}(r_{k:M}^2) \\leq C \\frac{\\Gamma(S+1)}{\\Gamma(S-3)}\n\\frac{\\Gamma(k-2)}{\\Gamma(k+1)}\\frac{\\Gamma(S-k-1)}{\\Gamma(S-k+1)}.\n\\]\nTo show that this doesn’t grow too quickly, we use the identity \\[\n\\frac{\\Gamma(x + a)}{\\Gamma(x + b)} \\propto x^{a-b}(1 +\n\\mathcal{O}(x^{-1})).\n\\] From this, it follows that \\[\n\\mathbb{E}(r_{k:M}^2) \\leq C S^4k^{-3}(S-k)^{-2}(1+\n\\mathcal{O}(S^{-1}))(1+ \\mathcal{O}(k^{-1}))(1+\n\\mathcal{O}((S+k)^{-1})).\n\\] In this case, we are interested in \\(k = S-M+1\\), so \\[\n\\mathbb{E}(r_{k:M}^2) \\leq C S^4S^{-3}M^{-2}(1 - M/S + 1/S)^{-3}(1 -\n1/M)^{-2}(1+ \\mathcal{O}(S^{-1}))(1+ \\mathcal{O}(S^{-1}))(1+\n\\mathcal{O}(M^{-1})).\n\\]\nHence the we get that \\(\\mathbb{E}(r_{k:M}^2) =\n\\mathcal{O}(SM^{-2})\\). This is increasing23\nin \\(S\\), but we will see that it is\nnot going up too fast.\nFor the second half of this shindig, we are going to attack \\[\n\\mathbb{E}\\left[\\left(1 - R(r_{s-M+1:S}) - \\frac{M}{S}\\right)^2\\right] =\n\\mathbb{E}\\left[\\left(1 - R(r_{s-M+1:S})\\right)^2 - 2\\left(1 -\nR(r_{s-M+1:S})\\right)\\frac{M}{S} +\\left(\\frac{M}{S}\\right)^2\\right].\n\\] A standard result24 from extreme value\ntheory is that \\(R(r_{k:S})\\) has the\nsame distribution as the \\(k\\)th order\nstatistics from a sample of \\(S\\) iid\n\\(\\text{Uniform}([0,1])\\) random\nvariables. Hence25, \\[\nR(r_{S-M+1:S}) \\sim \\text{Beta}(S-M+1, M).\n\\] If follows26 that \\[\n\\mathbb{E}(1- R(r_{S-M+1:S})) = \\frac{M}{S+1} =\n\\frac{M}{S}\\frac{1}{1+S^{-1}}\n\\] and \\[\n\\mathbb{E}((1- R(r_{S-M+1:S}))^2) = \\frac{M(M+1)}{(S+1)(S+2)} =\n\\frac{M^2}{S^2}\\left(\\frac{1 + M^{-1}}{1 + 3S^{-1} + 2S^{-2}}\\right).\n\\] Adding these together and doing some asymptotic expansions, we\nget \\[\n\\mathbb{E}\\left[\\left(1 - R(r_{s-M+1:S}) - \\frac{M}{S}\\right)^2\\right] =\n\\frac{M^2}{S^2} + \\mathcal{O}\\left(\\frac{M}{S^2}\\right),\n\\] which goes to zero27 like \\(\\mathcal{O}(S^{-1})\\) if \\(M = \\mathcal{O}(S^{1/2})\\).\nWe can multiply this rate together and get that the second term in\nthe bias is bounded above by \\[\n\\left[\\left(\\frac{S}{M^2} (1 + \\mathcal{O}(M^{-1} +\nMS^{-1}))\\right)\\left(\\frac{M^2}{S^2} (1 + \\mathcal{O}(M^{-1} +\nMS^{-1})\\right)\\right]^{1/2} = S^{-1/2}(1 + o(1)).\n\\]\nPutting all of this together we have proved the following\nCorollary.\nCorollary Let \\(\\theta_s\\), \\(s =\n1,\\ldots, S\\) be an iid sample from \\(G\\) and let \\(r_s\n= r(\\theta_s) \\sim R\\). Assume that\n\\(R\\) is absolutely continuous\nand satisfies the von Mises condition28\n\\[\n\\frac{rR'(r)}{1-R(r)} = \\frac{1}{k}(1 +\\mathcal{O}(r^{-1})).\n\\]\n\\(M = o(S)\\)\n\\(h\\) is bounded29\nWinsorized importance sampling converges in \\(L^1\\) with rate of, at most, \\(\\mathcal{O}(MS^{-1} + S^{-1/2})\\), which is\nbalanced when \\(M =\n\\mathcal{O}(S^{1/2})\\). Hence, WIS is30\n\\(\\sqrt{n}\\)-consistent.\nVariance of\nWinsorized Importance Sampling\nRight, that was a bit of a journey, but let’s keep going to the\nvariance.\nIt turns out that following the route I thought I was going to follow\ndoes not end well. That lovely set of tricks breaking up the variance\ninto two conditional terms turns out to be very very unnecessary. Which\nis good, because I thoroughly failed to make the argument work.\nIf you’re curious, the problem is that the random variable \\[\n\\frac{Mr_{S-M+1:S}}{S} \\mathbb{E}(H \\mid R \\geq r_{S-M+1:S}) =\n\\frac{Mr_{S-M+1:S}}{S(1-R(r_{S-M+1:S}))} \\mathbb{E}(H 1_{R \\geq\nr_{S-M+1:S}})\n\\] is an absolute bastard to bound. The problem is that\n\\(1- R({r_{S-M+1:S}}) \\approx M/S\\) and\nso the usual trick of bounding that truncated expectation by \\(\\|h\\|\\) or some such thing will prove that\nthe variance is finite but not that it goes to zero. There is a\nsolid chance that the Cauchy-Schwartz inequality \\[\n\\frac{Mr_{S-M+1:S}^{1/2}}{S(1-R(r_{S-M+1:S}))}\n\\mathbb{E}(r_{S-M+1:S}^{1/2}H 1_{R \\geq r_{S-M+1:S}})\n\\leq\\frac{Mr_{S-M+1:S}^{1/2}}{S(1-R(r_{S-M+1:S}))}R(r_{S-M+1:S})\\|h\\|_{L^2(p)}\n\\] would work. But truly that is just bloody messy31.\nSo let’s do it the easy way, shall we. Fundamentally, we will use\n\\[\n\\mathbb{V}\\left(I_\\text{WIS}^S\\right) \\leq\n\\mathbb{E}\\left([I_\\text{WIS}^S]^2\\right).\n\\] Noting that we can write \\(I_\\text{WIS}^S\\) compactly as \\[\nI_\\text{WIS}^S = \\frac{1}{S}\\sum_{s=1}^S h(\\theta_s)\\min\\{r(\\theta_s),\nr_{S-M+1:S}\\}.\n\\] Hence, \\[\\begin{align*}\n\\mathbb{E}\\left([I_\\text{WIS}^S]^2\\right) &= \\mathbb{E}_{T\\sim\nr_{S-M+1:S}}\\left[\\mathbb{E}\\left([I_\\text{WIS}^S]^2 \\mid r_{S-M+1:S} =\nT\\right)\\right]\\\\\n&=\\frac{1}{S^2}\\mathbb{E}_{T\\sim\nr_{S-M+1:S}}\\left[\\mathbb{E}\\left(H^2 \\min\\{R^2,T^2\\} \\mid r_{S-M+1:S} =\nT\\right)\\right]\\\\\n&\\leq\\frac{1}{S^2}\\mathbb{E}_{T\\sim\nr_{S-M+1:S}}\\left[\\mathbb{E}\\left(RTH^2 \\mid r_{S-M+1:S} =\nT\\right)\\right] \\\\\n&\\leq\\frac{1}{S^2}\\mathbb{E}_{T\\sim\nr_{S-M+1:S}}\\left[T\\|h\\|_{L^2(p)}^2\\right]\n\\end{align*}\\]\nThis goes to zero as long as \\(\\mathbb{E}(r_{S-M+1:S}) = o(S^2)\\).\nBickel\n(1967) shows that, noting that \\(\\mathbb{E}(R) < \\infty\\), \\[\n\\mathbb{E}(r_{S-M+1:S}) \\leq C\n(S-M+1)\\frac{\\Gamma(S+1)\\Gamma(S-M+1-1)\\Gamma(M)}{\\Gamma(S-M+1+1)\\Gamma(M+1)\\Gamma(S-1)}\n= \\frac{S}{M}(1 + o(1)),\n\\] and so the variance is bounded.\nThe previous argument shows that the variance is \\(\\mathcal{O}(M^{-1}S^{-1})\\). We can refine\nthat if we assume the von Mises condition hold. In that case we know\nthat \\(R(r) = 1- cr^{-1/k} + o(1)\\) as\n\\(r\\rightarrow \\infty\\) and therefore\n\\[\\begin{align*}\nR\\left(R^{-1}\\left(1-\\frac{M}{S}\\right)\\right) &= 1-\\frac{M}{S+1}\\\\\n1 - cR^{-1}\\left(1-\\frac{M}{S+1}\\right)^{-1/k}(1+o(1)) &= 1-\n\\frac{M}{S+1} \\\\\nR^{-1}\\left(1-\\frac{M}{S+1}\\right) &=\nc^{-k}\\left(\\frac{M}{S+1}\\right)^{-k}(1 + o(1)).\n\\end{align*}\\] Bickel (1967) shows that \\(\\mathbb{E}(r_{k:S}) = R^{-1}(1-M/(S+1)) +\no(1)\\) so combining this with the previous result gives a\nvariance of \\(\\mathcal{O}((M/S)^{k-2})\\). If we take\n\\(M =\\mathcal{O}(S^{1/2})\\), this gives\n\\(\\mathcal{S}^{k/2-1}\\), which is\nsmaller than the previous bound for \\(k<1\\). It’s worth noting that Hence the\nvariance goes to zero.\nThe argument that we used here is a modification of the argument in\nthe TIS paper. This lead to a great deal of panic: did I just make my\nlife extremely difficult? Could I have modified the TIS proof to show\nthe bias goes to zero? To be honest, someone might be able to, but I\ncan’t.\nSo anyway, we’ve proved the following theorem.\nTheorem Let \\(\\theta_s\\), \\(s =\n1,\\ldots, S\\) be an iid sample from \\(G\\) and let \\(r_s\n= r(\\theta_s) \\sim R\\). Assume that\n\\(R\\) is absolutely\ncontinuous\n\\(M \\rightarrow \\infty\\) and\n\\(M^{-1}S \\rightarrow 0\\)\n\\(h \\in L^2(p)\\).\nThe variance in Winsorized importance sampling is at most \\(\\mathcal{O}(M^{-1}S)\\).\nPareto-smoothed importance\nsampling\nPareto-smoothed importance sampling (or PSIS) takes the observation\nthat the tails are approximately Pareto distributed to add some bias\ncorrection to the mix. Essentially, it works by noting that\napproximating \\[\n(1-R(r_{S-M+1:S}))\\mathbb{E}(HR \\mid R>r_{S-M+1:S}) \\approx\n\\frac{1}{S}\\sum_{m=1}^M w_m h_{S-M+m:S},\n\\] where \\(w_m\\) is the median32 \\(m\\)th order statistic in an iid sample of\n\\(M\\) Generalised Pareto random\nvariables with tail parameters fitted to the distribution.\nThis is a … funky … quadrature rule. To see that, we can write \\[\n\\mathbb{E}(HR \\mid R>T) = \\mathbb{E}(R \\mathbb{E}(H \\mid R)).\n\\] If we approximate the distribution of \\(R > T\\) by \\[\n\\tilde{R}_\\text{PSIS}(r) = \\frac{1}{M}\\sum_{m=1}^M 1( w_m<r)\n\\] and approximate the conditional probability by \\[\n\\Pr(H < h\\mid R = w_m) \\approx 1(h_{S-M+m:S}< h).\n\\]\nEmpirically, this is a very good choice (with the mild caveat that\nyou need to truncate the largest expected order statistic by the\nobserved maximum in order to avoid some variability issues). I would\nlove to have a good analysis of why that is so, but honest I do not.\nBut, to the issue of this blog post the convergence and vanishing\nvariance still holds. To see this, we note that \\[\nw_m = r_{S-M+1}  + k^{-1}\\sigma\\left[\\left(1-\\frac{j-1/2}{M}\\right)^{-k}\n-1\\right].\n\\] So we are just re-weighting our tail \\(H\\) samples by \\[\n1 + \\frac{\\sigma}{kr_{S-M+1:S}}\\left[\\left(1-\\frac{j-1/2}{M}\\right)^{-k}\n-1\\right].\n\\]\nRecalling that when \\(R(r) = 1-\ncr^{-1/k}(1+ o(1))\\), we had \\(\\sigma =\n\\mathcal{O}(r_{S-M+1:S})\\), this term is at most \\(\\mathcal{O}(1 + M^{-k})\\). This will not\ntrouble either of our convergence proofs.\nThis leads to the following modification of our previous results.\nTheorem Let \\(\\theta_s\\), \\(s =\n1,\\ldots, S\\) be an iid sample from \\(G\\) and let \\(r_s\n= r(\\theta_s) \\sim R\\). Assume that\n\\(R\\) is absolutely\ncontinuous.\n\\(M =\n\\mathcal{O}(S^{1/2})\\)\n\\(h \\in L^2(p)\\)\n\\(k\\) and \\(\\sigma\\) are known with \\(\\sigma =\n\\mathcal{O}(r_{S-M+1:S})\\).\nPareto smoothed importance sampling converges in \\(L^1\\) and its variance goes to zero and it\nis consistent and asymptotically unbiased.\nCorollary\nAssume further that\nR satisfies the von Mises condition33\n\\[\n\\frac{rR'(r)}{1-R(r)} = \\frac{1}{k}(1 +\\mathcal{O}(r^{-1})).\n\\]\n\\(h\\) is bounded34.\nThen the L^1 convergence occurs at a rate of of, at most, \\(\\mathcal{O}(S^{-1/2})\\). Furthermore, the\nvariance of the PSIS estimator goes to zero at least as fast as \\(\\mathcal{O}(S^{k/2-1})\\).\nHence, under these additional conditions PSIS is35\n\\(\\sqrt{n}\\)-consistent.\nFinal thoughts\nSo that’s what truncation and winsorization does to importance\nsampling estimates. I haven’t touched on the fairly important topic of\nasymptotic normality. Essentially, Griffin\n(1988), in a fairly complex36 paper that suggests\nthat if you winsorize the product \\((h(\\theta_s)r(\\theta_s))\\) and\nwinsorize it at both ends, the von Mises condition37\nimply that the WIS estimator is asymptotically normal.\nWhy is this important, well the same proof shows that doubly\nwinsorized importance sampling (dWIS) applied to the vector valued\nfunction \\(\\tilde h(\\theta) =\n(h(\\theta),1)\\) will also be asymptotically normal, which\nimplies, via the delta method, that the self normalized dWIS\nestimator \\[\nI^S_\\text{SN-IS} = \\frac{\\sum_{s=1}^S\\max\\{\\min\\{h(\\theta_i)\nr(\\theta_i),T_{S-M+1:S}\\},\nT_{M:S}\\}}{\\sum_{s=1}^S\\max\\{\\min\\{r(\\theta_i),T_{S-M+1:S}\\},T_{M:S}\\}}\n\\] is consistent, where \\(T_{m:S}\\) is the \\(m\\)th order statistic of \\(\\max\\{h(\\theta_s)r(\\theta_s),\nr(\\theta_s)\\}\\).\nIt is very very likely that this can be shown (perhaps under some\nassumptions) for something closer to the version of PSIS we use in\npractice. But that is an open question.\n\nproportional to↩︎\nbecause \\(p(\\theta_s)\\) is very small↩︎\nbecause \\(p(\\theta_s)\\) is a reasonable size, but\n\\(g(\\theta_s)\\) is tiny.↩︎\nI have surreptitiously dropped the\n\\(h\\) subscript because I am gay and\nsneaky.↩︎\nThat it’s parameterised by \\(1/k\\) is an artefact of history.↩︎\nWe need \\(\\mathbb{E}(R)\\) to be finite, so we need\n\\(k<1\\).↩︎\nvery fucking complex↩︎\nI have used that old trick of using\nthe same letter for the CDF as the random variable when I have a lot of\nrandom variables. ↩︎\naka the tail index↩︎\nThis is a relevant case. But if you\nthink a little bit about it, our problem happens when \\(r(\\theta)\\) grows much faster than\n\\(h(\\theta)\\). For example if \\(P = \\operatorname{Exp}(1)\\) and \\(G = \\operatorname{Exp}(1/\\lambda)\\) for\n\\(\\lambda>1\\), then \\(k = 1-1/\\lambda\\), \\(r(\\theta) = \\exp((\\lambda-1)\\theta)\\) and\nif \\(|h(\\theta)| <\n|\\theta|^\\alpha\\), then \\(|h(\\theta)|\n\\leq C \\log(r)^\\alpha\\), which is a slowly growing function.↩︎\nBecause the truncation depends on\n\\(S\\), moving from the \\(S\\)th partial sum to the \\(S+1\\)th partial sum changes the\ndistribution of \\(z_ih_ir_i\\). This is\nexactly why the dead Russians gifted us with triangular arrays.↩︎\nAlso practical unbounded \\(h\\), but it’s just easier for bounded \\(h\\)↩︎\nShut up. I know. Don’t care.↩︎\nor, hell, even in a book↩︎\nStraight up, though, I spent 2 days\ndicking around with tail bounds on sums of Bernoulli random variables\nfor some bloody reason before I just looked at the damn formula.↩︎\nOk. I checked. And yeah. Same\ntechnique as below using Jensen in its \\(\\mathbb{E}(|X-\\mathbb{E}(X)|)^2 \\leq\n\\mathbb{V}(X)\\). If you put that together you get something that\ngoes to zero like \\(M^{1/2}S^{-1}\\),\nwhich is \\(\\mathcal{O}(S^{-3/4})\\) for\nour usual choice of \\(M\\). Which\nconfirms the suspicion that the first term in the bias goes to zero\nmuch faster than the second (remembering, of course, that\nJensen’s inequality is notoriously loose!).↩︎\nIt’s Pride month↩︎\nThe result holds exactly if \\(\\mathbb{E}(H \\mid R=r) =\n\\mathcal{O}(\\log^k(r))\\) and with a \\(k\\) turning up somewhere if it’s \\(o(r^{1/k - 1})\\).↩︎\n\\(1-R(r)\n\\sim c r^{(-1/k)}\\mathcal{L(r)}\\) for a slowly varying function\n(eg a power of a logarithm) \\(\\mathcal{L}(r)\\).↩︎\nA property that implies this is that\n\\(1-R(r)\\) is differentiable and\nconvex at infinity, which is to say that there is some finite\n\\(r_0\\) such that \\(R'(r)\\) exists for all \\(r \\geq r_0\\) and \\(1-R(r)\\) is a monotone function on \\([r_0, \\infty)\\).↩︎\nThere’s a condition here that \\(S\\) has to be large enough, but it’s enough\nif \\((S-M+1) > 2\\).↩︎\nThe first \\(k\\) in the equation below is missing in the\npaper. If you miss this, you suddenly get the expected value converging\nto zero, which would be very surprising. Always sense-check the\nproofs, people. Even if a famous person did it in the 60s.↩︎\nWe need to take \\(M = \\mathcal{O}(S^{1/2})\\) to be able to\nestimate the tail index \\(k\\) from a\nsample, which gives an upper bound by a constant.↩︎\nNote that if \\(U \\sim \\text{Unif}(0,1)\\), then \\(R^{-1}(U) \\sim R\\). Because this is\nmonotone, it doesn’t change ordering of the sample↩︎\nThis is, incidentally, how Bickel\ngot the upper bound on the moments. He combined this with an upper bound\non the quantile function.↩︎\nSave the cheerleader, save the\nworld. Except it’s one minus a beta is still beta but with the\nparameters reversed.↩︎\nAs long as \\(M = o(S)\\)↩︎\nThe rate here is probably not\noptimal, but it will guarantee that the error in the Pareto\napproximation doesn’t swamp the other terms.↩︎\nOr \\(\\mathbb{E}(h(\\theta) \\mid r(\\theta) = r)\\)\ndoesn’t grow to quickly, with some modification of the rates in the\nunlikely case that it grows polynomially.↩︎\nalmost, there’s an epsilon gap but I\ndon’t give a shit↩︎\nAnd girl do not get me started on\nmessy. I ended up going down a route where I used the [inequality]((https://www.sciencedirect.com/science/article/pii/0167715288900077)\n\\[\n\\mathbb{V}(g(U)) \\leq \\mathbb{E}(U)\\int_0^1\\left[F_U(u) -\n\\frac{\\mathbb{E}(U1_{U\\leq u})}{\\mathbb{E}(U)}\\right][g'(u)]^2\\,du\n\\] which holds for any \\(U\\)\nsupported on \\([0,1]\\) with\ndifferentiable density. And let me tell you. If you dick around with\nenough beta distributions you can get something. Is it what you want?\nFucking no. It is a lot of work, including having to\ndifferentiate the conditional expectation, and it gives you sweet bugger\nall.↩︎\nOr, the expected within \\(o(S^{-1/2})\\)↩︎\nThe rate here is probably not\noptimal, but it will guarantee that the error in the Pareto\napproximation doesn’t swamp the other terms.↩︎\nOr \\(\\mathbb{E}(h(\\theta) \\mid r(\\theta) = r)\\)\ndoesn’t grow to quickly, with some modification of the rates in the\nunlikely case that it grows polynomially.↩︎\nalmost, there’s an epsilon gap but I\ndon’t give a shit↩︎\nI mean, the tools are elementary.\nIt’s just a lot of detailed estimates and Berry-Esseen as far as the eye\ncan see.↩︎\nand more general things↩︎\n",
    "preview": "posts/2022-06-03-that-psis-proof/that-psis-proof_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-06-15T23:34:39+10:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/",
    "title": "Sparse matrices 6: To catch a derivative, first you've got to think like a derivative",
    "description": "Open up the kennels, Kenneth. Mamma’s coming home tonight.",
    "author": [
      {
        "name": "Dan Simpson",
        "url": "https://dpsimpson.github.io"
      }
    ],
    "date": "2022-05-30",
    "categories": [],
    "contents": "\nWelcome to part six!!! of our ongoing series on making sparse linear\nalgebra differentiable in JAX with the eventual hope to be able to do\nsome cool\nstatistical shit. We are nowhere near done.\nLast\ntime, we looked at making JAX primitives. We built four of them.\nToday we are going to implement the corresponding differentiation rules!\nFor three1 of them.\nSo strap yourselves in. This is gonna be detailed.\nIf you’re interested in the code2, the git repo for this\npost is linked at the bottom and in there you will find a folder with\nthe python code in a python file.\nShe\nis beauty and she is grace. She is queen of 50 states. She is elegance\nand taste. She is miss autodiff\nDerivatives are computed in JAX through the glory and power of\nautomatic differentiation. If you came to this blog hoping for a great\ndescription of how autodiff works, I am terribly sorry but I absolutely\ndo not have time for that. Might I suggest google? Or maybe flick\nthrough this survey by\nCharles Margossian..\nThe most important thing to remember about algorithmic\ndifferentiation is that it is not symbolic differentiation.\nThat is, it does not create the functional form of the derivative of the\nfunction and compute that. Instead, it is a system for cleverly\ncomposing derivatives in each bit of the program to compute the\nvalue of the derivative of the function.\nBut for that to work, we need to implement those clever little\nmini-derivatives. In particular, every function \\(f(\\cdot): \\mathbb{R}^n \\rightarrow\n\\mathbb{R}^m\\) needs to have a function to compute the\ncorresponding Jacobian-vector product \\[\n(\\theta, v) \\rightarrow J(\\theta) v,\n\\] where the \\(n \\times m\\)\nmatrix \\(J(\\theta)\\) has entries \\[\nJ(\\theta)_{ij} = \\frac{\\partial f_j }{\\partial \\theta_j}.\n\\]\nOk. So let’s get onto this. We are going to derive and implement some\nJacobian-vector products. And all of the assorted accoutrement. And by\ncrikey. We are going to do it all in a JAX-traceable way.\nJVP number one: The linear\nsolve.\nThe first of the derivatives that we need to work out is the\nderivative of a linear solve \\(A^{-1}b\\). Now, intrepid readers, the\nobvious thing to do is look the damn derivative up. You get exactly no\nhero points for computing it yourself.\nBut I’m not you, I’m a dickhead.\nSo I’m going to derive it. I could pretend there are reasons3, but that would just be lying. I’m\ndoing it because I can.\nBeyond the obvious fun of working out a matrix derivative from first\nprinciples, this is fun because we have two arguments instead\nof just one. Double the fun.\nAnd we really should make sure the function is differentiated with\nrespect to every reasonable argument. Why? Because if you write code\nother people might use, you don’t get to control how they use it (or\nwhat they will email you about). So it’s always good practice to limit\nsurprises (like a function not being differentiable wrt some argument)\nto cases4 where it absolutely necessary. This\nreduces the emails.\nTo that end, let’s take an arbitrary SPD matrix \\(A\\) with a fixed sparsity pattern.\nLet’s take another symmetric matrix \\(\\Delta\\) with the same sparsity\npattern and assume that \\(\\Delta\\)\nis small enough5 that \\(A +\n\\Delta\\) is still symmetric positive definite. We also need a\nvector \\(\\delta\\) with a small \\(\\|\\delta\\|\\).\nNow let’s get algebraing. \\[\\begin{align*}\nf(A + \\Delta, b + \\delta) &= (A+\\Delta)^{-1}(b + \\delta) \\\\\n&= (I + A^{-1}\\Delta)^{-1}A^{-1}(b + \\delta) \\\\\n&= (I - A^{-1}\\Delta + o(\\|\\Delta\\|))A^{-1}(b + \\delta) \\\\\n&= A^{-1}b + A^{-1}(\\delta - \\Delta A^{-1}b ) + o(\\|\\Delta\\| +\n\\|\\delta\\|)\n\\end{align*}\\]\nEasy6 as.\nWe’ve actually calculated the derivative now, but it’s a little more\nwork to recognise it.\nTo do that, we need to remember the practical definition of the\nJacobian of a function \\(f(x)\\) that\ntakes an \\(n\\)-dimensional input and\nproduces an \\(m\\)-dimensional output.\nIt is the \\(n \\times m\\) matrix \\(J_f(x)\\) such that \\[\nf(x + \\delta)  = f(x) + J_f(x)\\delta + o(\\|\\delta\\|).\n\\]\nThe formulas further simplify if we write \\(c = A^{-1}b\\). Then, if we want the\nJacobian-vector product for the first argument, it is \\[\n-A^{-1}\\Delta c,\n\\] while the Jacobian-vector product for the second argument is\n\\[\nA^{-1}\\delta.\n\\]\nThe only wrinkle in doing this is we need to remember that we are\nonly storing the lower triangle of \\(A\\). Because we need to represent \\(\\Delta\\) the same way, it is represented as\na vector Delta_x that contains only the lower triangle of\n\\(\\Delta\\). So we need to make sure we\nremember to form the whole matrix before we do the\nmatrix-vector product \\(\\Delta c\\)!\nBut otherwise, the implementation is going to be pretty\nstraightforward. The Jacobian-vector product costs one additional linear\nsolve (beyond the one needed to compute the value \\(c = A^{-1}b\\)).\nIn the language of JAX (and autodiff in general), we refer to \\(\\Delta\\) and \\(\\delta\\) as tangent vectors. In\nsearch of a moderately coherent naming convention, we are going to refer\nto the tangent associated with the variable x as\nxt.\nSo let’s implement this. Remember: it needs7 to\nbe JAX traceable.\nPrimitive two: The\ntriangular solve\nFor some sense of continuity, we are going to keep the naming of the\nprimitives from the last blog post, but we are not going to\nattack them in the same order. Why not? Because we work in order of\ncomplexity.\nSo first off we are going to do the triangular solve. As I have yet\nto package up the code (I promise, that will happen next8),\nI’m just putting it here under the fold.\n\nThe primal implementation\n\nfrom scipy import sparse\nimport numpy as np\nfrom jax import numpy as jnp\n/Users/dsim0009/miniforge3/envs/myjaxenv/lib/python3.10/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\nfrom jax import core\nfrom jax._src import abstract_arrays\nfrom jax import core\n\nsparse_triangular_solve_p = core.Primitive(\"sparse_triangular_solve\")\n\ndef sparse_triangular_solve(L_indices, L_indptr, L_x, b, *, transpose: bool = False):\n  \"\"\"A JAX traceable sparse  triangular solve\"\"\"\n  return sparse_triangular_solve_p.bind(L_indices, L_indptr, L_x, b, transpose = transpose)\n\n@sparse_triangular_solve_p.def_impl\ndef sparse_triangular_solve_impl(L_indices, L_indptr, L_x, b, *, transpose = False):\n  \"\"\"The implementation of the sparse triangular solve. This is not JAX traceable.\"\"\"\n  L = sparse.csc_array((L_x, L_indices, L_indptr)) \n  \n  assert L.shape[0] == L.shape[1]\n  assert L.shape[0] == b.shape[0]\n  \n  if transpose:\n    return sparse.linalg.spsolve_triangular(L.T, b, lower = False)\n  else:\n    return sparse.linalg.spsolve_triangular(L.tocsr(), b, lower = True)\n\n@sparse_triangular_solve_p.def_abstract_eval\ndef sparse_triangular_solve_abstract_eval(L_indices, L_indptr, L_x, b, *, transpose = False):\n  assert L_indices.shape[0] == L_x.shape[0]\n  assert b.shape[0] == L_indptr.shape[0] - 1\n  return abstract_arrays.ShapedArray(b.shape, b.dtype)\n\nThe Jacobian-vector product\n\nfrom jax._src import ad_util\nfrom jax.interpreters import ad\nfrom jax import lax\nfrom jax.experimental import sparse as jsparse\n\ndef sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent, *, transpose):\n  \"\"\"\n  A jax-traceable jacobian-vector product. In order to make it traceable, \n  we use the experimental sparse CSC matrix in JAX.\n  \n  Input:\n    arg_values:   A tuple of (L_indices, L_indptr, L_x, b) that describe\n                  the triangular matrix L and the rhs vector b\n    arg_tangent:  A tuple of tangent values (same lenght as arg_values).\n                  The first two values are nonsense - we don't differentiate\n                  wrt integers!\n    transpose:    (boolean) If true, solve L^Tx = b. Otherwise solve Lx = b.\n  Output:         A tuple containing the maybe_transpose(L)^{-1}b and the corresponding\n                  Jacobian-vector product.\n  \"\"\"\n  L_indices, L_indptr, L_x, b = arg_values\n  _, _, L_xt, bt = arg_tangent\n  value = sparse_triangular_solve(L_indices, L_indptr, L_x, b, transpose=transpose)\n  if type(bt) is ad.Zero and type(L_xt) is ad.Zero:\n    # I legit do not think this ever happens. But I'm honestly not sure.\n    print(\"I have arrived!\")\n    return value, lax.zeros_like_array(value) \n  \n  if type(L_xt) is not ad.Zero:\n    # L is variable\n    if transpose:\n      Delta = jsparse.CSC((L_xt, L_indices, L_indptr), shape = (b.shape[0], b.shape[0])).transpose()\n    else:\n      Delta = jsparse.CSC((L_xt, L_indices, L_indptr), shape = (b.shape[0], b.shape[0]))\n\n    jvp_Lx = sparse_triangular_solve(L_indices, L_indptr, L_x, Delta @ value, transpose = transpose) \n  else:\n    jvp_Lx = lax.zeros_like_array(value) \n\n  if type(bt) is not ad.Zero:\n    # b is variable\n    jvp_b = sparse_triangular_solve(L_indices, L_indptr, L_x, bt, transpose = transpose)\n  else:\n    jvp_b = lax.zeros_like_array(value)\n\n  return value, jvp_b - jvp_Lx\n\nad.primitive_jvps[sparse_triangular_solve_p] = sparse_triangular_solve_value_and_jvp\n\nBefore we see if this works, let’s first have talk about the\nstructure of the function I just wrote. Generally speaking, we want a\nfunction that takes in the primals and tangents at tuples and then\nreturns the value and the9 Jacobian-vector product.\nThe main thing you will notice in the code is that there is a\nlot of checking for ad.Zero. This is a special type\ndefined in JAX that is, essentially, telling the autodiff system that we\nare not differentiating wrt that variable. This is different to a\ntangent that just happens to be numerically equal to zero. Any code for\na Jacobian-vector product needs to handle this special value.\nAs we have two arguments, we have 3 interesting options:\nBoth L_xt and bt are\nad.Zero: This means the function is a constant and the\nderivative is zero. I am fairly certain that we do not need to manually\nhandle this case, but because I don’t know and I do not like surprises,\nit’s in there.\nL_xt is not ad.Zero: This\nmeans that we need to differentiate wrt the matrix. In this case we need\nto compute \\(\\Delta c\\) or \\(\\Delta^T c\\), depending on the\ntranspose argument. In order to do this, I used the\njax.experimental.sparse.CSC class, which has some very\nlimited sparse matrix support (basically matrix-vector products). This\nis extremely convenient because it means I don’t need to write\nthe matrix-vector product myself!\nbt is not ad.Zero: This means\nthat we need to differentiate wrt the rhs vector. This part of the\nformula is pretty straightforward: just an application of the\nprimal.\nIn the case that either L_xt or bt are\nad.Zero, we simply set the corresponding contribution to\nthe jvp to zero.\nIt’s worth saying that you can bypass all of this\nad.Zero logic by writing separate functions for the JVP\ncontribution from each input and then chaining them together using10 ad.defjvp2() to chain\nthem together. This is what the\nlax.linalg.triangular_solve() implementation does.\nSo why didn’t I do this? I avoided this because in the other\nprimitives I have to implement, there are expensive computations (like\nCholesky factorisations) that I want to share between the primal and the\nvarious tangent calculations. The ad.defjvp frameworks\ndon’t allow for that. So I decided not to demonstrate/learn two separate\npatterns.\nTransposition\nNow I’ve never actively wanted a Jacobian-vector product in my whole\nlife. I’m sorry. I want a gradient. Gimme a gradient. I am the Veruca\nSalt of gradients.\nIn may autodiff systems, if you want11 a\ngradient, you need to implement vector-Jacobian products12\nexplicitly.\nOne of the odder little innovations in JAX is that instead of forcing\nyou to implement this as well13, you only need to\nimplement half of it.\nYou see, some clever analysis that, as far as I far as I can tell14, is detailed in this paper shows that you\nonly need to form explicit vector-Jacobian products for the structurally\nlinear arguments of the function.\nIn JAX (and maybe elsewhere), this is known as a transposition\nrule. The combination of a transopition rule and a JAX-traceable\nJacobian-vector product is enough for JAX to compute all of the\ndirectional derivatives and gradients we could ever hope for.\nAs far as I understand, it is all about functions that are\nstructurally linear in some arguments. For instance, if \\(A(x)\\) is a matrix-valued function and\n\\(x\\) and \\(y\\) are vectors, then the function \\[\nf(x, y) = A(x)y + g(x)\n\\] is structurally linear in \\(y\\) in the sense that for every fixed value\nof \\(x\\), the function \\[\nf_x(y) = A(x) y + g(x)\n\\] is linear in \\(y\\). The\nresulting transpositon rule is then\n\ndef f_transpose(x, y):\n  Ax = A(x)\n  gx = g(x)\n  return (None, Ax.T @ y + gx)\n\nThe first element of the return is None because \\(f(x,y)\\) is not15\nstructurally linear in \\(x\\) so there\nis nothing to transpose. The second element simply takes the matrix in\nthe linear function and transposes it.\nIf you know anything about autodiff, you’ll think “this doesn’t\nfeel like enough” and it’s not. JAX deals with the non-linear\npart of \\(f(x,y)\\) by tracing the\nevaluation tree for its Jacobian-vector product and … manipulating16 it.\nWe already built the abstract evaluation function last time around,\nso the tracing part can be done. All we need is the transposition\nrule.\nThe linear solve \\(f(A, b) =\nA^{-1}b\\) is non-linear in the first argument but linear in the\nsecond argument. So we only need to implement \\[\nJ^T_b(A,b)w = A^{-T}w,\n\\] where the subscript \\(b\\)\nindicates we’re only computing the Jacobian wrt \\(b\\).\nInitially, I struggled to work out what needed to be implemented\nhere. The thing that clarified the process for me was looking at JAX’s\ninternal\nimplementation of the Jacobian-vector product for a dense matrix.\nFrom there, I understood what this had to look like for a vector-valued\nfunction and this is the result.\n\ndef sparse_triangular_solve_transpose_rule(cotangent, L_indices, L_indptr, L_x, b, *, transpose):\n  \"\"\"\n  Transposition rule for the triangular solve. \n  Translated from here https://github.com/google/jax/blob/41417d70c03b6089c93a42325111a0d8348c2fa3/jax/_src/lax/linalg.py#L747.\n  Inputs:\n    cotangent: Output cotangent (aka adjoint). (produced by JAX)\n    L_indices, L_indptr, L_x: Represenation of sparse matrix. L_x should be concrete\n    b: The right hand side. Must be an jax.interpreters.ad.UndefinedPrimal\n    transpose: (boolean) True: solve $L^Tx = b$. False: Solve $Lx = b$.\n  Output:\n    A 4-tuple with the adjoints (None, None, None, b_adjoint)\n  \"\"\"\n  assert not ad.is_undefined_primal(L_x) and ad.is_undefined_primal(b)\n  if type(cotangent) is ad_util.Zero:\n    cot_b = ad_util.Zero(b.aval)\n  else:\n    cot_b = sparse_triangular_solve(L_indices, L_indptr, L_x, cotangent, transpose = not transpose)\n  return None, None, None, cot_b\n\nad.primitive_transposes[sparse_triangular_solve_p] = sparse_triangular_solve_transpose_rule\n\nIf this doesn’t make a lot of sense to you, that’s because it’s\nconfusing.\nOne way to think of it is in terms of the more ordinary notation.\nMike Giles has a classic\npaper that covers these results for basic linear algebra. The idea\nis to imagine that, as part of your larger program, you need to compute\n\\(c = A^{-1}b\\).\nForward-mode autodiff computes the sensitivity of \\(c\\), usually denoted \\(\\dot c\\) from the sensitivies \\(\\dot A\\) and \\(\\dot b\\). These have already been computed.\nThe formula in Giles is \\[\n\\dot c = A^{-1}(\\dot b - \\dot A c).\n\\] The canny reader will recognise this as exactly17\nthe formula for the Jacobian-vector product.\nSo what does reverse-mode autodiff do? Well it moves through the\nprogram in the other direction. So instead of starting with the\nsensitivities \\(\\dot A\\) and \\(\\dot b\\) already computed, we instead start\nwith the18 adjoint sensitivity \\(\\bar c\\). Our aim is to compute \\(\\bar A\\) and \\(\\bar b\\) from \\(\\bar c\\).\nThe details of how to do this are19 beyond the\nscope, but without tooooooo much effort you can show that \\[\n\\bar b = A^{-T} \\bar c,\n\\] which you should recognise as the equation that was just\nimplemented.\nThe thing that we do not have to implement in JAX is the\nother adjoint that, for dense matrices20,\nis \\[\n\\bar{A} = -\\bar{b}c^T.\n\\] Through the healing power of … something?—Truly I do not\nknow.— JAX can work that bit out itself. woo.\nTesting\nthe numerical implementation of the Jacobian-vector product\nSo let’s see if this works. I’m not going to lie, I’m flying by the\nseat of my pants here. I’m not super familiar with the JAX internals, so\nI have written a lot of test cases. You may wish to skip this part. But\nrest assured that almost every single one of these cases was useful to\nme working out how this thing actually worked!\n\ndef make_matrix(n):\n    one_d = sparse.diags([[-1.]*(n-1), [2.]*n, [-1.]*(n-1)], [-1,0,1])\n    A = (sparse.kronsum(one_d, one_d) + sparse.eye(n*n)).tocsc()\n    A_lower = sparse.tril(A, format = \"csc\")\n    A_index = A_lower.indices\n    A_indptr = A_lower.indptr\n    A_x = A_lower.data\n    return (A_index, A_indptr, A_x, A)\n\nA_indices, A_indptr, A_x, A = make_matrix(10)\n\nThis is the same test case as the last blog. We will just use the\nlower triangle of \\(A\\) as the test\nmatrix.\nFirst things first, let’s check out the numerical implementation of\nthe function. We will do that by comparing the implemented\nJacobian-vector product with the definition of the\nJacobian-vector product (aka the forward21\ndifference approximation).\nThere are lots of things that we could do here to turn these into\nactual tests. For instance, the test suite inside JAX has a lot\nof nice convenience functions for checking implementations of\nderivatives. But I went with homespun because that was how I was\nfeeling.\nYou’ll also notice that I’m using random numbers here, which is fine\nfor a blog. Not so fine for a test that you don’t want to be\npotentially22 flaky.\nThe choice of eps = 1e-4 is roughly23\nbecause it’s the square root of the single precision machine epsilon24. A very rough back of the envelope\ncalculation for the forward difference approximation to the derivative\nshows that the square root of the machine epislon is about the size you\nwant your perturbation to be.\n\nb = np.random.standard_normal(100)\n\nbt = np.random.standard_normal(100)\nbt /= np.linalg.norm(bt)\n\nA_xt = np.random.standard_normal(len(A_x))\nA_xt /= np.linalg.norm(A_xt)\n\narg_values = (A_indices, A_indptr, A_x, b )\n\narg_tangent_A = (None, None, A_xt, ad.Zero(type(b)))\narg_tangent_b = (None, None, ad.Zero(type(A_xt)), bt)\narg_tangent_Ab = (None, None, A_xt, bt)\n\np, t_A = sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_A, transpose = False)\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n_, t_b = sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_b, transpose = False)\n_, t_Ab = sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_Ab, transpose = False)\npT, t_AT = sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_A, transpose = True)\n_, t_bT = sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_b, transpose = True)\n\neps = 1e-4\ntt_A = (sparse_triangular_solve(A_indices, A_indptr, A_x + eps * A_xt, b) - p) /eps\ntt_b = (sparse_triangular_solve(A_indices, A_indptr, A_x, b + eps * bt) - p) / eps\ntt_Ab = (sparse_triangular_solve(A_indices, A_indptr, A_x + eps * A_xt, b + eps * bt) - p) / eps\ntt_AT = (sparse_triangular_solve(A_indices, A_indptr, A_x + eps * A_xt, b, transpose = True) - pT) / eps\ntt_bT = (sparse_triangular_solve(A_indices, A_indptr, A_x, b + eps * bt, transpose = True) - pT) / eps\n\nprint(f\"\"\"\nTranspose = False:\n  Error A varying: {np.linalg.norm(t_A - tt_A): .2e}\n  Error b varying: {np.linalg.norm(t_b - tt_b): .2e}\n  Error A and b varying: {np.linalg.norm(t_Ab - tt_Ab): .2e}\n\nTranspose = True:\n  Error A varying: {np.linalg.norm(t_AT - tt_AT): .2e}\n  Error b varying: {np.linalg.norm(t_bT - tt_bT): .2e}\n\"\"\")\n\nTranspose = False:\n  Error A varying:  8.51e-08\n  Error b varying:  0.00e+00\n  Error A and b varying:  4.91e-07\n\nTranspose = True:\n  Error A varying:  1.06e-07\n  Error b varying:  0.00e+00\n\nBrilliant! Everythign correct withing single precision!\nChecking on the plumbing\nMaking the numerical implementation work is only half the battle. We\nalso have to make it work in the context of JAX.\nNow I would be lying if I pretended this process went smoothly. But\nthe first time is for experience. It’s mostly a matter of just reading\nthe documentation carefully and going through similar examples that have\nalready been implemented.\nAnd testing. I learnt how this was supposed to work by testing\nit.\n(For full disclosure, I also wrote a big block f-string in the\nsparse_triangular_solve() function at one point that told\nme the types, shapes, and what transpose was, which was how\nI worked out that my code was breaking because I forgot the first to\nNone outputs in the transposition rule. When it doubt,\nprint shit.)\nAs you will see from my testing code, I was not going for elegance. I\nwas running the damn permutations. If you’re looking for elegance, look\nelsewhere.\n\nfrom jax import jvp, grad\nfrom jax import scipy as jsp\n\ndef f(theta):\n  Ax_theta = jnp.array(A_x)\n  Ax_theta = Ax_theta.at[A_indptr[20]].add(theta[0])\n  Ax_theta = Ax_theta.at[A_indptr[50]].add(theta[1])\n  b = jnp.ones(100)\n  return sparse_triangular_solve(A_indices, A_indptr, Ax_theta, b, transpose = True)\n\ndef f_jax(theta):\n  Ax_theta = jnp.array(sparse.tril(A).todense())\n  Ax_theta = Ax_theta.at[20,20].add(theta[0])\n  Ax_theta = Ax_theta.at[50,50].add(theta[1])\n  b = jnp.ones(100)\n  return jsp.linalg.solve_triangular(Ax_theta, b, lower = True, trans = \"T\")\n\ndef g(theta):\n  Ax_theta = jnp.array(A_x)\n  b = jnp.ones(100)\n  b = b.at[0].set(theta[0])\n  b = b.at[51].set(theta[1])\n  return sparse_triangular_solve(A_indices, A_indptr, Ax_theta, b, transpose = True)\n\ndef g_jax(theta):\n  Ax_theta = jnp.array(sparse.tril(A).todense())\n  b = jnp.ones(100)\n  b = b.at[0].set(theta[0])\n  b = b.at[51].set(theta[1])\n  return jsp.linalg.solve_triangular(Ax_theta, b, lower = True, trans = \"T\")\n\ndef h(theta):\n  Ax_theta = jnp.array(A_x)\n  Ax_theta = Ax_theta.at[A_indptr[20]].add(theta[0]) \n  b = jnp.ones(100)\n  b = b.at[51].set(theta[1])\n  return sparse_triangular_solve(A_indices, A_indptr, Ax_theta, b, transpose = False)\n\ndef h_jax(theta):\n  Ax_theta = jnp.array(sparse.tril(A).todense())\n  Ax_theta = Ax_theta.at[20,20].add(theta[0])\n  b = jnp.ones(100)\n  b = b.at[51].set(theta[1])\n  return jsp.linalg.solve_triangular(Ax_theta, b, lower = True, trans = \"N\")\n\ndef no_diff(theta):\n  return sparse_triangular_solve(A_indices, A_indptr, A_x, jnp.ones(100), transpose = False)\n\ndef no_diff_jax(theta):\n  return jsp.linalg.solve_triangular(jnp.array(sparse.tril(A).todense()), jnp.ones(100), lower = True, trans = \"N\")\n\nA_indices, A_indptr, A_x, A = make_matrix(10)\nprimal1, jvp1 = jvp(f, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\nprimal2, jvp2 = jvp(f_jax, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\ngrad1 = grad(lambda x: jnp.mean(f(x)))(jnp.array([-142., 342.]))\ngrad2 = grad(lambda x: jnp.mean(f_jax(x)))(jnp.array([-142., 342.]))\n\nprimal3, jvp3 = jvp(g, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\nprimal4, jvp4 = jvp(g_jax, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\ngrad3 = grad(lambda x: jnp.mean(g(x)))(jnp.array([-142., 342.]))\ngrad4 = grad(lambda x: jnp.mean(g_jax(x)))(jnp.array([-142., 342.]))  \n\nprimal5, jvp5 = jvp(h, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\nprimal6, jvp6 = jvp(h_jax, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\ngrad5 = grad(lambda x: jnp.mean(h(x)))(jnp.array([-142., 342.]))\ngrad6 = grad(lambda x: jnp.mean(h_jax(x)))(jnp.array([-142., 342.]))\n\nprimal7, jvp7 = jvp(no_diff, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\nprimal8, jvp8 = jvp(no_diff_jax, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\ngrad7 = grad(lambda x: jnp.mean(no_diff(x)))(jnp.array([-142., 342.]))\ngrad8 = grad(lambda x: jnp.mean(no_diff_jax(x)))(jnp.array([-142., 342.]))\n\nprint(f\"\"\"\nVariable L:\n  Primal difference: {np.linalg.norm(primal1 - primal2): .2e}\n  JVP difference: {np.linalg.norm(jvp1 - jvp2): .2e}\n  Gradient difference: {np.linalg.norm(grad1 - grad2): .2e}\n\nVariable b:\n  Primal difference: {np.linalg.norm(primal3 - primal4): .2e}\n  JVP difference: {np.linalg.norm(jvp3 - jvp4): .2e}\n  Gradient difference: {np.linalg.norm(grad3 - grad4): .2e} \n\nVariable L and b:\n  Primal difference: {np.linalg.norm(primal5 - primal6): .2e}\n  JVP difference: {np.linalg.norm(jvp5 - jvp6): .2e}\n  Gradient difference: {np.linalg.norm(grad5 - grad6): .2e}\n\nNo diff:\n  Primal difference: {np.linalg.norm(primal7 - primal8)}\n  JVP difference: {np.linalg.norm(jvp7 - jvp8)}\n  Gradient difference: {np.linalg.norm(grad7 - grad8)}\n\"\"\")\n\nVariable L:\n  Primal difference:  1.98e-07\n  JVP difference:  2.58e-12\n  Gradient difference:  0.00e+00\n\nVariable b:\n  Primal difference:  7.94e-06\n  JVP difference:  1.83e-08\n  Gradient difference:  3.29e-10 \n\nVariable L and b:\n  Primal difference:  2.08e-06\n  JVP difference:  1.08e-08\n  Gradient difference:  2.33e-10\n\nNo diff:\n  Primal difference: 2.2101993124579167e-07\n  JVP difference: 0.0\n  Gradient difference: 0.0\n\nStunning!\nPrimitive one: The general \\(A^{-1}b\\)\nOk. So this is a very similar problem to the one that we just solved.\nBut, as fate would have it, the solution is going to look quite\ndifferent. Why? Because we need to compute a Cholesky factorisation.\nFirst things first, though, we are going to need a JAX-traceable way\nto compute a Cholesky factor. This means that we need25\nto tell our sparse_solve function the how many non-zeros\nthe sparse Cholesky will have. Why? Well. It has to do with how the\nfunction is used.\nWhen sparse_cholesky() is called with concrete inputs26, then it can quite happily work out\nthe sparsity structure of \\(L\\). But\nwhen JAX is preparing to transform the code, eg when it’s building a\ngradient, it calls sparse_cholesky() using abstract\narguments that only share the shape information from the inputs. This is\nnot enough to compute the sparsity structure. We need\nthe indices and indptr arrays.\nThis means that we need sparse_cholesky() to throw an\nerror if L_nse isn’t passed. This wasn’t implemented well\nlast time, so here it is done properly.\n(If you’re wondering about that None argument, it is the\nidentity transform. So if A_indices is a concrete value,\nind = A_indices. Otherwise an error is called.)\n\nsparse_cholesky_p = core.Primitive(\"sparse_cholesky\")\n\ndef sparse_cholesky(A_indices, A_indptr, A_x, *, L_nse: int = None):\n  \"\"\"A JAX traceable sparse cholesky decomposition\"\"\"\n  if L_nse is None:\n    err_string = \"You need to pass a value to L_nse when doing fancy sparse_cholesky.\"\n    ind = core.concrete_or_error(None, A_indices, err_string)\n    ptr = core.concrete_or_error(None, A_indptr, err_string)\n    L_ind, _ = _symbolic_factor(ind, ptr)\n    L_nse = len(L_ind)\n  \n  return sparse_cholesky_p.bind(A_indices, A_indptr, A_x, L_nse = L_nse)\n\n\nThe rest of the Choleksy code\n\n@sparse_cholesky_p.def_impl\ndef sparse_cholesky_impl(A_indices, A_indptr, A_x, *, L_nse):\n  \"\"\"The implementation of the sparse cholesky This is not JAX traceable.\"\"\"\n  \n  L_indices, L_indptr= _symbolic_factor(A_indices, A_indptr)\n  if L_nse is not None:\n    assert len(L_indices) == L_nse\n    \n  L_x = _structured_copy(A_indices, A_indptr, A_x, L_indices, L_indptr)\n  L_x = _sparse_cholesky_impl(L_indices, L_indptr, L_x)\n  return L_indices, L_indptr, L_x\n\ndef _symbolic_factor(A_indices, A_indptr):\n  # Assumes A_indices and A_indptr index the lower triangle of $A$ ONLY.\n  n = len(A_indptr) - 1\n  L_sym = [np.array([], dtype=int) for j in range(n)]\n  children = [np.array([], dtype=int) for j in range(n)]\n  \n  for j in range(n):\n    L_sym[j] = A_indices[A_indptr[j]:A_indptr[j + 1]]\n    for child in children[j]:\n      tmp = L_sym[child][L_sym[child] > j]\n      L_sym[j] = np.unique(np.append(L_sym[j], tmp))\n    if len(L_sym[j]) > 1:\n      p = L_sym[j][1]\n      children[p] = np.append(children[p], j)\n        \n  L_indptr = np.zeros(n+1, dtype=int)\n  L_indptr[1:] = np.cumsum([len(x) for x in L_sym])\n  L_indices = np.concatenate(L_sym)\n  \n  return L_indices, L_indptr\n\n\n\ndef _structured_copy(A_indices, A_indptr, A_x, L_indices, L_indptr):\n  n = len(A_indptr) - 1\n  L_x = np.zeros(len(L_indices))\n  \n  for j in range(0, n):\n    copy_idx = np.nonzero(np.in1d(L_indices[L_indptr[j]:L_indptr[j + 1]],\n                                  A_indices[A_indptr[j]:A_indptr[j+1]]))[0]\n    L_x[L_indptr[j] + copy_idx] = A_x[A_indptr[j]:A_indptr[j+1]]\n  return L_x\n\ndef _sparse_cholesky_impl(L_indices, L_indptr, L_x):\n  n = len(L_indptr) - 1\n  descendant = [[] for j in range(0, n)]\n  for j in range(0, n):\n    tmp = L_x[L_indptr[j]:L_indptr[j + 1]]\n    for bebe in descendant[j]:\n      k = bebe[0]\n      Ljk= L_x[bebe[1]]\n      pad = np.nonzero(                                                       \\\n          L_indices[L_indptr[k]:L_indptr[k+1]] == L_indices[L_indptr[j]])[0][0]\n      update_idx = np.nonzero(np.in1d(                                        \\\n                    L_indices[L_indptr[j]:L_indptr[j+1]],                     \\\n                    L_indices[(L_indptr[k] + pad):L_indptr[k+1]]))[0]\n      tmp[update_idx] = tmp[update_idx] -                                     \\\n                        Ljk * L_x[(L_indptr[k] + pad):L_indptr[k + 1]]\n            \n    diag = np.sqrt(tmp[0])\n    L_x[L_indptr[j]] = diag\n    L_x[(L_indptr[j] + 1):L_indptr[j + 1]] = tmp[1:] / diag\n    for idx in range(L_indptr[j] + 1, L_indptr[j + 1]):\n      descendant[L_indices[idx]].append((j, idx))\n  return L_x\n\n@sparse_cholesky_p.def_abstract_eval\ndef sparse_cholesky_abstract_eval(A_indices, A_indptr, A_x, *, L_nse):\n  return core.ShapedArray((L_nse,), A_indices.dtype),                   \\\n         core.ShapedArray(A_indptr.shape, A_indptr.dtype),             \\\n         core.ShapedArray((L_nse,), A_x.dtype)\n\nWhy\ndo we need a new pattern for this very very similar problem?\nOk. So now on to the details. If we try to repeat our previous\npattern it would look like this.\n\nef sparse_solve_value_and_jvp(arg_values, arg_tangents, *, L_nse):\n  \"\"\" \n  Jax-traceable jacobian-vector product implmentation for sparse_solve.\n  \"\"\"\n  \n  A_indices, A_indptr, A_x, b = arg_values\n  _, _, A_xt, bt = arg_tangents\n\n  # Needed for shared computation\n  L_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\n\n  # Make the primal\n  primal_out = sparse_triangular_solve(L_indices, L_indptr, L_x, b, transpose = False)\n  primal_out = sparse_triangular_solve(L_indices, L_indptr, L_x, primal_out, transpose = True)\n\n  if type(A_xt) is not ad.Zero:\n    Delta_lower = jsparse.CSC((A_xt, A_indices, A_indptr), shape = (b.shape[0], b.shape[0]))\n    # We need to do Delta @ primal_out, but we only have the lower triangle\n    rhs = Delta_lower @ primal_out + Delta_lower.transpose() @ primal_out - A_xt[A_indptr[:-1]] * primal_out\n    jvp_Ax = sparse_triangular_solve(L_indices, L_indptr, L_x, rhs)\n    jvp_Ax = sparse_triangular_solve(L_indices, L_indptr, L_x, jvp_Ax, transpose = True)\n  else:\n    jvp_Ax = lax.zeros_like_array(primal_out)\n\n  if type(bt) is not ad.Zero:\n    jvp_b = sparse_triangular_solve(L_indices, L_indptr, L_x, bt)\n    jvp_b = sparse_triangular_solve(L_indices, L_indptr, L_x, jvp_b, transpose = True)\n  else:\n    jvp_b = lax.zeros_like_array(primal_out)\n\n  return primal_out, jvp_b - jvp_Ax\n\nThat’s all well and good. Nothing weird there.\nThe problem comes when you need to implement the transposition rule.\nRemembering that \\(\\bar b = A^{-T}\\bar c =\nA^{-1}\\bar c\\), you might see the issue: we are going to need the\nCholesky factorisation. But we have no way to pass \\(L\\) to the transpose function.\nThis means that we would need to compute two Cholesky\nfactorisations per gradient instead of one. As the Cholesky\nfactorisation is our slowest operation, we do not want to do extra ones!\nWe want to compute the Cholesky triangle once and pass it around like a\nparty bottom27. We do not want each of our\nfunctions to have to make a deep and meaningful connection with the damn\nmatrix28.\nA different solution\nSo how do we pass around our Cholesky triangle? Well, I do love a\ngood class so my first thought was “fuck it. I’ll make a class and I’ll\npass it that way”. But the developers of JAX had a much better\nidea.\nTheir idea was to abstract the idea of a linear solve and its\ngradients. They do this through lax.custom_linear_solve.\nThis is a function that takes all of the bits that you would need to\ncompute \\(A^{-1}b\\) and all of its\nderivatives. In particular it takes29:\nmatvec: A function that matvec(x) that\ncomputes \\(Ax\\). This might seem a bit\nweird, but it’s the most common atrocity committed by mathematicians is\nabstracting30 a matrix to a linear mapping. So we\nmight as well just suck it up.\nb: The right hand side vector31\nsolve: A function that takes takes the\nmatvec and a vector so that32\nsolve(matvec, matvec(x)) == x\nsymmetric: A boolean indicating if \\(A\\) is symmetric.\nThe idea (happily copped from the implementation of\njax.scipy.linalg.solve) is to wrap our Cholesky\ndecomposition in the solve function. Through the never ending miracle of\npartial evaluation.\n\nfrom functools import partial\n\ndef sparse_solve(A_indices, A_indptr, A_x, b, *, L_nse = None):\n  \"\"\"\n  A JAX-traceable sparse solve. For this moment, only for vector b\n  \"\"\"\n  assert b.shape[0] == A_indptr.shape[0] - 1\n  assert b.ndim == 1\n  \n  L_indices, L_indptr, L_x = sparse_cholesky(\n    lax.stop_gradient(A_indices), \n    lax.stop_gradient(A_indptr), \n    lax.stop_gradient(A_x), L_nse = L_nse)\n  \n  def chol_solve(L_indices, L_indptr, L_x, b):\n    out = sparse_triangular_solve(L_indices, L_indptr, L_x, b, transpose = False)\n    return sparse_triangular_solve(L_indices, L_indptr, L_x, out, transpose = True)\n  \n  def matmult(A_indices, A_indptr, A_x, b):\n    A_lower = jsparse.CSC((A_x, A_indices, A_indptr), shape = (b.shape[0], b.shape[0]))\n    return A_lower @ b + A_lower.transpose() @ b - A_x[A_indptr[:-1]] * b\n\n  solver = partial(\n    lax.custom_linear_solve,\n    lambda x: matmult(A_indices, A_indptr, A_x, x),\n    solve = lambda _, x: chol_solve(L_indices, L_indptr, L_x, x),\n    symmetric = True)\n\n  return solver(b)\n\nThere are three things of note in that implementation.\nThe calls to lax.stop_gradient(): These tell JAX to\nnot bother computing the gradient of these terms. The relevant parts of\nthe derivatives are computed explicitly by\nlax.custom_linear_solve in terms of matmult\nand solve, neither of which need the explicit derivative of\nthe cholesky factorisation.!\nThat definition of matmult()33:\nLook. I don’t know what to tell you. Neither addition nor indexing is\nimplemented for jsparse.CSC objects. So we did it the\nsemi-manual way. (I am thankful that matrix-vector multiplication is\navailable)\nThe definition of solver(): Partial evaluation is a\nwonderful wonderful thing. functools.partial() transforms\nlax.custom_linear_solve() from a function that takes 3\narguments (and some keywords), into a function solver()\nthat takes one34 argument35\n(b, the only positional argument of\nlax.custom_linear_solve() that isn’t specified).\nDoes it work?\n\ndef f(theta):\n  Ax_theta = jnp.array(theta[0] * A_x)\n  Ax_theta = Ax_theta.at[A_indptr[:-1]].add(theta[1])\n  b = jnp.ones(100)\n  return sparse_solve(A_indices, A_indptr, Ax_theta, b)\n\ndef f_jax(theta):\n  Ax_theta = jnp.array(theta[0] * A.todense())\n  Ax_theta = Ax_theta.at[np.arange(100),np.arange(100)].add(theta[1])\n  b = jnp.ones(100)\n  return jsp.linalg.solve(Ax_theta, b)\n\ndef g(theta):\n  Ax_theta = jnp.array(A_x)\n  b = jnp.ones(100)\n  b = b.at[0].set(theta[0])\n  b = b.at[51].set(theta[1])\n  return sparse_solve(A_indices, A_indptr, Ax_theta, b)\n\ndef g_jax(theta):\n  Ax_theta = jnp.array(A.todense())\n  b = jnp.ones(100)\n  b = b.at[0].set(theta[0])\n  b = b.at[51].set(theta[1])\n  return jsp.linalg.solve(Ax_theta, b)\n\ndef h(theta):\n  Ax_theta = jnp.array(A_x)\n  Ax_theta = Ax_theta.at[A_indptr[:-1]].add(theta[0])\n  b = jnp.ones(100)\n  b = b.at[51].set(theta[1])\n  return sparse_solve(A_indices, A_indptr, Ax_theta, b)\n\ndef h_jax(theta):\n  Ax_theta = jnp.array(A.todense())\n  Ax_theta = Ax_theta.at[np.arange(100),np.arange(100)].add(theta[0])\n  b = jnp.ones(100)\n  b = b.at[51].set(theta[1])\n  return jsp.linalg.solve(Ax_theta, b)\n\nprimal1, jvp1 = jvp(f, (jnp.array([2., 3.]),), (jnp.array([1., 2.]),))\nprimal2, jvp2 = jvp(f_jax, (jnp.array([2., 3.]),), (jnp.array([1., 2.]),))\ngrad1 = grad(lambda x: jnp.mean(f(x)))(jnp.array([2., 3.]))\ngrad2 = grad(lambda x: jnp.mean(f_jax(x)))(jnp.array([2., 3.]))\n\n\nprimal3, jvp3 = jvp(g, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\nprimal4, jvp4 = jvp(g_jax, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\ngrad3 = grad(lambda x: jnp.mean(g(x)))(jnp.array([-142., 342.]))\ngrad4 = grad(lambda x: jnp.mean(g_jax(x)))(jnp.array([-142., 342.]))\n\nprimal5, jvp5 = jvp(h, (jnp.array([2., 342.]),), (jnp.array([1., 2.]),))\nprimal6, jvp6 = jvp(h_jax, (jnp.array([2., 342.]),), (jnp.array([1., 2.]),))\ngrad5 = grad(lambda x: jnp.mean(f(x)))(jnp.array([2., 342.]))\ngrad6 = grad(lambda x: jnp.mean(f_jax(x)))(jnp.array([2., 342.]))\n\nprint(f\"\"\"\nCheck the plumbing!\nVariable A:\n  Primal difference: {np.linalg.norm(primal1 - primal2): .2e}\n  JVP difference: {np.linalg.norm(jvp1 - jvp2): .2e}\n  Gradient difference: {np.linalg.norm(grad1 - grad2): .2e}\n  \nVariable b:\n  Primal difference: {np.linalg.norm(primal3 - primal4): .2e}\n  JVP difference: {np.linalg.norm(jvp3 - jvp4): .2e}\n  Gradient difference: {np.linalg.norm(grad3 - grad4): .2e} \n    \nVariable A and b:\n  Primal difference: {np.linalg.norm(primal5 - primal6): .2e}\n  JVP difference: {np.linalg.norm(jvp5 - jvp6): .2e}\n  Gradient difference: {np.linalg.norm(grad5 - grad6): .2e}\n  \"\"\")\n\nCheck the plumbing!\nVariable A:\n  Primal difference:  2.77e-07\n  JVP difference:  1.99e-07\n  Gradient difference:  2.27e-08\n  \nVariable b:\n  Primal difference:  1.14e-05\n  JVP difference:  1.26e-07\n  Gradient difference:  9.31e-10 \n    \nVariable A and b:\n  Primal difference:  4.59e-06\n  JVP difference:  1.47e-06\n  Gradient difference:  2.03e-12\n  \n\nYes.\nWhy\nis this better than just differentiating through the Cholesky\nfactorisation?\nThe other option for making this work would’ve been to implement the\nCholesky factorisation as a primitive (~which we are about to do!~ which\nwe will do another day) and then write the sparse solver directly as a\npure JAX function.\n\ndef sparse_solve_direct(A_indices, A_indptr, A_x, b, *, L_nse = None):\n  L_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\n  out = sparse_triangular_solve(L_indices, L_indptr, L_x, b)\n  return sparse_triangular_solve(L_indices, L_indptr, L_x, out, transpose = True)\n\nThis function is JAX-traceable36 and, therefore, we\ncould compute the gradient of it directly. It turns out that this is\ngoing to be a bad idea.\nWhy? Because the derivative of sparse_cholesky, which we\nwould have to chain together with the derivatives from the solver, is\npretty complicated. Basically, this means that we’d have to do a lot\nmore work37 than we do if we just implement the\nsymbolic formula for the derivatives.\nPrimitive 3: The\ndreaded log determinant\nOk, so now we get to the good one. The log-determinant of \\(A\\). The first thing that we need to do is\nwrench out a derivative. This is not as easy as it was for the linear\nsolve. So what follows is a modification for sparse matrices from\nAppendix A of Boyd’s\nconvex optimisation book.\nIt’s pretty easy to convince yourself that \\[\\begin{align*}\n\\log(|A + \\Delta|) &= \\log\\left( \\left|A^{1/2}(I + A^{-1/2}\\Delta\nA^{-1/2})A^{1/2}\\right|\\right) \\\\\n&= \\log(|A|) + \\log\\left( \\left|I + A^{-1/2}\\Delta\nA^{-1/2}\\right|\\right).\n\\end{align*}\\]\nIt is harder to convince yourself how this could possibly be a useful\nfact.\nIf we write \\(\\lambda_i\\), \\(i = 1, \\ldots, n\\) as the eigenvalues of\n\\(A^{-1/2}\\Delta A^{-1/2}\\), then we\nhave \\[\n\\log(|A + \\Delta |) = \\log(|A|) + \\sum_{i=1}^n \\log( 1 + \\lambda_i).\n\\] Remembering that \\(\\Delta\\)\nis very small, it follows that \\(A^{-1/2}\\Delta A^{-1/2}\\) will\nalso be small. That translates to the eigenvalues of \\(A^{-1/2}\\Delta A^{-1/2}\\) all being small.\nTherefore, we can use the approximation \\(\\log(1 + \\lambda_i) = \\lambda_i +\n\\mathcal{O}(\\lambda_i^2)\\).\nThis means that38 \\[\\begin{align*}\n\\log(|A + \\Delta |) &= \\log(|A|) + \\sum_{i=1}^n  \\lambda_i +\n\\mathcal{O}\\left(\\|\\Delta\\|^2\\right) \\\\\n&=\\log(|A|) + \\operatorname{tr}\\left(A^{-1/2} \\Delta A^{-1} \\right)\n+ \\mathcal{O}\\left(\\|\\Delta\\|^2\\right) \\\\\n&= \\log(|A|) + \\operatorname{tr}\\left(A^{-1} \\Delta \\right) +\n\\mathcal{O}\\left(\\|\\Delta\\|^2\\right),\n\\end{align*}\\] which follows from the cyclic property of the\ntrace.\nIf we recall the formula from the last section defining the\nJacobian-vector product, in our context \\(m =\n1\\), \\(x\\) is the vector of\nnon-zero entries of the lower triangle of \\(A\\) stacked by column, and \\(\\delta\\) is the vector of non-zero entries\nof the lower triangle of \\(\\Delta\\).\nThat means the Jacobian-vector product is \\[\nJ(x)\\delta = \\operatorname{tr}\\left(A^{-1} \\Delta \\right) =\n\\sum_{i=1}^n\\sum_{j=1}^n[A^{-1}]_{ij} \\Delta_{ij}.\n\\]\nRemembering that \\(\\Delta\\) is\nsparse with the same sparsity pattern as \\(A\\), we see that the Jacobian-vector\nproduct requires us to know the values of \\(A^{-1}\\) that correspond to non-zero\nelements of \\(A\\). That’s good news\nbecause we will see that these entries are relatively cheap and easy to\ncompute. Whereas the full inverse is dense and very expensive to\ncompute.\nBut before we get to that, I need to point out a trap for young\nplayers39. Lest your implementations go down\nfaster than me when someone asks politely.\nThe problem comes from how we store our matrix. A mathematician would\nsuggest that it’s our representation. A physicist40\nwould shit on about being coordinate free with such passion that he41 will keep going even after you\nquietly leave the room.\nThe problem is that we only store the non-zero entries of the\nlower-triangular part of \\(A\\). This\nmeans that we need to be careful that when we compute the\nJacobian-vector product that we properly compute the Matrix-vector\nproduct.\nLet A_indices and A_indptr define the\nsparsity structure of \\(A\\) (and \\(\\Delta\\)). Then if \\(A_x\\) is our input and \\(v\\) is our vector, then we need to do the\nfollow steps to compute the Jacobian-vector product:\nCompute Ainv_x (aka the non-zero elements of \\(A^{-1}\\) that correspond to the sparsity\npattern of \\(A\\))\nCompute the matrix vector product as\n\njvp = 2 * sum(Ainv_x * v) - sum(Ainv_x[A_indptr[:-1]] * v[A_indptr[:-1]])\n\nWhy does it look like that? Well we need to add the contribution from\nthe upper triangle as well as the lower triangle. And one way to do that\nis to just double the sum and then subtract off the diagonal terms that\nwe’ve counted twice.\n(I’m making a pretty big assumption here, which is fine in our\ncontext, that \\(A\\) has a non-zero\ndiagonal. If that doesn’t hold, it’s just a change of the indexing in\nthe second term to just pull out the diagonal terms.)\nUsing similar reasoning, we can compute the Jacobian as \\[\n[J_f(x)]_{i1} = \\begin{cases}\n\\operatorname{partial-inverse}(x)_i, \\qquad & x_i  \\text{ is a\ndiagonal element of }A \\\\\n2\\operatorname{partial-inverse}(x)_i, \\qquad & \\text{otherwise},\n\\end{cases}\n\\] where \\(\\operatorname{partial-inverse}(x)\\) is the\nvector that stacks the columns of the elements of \\(A^{-1}\\) that correspond to the non-zero\nelements of \\(A\\). (Yikes!)\nComputing the partial\ninverse\nSo now we need to actually work out how to compute this partial\ninverse of a symmetric positive definite matrix \\(A\\). To do this, we are going to steal a\ntechnique that goes back to Takahashi, Fagan, and Chen42\nin 1973. (For this presentation, I’m basically pillaging Håvard\nRue and Sara Martino’s 2007 paper.)\nTheir idea was that if we write \\(A =\nVDV^T\\), where \\(V\\) is a\nlower-triangular matrix with ones on the diagonal and \\(D\\) is diagonal. This links up with our\nusual Cholesky factorisation through the identity \\(L = VD^{1/2}\\). It follows that if \\(S = A^{-1}\\), then \\(VDV^TS = I\\). Then, we make some magic\nmanipulations43. \\[\\begin{align*}\nV^TS &= D^{-1}V^{-1} \\\\\nS + V^TS &= S + D^{-1}V^{-1} \\\\\nS &= D^{-1}V^{-1} + (I - V^T)S.\n\\end{align*}\\]\nOnce again, this does not look super-useful. The trick is to notice 2\nthings.\nBecause \\(V\\) is lower\ntriangular, \\(V^{-1}\\) is also lower\ntriangular and the elements of \\(V^{-1}\\) are the inverse of the diagonal\nelements of \\(V\\) (aka they are all 1).\nTherefore, \\(D^{-1}V^{-1}\\) is a lower\ntriangular matrix with a diagonal given by the diagonal of \\(D^{-1}\\).\n\\(I - V^T\\) is an upper\ntriangular matrix and \\([I - V^T]_{nn} =\n0\\).\nThese two things together lead to the somewhat unexpected situation\nwhere the upper triangle of \\(S = D^{-1}V^{-1}\n+ (I- V^T)S\\) defines a set of recursions for the upper triangle\nof \\(S\\). (And, therefore, all of \\(S\\) because \\(S\\) is symmetric!) These are sometimes\nreferred to as the Takahashi recursions.\nBut we don’t want the whole upper triangle of \\(S\\), we just want the ones that correspond\nto the non-zero elements of \\(A\\).\nUnfortunately, the set of recursions are not, in general, solveable\nusing only that subset of \\(S\\). But we\nare in luck: they are solveable using the elements of \\(S\\) that correspond to the non-zeros of\n\\(L + L^T\\), which, as we know from a\nfew posts ago, is a superset of the non-zero elements of \\(A\\)!\nFrom this, we get the recursions running from \\(i = n, \\ldots, 1\\), \\(j = n, \\ldots, i\\) (the order is\nimportant!) such that \\(L_{ji} \\neq 0\\)\n\\[\nS_{ji} =   \\begin{cases}\n\\frac{1}{L_{ii}^2} - \\frac{1}{L_{ii}}\\sum_{k=i+1}^{n} L_{ki} S_{kj}\n\\qquad&  \\text{if } i=j, \\\\         \n- \\frac{1}{L_{ii}}\\sum_{k=i+1}^{n} L_{ki} S_{kj}  &\n\\text{otherwise}.\n\\end{cases}\n\\]\nIf you recall our discussion way back when about the way the non-zero\nstructure of the \\(j\\) the column of\n\\(L\\) relates to the non-zero structure\nof the \\(i\\) th column for \\(j \\geq i\\), it’s clear that we have\ncomputed enough44 of \\(S\\) at every step to complete the\nrecursions.\nNow we just need to Python it. (And thanks to Finn Lindgren who\nhelped me understand how to implement this, which he may or may not\nremember because it happened about five years ago.)\nActually, we need this to be JAX-traceable, so we are going to\nimplement a very basic primitive. In particular, we don’t need to\nimplement a derivative or anything like that, just an abstract\nevaluation and an implementation.\n\nsparse_partial_inverse_p = core.Primitive(\"sparse_partial_inverse\")\n\ndef sparse_partial_inverse(L_indices, L_indptr, L_x, out_indices, out_indptr):\n  \"\"\"\n  Computes the elements (out_indices, out_indptr) of the inverse of a sparse matrix (A_indices, A_indptr, A_x)\n   with Choleksy factor (L_indices, L_indptr, L_x). (out_indices, out_indptr) is assumed to be either\n   the sparsity pattern of A or a subset of it in lower triangular form. \n  \"\"\"\n  return sparse_partial_inverse_p.bind(L_indices, L_indptr, L_x, out_indices, out_indptr)\n\n@sparse_partial_inverse_p.def_abstract_eval\ndef sparse_partial_inverse_abstract_eval(L_indices, L_indptr, L_x, out_indices, out_indptr):\n  return abstract_arrays.ShapedArray(out_indices.shape, L_x.dtype)\n\n@sparse_partial_inverse_p.def_impl\ndef sparse_partial_inverse_impl(L_indices, L_indptr, L_x, out_indices, out_indptr):\n  n = len(L_indptr) - 1\n  Linv = sparse.dok_array((n,n), dtype = L_x.dtype)\n  counter = len(L_x) - 1\n  for col in range(n-1, -1, -1):\n    for row in L_indices[L_indptr[col]:L_indptr[col+1]][::-1]:\n      if row != col:\n        Linv[row, col] = Linv[col, row] = 0.0\n      else:\n        Linv[row, col] = 1 / L_x[L_indptr[col]]**2\n      L_col  = L_x[L_indptr[col]+1:L_indptr[col+1]] / L_x[L_indptr[col]]\n \n      for k, L_kcol in zip(L_indices[L_indptr[col]+1:L_indptr[col+1]], L_col):\n         Linv[col,row] = Linv[row,col] =  Linv[row, col] -  L_kcol * Linv[k, row]\n        \n  Linv_x = sparse.tril(Linv, format = \"csc\").data\n  if len(out_indices) == len(L_indices):\n    return Linv_x\n\n  out_x = np.zeros(len(out_indices))\n  for col in range(n):\n    ind = np.nonzero(np.in1d(L_indices[L_indptr[col]:L_indptr[col+1]],\n      out_indices[out_indptr[col]:out_indptr[col+1]]))[0]\n    out_x[out_indptr[col]:out_indptr[col+1]] = Linv_x[L_indptr[col] + ind]\n  return out_x\n\nThe implementation makes use of the45\ndictionary of keys representation of a sparse matrix from\nscipy.sparse. This is an efficient storage scheme when you\nneed to modify the sparsity structure (as we are doing here) or do a lot\nof indexing. It would definitely be possible to implement this directly\non the CSC data structure, but it gets a little bit tricky to access the\nelements of L_inv that are above the diagonal. The\nresulting code is honestly a mess and there’s lots of non-local memory\naccess anyway, so I implemented it this way.\nBut let’s be honest: this thing is crying out for a proper symmetric\nmatrix class with sensible reverse iterators. But hey. Python.\nThe second chunk of the code is just the opposite of our\n_structured_copy() function. It takes a matrix with the\nsparsity pattern of \\(L\\) and returns\none with the sparsity pattern of out (which is assumed to\nbe a subset, and is usually the sparsity pattern of \\(A\\) or a diagonal matrix).\nLet’s check that it works.\n\nA_indices, A_indptr, A_x, A = make_matrix(15)\nn = len(A_indptr) - 1\n\n\nL_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\n\na_inv_L = sparse_partial_inverse(L_indices, L_indptr, L_x, L_indices, L_indptr)\n\ncol_counts_L = [L_indptr[i+1] - L_indptr[i] for i in range(n)]\ncols_L = np.repeat(range(n), col_counts_L)\n\ntrue_inv = np.linalg.inv(A.todense())\ntruth_L = true_inv[L_indices, cols_L]\n\na_inv_A = sparse_partial_inverse(L_indices, L_indptr, L_x, A_indices, A_indptr)\ncol_counts_A = [A_indptr[i+1] - A_indptr[i] for i in range(n)]\ncols_A = np.repeat(range(n), col_counts_A)\ntruth_A = true_inv[A_indices, cols_A]\n\nprint(f\"\"\"\nError in partial inverse (all of L): {np.linalg.norm(a_inv_L - truth_L): .2e}\nError in partial inverse (all of A): {np.linalg.norm(a_inv_A - truth_A): .2e}\n\"\"\")\n\nError in partial inverse (all of L):  1.36e-15\nError in partial inverse (all of A):  1.32e-15\n\nPutting the\nlog-determinant together\nAll of our bits are in place, so now all we need is to implement the\nprimitive for the log-determinant. One nice thing here is that we don’t\nneed to implement a transposition rule as the function is not\nstructurally linear in any of its arguments. At this point we take our\nsmall wins where we can get them.\nThere isn’t anything particularly interesting in the implementation.\nBut do note that the trace has been implemented in a way that’s aware\nthat we’re only storing the bottom triangle of \\(A\\).\n\nsparse_log_det_p = core.Primitive(\"sparse_log_det\")\n\ndef sparse_log_det(A_indices, A_indptr, A_x):\n  return sparse_log_det_p.bind(A_indices, A_indptr, A_x)\n\n@sparse_log_det_p.def_impl\ndef sparse_log_det_impl(A_indices, A_indptr, A_x):\n  L_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\n  return 2.0 * jnp.sum(jnp.log(L_x[L_indptr[:-1]]))\n\n@sparse_log_det_p.def_abstract_eval\ndef sparse_log_det_abstract_eval(A_indices, A_indptr, A_x):\n  return abstract_arrays.ShapedArray((1,), A_x.dtype)\n\ndef sparse_log_det_value_and_jvp(arg_values, arg_tangent):\n  A_indices, A_indptr, A_x = arg_values\n  _, _, A_xt = arg_tangent\n  L_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\n  value = 2.0 * jnp.sum(jnp.log(L_x[L_indptr[:-1]]))\n  Ainv_x = sparse_partial_inverse(L_indices, L_indptr, L_x, A_indices, A_indptr)\n  jvp = 2.0 * sum(Ainv_x * A_xt) - sum(Ainv_x[A_indptr[:-1]] * A_xt[A_indptr[:-1]])\n  return value, jvp\n\nad.primitive_jvps[sparse_log_det_p] = sparse_log_det_value_and_jvp\n\nFinally, we can test it out.\n\nld_true = np.log(np.linalg.det(A.todense())) #np.sum(np.log(lu.U.diagonal()))\nprint(f\"Error in log-determinant = {ld_true - sparse_log_det(A_indices, A_indptr, A_x): .2e}\")\nError in log-determinant =  0.00e+00\ndef f(theta):\n  Ax_theta = jnp.array(theta[0] * A_x) / n\n  Ax_theta = Ax_theta.at[A_indptr[:-1]].add(theta[1])\n  return sparse_log_det(A_indices, A_indptr, Ax_theta)\n\ndef f_jax(theta):\n  Ax_theta = jnp.array(theta[0] * A.todense()) / n \n  Ax_theta = Ax_theta.at[np.arange(n),np.arange(n)].add(theta[1])\n  L = jnp.linalg.cholesky(Ax_theta)\n  return 2.0*jnp.sum(jnp.log(jnp.diag(L)))\n\nprimal1, jvp1 = jvp(f, (jnp.array([2., 3.]),), (jnp.array([1., 2.]),))\nprimal2, jvp2 = jvp(f_jax, (jnp.array([2., 3.]),), (jnp.array([1., 2.]),))\n\neps = 1e-4\njvp_fd = (f(jnp.array([2.,3.]) + eps * jnp.array([1., 2.]) ) - f(jnp.array([2.,3.]))) / eps\n\ngrad1 = grad(f)(jnp.array([2., 3.]))\ngrad2 = grad(f_jax)(jnp.array([2., 3.]))\n\nprint(f\"\"\"\nCheck the Derivatives!\nVariable A:\n  Primal difference: {np.linalg.norm(primal1 - primal2)}\n  JVP difference: {np.linalg.norm(jvp1 - jvp2)}\n  JVP difference (FD): {np.linalg.norm(jvp1 - jvp_fd)}\n  Gradient difference: {np.linalg.norm(grad1 - grad2)}\n\"\"\")\n\nCheck the Derivatives!\nVariable A:\n  Primal difference: 0.0\n  JVP difference: 0.000885009765625\n  JVP difference (FD): 0.527069091796875\n  Gradient difference: 4.577776417136192e-05\n\nI’m not going to lie, I am not happy with that JVP\ndifference. I was somewhat concerned that there was a bug somewhere in\nmy code. I did a little bit of exploring and the error got larger as the\nproblem got larger. It also depended a little bit more than I was\ncomfortable on how I had implemented46 the baseline dense\nversion.\nThat second fact suggested to me that it might be a floating point\nproblem. By default, JAX uses single precision (32-bit) floating point.\nMost modern systems that don’t try and run on GPUs use double precision\n(64-bit) floating point. So I tried it with double precision and lo and\nbehold, the problem disappears.\nMatrix factorisations are bloody hard in single precision.\n\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\n\nld_true = np.log(np.linalg.det(A.todense())) #np.sum(np.log(lu.U.diagonal()))\nprint(f\"Error in log-determinant = {ld_true - sparse_log_det(A_indices, A_indptr, A_x): .2e}\")\nError in log-determinant =  0.00e+00\ndef f(theta):\n  Ax_theta = jnp.array(theta[0] * A_x, dtype = jnp.float64) / n\n  Ax_theta = Ax_theta.at[A_indptr[:-1]].add(theta[1])\n  return sparse_log_det(A_indices, A_indptr, Ax_theta)\n\ndef f_jax(theta):\n  Ax_theta = jnp.array(theta[0] * A.todense(), dtype = jnp.float64) / n \n  Ax_theta = Ax_theta.at[np.arange(n),np.arange(n)].add(theta[1])\n  L = jnp.linalg.cholesky(Ax_theta)\n  return 2.0*jnp.sum(jnp.log(jnp.diag(L)))\n\nprimal1, jvp1 = jvp(f, (jnp.array([2., 3.], dtype = jnp.float64),), (jnp.array([1., 2.], dtype = jnp.float64),))\nprimal2, jvp2 = jvp(f_jax, (jnp.array([2., 3.], dtype = jnp.float64),), (jnp.array([1., 2.], dtype = jnp.float64),))\n\neps = 1e-7\njvp_fd = (f(jnp.array([2.,3.], dtype = jnp.float64) + eps * jnp.array([1., 2.], dtype = jnp.float64) ) - f(jnp.array([2.,3.], dtype = jnp.float64))) / eps\n\ngrad1 = grad(f)(jnp.array([2., 3.], dtype = jnp.float64))\ngrad2 = grad(f_jax)(jnp.array([2., 3.], dtype = jnp.float64))\n\nprint(f\"\"\"\nCheck the Derivatives!\nVariable A:\n  Primal difference: {np.linalg.norm(primal1 - primal2)}\n  JVP difference: {np.linalg.norm(jvp1 - jvp2)}\n  JVP difference (FD): {np.linalg.norm(jvp1 - jvp_fd)}\n  Gradient difference: {np.linalg.norm(grad1 - grad2)}\n\"\"\")\n\nCheck the Derivatives!\nVariable A:\n  Primal difference: 0.0\n  JVP difference: 8.810729923425242e-13\n  JVP difference (FD): 4.455924994317684e-06\n  Gradient difference: 2.220446049250313e-16\n\nMuch better!\nWrapping up\nAnd that is where we will leave it for today. Next up, I’m probably\ngoing to need to do the autodiff for the Cholesky factorisation. It’s\nnot hard, but it is tedious47 and this post is\nalready very long.\nAfter that we need a few more things:\nCompilation rules for all of these things. For the most part, we\ncan just wrap the relevant parts of Eigen. The only non-trivial\ncode would be the partial inverse. That will allow us to JIT\nshit.\nWe need to beef up the sparse matrix class a little. In\nparticular, we are going to need addition and scalar multiplication at\nthe very minimum to make this useful.\nWork out how Aesara works so we\ncan try to prototype a PyMC model.\nThat will be a lot more blog posts. But I’m having fun. So\nwhy the hell not.\n\nI am sorry Cholesky factorisation,\nthis blog is already too long and there is simply too much code I need\nto make nicer to even start on that journey. So it will happen in a\nlater blog.↩︎\nWhich I have spent zero\neffort making pretty or taking to any level above scratch code↩︎\nLike making it clear how this works\nfor a sparse matrix compared to a general one↩︎\nTo the best of my knowledge, for\nexample, we don’t know how to differentiate with respect to the order\nparameter \\(\\nu\\) in the modified\nBessel function of the second kind \\(K_\\nu(x)\\). This is important in spatial\nstatistics (and general GP stuff).↩︎\nYou may need to convince\nyourself that this is possible. But it is. The cone of SPD matrices is\nvery nice.↩︎\nDon’t despair if you don’t recognise\nthe third line, it’s the Neumann series, which gives an approximation to\n\\((I + B)^{-1}\\) whenever \\(\\|B\\| \\ll 1\\).↩︎\nI recognise that I’ve not explained\nwhy everything needs to be JAX-traceable. Basically it’s because JAX\ndoes clever transformations to the Jacobian-vector product code to\nproduce things like gradients. And the only way that can happen is if\nthe JVP code can take abstract JAX types. So we need to make it\ntraceable because we really want to have gradients!↩︎\nWhy not now, Daniel? Why not now?\nWell mostly because I might need to do some tweaking down the line, so I\nam not messing around until I am done.↩︎\nThis is the primary difference\nbetween implementing forward mode and reverse mode: there is only one\noutput here. When we move onto reverse mode, we will output a tuple\nJacobian-transpose-vector products, one for each input. You can see the\nstructure of that reflected in the transposition rule we are going to\nwrite later.↩︎\nSome things: Firstly your function\nneeds to have the correct signature for this to work. Secondly, you\ncould also use ad.defjvp() if you didn’t need to use the\nprimal value to define the tangent (recall one of our tangents is \\(A^{-1}\\Delta c\\), where \\(c = A^{-1}b\\) is the primal value).↩︎\nThis is because it is the efficient\nway of computing a gradient. Forward-mode autodiff chains together\nJacobian-vector products in such a way that a single sweep of the entire\nfunction computes a single directional derivative. Reverse-mode autodiff\nchains together Jacobian-transpose-vector products (aka vector-Jacobian\nproducts) in such a way that a single sweep produces an entire gradient.\n(This happens at the cost of quite a bit of storage.) Depending on what\nyou are trying to do, you usually want one or the other (or sometimes a\nclever combination of both).↩︎\nor gradients or some sort of\nthing.↩︎\nto be honest, in Stan we sometimes\njust don’t dick around with the forward-mode autodiff, because gradients\nare our bread and butter.↩︎\nI mean, love you programming\nlanguage people. But fuck me this paper could’ve been written in\nBabylonic cuneiform for all I understood it.↩︎\nThat is, if you fix a value of \\(y\\), \\(f_y(x) =\nf(x, y)\\) is not an affine function.↩︎\nDetails bore me.↩︎\nIn general, there might need to be a\nlittle bit of reshaping, but it’s equivalent.↩︎\nHave you noticed this is like the\nthird name I’ve used for this equivalent concept. Or the fourth? The\ncode calls it a cotangent because that’s another damn synonym. I’m so\nvery sorry.↩︎\nnot difficult, I’m just lazy and\nMike does it better that I can. Read his paper.↩︎\nFor sparse matrices it’s just the\nnon-zero mask of that.↩︎\nYes. I know. Central differences. I\nam what I am.↩︎\nSome of the stuff I’ve done like\nnormalising all of the inputs would help make these tests more stable.\nYou should also just pick up Nick Higham’s backwards error analysis book\nto get some ideas of what your guarantees actually are in floating\npoint, but I truly cannot be bothered. This is scratch code.↩︎\nIt should be slightly bigger, it\nisn’t.↩︎\nThe largest number \\(\\epsilon\\) such that\nfloat(1.0) == float(1.0 + machine_eps) in single precision\nfloating point.↩︎\nFun fact: I implemented this and the\nerror never spawned, so I guess JAX is keeping the index arrays\nconcrete, which is very nice of it!↩︎\nactual damn numbers↩︎\nWe want that auld triangle to go jingle\nbloody jangle↩︎\nWe definitely do not want someone to\nwrite an eight hour, two part play that really seems to have the point\nof view that our Cholesky triangle deserved his downfall. Espoused while\nperiodically reading deadshit tumblr posts. I mean, it would win a Tony.\nBut we still do not want that.↩︎\nThere are more arguments. Read the\nhelp. This is what we need↩︎\nWhat if I told you that this would\nwork perfectly well if \\(A\\) was a\nlinear partial differential operator or an integral operator? Probably\nnot much because why would you give a shit?↩︎\nIt can be more general, but it\nisn’t↩︎\nI think there is a typo in the\ndocs↩︎\nFull disclosure: I screwed this up\nmultiple times today and my tests caught it. What does that look like?\nThe derivatives for \\(A\\) being off,\nbut everything else being good.↩︎\nAnd some optional keyword arguments,\nbut we don’t need to worry about those↩︎\nThis is not quite the same but\nsimilar to something that functional programming people call\ncurrying, which was named after famous Australian Olympic\nswimmer Lisa Curry.↩︎\nand a shitload simpler!↩︎\nAnd we have to store a bunch more.\nThis is less of a big deal when \\(L\\)\nis sparse, but for an ordinary linear solve, we’d be hauling around an\nextra \\(\\mathcal{O}(n^2)\\) floats\ncontaining tangents for no good reason.↩︎\nIf you are worrying about the\nsuppressed constant, remember that \\(A\\) (and therefore \\(n\\) and \\(\\|A\\|\\)) is fixed.↩︎\nI think I’ve made this mistake about\nfour times already while writing this blog. So I am going to write it\nout.↩︎\nNot to “some of my best friends are\nphysicists”, but I do love them. I just wished a man would talk about me\nthe way they talk about being coordinate free. Rather than with the same\nambivalence physicist use when speaking about a specific atlas. I’ve\nbeen listening to lesbian folk music all evening. I’m having feelings.↩︎\npronoun on purpose↩︎\nTakahashi, K., Fagan, J., Chen,\nM.S., 1973. Formation of a sparse bus impedance matrix and its\napplication to short circuit study. In: Eighth PICA Conference\nProceedings.IEEE Power Engineering Society, pp. 63–69 (Papers Presented\nat the 1973 Power Industry Computer Application Conference in\nMinneapolis, MN).↩︎\nThanks to Jerzy Baranowski for\nfinding a very very bad LaTeX error that made these questions quite\nwrong!↩︎\nIndeed, in the notation of post two\n\\(\\mathcal{L}_i \\cap \\{i+1, \\dots, n\\}\n\\subseteq \\mathcal{L}_j\\) for all \\(i\n\\leq j\\), where \\(\\mathcal{L}_i\\) is the set of non-zeros in\nthe \\(i\\)th column of \\(L\\).↩︎\nThe sparse matrix is stored as a\ndictionary {(i,j): value}, which is a very natural way to\nbuild a sparse matrix, even if its quite inefficient to do anything with\nit in that form.↩︎\nYou can’t just use\njnp.linalg.det() because there’s a tendency towards\nnans. (The true value is something like 6.1341871^{108}!)↩︎\nWould it be less tedious if my\nimplementation of the Cholesky was less shit? Yes. But hey. It was the\nfirst non-trivial piece of python code I’d written in more than a decade\n(or maybe ever?) so it is what it is. Anyway. I’m gonna run into the\nsame problem I had in Part\n3↩︎\n",
    "preview": {},
    "last_modified": "2022-06-24T14:40:52+10:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-14-sparse4-some-primatives/",
    "title": "Sparse Matrices 5: I bind you Nancy",
    "description": "A new JAX primitive? In this economy?",
    "author": [
      {
        "name": "Dan Simpson",
        "url": "https://dpsimpson.github.io"
      }
    ],
    "date": "2022-05-20",
    "categories": [],
    "contents": "\nThis is part five of our ongoing\nseries\non\nimplementing\ndifferentiable sparse linear algebra in JAX. In some sense this is the\nlast boring post before we get to the derivatives. Was this post going\nto include the derivatives? It sure was but then I realised that a\ndifferent choice was to go to bed so I can get up nice and early in the\nmorning and vote in our election.\nIt goes without saying that before I split the posts, it was more\nthan twice as long and I was nowhere near finished. So probably the\nsplit was a good choice.\nBut how do you add a\nprimative to JAX?\nWell, the first step is you read\nthe docs.\nThey tell you that you need to implement a few things:\nAn implementation of the call with “abstract types”\nAn implementation of the call with concrete types (aka evaluation\nthe damn function)\nThen,\nif you want your primitive to be JIT-able, you need to implement\na compilation rule.\nif you want your primitive to be batch-able, you need to\nimplement a batching rule.\nif you want your primitive to be differentiable, you need to\nimplement the derivatives in a way that allows them to be propagated\nappropriately.\nIn this post, we are going to do the first task: we are going to\nregister JAX-traceable versions of the four main primitives we are going\nto need for our task. For the most part, the implementations here will\nbe replaced with C++ bindings (because only a fool writes their own\nlinear algebra code). But this is the beginning1 of\nour serious journey into JAX.\nFirst things first, some\nprimitives\nIn JAX-speak, a primitive is a function that is JAX-traceable2. It is not necessary for every\npossible transformation to be implemented. In fact, today I’m not going\nto implement any transformations. That is a problem for future\nDan.\nWe have enough today problems.\nBecause today we need to write four new primitives.\nBut first of all, let’s build up a test matrix so we can at least\ncheck that this code runs. This is the same example from blog\n3. You can tell my PhD was in numerical analysis because I fucking\nlove a 2D Laplacian.\n\nfrom scipy import sparse\nimport numpy as np\n\ndef make_matrix(n):\n    one_d = sparse.diags([[-1.]*(n-1), [2.]*n, [-1.]*(n-1)], [-1,0,1])\n    A = (sparse.kronsum(one_d, one_d) + sparse.eye(n*n)).tocsc()\n    A_lower = sparse.tril(A, format = \"csc\")\n    A_index = A_lower.indices\n    A_indptr = A_lower.indptr\n    A_x = A_lower.data\n    return (A_index, A_indptr, A_x, A)\n\nA_indices, A_indptr, A_x, A = make_matrix(10)\n\nPrimitive one: \\(A^{-1}b\\)\nBecause I’m feeling lazy today and we don’t actually need the\nCholesky directly for any of this, I’m going to just use scipy. Why?\nWell, honestly, just because I’m lazy. But also so I can prove an\nimportant point: the implementation of the primitive does not\nneed to be JAX traceable. So I’m implementing it in a way that is not\nnow and will likely never be JAX traceable3.\nFirst off, we need to write the solve function and bind it4 to JAX. Specific information about\nwhat exactly some of these commands are doing can be found in\nthe docs, but the key thing is that there is no reason to\ndick around whit JAX types in any of these implementation functions.\nThey are only ever called using (essentially) numpy5\narrays. So we can just program like normal human beings.\n\nfrom jax import numpy as jnp\n/Users/dsim0009/miniforge3/envs/myjaxenv/lib/python3.10/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\nfrom jax import core\n\nsparse_solve_p = core.Primitive(\"sparse_solve\")\n\ndef sparse_solve(A_indices, A_indptr, A_x, b):\n  \"\"\"A JAX traceable sparse solve\"\"\"\n  return sparse_solve_p.bind(A_indices, A_indptr, A_x, b)\n\n@sparse_solve_p.def_impl\ndef sparse_solve_impl(A_indices, A_indptr, A_x, b):\n  \"\"\"The implementation of the sparse solve. This is not JAX traceable.\"\"\"\n  A_lower = sparse.csc_array((A_x, A_indices, A_indptr)) \n  \n  assert A_lower.shape[0] == A_lower.shape[1]\n  assert A_lower.shape[0] == b.shape[0]\n  \n  A = A_lower + A_lower.T - sparse.diags(A_lower.diagonal())\n  return sparse.linalg.spsolve(A, b)\n\n## Check it works\nb = jnp.ones(100)\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\nx = sparse_solve(A_indices, A_indptr, A_x, b)\n\nprint(f\"The error in the sparse sovle is {np.sum(np.abs(b - A @ x)): .2e}\")\nThe error in the sparse sovle is  0.00e+00\n\nIn order to facilitate its transformations, JAX will occasionally6 call functions using\nabstract data types. These data types know the shape of the\ninputs and their data type. So our next step is to specialise the\nsparse_solve function for this case. We might as well do\nsome shape checking while we’re just hanging around. But the essential\npart of this function is just saying that the output of \\(A^{-1}b\\) is the same shape as \\(b\\) (which is usually a vector, but the\ncode is no more complex if it’s a [dense] matrix).\n\nfrom jax._src import abstract_arrays\n\n@sparse_solve_p.def_abstract_eval\ndef sparse_solve_abstract_eval(A_indices, A_indptr, A_x, b):\n  assert A_indices.shape[0] == A_x.shape[0]\n  assert b.shape[0] == A_indptr.shape[0] - 1\n  return abstract_arrays.ShapedArray(b.shape, b.dtype)\n\nPrimitive two: The\ntriangular solve\nThis is very similar. We need to have a function that computes \\(L^{-1}b\\) and \\(L^{-T}b\\). The extra wrinkle from the last\ntime around is that we need to pass a keyword argument\ntranspose to indicate which system should be solved.\nOnce again, we are going to use the appropriate scipy\nfunction (in this case sparse.linalg.spsolve_triangular).\nThere’s a little bit of casting between sparse matrix types here as\nsparse.linalg.spsolve_triangular assumes the matrix is in\nCSR format.\n\nsparse_triangular_solve_p = core.Primitive(\"sparse_triangular_solve\")\n\ndef sparse_triangular_solve(L_indices, L_indptr, L_x, b, *, transpose: bool = False):\n  \"\"\"A JAX traceable sparse  triangular solve\"\"\"\n  return sparse_triangular_solve_p.bind(L_indices, L_indptr, L_x, b, transpose = transpose)\n\n@sparse_triangular_solve_p.def_impl\ndef sparse_triangular_solve_impl(L_indices, L_indptr, L_x, b, *, transpose = False):\n  \"\"\"The implementation of the sparse triangular solve. This is not JAX traceable.\"\"\"\n  L = sparse.csc_array((L_x, L_indices, L_indptr)) \n  \n  assert L.shape[0] == L.shape[1]\n  assert L.shape[0] == b.shape[0]\n  \n  if transpose:\n    return sparse.linalg.spsolve_triangular(L.T, b, lower = False)\n  else:\n    return sparse.linalg.spsolve_triangular(L.tocsr(), b, lower = True)\n\nNow we can check if it works. We can use the fact that our matrix\n(A_indices, A_indptr, A_x) is lower-triangular (because we\nonly store the lower triangle) to make our test case.\n\n## Check if it works\nb = np.random.standard_normal(100)\nx1 = sparse_triangular_solve(A_indices, A_indptr, A_x, b)\nx2 = sparse_triangular_solve(A_indices, A_indptr, A_x, b, transpose = True)\nprint(f\"\"\"Error in trianglular solve: {np.sum(np.abs(b - sparse.tril(A) @ x1)): .2e}\nError in triangular transpose solve: {np.sum(np.abs(b - sparse.triu(A) @ x2)): .2e}\"\"\")\nError in trianglular solve:  2.33e-15\nError in triangular transpose solve:  5.68e-15\n\nAnd we can also do the abstract evaluation.\n\n@sparse_triangular_solve_p.def_abstract_eval\ndef sparse_triangular_solve_abstract_eval(L_indices, L_indptr, L_x, b, *, transpose = False):\n  assert L_indices.shape[0] == L_x.shape[0]\n  assert b.shape[0] == L_indptr.shape[0] - 1\n  return abstract_arrays.ShapedArray(b.shape, b.dtype)\n\nGreat! Now on to the next one!\nPrimitive three: The sparse\ncholesky\nOk. This one is gonna be a pain in the arse. But we need to do it.\nWhy? Because we are going to need a JAX-traceable version further on\ndown the track.\nThe issue here is that the non-zero pattern of the Cholesky\ndecomposition is computed on the fly. This is absolutely not\nallowed in JAX. It must know the shape of all things at the\nmoment it is called.\nThis is going to make for a somewhat shitty user experience for this\nfunction. It’s unavoidable with JAX designed7 the\nway it is.\nThe code in jax.experimental.sparse.bcoo.fromdense has\nthis exact problem. In their case, they are turning a dense matrix into\na sparse matrix and they can’t know until they see the dense matrix how\nmany non-zeros there are. So they do the sensible thing and ask the user\nto specify it. They do this using the nse keyword\nparameter. If you’re curious what nse stands for, it turns\nout it’s not “non-standard evaluation” but rather “number of specified\nentries”. Most other systems use the abbreviation nnz for\n“number of non-zeros”, but I’m going to stick with the JAX notation.\nThe one little thing we need to add to this code is a guard to make\nsure that if the sparse_cholesky function is called without\nspecifying\n\nsparse_cholesky_p = core.Primitive(\"sparse_cholesky\")\n\ndef sparse_cholesky(A_indices, A_indptr, A_x, *, L_nse: int = None):\n  \"\"\"A JAX traceable sparse cholesky decomposition\"\"\"\n  if L_nse is None:\n    err_string = \"You need to pass a value to L_nse when doing fancy sparse_cholesky.\"\n    _ = core.concrete_or_error(None, A_x, err_string)\n  return sparse_cholesky_p.bind(A_indices, A_indptr, A_x, L_nse = L_nse)\n\n@sparse_cholesky_p.def_impl\ndef sparse_cholesky_impl(A_indices, A_indptr, A_x, *, L_nse = None):\n  \"\"\"The implementation of the sparse cholesky This is not JAX traceable.\"\"\"\n  \n  L_indices, L_indptr= _symbolic_factor(A_indices, A_indptr)\n  if L_nse is not None:\n    assert len(L_indices) == nse\n    \n  L_x = _structured_copy(A_indices, A_indptr, A_x, L_indices, L_indptr)\n  L_x = _sparse_cholesky_impl(L_indices, L_indptr, L_x)\n  return L_indices, L_indptr, L_x\n\nThe rest of the code is just the sparse Cholesky code from blog\n2 and I’ve hidden it under the fold. (You would think I would\npackage this up properly, but I simply haven’t. Why not? Who knows8.)\n\nClick here to see the implementation\n\ndef _symbolic_factor(A_indices, A_indptr):\n  # Assumes A_indices and A_indptr index the lower triangle of $A$ ONLY.\n  n = len(A_indptr) - 1\n  L_sym = [np.array([], dtype=int) for j in range(n)]\n  children = [np.array([], dtype=int) for j in range(n)]\n  \n  for j in range(n):\n    L_sym[j] = A_indices[A_indptr[j]:A_indptr[j + 1]]\n    for child in children[j]:\n      tmp = L_sym[child][L_sym[child] > j]\n      L_sym[j] = np.unique(np.append(L_sym[j], tmp))\n    if len(L_sym[j]) > 1:\n      p = L_sym[j][1]\n      children[p] = np.append(children[p], j)\n        \n  L_indptr = np.zeros(n+1, dtype=int)\n  L_indptr[1:] = np.cumsum([len(x) for x in L_sym])\n  L_indices = np.concatenate(L_sym)\n  \n  return L_indices, L_indptr\n\n\n\ndef _structured_copy(A_indices, A_indptr, A_x, L_indices, L_indptr):\n  n = len(A_indptr) - 1\n  L_x = np.zeros(len(L_indices))\n  \n  for j in range(0, n):\n    copy_idx = np.nonzero(np.in1d(L_indices[L_indptr[j]:L_indptr[j + 1]],\n                                  A_indices[A_indptr[j]:A_indptr[j+1]]))[0]\n    L_x[L_indptr[j] + copy_idx] = A_x[A_indptr[j]:A_indptr[j+1]]\n  return L_x\n\ndef _sparse_cholesky_impl(L_indices, L_indptr, L_x):\n  n = len(L_indptr) - 1\n  descendant = [[] for j in range(0, n)]\n  for j in range(0, n):\n    tmp = L_x[L_indptr[j]:L_indptr[j + 1]]\n    for bebe in descendant[j]:\n      k = bebe[0]\n      Ljk= L_x[bebe[1]]\n      pad = np.nonzero(                                                       \\\n          L_indices[L_indptr[k]:L_indptr[k+1]] == L_indices[L_indptr[j]])[0][0]\n      update_idx = np.nonzero(np.in1d(                                        \\\n                    L_indices[L_indptr[j]:L_indptr[j+1]],                     \\\n                    L_indices[(L_indptr[k] + pad):L_indptr[k+1]]))[0]\n      tmp[update_idx] = tmp[update_idx] -                                     \\\n                        Ljk * L_x[(L_indptr[k] + pad):L_indptr[k + 1]]\n            \n    diag = np.sqrt(tmp[0])\n    L_x[L_indptr[j]] = diag\n    L_x[(L_indptr[j] + 1):L_indptr[j + 1]] = tmp[1:] / diag\n    for idx in range(L_indptr[j] + 1, L_indptr[j + 1]):\n      descendant[L_indices[idx]].append((j, idx))\n  return L_x\n\nOnce again, we can check to see if this worked!\n\nL_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\nL = sparse.csc_array((L_x, L_indices, L_indptr))\nprint(f\"The error in the sparse cholesky is {np.sum(np.abs((A - L @ L.T).todense())): .2e}\")\nThe error in the sparse cholesky is  1.02e-13\n\nAnd, of course, we can do abstract evaluation. Here is where we\nactually need to use L_nse to work out the dimension of our\noutput.\n\n@sparse_cholesky_p.def_abstract_eval\ndef sparse_cholesky_abstract_eval(A_indices, A_indptr, A_x, *, L_nse):\n  return core.ShapedArray((L_nse,), A_indices.dtype),                   \\\n         core.ShapedArray(A_indptr.shape, A_indptr.dtype),             \\\n         core.ShapedArray((L_nse,), A_x.dtype)\n\nPrimitive four: \\(\\log(|A|)\\)\nAnd now we have our final primitive: the log determinant! Wow. So\nmuch binding. For this one, we compute the Cholesky factorisation and\nnote that \\[\\begin{align*}\n|A| = |LL^T| = |L||L^T| = |L|^2.\n\\end{align*}\\] If we successfully remember that the determinant\nof a triangular matrix is the product of its diagonal entries, we have a\nformula we can implement.\nSame deal as last time.\n\nsparse_log_det_p = core.Primitive(\"sparse_log_det\")\n\ndef sparse_log_det(A_indices, A_indptr, A_x):\n  \"\"\"A JAX traceable sparse log-determinant\"\"\"\n  return sparse_log_det_p.bind(A_indices, A_indptr, A_x)\n\n@sparse_log_det_p.def_impl\ndef sparse_log_det_impl(A_indices, A_indptr, A_x):\n  \"\"\"The implementation of the sparse log-determinant. This is not JAX traceable.\n  \"\"\"\n  L_indices, L_indptr, L_x = sparse_cholesky_impl(A_indices, A_indptr, A_x)\n  return 2.0 * sum(np.log(L_x[L_indptr[:-1]]))\n\nA canny reader may notice that I’m assuming that the first element in\neach column is the diagonal. This will be true as long as the diagonal\nelements of \\(L\\) are non-zero, which\nis true as long as \\(A\\) is symmetric\npositive definite.\nLet’s test9 it out.\n\nld = sparse_log_det(A_indices, A_indptr, A_x)\nLU = sparse.linalg.splu(A)\nld_true = sum(np.log(LU.U.diagonal()))\nprint(f\"The error in the log-determinant is {ld - ld_true: .2e}\")\nThe error in the log-determinant is  0.00e+00\n\nFinally, we can do the abstract evaluation.\n\n@sparse_log_det_p.def_abstract_eval\ndef sparse_log_det_abstract_eval(A_indices, A_indptr, A_x):\n  return core.ShapedArray((1,), A_x.dtype)\n\nWhere are we now but nowhere?\nSo we are done for today. Our next step will be to implement all of\nthe bits that are needed to make the derivatives work. So in the next\ninstalment we will differentiate log-determinants, Cholesky\ndecompositions, and all kinds of other fun things.\nIt should be a blast.\n\nThe second half of this post is half\nwritten but, to be honest, I want to go to bed more than I want to\nimplement more derivatives, so I’m splitting the post.↩︎\naka JAX can map out how the pieces of\nthe function go together and it can then use that map to make its weird\ntransformations↩︎\nBut mostly because although I’m going\nto have to implement the Cholesky and triangular solves later on down\nthe line, I’m writing this in order and I don’t wanna.↩︎\nThe JAX docs don’t use decorators for\ntheir bindings but I use decorators because I like decorators.↩︎\nSomething something duck type.\nThey’re arrays with numbers in them that work in numpy and scipy. Get\noff my arse.↩︎\nThis is mostly for JIT, so it’s not\nnecessary today, but to be very honest it’s the only easy thing to do\nhere and I’m not above giving myself a participation trophy.↩︎\nThis is a … fringe problem in\nJAX-land, so it makes sense that there is a less than beautiful solution\nto the problem. I think this would be less of a design problem in Stan,\nwhere it’s possible to make the number of unknowns in the autodiff tree\ndepend on int arrays is a complex way.↩︎\nWell, me. I’m who knows. I’m still\ntreating this like scratch code in a notepad. Although we are moving\ntowards the point where I’m going to have to set everything out\nproperly. Maybe that’s the next post?↩︎\nFull disclosure: first time out I\nforgot to multiply by two. This is why we test.↩︎\n",
    "preview": {},
    "last_modified": "2022-06-24T14:38:45+10:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-16-design-is-my-passion-sparse-matrices-part-four/",
    "title": "Design is my passion: sparse matrices part four",
    "description": "Just some harmeless notes. Like the ones Judy Dench took in that movie.",
    "author": [
      {
        "name": "Dan Simpson",
        "url": "https://dpsimpson.github.io"
      }
    ],
    "date": "2022-05-16",
    "categories": [],
    "contents": "\nThis is the fourth post in a series where I try to squeeze\nautodiffable sparse matrices into JAX with the aim to speed up some\nmodel classes in PyMC. So far, I have:\nOutlined the problem Post\n1\nWorked through a basic python implementation of a sparse Cholesky\ndecomposition Post\n2\nFailed to get JAX to transform some numpy code into efficient,\nJIT-compileable code Post\n3\nI am in the process of writing a blog on building new primitives1 into JAX, but as I was doing it I\naccidentally wrote a long section about options for exposing sparse\nmatrices. It really didn’t fit very well into that blog, so here it\nis.\nWhat are we trying to do\nhere?\nIf you recall from the first blog, we need to be able\nto compute the value and gradients of the (un-normalised) log-posterior\n\\[\n\\log(p(\\theta \\mid y)) = \\frac{1}{2} \\mu_{u\\mid y,\n\\theta}(\\theta)^TA^TW^{-1}y + \\frac{1}{2} \\log(|Q(\\theta)|) -\n\\frac{1}{2}\\log(|Q_{u\\mid y, \\theta}(\\theta)|) + \\text{const},\n\\] where \\(Q(\\theta)\\) is a\nsparse matrix, and \\[\n\\mu_{u\\mid y, \\theta}(\\theta) = \\frac{1}{\\sigma^2} Q_{u\\mid\ny,\\theta}(\\theta)^{-1} A^TW^{-1}y.\n\\]\nOverall, our task is to design a system where this un-normalised\nlog-posterior can be evaluated and differentiated efficiently. As with\nall design problems, there are a lot of different ways that we can\nimplement it. They share a bunch of similarities, so we will actually\nend up implementing the guts of all of the systems.\nTo that end, let’s think of all of the ways we can implement our\ntarget2.\nOption 1: The direct design\n\\(A \\rightarrow \\log(|A|)\\), for a\nsparse, symmetric positive definite matrix \\(A\\)\n\\((A,b) \\rightarrow A^{-1}b\\), for\na sparse, symmetric positive definite matrix \\(A\\) and a vector \\(b\\)\nThis option is, in some sense, the most straightforward. We implement\nprimitives for both of the major components of our target and combine\nthem using existing JAX primitives (like addition, scalar\nmultiplication, and dot products).\nThis is a bad idea.\nThe problem is that both primitives require the Cholesky\ndecomposition of \\(A\\), so if we take\nthis route we might end up computing an extra Cholesky decomposition.\nAnd you may ask yourself: what’s an extra Cholesky decomposition\nbetween friends?\nWell, Jonathan, it’s the most expensive operation we are doing for\nthese models, so perhaps we should avoid the 1/3 increase in running\ntime!\nThere are some ways around this. We might implement sparse, symmetric\npositive definite matrices as a class that, upon instantiation, computes\nthe Cholesky factorisation.\n\nclass SPDSparse: \n  def __init__(self, A_indices, A_indptr, A_x):\n    self._perm, self._iperm = _find_perm(A_indices, A_indptr)\n    self._A_indices, self._A_indptr, self._A_x = _twist(self._perm, A_indices, A_indptr, A_x)\n    try:\n      self._L_indices, self._L_indptr, self._L_x = _compute_cholesky()\n    except SPDError:\n      print(\"Matrix is not symmetric positive definite to machine precision.\")\n  \n  def _find_perm(self, indices, indptr):\n    \"\"\"Finds the best fill-reducing permutation\"\"\"\n    raise NotImplemented(\"_find_perm\")\n  \n  def _twist(self, perm, indices, indptr, x):\n    \"\"\"Returns A[perm, perm]\"\"\"\n    raise NotImplemented(\"_twist\")\n  \n  def _compute_cholesky():\n    \"\"\"Compute the Cholesky decomposition of the permuted matrix\"\"\"\n    raise NotImplemented(\"_compute_cholesky\")\n  \n  # Not pictured: a whole forest of gets\n\nIn contexts where we need a Cholesky decomposition of every SPD\nmatrix we instantiate, this design might be useful. It might also be\nuseful to write a constructor that takes a\njax.experimental.CSCMatrix, so that we could build a\ndifferentiable matrix and then just absolutely slam it into our\nfilthy little Cholesky context3.\nIn order to use this type of pattern with JAX, we would need to\nregister it as a Pytree class, which involves writing flatten and\nunflatten routines. The CSCSparse\nclass is a good example of how to implement this type of thing. Some\ncare would be needed to make sure the differentiation rules don’t try to\ndo something stupid like differentiate with respect to\nself.iperm or self.L_x. This is beyond the\nextra autodiff\nsugar in the experimental sparse library.\nImplementing this would be quite an undertaking, but it’s certainly\nan option. The most obvious downside of this pattern (plus a fully\nfunctional sparse matrix class) is that it may end up being quite\ndelicate to have this volume of auxillary information4 in\na pytree while making everything differentiate properly. This doesn’t\nseem to be how most parts of JAX has been built. There are also a couple\nof sharp\ncorners we could run into with instantiation.\nTo close this out, it’s worth noting a variation on this pattern that\ncomes up: the optional Cholesky. The idea is that rather than compute\nthe permutations and the Cholesky factorisation on initialisation, we\nstore a boolean flag in the class is_cholesky and, whenever\nwe need a Cholesky factor we check is_cholesky and if it’s\nTrue we use the computed Cholesky factor and otherwise we\ncompute it and set is_cholesky = True.\nThis pattern introduces state to the object: it is no longer set\nand forget. This will not work within JAX5,\nwhere objects need to be immutable. It’s also not an exceptional pattern\nin general: it is considerably easier to debug code with stateless\nobjects.\nOption\n2: Implement all of the combinations of functions that we need\nRather than dicking around with classes, we could just implement\nprimitives that compute\n\\(A \\rightarrow \\log(|A|)\\), for a\nsparse, symmetric positive definite matrix \\(A\\)\n\\((A,b, c) \\rightarrow \\log(|A|) +\nc^TA^{-1}b\\), for a sparse, symmetric positive definite matrix\n\\(A\\) and vectors \\(b\\) and \\(c\\).\nThis is exactly what we need to do our task and nothing more. It\nwon’t result in any unnecessary Cholesky factors. It doesn’t need us to\nstore computed Cholesky factors. We can simply eat, prey, love.\nThe obvious downside to this option is it’s going to just massively\nexpand the codebase if there are more things that we want to do. It’s\nalso not obvious why we would do this instead of just making \\(\\log p(\\theta \\mid y)\\) a primitive6.\nOption 3: Just compute the\nCholesky\nOur third option is to simply compute (and differentiate) the\nCholesky factor directly. We can then compute \\(\\log(|A|)\\) and \\(A^{-1}b\\) through a combination of\ndifferentiable operations on the elements of the Cholesky factor (for\n\\(\\log(|A|)\\)) and triangular linear\nsolves \\(L^{-1}b\\) and \\(L^{-T}c\\) (for \\(A^{-1}b\\)).\nHence we require the following two7 JAX primitives:\n\\(A \\rightarrow\n\\operatorname{chol}(A)\\), where \\(\\operatorname{chol}(A)\\) is the Cholesky\nfactor of \\(A\\),\n\\((L, b) \\rightarrow L^{-1} b\\) and\n\\((L, b) \\rightarrow L^{-T}b\\) for\nlower-triangular sparse matrix \\(L\\).\nThis is pretty close to how the dense version of this function would\nbe implemented.\nThere are two little challenges with this pattern:\nWe are adding another large-ish node \\(L\\) to our autodiff tree. As we saw in\nother patterns, this is unnecessary storage for our problem at\nhand.\nThe number of non-zeros in \\(L\\)\nis a function of the non-zero pattern of \\(A\\). This means the Cholesky will need to\nbe implemented very carefully to ensure that its traceable\nenough.\nThe second point here might actually be an issue. To be honest, I\nhave no idea. I think maybe it’s fine? But I need to do a close read on\nthe\nadding primitives doc. Essentially, as long as the abstract traces\njust need shapes but not dimensions, we should be ok.\nFor adding this to something like Stan, however, we will likely need\nto do some extra work to make sure we know the number of parameters.\nThe advantage of this type of design pattern is that it gives users\nthe flexibility to do whatever perverted thing they want to do with the\nCholesky triangle. For example, they might want to do a\ncentring/non-centring transformation. In Option 1, we would need to\nwrite explicit functions to let them do that (not difficult, but there’s\na lot of code to write, which has the annoying tendency to increases the\nmaintainence burden).\nOption 4: Functors!\nA slightly wilder design pattern would be to abandon sparse matrices\nand just make functions A(theta, ...) that return a sparse\nmatrix. If that function is differentiable wrt its first argument, then\nwe can build this whole thing up that way.\nIn reality, the only way I can think of to implement this pattern\nwould be to implement a whole differentiable sparse matrix arithmetic\n(make operations like alpha * A + beta * B,\nC * D work for sparse matrices). At which point, we’ve\nbasically just recreated option 1.\nI’m really only bringing up functors because unlike sparse matrices,\nit is actually a pretty good model for implementing Gaussian Processes\nwith general covariance functions. There’s a little bit of the idea in\nthis Stan\nissue that, to my knowledge, hasn’t gone anywhere. More recently, a\nvariant has been used successfully in the (as yet un-merged) Laplace\napproximation feature in Stan.\nWhich one should we use?\nWe don’t really need to make that choice yet. So we won’t.\nBut personally, I like option 1. I expect everyone else on earth\nwould prefer option 3. For densities that see a lot of action, it would\nmake quite a bit of sense to consider making that density a primitive\nwhen it has a complex derivative (à la option 2).\nBut for now, let’s park this and start getting in on the\nimplementations.\n\nfunctions that have explicit\ntransformations written for them (eg explicit instruction on how to JIT\nor how to differentiate)↩︎\nI get sick of typing “unnormalised\nlog-posterior”↩︎\nI am sorry. I have had some wine.↩︎\nPermuations, cholesky, etc↩︎\nThis also won’t work in Stan, because\nall Stan objects are stateless.↩︎\nThis is actually what Stan has done\nfor a bunch of its GLM-type\nmodels. It’s very efficient and fast. But with a maintainance\nburden.↩︎\nor three, but you can implement both\ntriangular solves in one function↩︎\n",
    "preview": {},
    "last_modified": "2022-05-16T17:22:49+10:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/",
    "title": "Sparse Matrices 3: Failing at JAX",
    "description": "_Takes a long drag on cigarette._ JAX? Where was he when I had my cancer?",
    "author": [
      {
        "name": "Dan Simpson",
        "url": "https://dpsimpson.github.io"
      }
    ],
    "date": "2022-05-14",
    "categories": [],
    "contents": "\nThis is part three of an ongoing exercise in hubris. Part\none is here. Part\ntwo is here. The overall aim of this series of posts is to look at\nhow sparse Cholesky factorisations work, how JAX works, and how to marry\nthe two with the ultimate aim of putting a bit of sparse matrix support\ninto PyMC, which should allow for faster inference in linear mixed\nmodels, Gaussian spatial models. And hopefully, if anyone ever gets\naround to putting the Laplace approximation in, all sorts of GLMMs and\nnon-Gaussian models with splines and spatial effects.\nIt’s been a couple of weeks since the last blog, but I’m going to\njust assume that you are fully on top of all of those details. To that\nend, let’s jump in.\nWhat is JAX?\nJAX is\na minor miracle. It will take python+numpy code and make it cool. It\nwill let you JIT1 compile it! It will let you\ndifferentiate it! It will let you batch2.\nJAX refers to these three operations as transformations.\nBut, as The Mountain Goats tell us God is present in\nthe sweeping gesture, but the devil is in the details. And oh\nboy are those details going to be really fucking important to us.\nThere are going to be two key things that will make our lives more\ndifficult:\nNot every operation can be transformed by every operation. For\nexample, you can’t always JIT or take gradients of a for\nloop. This means that some things have to be re-written carefully to\nmake sure it’s possible to get the advantages we need.\nJAX arrays are immutable. That means that once a\nvariable is defined it cannot be changed. This means that\nthings like a = a + 1 is not allowed! If you’ve come from\nan R/Python/C/Fortran world, this is the weirdest thing to deal\nwith.\nThere are really excellent reasons for both of these restrictions.\nAnd looking into the reasons is fascinating. But not a topic for this\nblog3\nJAX has some pretty decent4 documentation, a core\npiece of which outlines some of the sharp\nedges you will run into. As you read through the documentation, the\ndesign choices become clearer.\nSo let’s go and find some sharp edges together!\nTo JAX or not to JAX\nBut first, we need to ask ourselves which functions do we need to\nJAX?\nIn the context of our problem we, so far, have three functions:\n_symbolic_factor_csc(A_indices, A_indptr), which finds\nthe non-zero indices of the sparse Cholesky factor and return them in\nCSC format,\n_deep_copy_csc(A_indices, A_indptr, A_x, L_indices, L_indptr),\nwhich takes the entries of the matrix \\(A\\) and re-creates them so they can be\nindexed within the larger pattern of non-zero elements of \\(L\\),\n_sparse_cholesky_csc_impl(L_indices, L_indptr, L_x),\nwhich actually does the sparse Cholesky factorisation.\nLet’s take them piece by piece, which is also a good opportunity to\nremind everyone what the code looked like.\nSymbolic factorisation\n\ndef _symbolic_factor_csc(A_indices, A_indptr):\n  # Assumes A_indices and A_indptr index the lower triangle of $A$ ONLY.\n  n = len(A_indptr) - 1\n  L_sym = [np.array([], dtype=int) for j in range(n)]\n  children = [np.array([], dtype=int) for j in range(n)]\n  \n  for j in range(n):\n    L_sym[j] = A_indices[A_indptr[j]:A_indptr[j + 1]]\n    for child in children[j]:\n      tmp = L_sym[child][L_sym[child] > j]\n      L_sym[j] = np.unique(np.append(L_sym[j], tmp))\n    if len(L_sym[j]) > 1:\n      p = L_sym[j][1]\n      children[p] = np.append(children[p], j)\n        \n  L_indptr = np.zeros(n+1, dtype=int)\n  L_indptr[1:] = np.cumsum([len(x) for x in L_sym])\n  L_indices = np.concatenate(L_sym)\n  \n  return L_indices, L_indptr\n\nThis function only needs to be computed once per non-zero pattern. In\nthe applications I outlined in the first post, this non-zero pattern is\nfixed. This means that you only need to run this function\nonce per analysis (unlike the others, that you will have to run\nonce per iteration!).\nAs a general rule, if you only do something once, it isn’t all that\nnecessary to devote too much time into optimising it. There\nare, however, some obvious things we could do.\nIt is, for instance, pretty easy to see how you would implement this\nwith an explicit tree5 structure instead of constantly\nnp.appending the children array. This is\nfar better from a memory standpoint.\nIt’s also easy to imagine this as a two-pass algorithm, where you\nbuild the tree and count the number of non-zero elements in the first\npass and then build and populate L_indices in the second\npass.\nThe thing is, neither of these things fixes the core problem for\nusing JAX to JIT this: the dimensions of the internal arrays depend on\nthe values of the inputs. This is not possible.\nIt seems like this would be a huge limitation, but in reality it\nisn’t. Most functions aren’t like this one! And, if we remember that JAX\nis a domain language focussing mainly on ML applications, this is\nvery rarely the case. It is always good to remember\ncontext!\nSo what are our options? We have two.\nLeave it in Python and just eat the speed.\nBuild a new\nJAX primitive and write the XLA compilation rule6.\nToday are opting for the first option!\nThe structure-changing copy\n\ndef _deep_copy_csc(A_indices, A_indptr, A_x, L_indices, L_indptr):\n  n = len(A_indptr) - 1\n  L_x = np.zeros(len(L_indices))\n  \n  for j in range(0, n):\n    copy_idx = np.nonzero(np.in1d(L_indices[L_indptr[j]:L_indptr[j + 1]],\n                                  A_indices[A_indptr[j]:A_indptr[j+1]]))[0]\n    L_x[L_indptr[j] + copy_idx] = A_x[A_indptr[j]:A_indptr[j+1]]\n  return L_x\n\nThis is, fundamentally, a piece of bookkeeping. An annoyance of\nsparse matrices. Or, if you will, explicit cast between\ndifferent sparse matrix types7. This is a thing that we\ndo actually need to be able to differentiate, so it needs to live in\nJAX.\nSo where are the potential problems? Let’s go line by line.\nn = len(A_indptr) - 1: This is lovely.\nn is used in a for loop later, but because it is a function\nof the shape of A_indptr, it is considered static\nand we will be able to JIT over it!\nL_x = np.zeros(len(L_indices)): Again, this is fine.\nSizes are derived from shapes, life is peachy.\nfor j in range(0, n):: This could be a problem if\nn was an argument or derived from values of the\narguments, but it’s derived from a shape so it is static. Praise be!\nWell, actually it’s a bit more involved than that.\nThe problem with the for loop is what will happen when\nit is JIT’d. Essentially, the loop will be statically unrolled8. That is fine for small loops, but\nit’s a bit of a pain in the arse when n is large.\nIn this case, we might want to use the structured control flow in\njax.lax9 In this case we would need\njax.lax.fori_loop(start, end, body_fun, init_value). This\nmakes the code look less pythonic, but probably should make it\nfaster. It is also, and I cannot stress this enough, an absolute dick to\nuse.\n(In actuality, we will see that we do not need this particular corner\nof the language here!)\ncopy_idx = np.nonzero(...): This looks like it’s going\nto be complicated, but actually it is a perfectly reasonable composition\nof numpy functions. Hence, we can use the same\njax.numpy functions with minimal changes. The one change\nthat we are going to need to make in order to end up with a JIT-able and\ndifferentiable function is that we need to tell JAX how many non-zero\nelements there are. Thankfully, we know this! Because the non-zero\npattern of \\(A\\) is a subset of the\nnon-zero pattern of \\(L\\), we know\nthat\n\nnp.in1d(L_indices[L_indptr[j]:L_indptr[j + 1]], A_indices[A_indptr[j]:A_indptr[j+1]])\n\nwill have exactly\nlen(A_indices[A_indptr[j]:A_indptr[j+1]]) True\nvalues, and so np.nonzero(...) will have that many. We can\npass this information to jnp.nonzero() using the optional\nsize argument.\nOh no! We have a problem! This return size is a\nfunction of the values of A_indptr rather than a\nfunction of the shape. This means we’re a bit fucked.\nThere are two routes out:\nDeclare A_indptr to be a static parameter, or\nChange the representation from CSC to something more\nconvenient.\nIn this case we could do either of these things, but I’m going to opt\nfor the second option, as it’s going to be more useful going\nforward.\nBut before we do that, let’s look at the final line in the code.\nL_x[L_indptr[j] + copy_idx] = A_x[A_indptr[j]:A_indptr[j+1]]:\nThe final non-trivial line of the code is also a problem. The issue is\nthat these arrays are immutable and we are asking to change the\nvalues! That is not allowed!\nThe solution here is to use a clunkier syntax. In JAX, we need to\nreplace\n\nx[ind] = a\n\nwith the less pleasant\n\nx = x.at[ind].set(a)\n\nWhat is going on under the hood to make the second option ok while\nthe first is an error is well beyond the scope of this little post. But\nthe important thing is that they compile down to an in-place10 update, which is all we really care\nabout.\nRe-doing the data structure.\nOk. So we need a new data structure. That’s annoying. The rule, I\nguess, is always that if you need to innovate, you should innovate very\nlittle if you can get away with it, or a lot if you have to.\nWe are going to innovate only the tiniest of bits.\nThe idea is to keep the core structure of the CSC data structure, but\nto replace the indptr array with explicitly storing the row\nindices and row values as a list of np.arrays. So\nA_index will now be a list of n\narrays that contain the row indices of the non-zero elements of \\(A\\), while A_xwill now be a\nlist of n arrays that contain the values of the\nnon-zero elements of \\(A\\).\nThis means that the matrix \\[\nB = \\begin{pmatrix}\n1 &&5 \\\\\n2&3& \\\\\n&4&6\n\\end{pmatrix}\n\\] would be stored as\n\nB_index = [np.array([0,1]), np.array([1,2]), np.array([0,2])]\nB_x = [np.array([1,2]), np.array([3,4]), np.array([5,6])]\n\nThis is a considerably more pythonic11\nversion of CSC. So I guess that’s an advantage.\nWe can easily go from CSC storage to this modified storage.\n\ndef to_pythonic_csc(indices, indptr, x):\n  index = np.split(indices, indptr[1:-1])\n  x = np.split(x, indptr[1,-1])\n  return index, x\n\nA JAX-tracable\nstructure-changing copy\nSo now it’s time to come back to that damn for loop. As\nflagged earlier, for loops can be a bit picky in JAX. If we\nuse them as is, then the code that is generated and then\ncompiled is unrolled. You can think of this as if the JIT\ncompiler automatically writes a C++ program and then compiles it. If you\nwere to examine that code, the for loop would be replaced by\nn almost identical blocks of code with only the index\nj changing between them. This leads to a potentially very\nlarge program to compile12 and it limits the\ncompiler’s ability to do clever things to make the compiled code run\nfaster13.\nThe lax.fori_loop() function, on the other hand,\ncompiles down to the equivalent of a single operation14.\nThis lets the compiler be super clever.\nBut we don’t actually need this here. Because if you take a look at\nthe original for loop we are just applying the same two lines of code to\neach triple of lists in A_index, A_x, and\nL_index (in our new15 data structure).\nThis just screams out for a map applying a single function\nindependently to each column.\nThe challenge is to find the right map function. An obvious hope\nwould be jax.vmap. Sadly, jax.vmap does not do\nthat. (At least not without more padding16\nthan a drag queen.) The problem here is a misunderstanding of what\ndifferent parts of JAX are for. Functions like jax.vmap are\nmade for applying the same function to arrays of the same size.\nThis makes sense in their context. (JAX is, after all, made for machine\nlearning and these shape assumptions fit really well in that paradigm.\nThey just don’t fit here.)\nAnd I won’t lie. After this point I went wild.\nlax.map did not help. And I honest to god tried\nlax.scan, which is will solve the problem but at what\ncost?.\nBut at some point, you read enough of the docs to find the\nanswer.\nThe correct answer here is to use the JAX concept of a\npytree. Pytrees are essentially17\nlists of arrays. They’re very flexible and they have a\njax.tree_map function that lets you map over them! We are\nsaved!\n\nimport numpy as np\nfrom jax import numpy as jnp\n/Users/dsim0009/miniforge3/envs/myjaxenv/lib/python3.10/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\nfrom jax import tree_map\n\ndef _structured_copy_csc(A_index, A_x, L_index):\n    def body_fun(A_rows, A_vals, L_rows):\n      out = jnp.zeros(len(L_rows))\n      copy_idx =  jnp.nonzero(jnp.in1d(L_rows, A_rows), size = len(A_rows))[0] \n      out = out.at[copy_idx].set(A_vals)\n      return out\n    L_x = tree_map(body_fun, A_index, A_x, L_index)\n    return L_x\n\nTesting it out\nOk so now lets see if it works. To do that I’m going to define a very\nsimple function \\[\nf(A, \\alpha, \\beta) = \\|\\alpha I + \\beta \\operatorname{tril}(A)\\|_F^2,\n\\] that is the sum of the squares of all of the elements of \\(\\alpha I + \\beta \\operatorname{tril}(A)\\).\nThere’s obviously an easy way to do this, but I’m going to do it in a\nway that uses the function we just built.\n\ndef test_func(A_index, A_x, params):\n  I_index = [jnp.array([j]) for j in range(len(A_index))]\n  I_x = [jnp.array([params[0]]) for j in range(len(A_index))]\n  I_x2 = _structured_copy_csc(I_index, I_x, A_index)\n  return jnp.sum((jnp.concatenate(I_x2) + params[1] * jnp.concatenate(A_x))**2)\n\nNext, we need a test case. Once again, we will use the 2D Laplacian\non a regular \\(n \\times n\\) grid (up to\na scaling). This is a nice little function because it’s easy to make\ntest problems of different sizes.\n\nfrom scipy import sparse\n\ndef make_matrix(n):\n    one_d = sparse.diags([[-1.]*(n-1), [2.]*n, [-1.]*(n-1)], [-1,0,1])\n    A_lower = sparse.tril(sparse.kronsum(one_d, one_d) + sparse.eye(n*n), format = \"csc\")\n    A_index = jnp.split(jnp.array(A_lower.indices), A_lower.indptr[1:-1])\n    A_x = jnp.split(jnp.array(A_lower.data), A_lower.indptr[1:-1])\n    return (A_index, A_x)\n\nWith our test case in hand, we can check to see if JAX will\ndifferentiate for us!\n\nfrom jax import grad, jit\nfrom jax.test_util import check_grads\n\ngrad_func = grad(test_func, argnums = 2)\n\nA_index, A_x = make_matrix(50)\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\nprint(f\"The value at (2.0, 2.0) is {test_func(A_index, A_x, (2.0, 2.0))}.\")\nThe value at (2.0, 2.0) is 379600.0.\nprint(f\"The gradient is {np.array(grad_func(A_index, A_x, (2.0, 2.0)))}.\")\nThe gradient is [ 60000. 319600.].\n\nFabulous! That works!\nBut what about JIT?\nJIT took fucking ages. I’m talking “it threw a message”\namounts of time. I’m not even going to pretend that I understand why.\nBut I can hazard a guess.\nMy running assumption, taken from the docs, is that as long as the\nfunction only relies of quantities that are derived from the\nshapes of the inputs (and not the values), then JAX will be\nable to trace through and JIT through the functions with ease.\nThis might not be true for tree_maps. The docs are, as\nfar as I can tell, silent on this matter. And a cursory look through the\ngithub repo did not give me any hints as to how tree_map()\nis translated.\nLet’s take a look to see if this is true.\n\nimport timeit\nfrom functools import partial\njit_test_func = jit(test_func)\n\nA_index, A_x = make_matrix(5)\ntimes = timeit.repeat(partial(jit_test_func, A_index, A_x, (2.0, 2.0)), number = 1)\nprint(f\"n = 5: {[round(t, 4) for t in times]}\")\nn = 5: [1.3092, 0.0, 0.0, 0.0001, 0.0]\n\nWe can see that the first run includes compilation time, but after\nthat it runs a bunch faster. This is how a JIT system is supposed to\nwork! But the question is: will it recompile when we run it for a\ndifferent matrix?\n\n_ = jit_test_func(A_index, A_x, (2.0, 2.0)) \nA_index, A_x = make_matrix(20)\ntimes = timeit.repeat(partial(jit_test_func, A_index, A_x, (2.0, 2.0)), number = 1)\nprint(f\"n = 20: {[round(t, 4) for t in times]}\")\nn = 20: [24.7397, 0.0005, 0.0003, 0.0002, 0.0003]\n\nDamn. It recompiles. But, as we will see, it does not recompile if we\nonly change A_x.\n\n# What if we change A_x only\n_ = jit_test_func(A_index, A_x, (2.0, 2.0)) \nA_x2 = tree_map(lambda x: x + 1.0, A_x)\ntimes = timeit.repeat(partial(jit_test_func, A_index, A_x2, (2.0, 2.0)), number = 1)\nprint(f\"n = 20, new A_x: {[round(t, 4) for t in times]}\")\nn = 20, new A_x: [0.0004, 0.0002, 0.0003, 0.0003, 0.0002]\n\nThis gives us some hope! This is because the structure of A\n(aka A_index) is fixed in our application, but the values\nA_x changes. So as long as the initial JIT compilation is\nreasonable, we should be ok.\nUnfortunately, there is something bad happening with the compilation.\nFor \\(n=10\\), it takes (on my machine)\nabout 2 seconds for the initial compilation. For \\(n=20\\), that increases to 16 seconds. Once\n\\(n = 30\\), this balloons up to 51\nseconds. Once we reach the lofty peaks18\nof \\(n=40\\), we are up at 149 seconds\nto compile.\nThis is not good. The function we are JIT-ing is very\nsimple: just one tree_map. I do not know enough19 about the internals of JAX, so I\ndon’t want to speculate too wildly. But it seems like it might be\nunrolling the tree_map before compilation, which is …\nbad.\nLet’s admit failure\nOk. So that didn’t bloody work. I’m not going to make such broad\nstatements as you can’t use the JAX library in python to write a\ntransformable sparse Cholesky factorisation, but I am more than\nprepared to say that I cannot do such a thing.\nBut, if I’m totally honest, I’m not enormously surprised.\nEven in looking at the very simple operation we focussed on today, it’s\npretty clear that the operations required to work on a sparse matrix\ndon’t look an awful lot like the types of operations you need to do the\ntypes of machine learning work that is JAX’s raison d’être.\nAnd it is never surprising to find that a library designed\nto do a fundamentally different thing does not easily adapt to whatever\nrandom task I decide to throw at it.\nBut there is a light: JAX is an extensible language. We can build a\nnew JAX primitive (or, new JAX primitives) and manually write all of the\ntransformations (batching, JIT, and autodiffing).\nAnd that is what we shall do next! It’s gonna be a blast!\n\nIf you’ve never come across this term\nbefore, you can Google it for actual details, but the squishy version is\nthat it will compile your code so it runs fast (like C code)\ninstead of slow (like python code). JIT stands for just in\ntime, which means that the code is compiled when it’s needed rather\nthan before everything else is run. It’s a good thing. It makes the\nmachine go bing faster.↩︎\nI give less of a shit about the third\ntransformation in this context. I’m not completely sure what you would\nbatch when you’re dealing with a linear mixed-ish model. But hey. Why\nnot.↩︎\nIf you’ve ever spoken to a Scala\nadvocate (or any other pure functional language), you can probably see\nthe edges of why the arrays need to be immutable\\[\n\\phantom{a}\n\\] Restrictions to JIT-able control flow has to do with how it’s\ntranslated onto the XLA compiler, which involves tracing\nthrough the code with an abstract data type with the same shape as the\none that it’s being called with. Because this abstract data type does\nnot have any values, structural parts of the code that require\nknowledge of specific values of the arguments will be lost. You can get\naround this partially by declaring those important values to be\nstatic, which would make the JIT compiler re-compile the\nfunction each time that value changes. We are not going to do that.\n\\[\n\\phantom{a}\n\\] Restrictions to gradients have to do (I assume) with\nreverse-mode autodiff needing to construct the autodiff tree at compile\ntime, which means you need to be able to compute the number of\noperations from the types and shapes of the input variables and not from\ntheir values.↩︎\nCoverage is pretty good on the\nusing bit, but, as is usual, the bits on extending the system\nare occasionally a bit … sparse. (What in the hairy Christ is a transposition\nrule actually supposed to do????)↩︎\nForest↩︎\naka implement the damn thing in C++\nand then do some proper work on it.↩︎\nIt is useful to think of a sparse\nmatrix type as the triple (value_type, indices, indptr).\nThis means that if we are going to do something like add sparse\nmatrices, we need to first cast them both to have the same type. After\nthe cast, addition of two different sparse matrices becomes the addition\nof their x attributes. The same holds for scalar\nmultiplication. Sparse matrix-matrix multiplication is a bit different\nbecause you once again need to symbolically work out the sparsity\nstructure (aka the type) of the product. ↩︎\nI think. That’s certainly what’s\nimplied by\nthe docs, but I don’t want to give the impression that I’m sure.\nBecause this is complicated.↩︎\nWhat is jax.lax? Oh\nhoney you don’t want to know.↩︎\naka there’s no weird copying↩︎\nWhatever that means\nanyway↩︎\nslowwwwww to compile↩︎\nThe XLA compiler does very clever\nthings. Incidentally, loop unrolling is actually one of the\noptimisations that compilers have in their pocket. Just not one that’s\nusually used for loops as large as this.↩︎\nRead about XLA High Level Operations\n(HLOs) here.\nThe XLA documentation is not extensive, but there’s still a lot to\nread.↩︎\nThis is why we have a new data\nstructure.↩︎\nMy kingdom for a ragged array.↩︎\nYes. They are more complicated than\nthis. But for our purposes they are lists of arrays.↩︎\n\\(n=50\\) takes so long it prints a message\ntelling us what to do if we need to do if we want to file a bug!\nCompilation eventually clocks in at 361 seconds.↩︎\naka I know sweet bugger all↩︎\n",
    "preview": {},
    "last_modified": "2022-06-24T14:37:45+10:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-23-getting-jax-to-love-sparse-matrices/",
    "title": "Sparse Matrices 2: An invitation to a sparse Cholesky factorisation",
    "description": "Come for the details, stay for the shitty Python, leave with disappointment. Not unlike the experience of dating me.",
    "author": [
      {
        "name": "Dan Simpson",
        "url": "https://dpsimpson.github.io"
      }
    ],
    "date": "2022-03-31",
    "categories": [],
    "contents": "\nThis is part two of an ongoing exercise in hubris. Part\none is here.\nThe Choleksy factorisation\nSo first things first: Cholesky wasn’t Russian. I don’t know why I\nalways thought he was, but you know. Sometime you should do a little\ngoogling first. Cholesky was French and died in the First World War.\nBut now that’s out of the way, let’s talk about matrices. If \\(A\\)1 is a symmetric positive\ndefinite matrix, then there is a unique lower-triangular matrix \\(L\\) such that \\(A\n= LL^T\\).\nLike all good theorems in numerical linear algebra, the proof of the\nexistence of the Cholesky decomposition gives a pretty clear algorithm\nfor constructing \\(L\\). To sketch2 it, let us see what it looks like if\nbuild up our Choleksy factorisation from left to right, so the first\n\\(j-1\\) columns have been modified and\nwe are looking at how to build the \\(j\\)th column. In order to make \\(L\\) lower-triangular, we need the first\n\\(j-1\\) elements of the \\(j\\)th column to be zero. Let’s see if we\ncan work out what the other columns have to be.\nWriting this as a matrix equation, we get \\[\n\\begin{pmatrix} A_{11} & a_{12} & A_{32}^T \\\\\na_{12}^T & a_{22} & a_{32}^T \\\\\nA_{31} & a_{32} & A_{33}\\end{pmatrix} =\n\\begin{pmatrix} L_{11}&& \\\\\nl_{12}^T & l_{22}&\\\\\nL_{31} & l_{32} & L_{33}\\end{pmatrix}\n\\begin{pmatrix}L_{11}^T  &l_{12} & L_{31}^T\\\\\n& l_{22}&l_{32}^T\\\\\n&  & L_{33}^T\\end{pmatrix},\n\\] where \\(L_{11}\\) is\nlower-triangular (and \\(A_{11} =\nL_{11}L_{11}^T\\)) and lower-case letters are vectors3 and everything is of the appropriate\ndimension to make \\(A_{11}\\) the\ntop-left \\((j-1) \\times (j-1)\\)\nsubmatrix of \\(A\\).\nIf we can find equations for \\(l_{22}\\) and \\(l_{32}\\) that don’t depend on \\(L_{33}\\) (ie we can express them in terms\nof things we already know), then we have found an algorithm that marches\nfrom the left of the matrix to the right leaving a Choleksy\nfactorisation in its wake!\nIf we do our matrix multiplications, we get the following equation\nfor \\(a_{22} = A_{jj}\\): \\[\na_{22} = l_{12}^Tl_{12} + l_{22}^2.\n\\] Rearranging, we get \\[\nl_{22}  = \\sqrt{a_{22} - l_{12}^Tl_{12}}.\n\\] The canny amongst you will be asking “yes but is that a real\nnumber”. The answer turns out to be “yes” for all diagonals if and only\nif4 \\(A\\) is symmetric positive definite.\nOk! We have expressed \\(l_{22}\\) in\nterms of things we know, so we are half way there. Now to attack the\nvector \\(l_{3,2}\\). Looking at the\n(3,2) equation implied by the above block matrices, we get \\[\na_{32} = L_{31}l_{12} + l_{32} l_{22}.\n\\] Remembering that \\(l_{22}\\)\nis a scalar (that we have already computed!), we get \\[\nl_{32} = (a_{32} - L_{31}l_{12}) / l_{22}.\n\\]\nSuccess!\nThis then gives us the5 Cholesky factorisation6:\nfor j in range(0,n) (using python slicing notation because why notation)\n  L[j,j] = sqrt(A[j,j] - L[j, 1:(j-1)] * L[j, 1:(j-1)]')\n  L[(j+1):n, j] = (A[(j+1):n, j] - L[(j+1):n, 1:(j-1)] * L[j, 1:(j-1)]') / L[j,j]\nEasy as.\nWhen \\(A\\) is a dense matrix, this\ncosts \\(\\mathcal{O}(n^3)\\) floating\npoint operations7.\nSo how can we take advantage of the observation that most of the\nentries of \\(A\\) are zero (aka \\(A\\) is a sparse matrix)? Well. That is the\ntopic of this post. In order, we are going to look at the following:\nStoring a sparse matrix so it works with the algorithm\nHow sparse is a Cholesky factor?\nWhich elements of the Cholesky factor are non-zero (aka symbolic\nfactorisation)\nComputing the Cholesky factorisation\nWhat about JAX? (or: fucking immutable arrays are trying to\nruin my fucking life) (This did not happen. Next time. The post is\nlong enough.)\nSo how do we store a sparse\nmatrix?\nIf we look at the Cholesky algorithm, we notice that we are scanning\nthrough the matrix column-by-column. When a computer stores a matrix, it\nstores it as a long 1D array with some side information. How this array\nis constructed from the matrix depends on the language.\nThere are (roughly) two options: column-major or row-major storage.\nColumn major storage (used by Fortran8, R, Matlab, Julia,\nEigen, etc) stacks a matrix column by column. A small example: \\[\n\\begin{pmatrix}1&3&5\\\\2&4&6 \\end{pmatrix} \\Rightarrow\n[1,2,3,4,5,6].\n\\] Row-mjor ordering (C/C++ arrays, SAS, Pascal, numpy9) stores things row-by-row.\nWhich one do we use? Well. If you look at the Cholesky algorithm, it\nscans through the matrix column-by-column. It is much much much more\nmemory efficient in this case to have the whole column available in one\ncontiguous chunk of memory. So we are going to use column-major\nstorage.\nBut there’s an extra wrinkle: Most of the entries in our matrix are\nzero. It would be very inefficient to store all of those zeros. You may\nbe sceptical about this, but it’s true. It helps to realize that even in\nthe examples at the bottom of this post that are not trying very hard to\nminimise the fill in, only 3-4% of the potential elements in \\(L\\) are non-zero.\nIt is far more efficient to just store the locations10\nof the non-zeros and their values. If only 4% of your matrix is\nnon-zero, you are saving11 a lot of memory!\nThe storage scheme we are inching towards is called compressed\nsparse column (CSC) storage. This stores the matrix in three\narrays. The first array indices (which has as many entries\nas there are non-zeros) stores the row numbers for each non-zero\nelement. So if \\[\nB = \\begin{pmatrix}\n1 &&5 \\\\\n2&3& \\\\\n&4&6\n\\end{pmatrix}\n\\] then (using zero-based indices because I’ve to to make this\nwork in Python)\n\nB_indices = [0,1,1,2,0,3]\n\nThe second array indptr is an \\(n+1\\)-dimensional array that indexes the\nfirst element of each row. The final element of indptr is\nnnz(B)12. This leads to\n\nB_indptr = [0,2,4,6]\n\nThis means that the entries in column13 j\nare have row numbers\n\nB_indices[B_indptr[j]:B_indptr[j+1]]\n\nThe third and final array is x, which stores the\nvalues of the non-negative entries of \\(A\\) column-by-column. This\ngives\n\nB_x = [1,2,3,4,5,6]\n\nUsing these three arrays we can get access to the jth\nrow of \\(B\\) by accessing\n\nB_x[B_indptr[j]:B_indptr[j+1]]\n\nThis storage scheme is very efficient for what we are about to do.\nBut it is fundamentally a static scheme: it is extremely\nexpensive to add a new non-zero element. There are other sparse matrix\nstorage schemes that make this work better.\nHow sparse\nis a Cholesky factor of a sparse matrix?\nOk. So now we’ve got that out of the way, we need to work out the\nsparsity structure of a Choleksy factorisation. At this point we need to\nclose our eyes, pray, and start thinking about graphs.\nWhy graphs? I promise, it is not because I love discrete14 maths. It is because symmetric\nsparse matrices are strongly related to graphs.\nTo remind people, a graph15 (in a mathematical\nsense) \\(\\mathcal{G} = (\\mathcal{V},\n\\mathcal{E})\\) consists of two lists:\nA list of vertices \\(\\mathcal{V}\\)\nnumbered from \\(1\\) to \\(n\\)16.\nA list of edges \\(\\mathcal{E}\\) in\nthe graph (aka all the pairs \\((i,j)\\)\nsuch that \\(i<j\\) and there is an\nedge between \\(i\\) and \\(j\\)).\nEvery symmetric sparse matrix \\(A\\)\nhas a graph naturally associated with it. The relationship is that \\((i,j)\\) (for \\(i\\neq j\\)) is an edge in \\(\\mathcal{G}\\) if and only if \\(A_{ij} \\neq 0\\).\nSo, for instance, if \\[\nA = \\begin{pmatrix}\n1&2&&8 \\\\\n2&3&& 5\\\\\n&&4&6 \\\\\n8&5&6&7\n\\end{pmatrix},\n\\]\nthen we can plot the associated graph, \\(\\mathcal{G}\\).\n\n\n\nBut why do we care about graphs?\nWe care because they let us answer our question for this section:\nwhich elements of the Cholesky factor \\(L\\) are non-zero?\nIt is useful to write the algorithm out for a second time17, but this time closer to how we\nwill implement it.\n\n\n\n\nL = np.tril(A)\nfor j in range(n):\n  for k in range(j-1):\n    L[j:n, j] -= L[j, k] * L[j:n, k]\n  L[j,j]= np.sqrt(L[j,j])\n  L[j+1:n, j] = L[j+1:n] / L[j, j]\n\nIf we stare at this long enough we can work out when \\(L_{ij}\\) is going to be potentially\nnon-zero.\nAnd here is where we have to take a quick zoom out. We are\nnot interested if the numerical entry \\(L_{ij}\\) is actually non-zero. We\nare interested if it could be non-zero. Why? Because this will\nallow us to set up our storage scheme for the sparse Cholesky factor.\nAnd it will tell us exactly which bits of the above loops we actually\nneed to do!\nSo with that motivation in mind, can we spot the non-zeros? Well.\nI’ll be honest with you. I struggle at this game. This is part of why I\ndo not like thinking about graphs18. But with a piece of\npaper and a bit of time, I can convince myslef that \\(L_ij\\) is potentially non-zero (or a\nstructural non-zero) if:\n\\(A_{ij}\\) is non-zero (because\ntmp[i-j] is non-zero!), or\n\\(L_{ik} \\neq 0\\) and\n\\(L_{jk} \\neq 0\\) for some \\(k < \\min\\{i, j\\}\\) (because that is the\nonly time an element of tmp is updated through\ntmp[i] = tmp[i] - L[i, k] * L[j, k])\nIf we dig into the second condition a bit more,19\nwe notice that the second case can happen if and only if there is a path\nin \\(\\mathcal{G}\\)20\nfrom node \\(i\\) to node \\(j\\) \\[\ni \\rightarrow v_1 \\rightarrow v_2 \\rightarrow \\ldots \\rightarrow\nv_{\\ell-1} \\rightarrow j\n\\] with \\(v_1, \\ldots v_{\\ell-1} <\n\\min\\{i,j\\}\\). The proof is an induction on \\(\\min\\{i,j\\}\\) that I can’t be arsed typing\nout.\n(As an aside, Theorem 2.8 in Rue\nand Held’s book gives a very clearn nice statistical proof of this\nresult.)\nThis is enough to see that fill in patterns are going to be a complex\nthing.\nA toy example\nConsider the following graph\n\n\n\nIt’s pretty clear that there is a path between \\((i,j)\\) for every pair \\((i,j)\\) (the path goes through the fully\nconnected vertex, which is labelled 1).\nAnd indeed, we can check this numerically21\n\n\nlibrary(Matrix)\nn <- 6\nA <- sparseMatrix(i = c(1:n, rep(1,n)), \n                  j = c(rep(1,n),1:n), \n                  x = -0.2, \n                  dims = c(n,n)) + \n      Diagonal(n)\nA != 0 #print the non-zero structrure\n\n\n6 x 6 sparse Matrix of class \"lgCMatrix\"\n                \n[1,] | | | | | |\n[2,] | | . . . .\n[3,] | . | . . .\n[4,] | . . | . .\n[5,] | . . . | .\n[6,] | . . . . |\n\nL = t(chol(as.matrix(A))) # transpose is for R reasons\nround(L, digits = 1) # Fully dense!\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]  0.8  0.0  0.0  0.0  0.0    0\n[2,] -0.3  1.0  0.0  0.0  0.0    0\n[3,] -0.3 -0.1  1.0  0.0  0.0    0\n[4,] -0.3 -0.1 -0.1  1.0  0.0    0\n[5,] -0.3 -0.1 -0.1 -0.1  1.0    0\n[6,] -0.3 -0.1 -0.1 -0.1 -0.1    1\n\nBut what if we changed the labels of our vertices? What is the fill\nin pattern implied by a labelling where the fully collected vertex is\nlabelled last instead of first?\n\n\n\nThere are now no paths from \\(i\\) to \\(j\\) that only go through lower-numbered\nvertices. So there is no fill in! We can check this numerically!22\n\n\nA2 <- A[n:1,n:1]\nL2 <- t(chol(A2))\nL2!=0\n\n\n6 x 6 sparse Matrix of class \"ltCMatrix\"\n                \n[1,] | . . . . .\n[2,] . | . . . .\n[3,] . . | . . .\n[4,] . . . | . .\n[5,] . . . . | .\n[6,] | | | | | |\n\nSo what is the lesson here?\nThe lesson is that the sparse Cholesky algorithm cares\ndeeply about what order the rows and columns of the matrix are\nin. This is why, in\nthe previous post, we put the dense rows and columns of \\(Q_{u \\mid y, \\theta}\\) at the end\nof the matrix!\nLuckily, a lot of clever graph theorists got on the job a while back\nand found a number of good algorithms for finding decent23\nways to reorder the vertices of a graph to minimise fill in. There are\ntwo particularly well-known reorderings: the approximate minimum degree\n(AMD) reordering and the nested-dissection reordering. Neither of these\nare easily available in Python24.\nAMD is a bog-standard black box that is a greedy reordering that\ntries to label the next vertex so that graph you get after removing that\nvertex and adding edges between all of the nodes that connect to that\nvertex isn’t too fucked.\nNested dissection tries to generalise the toy example above by\nfinding nodes that separate the graph into two minimally connected\ncomponents. The separator node is then labelled last. The process is\nrepeated until you run out of nodes. This algorithm can be very\nefficient in some cases (eg if the graph is planar25,\nthe sparse Cholesky algorithm using this reordering provably\ncosts at most \\(\\mathcal{O}(n^{3/2})\\)).\nTypically, you compute multiple reorderings26\nand pick the one that results in the least fill in.\nWhich\nelements of the Cholesky factor are non-zero (aka symbolic\nfactorisation)\nOk. So I guess we’ve got to work out an algorithm for computing the\nnon-zero structure of a sparse Cholesky factor. Naively, this seems\neasy: just use the Cholesky algorithm and mark which elements are\nnon-zero.\nBut this is slow and inefficient. You’re not thinking like a\nprogrammer! Or a graph theorist. So let’s talk about how to do this\nefficiently.\nThe elimination tree\nLet’s consider the graph \\(\\mathcal{G}_L\\) that contains the sparsity\npattern of \\(L\\). We know that\nthe non-zero structure consists of all \\((i,j)\\) such that \\(i < j\\) and there is a path \\(in \\mathcal{G}\\) from \\(i\\) to \\(j\\). This means we could just compute that\nand make \\(\\mathcal{G}_L\\).\nThe thing that you should notice immediately is that there is a lot\nof redundancy in this structure. Remember that if \\(L_{ik}\\) is non-zero and \\(L_{jk}\\) is also non-zero, then \\(L_{ij}\\) is also non-zero.\nThis suggests that if we have \\((i,k)\\) and \\((j,k)\\) in the graph, we can remove the\nedge \\((i,j)\\) from \\(\\mathcal{G}_L\\) and still be able to work\nout that \\(L_{ij}\\) is non-zero. This\nnew graph is no longer the graph associated with \\(L\\) but, for our purposes, it contains the\nsame information.\nIf we continue pruning the graph this way, we are going to end up\nwith a27 rooted tree! From this tree, which\nis called the elimination tree of \\(A\\)28 we can easily work out\nthe non-zero structure of \\(L\\).\nThe elimination tree is the fundamental structure needed to build an\nefficient sparse Cholesky algorithm. We are not going to use it to its\nfull potential, but it is very cheap to compute (roughly29\n\\(\\mathcal{O}(\\operatorname{nnz}(A))\\)\noperations).\nOnce we have the elimination tree, it’s cheap to compute properties\nof \\(L\\) like the number of non-zeros\nin a column, the exact sparsity pattern of every column, which columns\ncan be grouped together to form supernodes30,\nand the approximate minimum degree reordering.\nAll of those things would be necessary for a modern,\nindustrial-strength sparse Cholesky factorisation. But, and I cannot\nstress this enough, fuck that shit.\nThe symbolic factorisation\nWe are doing the easy version. Which is to say I refuse to\ndo anything here that couldn’t be easily done in the early 90s.\nSpecifically, we are going to use the version of this thatGeorge,\nLiu, and Ng wrote about31 in the 90s.\nUnderstanding this is, I think, enough to see how things like supernodal\nfactorisations work, but it’s so much less to keep track of.\nThe nice thing about this method is that we compute the elimination\ntree implicitly as we go along.\nLet \\(\\mathcal{L}_j\\) be the\nnon-zero entries in the \\(j\\)th column\nof \\(L\\). Then our discussion in the\nprevious section tells us that we need to determine the reach\nof the node i \\[\n\\text{Reach}(j, S_j) = \\left\\{i: \\text{there is a path from } i\\text{ to\n}j\\text{ through }S_j\\right\\},\n\\] where \\(S_j = \\{1,\\ldots,\nj-1\\}\\).\nIf we can compute the reach, then \\(\\mathcal{L}_j = \\text{Reach}(j, S_j)\n\\cup\\{j\\}\\)!\nThis is where the elimination tree comes in: it is an efficient\nrepresentation of these sets. Indeed, \\(i \\in\n\\text{Reach}(j, S_j)\\) if and only if there is a\ndirected32 path from \\(j\\) to \\(i\\) in the elimination tree! Now this tree\nis ordered33 so that if \\(i\\) is a child of \\(j\\) (aka directly below it in the tree),\nthen \\(i < j\\). This means that its\ncolumn in the Cholesky factorisation has already been computed. So all\nof the nodes that can be reached from \\(j\\) by going through \\(i\\) are in \\(\\mathcal{L}_{i} \\cap \\{j+1, \\ldots,\nn\\}\\).\nThis means that we can compute the non-zeros of the \\(j\\)th column of \\(L\\) efficiently from the non-zeros of all\nof the (very few, hopefully) columns associated with the child nodes of\n\\(j\\).\nSo all that’s left is to ask “how can we find the child?” (as phones\naround the city start buzzing). Well, a little bit of thinking time\nshould convince you that if \\[\np = \\min\\{i : i \\in \\text{Reach}(j, S_j) \\},\n\\] then \\(p\\) is the parent of\n\\(i\\). Or, the parent of column \\(j\\) is the index of its first34 non-zero below the diagonal.\nWe can put all of these observations together into the following\nalgorithm. We assume that we are given the non-zero structure of\ntril(A) (aka the lower-triangle of \\(A\\)).\n\nimport numpy as np\n\ndef _symbolic_factor_csc(A_indices, A_indptr):\n  # Assumes A_indices and A_indptr index the lower triangle of $A$ ONLY.\n  n = len(A_indptr) - 1\n  L_sym = [np.array([], dtype=int) for j in range(n)]\n  children = [np.array([], dtype=int) for j in range(n)]\n  \n  for j in range(n):\n    L_sym[j] = A_indices[A_indptr[j]:A_indptr[j + 1]]\n    for child in children[j]:\n      tmp = L_sym[child][L_sym[child] > j]\n      L_sym[j] = np.unique(np.append(L_sym[j], tmp))\n    if len(L_sym[j]) > 1:\n      p = L_sym[j][1]\n      children[p] = np.append(children[p], j)\n        \n  L_indptr = np.zeros(n+1, dtype=int)\n  L_indptr[1:] = np.cumsum([len(x) for x in L_sym])\n  L_indices = np.concatenate(L_sym)\n  \n  return L_indices, L_indptr\n  \n\nThis was the first piece of Python I’ve written in about 13 years35, so it’s a bit shit. Nevertheless,\nit works. It is possible to replace the children structure\nby a linked list implemented in an n-dimensional integer array36, but why bother. This function is\nrun once.\nIt’s also worth noting that the children array expresses\nthe elimination tree. If we were going to do something with it\nexplicitly, we could just spit it out and reshape it into a more useful\ndata structure.\nThere’s one more piece of tedium before we can get to the main event:\nwe need to do a deep copy of \\(A\\) into\nthe data structure of \\(L\\). There is\nno37 avoiding this.\nHere is the code.\n\ndef _deep_copy_csc(A_indices, A_indptr, A_x, L_indices, L_indptr):\n  n = len(A_indptr) - 1\n  L_x = np.zeros(len(L_indices))\n  \n  for j in range(0, n):\n    copy_idx = np.nonzero(np.in1d(L_indices[L_indptr[j]:L_indptr[j + 1]],\n                                  A_indices[A_indptr[j]:A_indptr[j+1]]))[0]\n    L_x[L_indptr[j] + copy_idx] = A_x[A_indptr[j]:A_indptr[j+1]]\n  return L_x\n\nComputing the Cholesky\nfactorisation\nIt feels like we’ve been going for a really long time and we still\ndon’t have a Cholesky factorisation. Mate. I feel your pain. Believe\nme.\nBut we are here now: everything is in place. We can now write down\nthe Cholesky algorithm!\nThe algorithm is as it was before, with the main difference being\nthat we now know two things:\nWe only need to update tmp with descendent of\nj in the elimination tree.\nThat’s it. That is the only thing we know.\nOf course, we could use the elimination tree to do this very\nefficiently, but, as per my last email, I do not care. So we\nwill simply build up a copy of all of the descendants. This will\nobviously be less efficient, but it’s fine for our purposes. Let’s face\nit, we’re all going to die eventually.\nSo here it goes.\n\ndef _sparse_cholesky_csc_impl(L_indices, L_indptr, L_x):\n    n = len(L_indptr) - 1\n    descendant = [[] for j in range(0, n)]\n    for j in range(0, n):\n        tmp = L_x[L_indptr[j]:L_indptr[j + 1]]\n        for bebe in descendant[j]:\n            k = bebe[0]\n            Ljk= L_x[bebe[1]]\n            pad = np.nonzero(                                                \\\n              L_indices[L_indptr[k]:L_indptr[k+1]] == L_indices[L_indptr[j]])[0][0]\n            update_idx = np.nonzero(np.in1d(                                 \\\n              L_indices[L_indptr[j]:L_indptr[j+1]],                          \\\n              L_indices[(L_indptr[k] + pad):L_indptr[k+1]]))[0]\n            tmp[update_idx] = tmp[update_idx] -                              \\\n              Ljk * L_x[(L_indptr[k] + pad):L_indptr[k + 1]]\n            \n        diag = np.sqrt(tmp[0])\n        L_x[L_indptr[j]] = diag\n        L_x[(L_indptr[j] + 1):L_indptr[j + 1]] = tmp[1:] / diag\n        for idx in range(L_indptr[j] + 1, L_indptr[j + 1]):\n            descendant[L_indices[idx]].append((j, idx))\n    return L_x\n\nThe one thing that you’ll note in this code38\nis that we are implicitly using things that we know about the sparsity\nstructure of the \\(j\\)th column. In\nparticular, we know that the sparsity structure of the \\(j\\)th column is the union of the\nrelevant parts of the sparsity structure of their dependent columns.\nThis allows a lot of our faster indexing to work.\nFinally, we can put it all together.\n\ndef sparse_cholesky_csc(A_indices, A_indptr, A_x):\n    L_indices, L_indptr= _symbolic_factor_csc(A_indices, A_indptr)\n    L_x = _deep_copy_csc(A_indices, A_indptr, A_x, L_indices, L_indptr)\n    L_x = _sparse_cholesky_csc_impl(L_indices, L_indptr, L_x)\n    return L_indices, L_indptr, L_x\n\nRight. Let’s test it. We’re going to work on a particular39 sparse matrix.\n\nfrom scipy import sparse\n\nn = 50\none_d = sparse.diags([[-1.]*(n-1), [2.]*n, [-1.]*(n-1)], [-1,0,1])\nA = sparse.kronsum(one_d, one_d) + sparse.eye(n*n)\nA_lower = sparse.tril(A, format = \"csc\")\nA_indices = A_lower.indices\nA_indptr = A_lower.indptr\nA_x = A_lower.data\n\nL_indices, L_indptr, L_x = sparse_cholesky_csc(A_indices, A_indptr, A_x)\nL = sparse.csc_array((L_x, L_indices, L_indptr), shape = (n**2, n**2))\n\nerr = np.sum(np.abs((A - L @ L.transpose()).todense()))\nprint(f\"Error in Cholesky is {err}\")\nError in Cholesky is 3.871041263071504e-12\nnnz = len(L_x)\nprint(f\"Number of non-zeros is {nnz} (fill in of {len(L_x) - len(A_x)})\")\nNumber of non-zeros is 125049 (fill in of 117649)\n\nFinally, let’s demonstrate that we can reduce the amount of fill-in\nwith a reordering. Obviously, the built in permutation in\nscipy is crappy, so we will not see much of a difference.\nBut nevertheless. It’s there.\n\nperm = sparse.csgraph.reverse_cuthill_mckee(A, symmetric_mode=True)\nprint(perm)\n[2499 2498 2449 ...   50    1    0]\nA_perm = A[perm[:,None], perm]\nA_perm_lower = sparse.tril(A_perm, format = \"csc\")\nA_indices = A_perm_lower.indices\nA_indptr = A_perm_lower.indptr\nA_x = A_perm_lower.data\n\nL_indices, L_indptr, L_x = sparse_cholesky_csc(A_indices, A_indptr, A_x)\nL = sparse.csc_array((L_x, L_indices, L_indptr), shape = (n**2, n**2))\nerr = np.sum(np.abs((A_perm - L @ L.transpose()).todense()))\nprint(f\"Error in Cholesky is {err}\")\nError in Cholesky is 3.0580421951974465e-12\nnnz_rcm = len(L_x)\nprint(f\"Number of non-zeros is {nnz_rcm} (fill in of {len(L_x) - len(A_x)}),\\nwhich is less than the unpermuted matrix, which had {nnz} non-zeros.\")\nNumber of non-zeros is 87025 (fill in of 79625),\nwhich is less than the unpermuted matrix, which had 125049 non-zeros.\n\nAnd finally, let’s check that we’ve not made some fake non-zeros. To\ndo this we need to wander back into R because\nscipy doesn’t have a sparse Cholesky40\nfactorisation.\n\n\nind <- py$A_indices\nindptr <- py$A_indptr\nx <- as.numeric(py$A_x)\nA = sparseMatrix(i = ind + 1, p = indptr, x=x, symmetric = TRUE)\n\nL = t(chol(A))\nsum(L@i - py$L_indices)\n\n\n[1] 0\n\nsum(L@p - py$L_indptr)\n\n\n[1] 0\n\nPerfect.\nOk we are done for today.\nI was hoping that we were going to make it to the JAX implementation,\nbut this is long enough now. And I suspect that there will be some\nissues that are going to come up.\nIf you want some references, I recommend:\nGeorge,\nLiu, and Ng’s notes (warning: FORTRAN).\nTimothy\nDavis’ book (warning: pure C).\nLiu’s survey\npaper about elimination trees (warning: trees).\nRue\nand Held’s book (Statistically motivated).\nObviously this is a massive area and I obviously did not do it\njustice in a single blog post. It’s well worth looking further into. It\nis very cool. And obviously, I go through all this41 to get a prototype that I can play\nwith all of the bits of. For the love of god, use Cholmod or Eigen or\nMUMPS or literally anything else. The only reason to write these\nyourself is to learn how to understand it.\n\nThe old numerical linear algebra\nnaming conventions: Symmetric letters are symmetric matrices, upper case\nis a matrix, lower case is a vector, etc etc etc. Obviously, all\nconventions in statistics go against this so who really cares. Burn it\nall down.↩︎\nGo girl. Give us nothing.↩︎\nor scalars↩︎\nThis is actually how you check if a\nmatrix is SPD. Such a useful agorithm!↩︎\nThis variant is called the\nleft-looking Cholesky. There are 6 distinct ways to rearrange these\ncomputations that lead to algorithms that are well-adapted to different\nstructures. The left-looking algorithm is well adapted to matrices\nstored column-by-column. But it is not the only one! The variant of the\nsparse Cholesky in Matlab and Eigen is the upward-looking Cholesky.\nCHOLMOD uses the left-looking Cholesky (because that’s how you get\nsupernodes). MUMPS uses the right-looking variant. Honestly this is a\nfucking fascinating wormhole you can fall down. A solid review of some\nof the possibilities is in Chapter 4 of Tim Davis’ book.↩︎\nHere A is a \\(n\\times n\\) matrix and u' is\nthe transpose of the vector u.↩︎\nYou can also see that if \\(A\\) is stored in memory by stacking the\ncolumns, this algorithm is set up to be fairly memory efficient. Of\ncourse, if you find yourself caring about what your cache is doing,\nyou’ve gone astray somewhere. That is why professionals have coded this\nup (only a fool competes with LAPACK).↩︎\nThe ultimate language of scientific\ncomputing. Do not slide into my DMs and suggest Julia is.↩︎\nYou may be thinking well surely\nwe have to use a row-major ordering. But honey let me tell you. We\nare building our own damn storage method, so we can order it however we\nbloody want. Also, somewhere down the line I’m going to do this in\nEigen, which is column major by default.↩︎\nIf you look at the algorithm, you’ll\nsee that we only need to store the diagonal and the entries below. This\nis enough (in general) because we know the matrix is symmetric!↩︎\nCPU operations are a lot less\nmemory-limited than they used to be, but nevertheless it piles up. GPU\noperations still very much are, but sparse matrix operations mostly\ndon’t have the arithmetic intensity to be worth putting on a GPU.↩︎\n(NB: zero-based indexing!) This is a\nsuperfluous entry (the information is available elsewhere), but having\nit in makes life just a million times easier because you don’t have to\ntreat the final column separately!.↩︎\nZERO BASED, PYTHON SLICES↩︎\nI am not a headless torso that can’t\nhost. I differentiate.↩︎\nWe only care about undirected\ngraphs↩︎\nOr from \\(0\\) to \\(n-1\\) if you have hate in your heart and\ndarkness in your soul.↩︎\nTo get from the previous version of\nthe algorithm to this, we unwound all of those beautiful vectorised\nmatrix-vector products. This would be a terrible idea if we were doing a\ndense Cholesky, but as general rule if you are implementing your own\ndense Cholesky factorisation you have already committed to a terrible\nidea. (The same, to be honest, is true for sparse Choleskys. But\nnevertheless, she persisted.)↩︎\nor trees or really any discrete\nstructure.↩︎\nDon’t kid yourself, we look this shit\nup.↩︎\nThis means that all of the pairs\n\\((i, v_1)\\), \\((v_i, v_{i+1})\\) and \\((v_{\\ell-1}, v_j)\\) are all in the edge set\n\\(\\mathcal{E}\\)↩︎\nThe specific choices building this\nmatrix are to make sure it’s positive definite. The transpose is there\nbecause in R, R <- chol(A) returns an upper\ntriangular matrix that satisfies \\(A =\nR^TR\\). I assume this is because C has row-major storage, but I\nhonestly don’t care enough to look it up.↩︎\nHere the pivot = FALSE\noption is needed because the default for a sparse Cholesky decomposition\nin R is to re-order the vertices to try to minimise the fill-in. But\nthat goes against the example!↩︎\nFinding the minimum fill reordering\nis NP-hard, so everything is heuristic.↩︎\nscipy has the reverse Cuthill-McKee\nreordering—which is shit—easily available. As far as I can tell, the\neasiest way to get AMD out is to factorise a sparse matrix in scipy and\npull the reordering out. If I were less lazy, I’d probably just bind\nSuiteSparse’s AMD algorithm, which is permissively licensed. But nah.\nThe standard nested-dissection implementation is in the METIS package,\nwhich used to have a shit license but is now Apache2.0. Good on you\nMETIS!↩︎\nand some other cases↩︎\nThey are cheap to compute↩︎\nActually, you get a forest in\ngeneral. You get a tree if \\(\\mathcal{G}\\) has a single connected\ncomponent, otherwise you get a bunch of disjoint trees. But we still\ncall it a tree because maths is wild.↩︎\nFun fact: it is the spanning tree of\nthe graph of \\(L + L^T\\). Was that fun?\nI don’t think that was fun.↩︎\nThis is morally but not actually\ntrue. There is a variant (slower in practice, faster asymptotically),\nthat costs \\(\\mathcal{O}\\left(\\operatorname{nnz}(A)\\alpha(\\operatorname{nnz}(A),\nn)\\right)\\), where \\(\\alpha(m,n)\\) is the inverse Ackerman\nfunction, which is a very slowly growing function that is always equal\nto 4 for our purposes. The actual version that people use is technically\n\\(\\mathcal{O}(\\operatorname{nnz}(A) \\log\nn)\\), but is faster and the \\(\\log\nn\\) is never seen in practice.↩︎\nThis is beyond the scope, but\nbasically it’s trying to find groups of nodes that can be eliminated as\na block using dense matrix operations. This leads to a much more\nefficient algorithm.↩︎\nThere is, of course, a typo in the\nalgorithm we’re about to implement. We’re using the correct version from\nhere.↩︎\nfrom parent to child (aka in\ndescending node order)↩︎\nby construction↩︎\nIf there are no non-zeros below the\ndiagonal, then we have a root of one of the trees in the forest!↩︎\nI did not make it prettier because\na) I think it’s useful to show bad code sometimes, and b) I can’t be\narsed. The real file has some comments in it because I am not a monster,\nbut in some sense this whole damn blog is a code comment.↩︎\nThe George, Liu, Ng book does that\nin FORTRAN. Enjoy decoding it.↩︎\nWell, there is some avoiding this.\nIf the amount of fill in is small, it may be more efficient to do\ninsertions instead. But again, I am not going to bother. And anyway. If\nA_x is a JAX array, it’s going to be immutable and we are\nnot going to be able to avoid the deep copy.↩︎\nand in the deep copy code↩︎\nThis is the discretisation of a 2D\nlaplacian on a square with some specific boundary conditions↩︎\nCholmod, which is the natural\nchoice, is GPL’d, which basically means it can’t be used in something\nlike Scipy. R does not have this problem.↩︎\nBjörk voice↩︎\n",
    "preview": "posts/2022-03-23-getting-jax-to-love-sparse-matrices/getting-jax-to-love-sparse-matrices_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2022-06-24T14:34:49+10:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-03-22-a-linear-mixed-effects-model/",
    "title": "Sparse Matrices 1: The linear algebra of linear mixed effects models and their generalisations",
    "description": "Hubris. Just hubris. But before the fall comes the statement of purpose. This is that statement.",
    "author": [
      {
        "name": "Dan Simpson",
        "url": "https://dpsimpson.github.io"
      }
    ],
    "date": "2022-03-22",
    "categories": [],
    "contents": "\nBack in the early days of the pandemic I though “I’ll have a pandemic\nproject”. I never did my pandemic project.\nBut I did think briefly about what it would be. I want to get the\ntypes of models I like to use in everyday life efficiently implemented\ninside Stan. These models encapsulate (generalised) linear mixed\nmodels1, (generalised) additive models,\nMarkovian spatial models2, and other models. A good\ndescription of the types of models I’m talking about can be found here.\nMany of these models can be solved efficiently via INLA3, a\ngreat R package for fast posterior inference for an extremely useful set\nof Bayesian models. In focussing on a particular class of Bayesian\nmodels, INLA leverages a bunch of structural features to make a very\nvery fast and accurate posterior approximation. I love this stuff. It’s\nwhere I started my stats career.\nNone of the popular MCMC packages really implement the lessons learnt\nfrom INLA to help speed up their inference. I want to change that.\nThe closest we’ve gotten so far is the nice work Charles Margossian has\nbeen doing to get Laplace approximations into Stan.\nBut I want to focus on the other key tool in INLA: using sparse\nlinear algebra to make things fast and scalable.\nI usually work with Stan, but the scale of the C++ coding4 required to even tell if these ideas\nare useful in Stan was honestly just too intimidating.\nBut the other day I remembered Python. Now I am a shit Python\nprogrammer5 and I’m not fully convinced I ever\nachieved object permanence. So it took me a while to remember it\nexisted. But eventually I realised that I could probably make a decent\nprototype6 of this idea using some modern\nPython tools (specifically JAX). I checked with some PyMC devs and they\npointed me at what the appropriate bindings would look like.\nSo I decided to go for it.\nOf course, I’m pretty busy and these sort of projects have a way of\ndying in the arse. So I’m motivating myself by blogging it. I do not\nknow if these ideas will work7. I do not know if my\ncoding skills are up to it8. I do not know if I will\nlose interest. But it should be fun to find out.\nSo today I’m going to do the easiest part: I’m going to scope out the\nproject. Read on, MacDuff.\nA generalised\nlinear mixed effects-ish model\nIf you were to open the correct textbook, or the Bates, Mächler,\nBoler, and Walker 2015 masterpiece paper that describes the workings\nof lme4, you will see the linear mixed model written as\n\\[\ny = X\\beta + Zb + \\epsilon,\n\\] where\nthe columns of \\(X\\) contain the\ncovariates9,\n\\(\\beta\\) is a vector of unknown\nregression coefficients,\n\\(Z\\) is a known matrix that\ndescribes the random effects (basically which observation is linked to\nwhich random effect),\n\\(b \\sim N(0, \\Sigma_b)\\) is the\nvector of random effects with some unknown covariance matrix \\(\\Sigma_b\\),\nand \\(\\epsilon \\sim N(0 ,\\sigma^2\nW)\\) is the observation noise (here \\(W\\) is a known diagonal matrix10).\nBut unlike Doug Bates and his friends, my aim is to do Bayesian\ncomputation. In this situation, \\(\\beta\\) also has a prior on it! In\nfact, I’m going to put a Gaussian prior \\(\\beta \\sim N(0, R)\\) on it, for some\ntypically known11 matrix \\(R\\).\nThis means that I can treat \\(\\beta\\) and \\(b\\) the same12\nway! And I’m going to do just that. I’m going to put them together into\na vector \\(u = (\\beta^T, b^T)^T\\).\nBecause the prior on \\(u\\) is\nGaussian13, I’m sometimes going to call \\(u\\) the Gaussian component or even\nthe latent14 Gaussian component.\nNow that I’ve smooshed my fixed and random effects together, I don’t\nreally need to keep \\(X\\) and \\(Z\\) separate. So I’m going push them\ntogether into a rectangular matrix \\[\nA = [X \\vdots Z].\n\\]\nThis allows us to re-write the model as \\[\\begin{align*}\ny \\mid u, \\sigma & \\sim N(A u, \\sigma^2 W)\\\\\nu \\mid \\theta &\\sim N(0, Q(\\theta)^{-1}).\n\\end{align*}\\]\nWhat the hell is \\(Q(\\theta)\\)\nand why are we suddenly parameterising a multivariate normal\ndistribution by the inverse of its covariance matrix (which, if you’re\ncurious, is known as a precision matrix)???\nI will take your questions in reverse order.\nWe are parameterising by the precision15\nmatrix because it will simplify our formulas and lead to faster\ncomputations. This will be a major topic for us later!\nAs to what \\(Q(\\theta)\\) is, it is\nthe matrix \\[\nQ(\\theta) = \\begin{pmatrix} \\Sigma_b^{-1} & 0 \\\\ 0 &\nR^{-1}\\end{pmatrix}\n\\] and \\(\\theta = (\\sigma,\n\\Sigma_b)\\) is the collection of all16\nnon-Gaussian parameters in the model. Later, we will assume17 that \\(\\Sigma_b\\) has quite a lot of\nstructure.\nThis is a very generic model. It happily contains things\nlike\nLinear regression!\nLinear regression with horseshoe priors!\nLinear mixed effects models!\nLinear regression with splines (smoothing or basis)!\nSpatial models like ICARs, BYMs, etc etc\netc\nGaussian processes (with the caveat that we’re mostly focussing on\nthose that can be formulated via precision matrices rather than\ncovariance matrices. A\nwhole blog post, I have.)\nAny combination of these things!\nSo if I manage to get this implemented efficiently, all of these\nmodels will become efficient too. All it will cost is a truly\nshithouse18 interface.\nThe only downside of this degree of flexibility compared to just\nimplementing a straight linear mixed model with \\(X\\) and \\(Z\\) and \\(\\beta\\) and \\(b\\) all living separately is that there are\na couple of tricks19 to improve numerical stability that\nwe can’t use.\nLet’s get the posterior!\nThe nice thing about thing about this model is that it is a normal\nlikelihood with a normal prior, so we can directly compute two key\nquantities:\nThe “full conditional” distribution \\(p(u \\mid y, \\theta)\\), which is useful for\ngetting posterior information about \\(b\\) and \\(\\beta\\), and\nThe marginal posterior \\(p(\\theta \\mid\ny)\\).\nThis means that we do not need to do MCMC on the joint space \\((u, \\theta)\\)! We can instead write a model\nto draw samples from \\(p(\\theta \\mid\ny)\\), which is much lower-dimensional and easier20\nto sample from, and then compute the joint posterior by sampling from\nthe full conditional.\nI talked a little about the mechanics of this in a previous\nblog post about conjugate priors, but let’s do the derivations. Why?\nBecause they’re not too hard and it’s useful to have them written out\nsomewhere.\nThe full conditional\nFirst we need to compute \\(p(u \\mid y ,\n\\theta)\\). The first thing that we note is that conditional\ndistributions are always proportional to the joint distribution (we’re\nliterally just pretending some things are constant), so we get \\[\\begin{align*}\np(u \\mid y , \\theta) &\\propto p(y \\mid u, \\theta) p(u \\mid \\theta)\np(\\theta) \\\\\n&\\propto \\exp\\left[-\\frac{1}{2\\sigma^2} (y -\nAu)^TW^{-1}(y-Au)\\right]\\exp\\left[-\\frac{1}{2}u^TQ(\\theta)u\\right].\n\\end{align*}\\]\nNow we just need to expand things out and work out what the mean and\nthe precision matrix of \\(p(u \\mid y, \\theta\n)\\) (which is Gaussian by conjugacy!) are.\nComputing posterior distributions by hand is a dying21\nart. So my best and only advice to you: don’t be a hero. Just pattern\nmatch like the rest of us. To do this, we need to know what the density\nof a multivarite normal distribution looks like deep down in\nits soul.\nBehold: the ugly div box!22\n\nIf \\(u \\sim N(m, P^{-1})\\), then\n\\[\\begin{align*}\np(u) &\\propto \\exp\\left[- \\frac{1}{2}(u - m)^TP(u-m)\\right] \\\\\n&\\propto \\exp\\left[- \\frac{1}{2}u^TPu + m^TPu\\right],\n\\end{align*}\\] where I just dropped all of the terms that didn’t\ninvolve \\(u\\).\n\nThis means the plan is to\nExpand out the quadratics in the exponential term so we get\nsomething that looks like \\(\\exp\\left[-\\frac{1}{2}u^TPu +\nz^Tu\\right]\\)\nThe matrix \\(P\\) will be the\nprecision matrix of \\(u \\mid y,\n\\theta\\).\nThe mean of \\(\\mu \\mid y, \\theta\\)\nis \\(P^{-1}z\\).\nSo let’s do it!\n\\[\\begin{align*}\np(u \\mid y , \\theta) &\\propto \\exp\\left[-\\frac{1}{2\\sigma^2}\nu^TA^TW^{-1}Au +\n\\frac{1}{\\sigma^2}(A^TW^{-1}y)^Tu\\right]\\exp\\left[-\\frac{1}{2}u^TQ(\\theta)u\\right]\n\\\\\n&\\propto \\exp\\left[-\\frac{1}{2}u^T\\left(Q +\n\\frac{1}{\\sigma^2}A^TW^{-1}A\\right)u\n+  \\frac{1}{\\sigma^2}(A^TW^{-1}y)^Tu\\right].\n\\end{align*}\\]\nThis means that \\(p(u \\mid y\n,\\theta)\\) is multivariate normal with\nprecision matrix \\(Q_{u\\mid\ny,\\theta}(\\theta) = \\left(Q(\\theta) +\n\\frac{1}{\\sigma^2}A^TW^{-1}A\\right)\\) and\nmean23 \\(\\mu_{u\\mid y,\\theta}(\\theta) = \\frac{1}{\\sigma^2}\nQ_{u\\mid y,\\theta}(\\theta)^{-1} A^TW^{-1}y\\).\nThis means if I build an MCMC scheme to give me \\(B\\) samples \\(\\theta_b \\sim p(\\theta \\mid y)\\), \\(b = 1, \\ldots, B\\), then I can turn them\ninto \\(B\\) samples \\((\\theta_b, u_b)\\) from \\(p(\\theta, u \\mid y)\\) by doing the\nfollowing.\n\nFor \\(b = 1, \\ldots, B\\)\nSimulate \\(u_b \\sim N\\left(\\mu_{u\\mid\ny,\\theta}(\\theta_b), Q_{u\\mid\ny,\\theta}(\\theta_b)^{-1}\\right)\\)\nStore the pair \\((\\theta_b,\nu_b)\\)\n\nEasy24 as!\nWriting down \\(p(\\theta \\mid y)\\)\nSo now we just25 have to get the marginal posterior\nfor the non-Gaussian parameters \\(\\theta\\). We only need it up to a constant\nof proportionality, so we can express the joint probability \\(p(y, u, \\theta)\\) in two equivalent ways to\nget \\[\\begin{align*}\np(y, u , \\theta) &= p(y, u, \\theta) \\\\\np(u \\mid \\theta, y) p(\\theta \\mid y) p(y) &= p(y \\mid u, \\theta) p(u\n\\mid \\theta)p(\\theta). \\\\\n\\end{align*}\\]\nRearranging, we get \\[\\begin{align*}\np(\\theta \\mid y) &= \\frac{p(y \\mid u, \\theta) p(u \\mid\n\\theta)p(\\theta)}{p(u \\mid \\theta, y)p(y)} \\\\\n&\\propto \\frac{p(y \\mid u, \\theta) p(u \\mid \\theta)p(\\theta)}{p(u\n\\mid \\theta, y)}.\n\\end{align*}\\]\nThis is a very nice relationship between the functional forms of the\nvarious densities we happen to know and the density we are trying to\ncompute. This means that if you have access to the full conditional\ndistribution26 for \\(u\\) you can marginalise \\(u\\) out. No weird integrals required.\nBut there’s one oddity: there is a \\(u\\) on the right hand side, but no \\(u\\) on the left hand side. What we have\nactually found is a whole continuum of functions that are proportional\nto \\(p(\\theta \\mid y)\\). It truly does\nnot matter which one we choose.\nBut some choices make the algebra slightly nicer. (And remember, I’m\ngonna have to implement this later, so I should probably keep and eye on\nthat.)\nA good27 generic choice is \\(u = \\mu_{u\\mid y, \\theta}(\\theta)\\).\nThe algebra here can be a bit tricky28,\nso let’s write out each function evaluated at \\(u = \\mu_{u\\mid y, \\theta}(\\theta)\\).\nThe bit from the likelihood is \\[\\begin{align*}\np(y \\mid u = \\mu_{u\\mid y, \\theta}(\\theta), \\theta) &\\propto\n\\sigma^{-n} \\exp\\left[-\\frac{1}{2\\sigma^2}(y - A\\mu_{u\\mid y,\n\\theta}(\\theta))^TW^{-1}(y-  A\\mu_{u\\mid y, \\theta}(\\theta))\\right]\\\\\n&\\propto \\sigma^{-n}\\exp\\left[\\frac{-1}{2\\sigma^2} \\mu_{u\\mid y,\n\\theta}(\\theta)^TA^TW^{-1}A\\mu_{u\\mid y, \\theta}(\\theta) +\n\\frac{1}{\\sigma^2} y^T W^{-1}A \\mu_{u\\mid y, \\theta}(\\theta)\\right],\n\\end{align*}\\] where \\(n\\) is\nthe length of \\(y\\).\nThe bit from the prior on \\(u\\) is\n\\[\\begin{align*}\np(\\mu_{u\\mid y, \\theta}(\\theta) \\mid \\theta )\n\\propto |Q(\\theta)|^{1/2}\\exp\\left[-\\frac{1}{2} \\mu_{u\\mid y,\n\\theta}(\\theta)^TQ(\\theta)\\mu_{u\\mid y, \\theta}(\\theta)\\right].\n\\end{align*}\\]\nFinally, we get that the denominator is \\[\np(\\mu_{u\\mid y, \\theta}(\\theta) \\mid y, \\theta) \\propto |Q_{u\\mid y,\n\\theta}(\\theta)|^{1/2}\n\\] as the exponential term29 cancels!\nOk. Let’s finish this. (Incidentally, if you’re wondering why\nBayesians love MCMC, this is why.)\n\\[\\begin{align*}\np(\\theta \\mid y) &\\propto p(\\theta) \\frac{|Q(\\theta)|}{|Q_{u\\mid y,\n\\theta}(\\theta)|} \\exp\\left[-\\frac{1}{2} \\mu_{u\\mid y,\n\\theta}(\\theta)^T(Q(\\theta) + \\frac{1}{\\sigma^2}A^TW^{-1}A)\\mu_{u\\mid y,\n\\theta}(\\theta) + \\frac{1}{\\sigma^2} y^T W^{-1}A \\mu_{u\\mid y,\n\\theta}(\\theta)\\right] \\\\\n&=  p(\\theta) \\frac{|Q(\\theta)|}{|Q_{u\\mid y, \\theta}(\\theta)|}\n\\exp\\left[-\\frac{1}{2} \\mu_{u\\mid y, \\theta}(\\theta)^TQ_{u\\mid y,\n\\theta}(\\theta)\\mu_{u\\mid y, \\theta}(\\theta) + \\frac{1}{\\sigma^2} y^T\nW^{-1}A \\mu_{u\\mid y, \\theta}(\\theta)\\right].\n\\end{align*}\\]\nWe can now use the fact that \\(Q_{u\\mid y,\n\\theta}(\\theta)\\mu_{u\\mid y, \\theta}(\\theta) = A^TW^{-1}y\\) to\nget\n\\[\\begin{align*}\np(\\theta \\mid y) &\\propto p(\\theta) \\frac{|Q(\\theta)|}{|Q_{u\\mid y,\n\\theta}(\\theta)|} \\exp\\left[-\\frac{1}{2} \\mu_{u\\mid y,\n\\theta}(\\theta)^TA^TW^{-1}y + \\frac{1}{\\sigma^2} y^T W^{-1}A \\mu_{u\\mid\ny, \\theta}(\\theta)\\right] \\\\\n&=\\frac{|Q(\\theta)|}{|Q_{u\\mid y, \\theta}(\\theta)|}\n\\exp\\left[\\frac{1}{2} \\mu_{u\\mid y, \\theta}(\\theta)^TA^TW^{-1}y \\right]\n.\n\\end{align*}\\]\nFor those who just love a log-density, this is \\[\n\\log(p(\\theta \\mid y)) = \\frac{1}{2} \\mu_{u\\mid y,\n\\theta}(\\theta)^TA^TW^{-1}y +\\frac{1}{2} \\log(|Q(\\theta)|) -\n\\frac{1}{2}\\log(|Q_{u\\mid y, \\theta}(\\theta)|).\n\\] A fairly simple expression30 for all of that\nwork.\nSo why isn’t this just\na Gaussian process?\nThese days, people31 are more than passingly familiar32 with Gaussian processes. And so\nthey’re quite possibly wondering why this isn’t all just an extremely\ninconvenient way to do the exact same computations you do with a GP.\nLet me tell you. It is all about \\(Q(\\theta)\\) and \\(A\\).\nThe prior precision matrix \\(Q(\\theta)\\) is typically block diagonal.\nThis special structure makes it pretty easy to compute the \\(|Q(\\theta)|\\) term33.\nBut, of course, there’s more going on here.\nIn linear mixed effects models, these blocks on the diagonal matrix\nare typically fairly small (their size is controlled by the number of\nlevels in the variable you’re stratifying by). Moreover, the matrices on\nthe diagonal of \\(Q(\\theta)\\) are the\ninverses of either diagonal or block diagonal matrices that themselves\nhave quite small blocks34.\nIn models that have more structured random effects35,\nthe diagonal blocks of \\(Q(\\theta)\\)\ncan get quite large36. Moreover, the matrices on these\nblocks are usually not block diagonal.\nThankfully, these prior precision matrices do have something going\nfor them: most of their entries are zero. We refer to these types of\nmatrices as sparse matrices. There are some marvelous\nalgorithms for factorising sparse matrices that are usually a lot more\nefficient37 than algorithms for dense\nmatrices.\nMoreover, the formulation here decouples the dimension of the latent\nGaussian component from the number of observations. The data only enters\nthe posterior through the reduction \\(A^Ty\\), so if the number of observations is\nmuch larger than the number of latent variables38\nand \\(A\\) is sparse39,\nthe operation scales linearly in the number of observations\n(and obviously superlinearly40 in the row-dimension\nof \\(A\\)).\nSo the prior precision41 is a sparse matrix.\nWhat about the precision matrix of \\([u \\mid\ny, \\theta]\\)?\nIt is also sparse! Recall that \\(A = [Z\n\\vdots X]\\). This means that \\[\n\\frac{1}{\\sigma^2}A^TW^{-1}A = \\frac{1}{\\sigma^2}\\begin{pmatrix} Z^T\nW^{-1}Z & Z^T W^{-1}X \\\\ X^T W^{-1} Z & X^TW^{-1}X\n\\end{pmatrix}.\n\\] \\(Z\\) is a matrix that links\nthe stacked vector of random effects \\(b\\) to each observation. Typically, the\nlikelihood \\(p(y_i \\mid \\theta)\\) will\nonly depend on a small number of entries of \\(b\\), which suggests that most elements in\neach row of \\(Z\\) will be zero. This,\nin turn, implies that \\(Z\\) is sparse\nand so is42 \\(Z^TW^{-1}Z\\).\nOn the other hand, the other three blocks are usually43\nfully dense. Thankfully, though, the usual situation is that \\(b\\) has far more elements that\n\\(\\beta\\), which means that \\(A^TW^{-1}A\\) is still sparse and we can\nstill use our special algorithms44\nAll of this suggests that, under usual operating conditions, \\(Q_{u\\mid y, \\theta}\\) is also a\nsparse matrix.\nAnd that’s great because that means that we can compute the\nlog-posterior using only 3 main operations:\nComputing \\(\\log(|Q(\\theta)|)\\).\nThis matrix is block diagonal so you can just multiply together the\ndeterminants45 of the diagonal blocks, which are\nrelatively cheap to compute.\nComputing \\(\\mu_{u \\mid y,\n\\theta}(\\theta)\\). This requires solving the sparse linear system\n\\(Q_{u \\mid y, \\theta} \\mu_{u \\mid y, \\theta}\n= \\frac{1}{\\sigma^2}A^TW^{-1}y\\). This is going to rely on some\nfancy pants sparse matrix algorithm.\nComputing \\(\\log(|Q_{u \\mid y,\n\\theta}(\\theta)|)\\). This is, thankfully, a by-product of the\nthings we need to compute to solve the linear system in the previous\ntask.\nWhat\nI? What I? What I gotta do? What I gotta do to\nget this model in PyMC?\nSo this is where shit gets real.\nEssentially, I want to implement a new distribution in PyMC that will\ntake approprite inputs and output the log-density and its gradient.\nThere are two ways to do this:\nPanic\nPray\nFor the first option, you write a C++46\nbackend and register it as an Aesara node. This is how, for example,\ndifferential equation solvers migrated into PyMC.\nFor the second option, which is going to be our goal, we light our\nSinead O’Connor votive candle and program up the model using JAX. JAX is\na glorious feat of engineering that makes compilable and autodiff-able\nPython code. In a lot of cases, it seamlessly lets you shift from CPUs\nto GPUs and is all around quite cool.\nIt also has approximately zero useful sparse matrix support. (It will\nlet you do very basic things47\nbut nothing as complicated as we are going to need.)\nSo why am I taking this route? Well firstly I’m curious to see how\nwell it works. So I am going to write JAX code to do all of my sparse\nmatrix operations and see how efficiently it autodiffs it.\nNow I’m going to pre-register my expectations. I expect it to be a\nlittle bit shit. Or, at least, I expect to be able to make it do\nbetter.\nThe problem is that computing a gradient requires a single\nreverse-mode48 autodiff sweep. This does not seem\nlike a problem until you look at how this sort of thing needs to be\nimplemented and you realise that every gradient call is going to need to\ngenerate and store the entire damn autodiff tree for the\nlog-density evaluation. And that autodiff tree is going to be\nlarge. So I am expecting the memory scaling on this to be truly\nshite.\nThankfully there are two ways to fix this. One of them is to\nimplement a custom Jacobian-vector product49\nand register it with JAX so it knows most of how to do the\nderivative. The other way is to implement this shit in C++ and register\nit as a JAX primitive. And to be honest I’m very tempted. But that is\nnot where I am starting.\nThe other problem is going to be exposing this to users. The internal\ninterface is going to be an absolute shit to use. So we are gonna have\nto get our Def Leppard on and sprinkle some syntactical sugar all over\nit.\nI’m honestly less concerned about this challenge. It’s important but\nI am not expecting to produce anything good enough to put into PyMC (or\nany other package). But I do think it’s a good idea to keep this sort of\nquestion in mind: it can help you make cleaner, more useful code.\nWhat comes next?\nWell you will not get a solution today. This blog post is more than\nlong enough.\nMy plan is to do three things.\nImplement the relevant sparse matrix solver in a JAX-able form.\n(This is mostly gonna be me trying to remember how to do something I\nhaven’t done in a very long time.)\nBind50 the (probably) inefficient version\ninto PyMC to see how that process works.\nTry the custom jvp and vjp interfaces\nin JAX to see if they speed things up relative to just autodiffing\nthrough my for loops.\n(Maybe) Look into whether hand-rolling some C++ is worth the\neffort.\nWill I get all of this done? I mean, I’m skeptical. But hey. If I do\nit’ll be nice.\n\naka linear multilevel models↩︎\nPopular in epidemiology↩︎\nINLA = Laplace approximations +\nsparse linear algebra to do fast, fairly scalable, and accurate Bayesian\ninference on a variety of Bayesian models. It’s particularly good at\nthings like spatial models.↩︎\nIn its guts, Stan is a fully\ntemplated C++ autodiff library, so I would need to add specific sparse\nmatrix support. And then there’s be some truly gross stuff with the Stan\nlanguage and its existing types. And so on and so on and honestly it\njust broke my damn brain. So I started a few times but never finished.↩︎\nI just don’t ever use it. I\nsemi-regularly read and debug other people’s code, but I don’t typically\nwrite very much myself. I use R because that’s what my job needs me to\nuse. So a shadow aim here is to just put some time into my Python. By\nthe end of this I’ll be like Britney doing I’m a Slave 4 U.↩︎\nOr maybe more, but let’s not be too\nambitious.↩︎\nI’m pretty sure they will.↩︎\nMy sparse matrix data structures are\nrusty as fuck.↩︎\nand the intercept if it’s needed↩︎\nReally this costs me nothing and can\nbe useful with multiple observations.↩︎\nDefault options include the identity\nmatrix or some multiple of the identity matrix.↩︎\nREML heads don’t dismay. You can do\nall kinds of weird shit by choosing some of these matrices in certain\nways. I’m not gonna stop you. I love and support you. Good vibes only.↩︎\nThe priors on \\(\\beta\\) and \\(b\\) are independent Gaussian so it has to\nbe.↩︎\nhomosexual↩︎\nInverse correlation matrix↩︎\nexcluding the fixed ones, like \\(W\\) and \\(A\\) and \\(R\\). ↩︎\nSuch a dirty word. For all of the\nmodels we care about, this is block diagonal. So this assumption is our\nrestriction to a specific class of models.↩︎\nI would suggest a lot of syntactic\nsugar if you were ever going to expose this stuff to users.↩︎\nSee the Bates et al. paper.\nTheir formulation is fabulous but doesn’t extend nicely to the\nsituations I care about! Basically they optimise for the situation where\n\\(\\Sigma_b\\) can be singular, which is\nan issue when you’re doing optimisation. But I’m not doing optimisation\nand I care about the case where the precision matrix is defined as a\nsingular matrix (and therefore \\(\\Sigma_b\\) does not exist. This seems like\na truly wild idea, but it occurs quite naturally in many important\nmodels like smoothing splines and ICAR models (which are extremely\npopular in spatial epidemiology).↩︎\nIt’s easier in two ways. Firstly,\nMCMC likes lower-dimensional targets. They are typically easier to\nsample from! Secondly, the posterior geometry of \\(p(\\theta \\mid y)\\) is usually pretty\nsimple, while the joint posterior \\(p(\\theta,\nu \\mid y)\\) has an annoying tendency to have a funnel in it,\nwhich forces us to do all kinds of annoying reparameterisation tricks to\nstop the sampler from shitting the bed.↩︎\nComputers!↩︎\nCSS is my passion.↩︎\nIt’s possible to rearrange things to\nlose that \\(\\frac{1}{\\sigma^2}\\), which\nI admit looks a bit weird. It cancels out down the line.↩︎\nI have, historically, not had the\ngreatest grip on whether or not things are easy.↩︎\nSee previous footnote.↩︎\nOr a good approximation to it.\nLaplace approximations work very well for this to extend everything\nwe’re doing here from a linear mixed-ish model to a generalised linear\nmixed-ish model.↩︎\nThis is actually a bit dangerous on\nthe face of it because it depends on \\(\\theta\\). You can convince yourself it’s\nok. Choosing \\(u=0\\) is less stress\ninducing, but I wanted to bring out the parallel to using a Laplace\napproximation to \\(p(u \\mid \\theta,\ny)\\), in which case we really want to evaluate the ratio at the\npoint where the approximation is the best (aka the conditional mean).↩︎\nA common mistake is to forget the\nparameter dependent proportionality constants from the normal\ndistribution. You didn’t need them before because you were conditioning\non \\(\\theta\\) so they were all\nconstant. But now \\(\\theta\\) is unknown\nand if we forget them an angel will cry.↩︎\nHonest footnote: This started as\n\\(p(\\mu_{u\\mid y, \\theta}(\\theta) \\mid y,\n\\theta) \\propto 1\\) because I don’t read my own warnings.↩︎\nThe brave or foolish amongst you\nmight want to convince yourselves that this collapses to\nexactly the marginal likelihood we would’ve gotten from\nRasmussen and Williams had we made a sequence of different life choices.\nIn particular if \\(A = I\\) and \\(Q(\\theta) = \\Sigma(\\theta)^{-1}\\).↩︎\nOr, at least, people who have made\nit this far into the post.↩︎\nYou like GPs bro? Give\nme a sequence of increasingly abstract definitions. I’m waiting.↩︎\nMultiply the determinants of the\nmatrices along the diagonal.↩︎\nLook at the Bates et al paper.\nSpecifically section 2.2. lme4 is a really clever thing.↩︎\nexamples: smoothing splines, AR(p)\nmodels, areal spatial models, some\nGaussian processes if you’re careful↩︎\n\\(10^4\\)–\\(10^6\\) is not unheard of↩︎\nA dense matrix factorisation of an\n\\(n\\times n\\) matrix costs \\(\\mathcal{O}(n^3)\\). The same factorisation\nof a sparse matrix can cost as little as \\(\\mathcal{O}(n)\\) if you’re very lucky. More\ntypically it clocks in a \\(\\mathcal{O}(n^{1.5})\\)–\\(\\mathcal{O}(n^{2})\\), which is still a\nsubstantial saving!↩︎\nThis happens for a lot of designs,\nor when a basis spline or a Markovian Gaussian process is being used↩︎\nThis happens a lot, but not always.\nFor instance subset-of-regressors/predictive process-type models have a\ndense \\(A\\). In this case, if \\(A\\) has \\(m\\) rows an \\(n\\) columns, this is an \\(\\mathcal{O}(mn)\\), which is more expensive\nthan a sparse \\(A\\) unless \\(A\\) has roughly \\(m\\) non-zeros per row..↩︎\nbut usually not cubically. See above\nfootnote.↩︎\nIt’s important that we are talking\nabout precision matrices here and not covariance matrices as\nthe inverse of a sparse matrix is typically dense. For instance, an\nAR(1) prior with autocorrelation parameter \\(\\rho\\) has a prior has a sparse precision\nmatrix that looks something like \\[\nQ = \\frac{1}{\\tau^2}\\begin{pmatrix}\n1 & -\\rho &&&&& \\\\\n-\\rho&1 + \\rho^2& -\\rho&&&& \\\\\n&-\\rho& 1 + \\rho^2 &- \\rho&&& \\\\\n&&-\\rho& 1 + \\rho^2&-\\rho&& \\\\\n&&&-\\rho&1+\\rho^2 &-\\rho & \\\\\n&&&&-\\rho&1 + \\rho^2& - \\rho \\\\\n&&&&&-\\rho&1\n\\end{pmatrix}.\n\\] On the other hand, the covariance matrix is fully\ndense \\[\nQ^{-1} = \\tau^2\\begin{pmatrix}\n\\rho&\\rho^2&\\rho^3&\\rho^4&\\rho^5&\\rho^6&\\rho^7\n\\\\\n\\rho^2&\\rho&\\rho^2&\\rho^3&\\rho^4&\\rho^5&\\rho^6\n\\\\\n\\rho^3&\\rho^2&\\rho&\\rho^2&\\rho^3&\\rho^4&\\rho^5\n\\\\\n\\rho^4&\\rho^3&\\rho^2&\\rho&\\rho^2&\\rho^3&\\rho^4\n\\\\\n\\rho^5&\\rho^4&\\rho^3&\\rho^2&\\rho&\\rho^2&\\rho^3\n\\\\\n\\rho^6&\\rho^5&\\rho^4&\\rho^3&\\rho^2&\\rho&\\rho^2\n\\\\\n\\rho^7&\\rho^6&\\rho^5&\\rho^4&\\rho^3&\\rho^2&\\rho\n\\\\\n\\end{pmatrix},\n\\] which is completely dense. This is a generic property: the\ninverse of a sparse matrix is usually dense (it’s dense as long as the\ngraph associated with the sparse matrix has a single connected component\nthere’s a matrix with the same pattern of non-zeros that has a fully\ndense inverse) and the entries satisfy geometric decay\nbounds.↩︎\nRemember: \\(W\\) is diagonal and known.↩︎\nNot if you’re doing some wild dummy\ncoding shit or modelling text, but typically.↩︎\nYou’d think that dense rows and\ncolumns would be a problem but they’re not. A little graph theory and a\nlittle numerical linear algebra says that as long as they are the last\nvariables in the model, the algorithms will still be efficient. That\nsaid, if you want to dig in, it is possible to use supernodal\n(eg CHOLMOD) and multifrontal (eg MUMPS) methods to group the operations\nin such a way that it’s possible to use level-3 BLAS operations. CHOLMOD\neven spins this into a GPU acceleration scheme, which is fucking wild if\nyou think about it: sparse linear algebra rarely has the arithmetic\nintensity or data locality required to make GPUs worthwhile (you spend\nall of your time communicating, which is great in a marriage, terrible\nin a GPU). But some clever load balancing, tree-based magic, and\nmultithreading apparently\nmakes it possible. Like truly, I am blown away by this. \\[\n\\phantom{abcde}\n\\]We are not going to do any of this because absolutely\nfucking not. And anyway. It’s kinda rare to have a huge number of\ncovariates in the sorts of models that use these complex random effects.\n(Or if you do, you better light your Sinead O’Connor votive candle\nbecause honestly you have a lot of problems and you’re gonna need\nhealing.)↩︎\nIf you’ve been reading the\nfootnotes, you’ll recall that sometimes one of these precision matrices\non the diagonal will be singular. Sometimes that’s because you fucked up\nyour programming. But other times it’s because you’re using something\nlike an ICAR (intrinsic conditional autoregressive) prior on one of your\ncomponents. The precision matrix for this model is \\(Q_\\text{ICAR} = \\tau_\\text{ICAR} = \\tau\n\\text{Adj}(\\mathcal{G})\\), where \\(\\operatorname{Adj}(\\mathcal{G})\\) is the\nadjacency matrix of some fixed graph \\(\\mathcal{G}\\) (typically describing\nsomething like which postcodes are next to each other). Some\ntheory suggests that if \\(\\mathcal{G}\\) has \\(d\\) connected components, the zero\ndeterminant should be replaced with \\(\\tau^{(m\n- d)/2}\\), where \\(m\\) is the\nnumber of vertices in \\(\\mathcal{G}\\).↩︎\nI guess there’s nothing really\nstopping you from writing in pure Python except a creeping sense of\ninadequacy.↩︎\neg build a sparse matrix↩︎\nHoney, we do not have time.\nUnderstanding autodiff is not massively important in the grand scheme of\nthis blogpost (or, you know, probably in real life unless you do some\nfairly specific things). I’ll\nlet Charles explain it.↩︎\nOr, a custom vector-Jacobian\nproduct, which is not a symmetrical choice.↩︎\nI bind you Nancy!↩︎\n",
    "preview": {},
    "last_modified": "2022-06-24T14:33:57+10:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-26-barry-gibb-came-fourth-in-a-barry-gibb-look-alike-contest-repost/",
    "title": "Barry Gibb came fourth in a Barry Gibb look alike contest (Repost)",
    "description": "A repost from Andrew's blog about comparing computational methods for performing a task. (Lightly edited.) Original posted 20 October, 2017.",
    "author": [
      {
        "name": "Dan Simpson",
        "url": "https://dpsimpson.github.io"
      }
    ],
    "date": "2022-01-26",
    "categories": [],
    "contents": "\n\nEvery day a little death, in the parlour, in the bed. On the lips and in the eyes. In the curtains in the silver, in the buttons, in the bread, in the murmurs, in the pauses, in the gestures, in the sighs. Sondheim\n\nThe most horrible sound in the world is that of a reviewer asking you to compare your computational method to another, existing method. Like bombing countries in the name of peace, the purity of intent drowns out the voices of our better angels as they whisper: at what cost.\nBefore the unnecessary drama of that last sentence1 sends you running back to the still-open browser tab documenting the world’s slow slide into a deeper, danker, more complete darkness that we’ve seen before, I should say that I understand that for most people this isn’t a problem. Most people don’t do research in computational statistics. Most people are happy2.\nSo why does someone asking for a comparison of two methods for allegedly computing the same thing fill me with the sort of dread usually reserved for climbing down the ladder into my basement to discover, by the the light of a single, swinging, naked light bulb, that the evil clown I keep chained in the corner has escaped? Because it’s almost impossible to do well.\nI go through all this before you wake up so I can feel happier to be safe again with you\nMany many years ago, when I still had all my hair and thought it was impressive when people proved things, I did a PhD in numerical analysis. These all tend to have the same structure:\nsurvey your chosen area with a simulation study comparing all the existing methods,\npropose a new method that should be marginally better than the existing ones,\nanalyse the new method, show that it’s at least not worse than the existing ones (or worse in an interesting way),\nconstruct a simulation study that shows the superiority of your method on a problem that hopefully doesn’t look too artificial,\nwrite a long discussion blaming the inconsistencies between the maths and the simulations on “pre-asymptotic artefacts”.\nWhich is to say, I’ve done my share of simulation studies comparing algorithms.\nSo what changed? When did I start to get the fear every time someone mentioned comparing algorithms?\nWell, I left numerical analysis and moved to statistics and I learnt the one true thing that all people who come to statistics must learn: statistics is hard.\nWhen I used to compare deterministic algorithms it was easy. I would know the correct answer and so I could compare algorithms by comparing the error in their approximate solutions (perhaps taking into account things like how long it took to compute the answer).\nBut in statistics, the truth is random. Or the truth is a high-dimensional joint distribution that you cannot possibly know. So how can you really compare your algorithms, except possibly by comparing your answer to some sort of “gold standard” method that may or may not work.\nInte ner för ett stup. Inte ner från en bro. Utan från vattentornets topp3.\nThe first two statistical things I ever really worked on (in an office overlooking a fjord) were computationally tractable ways of approximating posterior distributions for specific types of models. The first of these was INLA4. For those of you who haven’t heard of it, INLA (and it’s popular R implementation R-INLA) is a method for doing approximate posterior computation for a lot of the sorts of models you can fit in rstanarm and brms. So random effect models, multilevel models, models with splines, and spatial effects.\nAt the time, Stan didn’t exist (later, it barely existed), so I would describe INLA as being Bayesian inference for people who lacked the ideological purity to wait 14 hours for a poorly mixing BUGS chain to run, instead choosing to spend 14 seconds to get a better “approximate” answer. These days, Stan exists in earnest and that 14 hours is 20 minutes for small-ish models with only a couple of thousand observations, and the answer that comes out of Stan is probably as good as INLA.\nWorking on INLA I learnt a new fear: the fear that someone else was going to publish a simulation study comparing INLA with something else without checking with us first.\nNow obviously, we wanted people to run their comparisons past us so we could ruthlessly quash any dissent and hopefully exile the poor soul who thought to critique our perfect method to the academic equivalent of a Siberian work camp.\nOr, more likely, because comparing statistical models is really hard, and we could usually make the comparison much better by asking some questions about how it was being done.\nSometimes, learning from well-constructed simulation studies how INLA was failing lead to improvements in the method.\nBut nothing could be learned if, for instance, the simulation study was reporting runs from code that wasn’t doing what the authors thought it was5. And I don’t want to suggest that bad or unfair comparisons comes from malice (for the most part, we’re all quite conscientious and fairly nice), but rather that they happen because comparing statistical algorithms is hard.\nAnd comparing algorithms fairly where you don’t understand them equally well is almost impossible.\nWell did you hear the one about Mr Ed? He said I’m this way because of the things I’ve seen\nWhy am I bringing this up? It’s because of the second statistical thing that I worked on while I was living in sunny Trondheim (in between looking at the fjord and holding onto the sides of buildings for dear life because for 8 months of the year Trondheim is a very pretty mess of icy hills).\nDuring that time, I worked with Finn Lindgren and Håvard “INLA” Rue on computationally efficient approximations to Gaussian random fields (which is what we’re supposed to call Gaussian Processes when the parameter space is more complex than just “time” [shakes fist at passing cloud]). Finn (with Håvard and Johan Lindström) had proposed a new method, cannily named the Stochastic Partial Differential Equation (SPDE) method, for exploiting the continuous-space Markov property in higher dimensions. Which all sounds very maths-y, but it isn’t.\nThe guts of the method says “all of our problems with working computationally with Gaussian random fields comes from the fact that the set of all possible functions is too big for a computer to deal with, so we should do something about that”.  The “something” is replace the continuous function with a piecewise linear one defined over a fairly fine triangulation on the domain of interest.\nBut why am I talking about this?\n(Sorry. One day I’ll write a short post.)\nA very exciting paper popped up on arXiv on Monday6 comparing a fairly exhaustive collection of recent methods for making spatial Gaussian random fields more computationally efficient.\nWhy am I not cringing in fear? Because if you look at the author list, they have included an author from each of the projects they have compared! This means that the comparison will probably be as good as it can be. In particular, it won’t suffer from the usual problem of the authors understanding some methods they’re comparing better than others.\n#The world is held together by the wind that blows through Gena Rowland’s hair\nSo how did they go? Well, actually, they did quite well. I like that\nThey describe each problem quite well\nThe simulation study and the real data analysis uses a collection of different evaluations metrics\nSome of these are proper scoring rules, which is the correct framework for evaluating probabilistic predictions\nThey acknowledge that the wall clock timings are likely to be more a function of how hard a team worked to optimise performance on this one particular model than a true representation of how these methods would work in practice.\nNot the lovin’ kind\nBut I’m an academic statistician. And our key feature, as a people, is that we loudly and publicly dislike each other’s work. Even the stuff we agree with.  Why? Because people with our skills who also have impulse control tend to work for more money in the private sector.\nSo with that in mind, let’s have some fun.\n(Although seriously, this is the best comparison of this type I’ve ever seen. So, really, I’m just wanting it to be even bester.)\nSo what’s wrong with it?\nIt’s gotta be big. I said it better be big\nThe most obvious problem with the comparison is that the problem that these methods are being compared on is not particularly large or complex. You can see that from the timings. Almost none of these implementations are sweating, which is a sign that we are not anywhere near the sort of problem that would really allow us to differentiate between methods.\nSo how small is small? The problem had 105,569 observations and required prediction at at most  4,431 other locations. To be challenging, this data needed to be another order of magnitude bigger.\nGod knows I know I’ve thrown away those graces\n(Can you tell what I’m listening to?)\nThe second problem with the comparison is that the problem is tooooooo easy. As the data is modelled with a Gaussian observation noise and a multivariate Gaussian latent random effect, it is a straightforward piece of algebra to eliminate all of the latent Gaussian variables from the model. This leads to a model with only a small number of parameters, which should make inference much easier.\nHow do you do that? Well, if the data is \\(y\\), the Gaussian random field is \\(x\\) and and all the hyperparmeters \\(\\theta\\). In this case, we can use conditional probability to write that \\[\np(\\theta \\mid y) \\propto \\frac{p(y,x,\\theta)}{p(x \\mid y, \\theta)},\n\\]\nwhich holds for every value of \\(x\\) and particularly \\(x=0\\). Hence if you have a closed form full conditional (which is the case when you have Gaussian observations), you can write the marginal posterior out exactly without having to do any integration.\nA much more challenging problem would have had Poisson or binomial data, where the full conditional doesn’t have a known form. In this case you cannot do this marginalisation analytically, so you put much more stress on your inference algorithm.\nI guess there’s an argument to be made that some methods are really difficult to extend to non-Gaussian observations. But there’s also an argument to be made that I don’t care. Shit or get off the pot, as American would say.\nDon’t take me back to the range\nThe prediction quality is measured in terms of mean squared error and mean absolute error (which are fine), the continuous rank probability score (CRPS) and and the Interval Score (INT), both of which are proper scoring rules. Proper scoring rules (and follow the link or google for more if you’ve never heard of them) are the correct way to compare probabilistic predictions, regardless of the statistical framework that’s used to make the predictions. So this is an excellent start!\nBut one of these measures does stand out: the prediction interval coverage (CVG) which is defined in the paper as “the percent of intervals containing the true predicted value”. I’m going to parse that as “the percent of prediction intervals containing the true value”. The paper suggests (through use of bold in the tables) that the correct value for CVG is 0.95. That is, the paper suggests the true value should lie within the 95% interval 95% of the time.\nThis is not true.\nOr, at least, this is considerably more complex than the result suggests.\nOr, at least, this is only true if you compute intervals that are specifically built to do this, which is mostly very hard to do. And you definitely don’t do it by providing a standard error (which is an option in this competition).\n#Boys on my left side. Boys on my right side. Boys in the middle. And you’re not here.\nSo what’s wrong with CVG?\nWhy? Well first of all it’s a multiple testing problem. You are not testing the same interval multiple times, you are checking multiple intervals one time each. So it can only be meaningful if the prediction intervals were constructed jointly to solve this specific multiple testing problem.\nSecondly, it’s extremely difficult to know what is considered random here. Coverage statements are statements about repeated tests, so how you repeat them7 will affect whether or not a particular statement is true. It will also affect how you account for the multiple testing when building your prediction intervals. (Really, if anyone did opt to just return standard errors, nothing good is going to happen for them in this criterion!)\nThirdly, it’s already covered by the interval score. If your interval is \\([l,u]\\) with nominal level \\(\\alpha\\), the interval score is for an observation \\(y\\) is \\[\n\\text{INT}_\\alpha(l, u, y) = u - l + \\frac{2}{\\alpha}(l-y) \\mathbf{1}\\{y < l\\} + \\frac{2}{\\alpha}(y-u)\\mathbf{1}\\{y>u\\}.\n\\]\nThis score (where smaller is better) rewards you for having a narrow prediction interval, but penalises you every time the data does not lie in the interval. The score is minimised when \\(\\Pr(y \\in [l,u]) = \\alpha\\). So this really is a good measure of how well the interval estimate is calibrated that also checks more aspects of the interval than CVG (which lacks the first term) does.\nThere’s the part you’ve braced yourself against, and then there’s the other part\nAny conversation about how to evaluate the quality of an interval estimate really only makes sense in the situation where everyone has constructed their intervals the same way. The authors’ code is here, but even without seeing it we know there are essentially four options:\nCompute pointwise prediction means \\(\\hat{\\mu}_i\\) and standard errors \\(\\hat{\\sigma}_i\\) and build the pointwise intervals \\(\\hat{\\mu}_i \\pm 1.96\\hat{\\sigma}\\).\nCompute the pointwise Bayesian prediction intervals, which are formed from the appropriate quantiles (or the HPD region if you are Tony O’Hagan) of \\(\\int \\int p(\\hat{y} \\mid x,\\theta) p(x,\\theta \\mid y)\\,dx d\\theta\\).\nAn interval of the form \\(\\hat{\\mu}_i \\pm c\\hat{\\sigma}\\), where \\(c\\) is chosen to ensure coverage.\nSome sort of clever thing based on functional data analysis.\nBut how well these different options work will depend on how they’re being assessed (or what they’re being used for).\nOption 1: We want to fill in our sparse observation by predicting at more and more points\n(This is known as “in-fill asymptotics”). This type of question occurs when, for instance, we want to fill in the holes in satellite data (which are usually due to clouds).\nThis is the case that most closely resembles the design of the simulation study in this paper. In this case you refine your estimated coverage by computing more prediction intervals and checking if the true value lies within the interval.\nMost of the easy to find results about coverage in these is from the 1D literature (specifically around smoothing splines and non-parametric regression). In these cases, it’s known that the first option is bad, the second option will lead to conservative regions (the coverage will be too high), the third option involves some sophisticated understanding of how Gaussian random fields work, and the fourth is not something I know anything about.\nOption 2: We want to predict at one point, where the field will be monitored multiple times\nThis second option comes up when we’re looking at a long-term monitoring network. This type data is common in environmental science, where a long term network of sensors is set up to monitor, for example, air pollution. The new observations are not independent of the previous ones (there’s usually some sort of temporal structure), but independence can often be assumed if the observations are distant enough in time.\nIn this case as you are repeating observations at a single site, Option 1 will be the right way to construct your interval, option 2 will probably still be a bit broad but might be ok, and options 3 and 4 will probably be too narrow if the underlying process is smooth.\nOption 3: Mixed asymptotics! You do both at once\nSimulation studies are the last refuge of the damned.\nI see the sun go down. I see the sun come up. I see a light beyond the frame.\nSo what are my suggestions for making this comparison better (other than making it bigger, harder, and dumping the weird CVG criterion)?\nrandomise\nrandomise\nrandomise\nWhat do I mean by that? Well in the simulation study, the paper only considered one possible set of data simulated from the correct model. All of the results in their Table 2, which contains the scores, and timings on the simulated data, depends on this particular realisation. And hence Table 2 is a realisation of a random variable that will have a mean and standard deviation.\nThis should not be taken as an endorsement of the frequentist view that the observed data is random and estimators should be evaluated by their average performance over different realisation of the data. This is an acknowledgement of the fact that in this case the data is actually a realisation of a random variable. Reporting the variation in Table 2 would give an idea of the variation in the performance of the method. And would lead to a more nuanced and realistic comparison of the methods. It is not difficult to imagine that for some of these criteria there is no clear winner when averaged over data sets.\nWhere did you get that painter in your pocket?\nI have very mixed feelings about the timings column in the results table. On one hand, an “order of magnitude” estimate of how long this will actually take to fit is probably a useful thing for a person considering using a method. On the other hand, there is just no way for these results not to be misleading. And the paper acknowledges this.\nSimilarly, the competition does not specify things like priors for the Bayesian solutions. This makes it difficult to really compare things like interval estimates, which can strongly depend on the specified priors. You could certainly improve your chances of winning on the CVG computation for the simulation study by choosing your priors carefully!\nWhat is this six-stringed instrument but an adolescent loom?\nI haven’t really talked about the real data performance yet. Part of this is because I don’t think real data is particularly useful for evaluating algorithms. More likely, you’re evaluating your chosen data set as much as, or even more than, you are evaluating your algorithm.\nWhy? Because real data doesn’t follow the model, so even if a particular method gives a terrible approximation to the inference you’d get from the “correct” model, it might do very very well on the particular data set. I’m not sure how you can draw any sort of meaningful conclusion from this type of situation.\nI mean, I should be happy I guess because the method I work on “won” three of the scores, and did fairly well in the other two. But there’s no way to say that wasn’t just luck.\nWhat does luck look like in this context? It could be that the SPDE approximation is a better model for the data than the “correct” Gaussian random field model. It could just be Finn appealing to the old Norse gods. It’s really hard to tell.\nIf any real data is to be used to make general claims about how well algorithms work, I think it’s necessary to use a lot of different data sets rather than just one.\nSimilarly, a range of different simulation study scenarios would give a broader picture of when different approximations behave better.\nDon’t dream it’s over\nOne more kiss before we part: This field is still alive and kicking. One of the really exciting new ideas in the field (that’s probably too new to be in the comparison) is that you can speed up the computation of the unnormalised log-posterior through hierarchical decompositions of the covariance matrix (there is also code). This is a really neat method for solving the problem and a really exciting new idea in the field.\nThere are a bunch of other things that are probably worth looking at in this article, but I’ve run out of energy for the moment. Probably the most interesting thing for me is that a lot of the methods that did well (SPDEs, Predictive Processes, Fixed Rank Kriging, Multi-resolution Approximation, Lattice Krig, Nearest-Neighbour Predictive Processes) are cut from very similar cloth. It would be interesting to look deeper at the similarities and differences in an attempt to explain these results.\n\n2021: Oh my giddy aunt what even was that?!↩︎\n2021: The around that time is notable to me, but not interesting to others. So I’m sorry extent to which these blog posts captured the variations in my mental state about that. But also\nthey give a small glimpse at just how bleak my sense of humour can be.↩︎\nNo I don’t speak Swedish, but one of my favourite songwriters/lyricists does. And sometimes I’m just that unbearable. Also the next part of this story takes place in Norway, which is near Sweden but produces worse music (Susanne Sunfør and M2M being notable exceptions)↩︎\nI once gave a truly mortifying talk called INLA: Past, Present, and Future at a conference in Dublin.↩︎\nOr, as happened one time, they compared computation for a different model with an algorithm that failed its convergence checks and assumed that all of the hyperparameters were fixed. All of that is bad but the last part is like saying lm is faster than lme4::lmer for fitting mixed effects models because we only checked when the almost always unknown variance parameters were assumed known.↩︎\nIn 2017. A long time ago.↩︎\nRepeat the same test or make a new test for different data↩︎\n",
    "preview": {},
    "last_modified": "2022-02-10T23:28:50+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-09-why-wont-you-cheat-with-me-repost/",
    "title": "Why won't you cheat with me? (Repost)",
    "description": "A repost from Andrew's blog about how design information infects multivariate priors.\n(Lightly edited. Well, a bit more than lightly because the last version didn't \nfully make sense. But whatever. Blogs, eh.) Original posted 5 November, 2017.",
    "author": [
      {
        "name": "Dan Simpson",
        "url": "https://dpsimpson.github.io"
      }
    ],
    "date": "2021-12-09",
    "categories": [],
    "contents": "\n\nBut I got some ground rules \nI’ve found to be sound rules\nand you’re not the one I’m exempting.\nNonetheless, I confess it’s tempting.\n– Jenny Toomey sings Franklin Bruno\n\nIt turns out that I did something a little controversial in last week’s1 post. As these things always go, it wasn’t the thing I was expecting to get push back from, but rather what I thought was a fairly innocuous scaling of the prior. One commenter (and a few other people on other communication channels) pointed out that the dependence of the prior on the design didn’t seem kosher. Of course, we (Andrew, Mike and I) wrote a paper that was sort of about this a few months ago2, but it’s one of those really interesting topics that we can probably all deal with thinking more about.\nSo in this post, I’m going to go into a couple of situations where it makes sense to scale the prior based on fixed information about the experiment. (The emerging theme for these posts is “things I think are interesting and useful but are probably not publishable” interspersed with “weird digressions into musical theatre / the personal mythology of Patti LuPone”.)\nIf you haven’t clicked yet, this particular post is going to be drier than Eve Arden in Mildred Pierce. If you’d rather be entertained, I’d recommend Tempting: Jenny Toomey sings the songs of Franklin Bruno. (Franklin Bruno is today’s stand in for Patti, because I’m still sad that War Paint closed3. I only got to see it twice.)\n(Jenny Toomey was one of the most exciting American indie musicians in the 90s both through her bands [Tsunami was the notable one, but there were others] and her work with Simple Machines, the label she co-founded. These days she’s working in musician advocacy and hasn’t released an album since the early 2000s. Bruno’s current band is called The Human Hearts. He has had a long solo career and was also in an excellent powerpop band called Nothing Painted Blue, who had an album called The Monte Carlo Method. And, now4 that I live in Canada, I should say that that album has a fabulous cover of Mark Szabo’s I Should Be With You. To be honest, the only reason I work with Andrew and the Stan crew is that I figure if I’m in New York often enough I’ll eventually coincide with a Human Hearts concert5.)\nSparsity\n\nWhy won’t you cheat with me? You and I both know you’ve done it before. – Jenny Toomey sings Franklin Bruno\n\nThe first object of our affliction are priors that promote sparsity in high-dimensional models. There has been a lot of work on this topic, but the cheaters guide is basically this:\n\nWhile spike-and-slab models can exactly represent sparsity and have excellent theoretical properties, they are basically useless from a computational point of view. So we use scale-mixture of normal priors (also known as local-global priors) to achieve approximate sparsity, and then use some sort of decision rule to take our approximately sparse signal and make it exactly sparse.\n\nWhat is a scale-mixture of normals? Well it has the general form \\[\n\\beta_j \\sim N(0, \\tau^2 \\psi^2_j),\n\\]\nwhere \\(\\tau\\) is a global standard deviation parameter, controlling how large the \\(\\beta_j\\) parameters are in general6, while the local standard deviation parameters \\(\\psi_j\\) control how big \\(\\beta_j\\) is relative to the other \\(\\beta\\)s.\nThe priors for \\(\\tau\\) and the \\(\\psi_j\\) are typically set to be independent. A lot of theoretical work just treats \\(\\tau\\) as fixed (or as otherwise less important than the local parameters), but this is wrong.\nPedant’s corner: Andrew likes define mathematical statisticians as those who use \\(x\\) for their data rather than \\(y\\). I prefer to characterise them by those who think it’s a good idea to put a prior on variance (an un-elicitable quantity) rather than standard deviation (which is easy to have opinions about). Please people just stop doing this. You’re not helping yourselves!\nActually, maybe that last point isn’t for Pedant’s Corner after all. Because if you parameterise by standard deviation it’s pretty easy to work out what the marginal prior on \\(\\beta_j\\) (with \\(\\tau\\) fixed) is.\nThis is quite useful because, with the notable exception of the “Bayesian” “Lasso” which-does-not-work-but-will-never-die-because-it-was-inexplicably-published-in-the leading-stats-journal-by-prominent-statisticians-and-has-the-word-Lasso-in-the-title-even-though-a-back-of-the-envelope-calculation-or-I-don’t-know-a-fairly-straightforward-simulation-by-the-reviewers-should-have-nixed-it (to use its married name), we can’t compute the marginal prior for most scale-mixtures of normals.\nThe following result, which was killed by reviewers at some point during the PC prior papers long review process, but lives forever in the arXiv’d first version, tells you everything you need to know. It’s a picture because frankly I’ve had a glass of wine and I’m not bloody typing it all again7.\n\n\n\nTheorem 1  Let \\(\\pi_d(r)\\) be a prior on the standard deviation of \\(v \\sim {\\mathcal N}(0,r^2)\\). The induced prior\n\\[\n\\pi(v) = \\int_0^\\infty\n\\frac{1}{2\\pi r}\\exp\\left({-\\frac{v^2}{2r^2}}\\right)\\pi_d(r)\\,dr\n\\]\nhas the following properties. Fix \\(\\delta> 0\\).\nIf \\(\\pi_d(r) \\leq Cr^t\\) for all \\(r \\in [0,\\delta]\\) and for\nsome \\(C,t >0\\), then \\(\\pi(v)\\) is finite at\n\\(v=0\\).\nIf \\(\\pi_d(r) \\in (0,\\infty)\\) for every \\(r \\in [0,\\delta]\\), then \\(\\pi(v)\\) has a weak logarithmic spike at zero,\nthat is\n\\[\n\\pi(v) = \\mathcal{O}\\left[\\log\\left(1 +\n                                   \\frac{\\delta^2}{v^2}\\right)\\right], \\qquad v \\rightarrow 0.\n\\]\nIf \\(\\int_0^\\delta \\frac{1}{2\\pi  r}\\exp\\left({-\\frac{v^2}{2r^2}}\\right)\\pi_d(r)\\,dr <  \\infty\\), then\n\\[\n\\pi(v) \\geq\n\\mathcal{O}\\left(v^{-2}\\exp\\left(-\\frac{v^2}{2\\delta^2}\\right)\\right),\n\\qquad |v| \\rightarrow \\infty.\n\\]\nIf \\(\\pi_d(r) {\\leq}({\\geq}) Cr^{-t}\\) for all\n\\(r \\in [0,\\delta]\\) and for some \\(C,t >0\\), then \\[\n\\pi(v)\n{\\leq}({\\geq}) \\mathcal{O}(|v|^{-t}),\\qquad v \\rightarrow 0.\n\\]\nIf \\(\\pi_d(r) {\\leq}({\\geq}) Cr^{-t}\\) for all\n\\(r >\\delta\\) and for some \\(C,t >0\\), then\n\\[\n\\pi(v)\n{\\leq}({\\geq}) \\mathcal{O}(|v|^{-t}),\\qquad |v| \\rightarrow\n\\infty.\n\\]\n\n\n\n\nThe proof is here.\nFor any \\(\\delta > 0\\),\n\\[\n\\pi(v) =\n\\int_0^\\delta\\frac{1}{2\\pi r}\n\\exp\\left({-\\frac{v^2}{2r^2}}\\right)\n\\pi_d(r)\\,dr +\n\\int_\\delta^\\infty\\frac{1}{2\\pi\nr}\\exp\\left({-\\frac{v^2}{2r^2}}\\right)\n\\pi_d(r)\\,dr = I_1 + I_2.\n\\]\nExamining this splitting, we note that \\(I_1\\) will control the\nbehaviour of \\(\\pi(v)\\) near zero, while \\(I_2\\) will control the\ntails.\nAssuming that \\(\\int_\\delta^\\infty r^{-1}\\pi_d(r)\\,dr < \\infty\\), we\ncan bound \\(I_2\\) as\n\\[\n\\frac{1}{2\\pi }\\exp\\left({-\\frac{v^2}{2\\delta^2}}\\right)\n\\int_\\delta^\\infty r^{-1}\\pi_d(r)\\,dr \\leq I_2 \\leq \\frac{1}{2\\pi}\n\\int_\\delta^\\infty r^{-1}\\pi_d(r)\\,dr.\n\\]\nTo prove part 1, let \\(\\pi_d(r) \\leq Cr^t\\), \\(r \\in [0,\\delta]\\) for some \\(t>0\\). Substituting this into \\(I_1\\) and\ncomputing the resulting integral using Maple8, we get\n\\[\\begin{align*}\nI_1 &\\leq - \\frac{C}{2\\pi t}\\left( {2}^{-1/2\\,t}{|v|}^{t}\\Gamma\n\\left( 1-1/2\\,t,1/2\\,{\\frac {v^2}{{\\delta}^{2}}} \\right) -{{\\rm\ne}^{-1/2\\,{\\frac {v^2}{{\\delta}^{2}}}} }{\\delta}^{t}\n\\right) = \\mathcal{O}(1),\n\\end{align*}\\]\nwhere \\(\\Gamma(a,x) = \\int_x^\\infty \\exp\\left({-t}\\right)t^{a-1}\\,dt\\) is the incomplete Gamma\nfunction.\nTo prove parts 2 and 3, we bound \\(I_1\\) as\nfollows.\n\\[\\begin{align*}\n\\left(\\inf_{r\\in[0,\\delta]} \\pi_d(r)\n\\right)\\int_0^\\delta\\frac{1}{2\\pi\nr}\\exp\\left({-\\frac{v^2}{2r^2}}\\right) \\,dr &\\leq I_1 \\leq\n\\left(\\sup_{r\\in[0,\\delta]} \\pi_d(r)\n\\right)\\int_0^\\delta\\frac{1}{2\\pi r}\\exp\\left({-\\frac{v^2}{2r^2}}\\right) \\\\\n\\frac{1}{4\\pi}\\left(\\inf_{r\\in[0,\\delta]} \\pi_d(r)\\right)\n\\text{E}_1\\left(\\frac{v^2}{2\\delta^2}\\right) & \\leq I_1 \\leq\n\\frac{1}{4\\pi}\\left(\\sup_{r\\in[0,\\delta]} \\pi_d(r)\\right)\n\\text{E}_1\\left(\\frac{v^2}{2\\delta^2}\\right) \\\\\n\\frac{1}{8\\pi}\\left(\\inf_{r\\in[0,\\delta]} \\pi_d(r)\\right)\n\\exp\\left({-\\frac{v^2}{2\\delta^2}}\\right)\\log\\left( 1 +\n\\frac{4\\delta^2}{v^2}\\right) &\\leq I_1\n\\leq\\frac{1}{4\\pi}\\left(\\sup_{r\\in[0,\\delta]} \\pi_d(r)\\right)\n\\exp\\left({-\\frac{v^2}{2\\delta^2}}\\right)\\log\\left( 1 +\n\\frac{2\\delta^2}{v^2}\\right),\n\\end{align*}\\]\nwhere \\(\\text{E}_1(x) = \\int_1^\\infty t^{-1}\\exp\\left({-tx}\\right)\\,dt\\) and the\nthird line of inequalities follows using standard bounds in the\nexponential integral9.\nCombining the lower and upper bounds, it follows that if \\(0 <\\inf_{r\\in[0,\\delta]} \\pi_d(r) \\leq \\sup_{r\\in[0,\\delta]} \\pi_d(r) < \\infty\\), then \\(\\pi(v)\\) has a logarithmic spike near\nzero. Similarly, the lower bounds show that \\(\\pi(v) \\geq C v^{-2}\\exp\\left(-\\frac{v^2}{2\\delta^2}\\right)\\) as \\(v\\rightarrow \\infty\\).\nPart 4 follows by considering let \\(\\pi_d(r) = Cr^{-t}\\),\n\\(r \\in [0,\\delta]\\) for some \\(t>0\\). Substituting this into \\(I_1\\)\nand computing the resulting integral using Maple, we get\n\\[\\begin{align*}\nI_1 & = \\frac{C}{2\\pi t}\\left( {|v|}^{-t}\\Gamma \\left(\n1+1/2\\,t,1/2\\,{\\frac {v^2}{{\\delta}^{2}}} \\right)\n{2}^{t/2}-{\\delta}^{-t}{{\\rm e}^{-1/2\\,{\\frac\n{v^2}{{\\delta}^{2}}}}} \\right) \\sim\n\\mathcal{O}(v^{-t})\n\\end{align*}\\]\nas \\(v \\rightarrow 0\\). We note that \\(I_1 = \\mathcal{O}\\left(\\exp\\left(-v^2/(2\\delta^2)\\right)\\right)\\) as \\(|v| \\rightarrow \\infty\\).\nTo prove part 5, let \\(\\pi_d(r) = Cr^{-t}\\), \\(r \\in (\\delta,\\infty)\\) for some \\(t>0\\). Substituting this into \\(I_2\\), we\nget\n\\[\\begin{align*}\nI_2 = \\frac{C}{8\\pi^2}\\,{2}^{1/2\\,t}{|v|}^{-t} \\left( \\Gamma\n\\left( 1/2\\,t \\right) - \\Gamma \\left( 1/2\\,t,1/2\\,{\\frac\n{{v}^{2}}{{\\delta}^{2}}} \\right) \\right) =\n\\mathcal{O}(|v|^{-t}),\n\\end{align*}\\]\nwhere we used the identity \\[\n\\Gamma \\left( 1/2\\,t \\right) - \\Gamma\n\\left( 1/2\\,t,1/2\\,{\\frac {{v}^{2}}{{\\delta}^{2}}} \\right)\n\\rightarrow \\Gamma\\left( 1/2\\,t \\right) \\]\nas \\(|v|\\rightarrow \\infty\\).\nDone.\nAll of this basically says the following:\nIf the density of the prior on the standard deviation is finite at zero, then the implied prior on \\(\\beta_j\\) has a logarithmic spike at zero.\nIf the density of the prior on the standard has a polynomial tail, then the implied prior on \\(\\beta_j\\) has the same polynomial tail.\nNot in the result, but computed at the time: if the prior on the standard deviation is exponential, the prior on \\(\\beta_j\\) still has Gaussian-ish tails. I couldn’t work out what happened in the hinterland between exponential tails and polynomial tails, but I suspect at some point the tail on the standard deviation does eventually get heavy enough to be seen in the marginal, but I can’t tell you when.)\nWith this sort of information, you can compute the equivalent of the bounds that I did on the Laplace prior for the general case (or, actually, for the case that will have at least a little bit of a chance, which is the monotonically decreasing priors on the standard deviation).\nIn particular, if you run the argument from the last post, you see that you need a quite heavy tail on the standard deviation prior to get a reasonable prior on the implied sparsity. In particular, we showed that applying this reasoning to the horseshoe prior, where the prior on the local standard deviation is half-Cauchy, you can see that there\nis a \\(\\lambda\\) that gives a priori weight on \\(p^{-1}\\)-sparse signals, while also letting you have a few very large \\(\\beta_j\\)s.\nThe design-scaling in these priors links directly to an implied decision process\n\nYou’d look better if your shadow didn’t follow you around, but it looks as though you’re tethered to the ground, just like every pound of flesh I’ve ever found. – Franklin Bruno in a sourceless light.\n\nFor a very simple decision process (the deterministic threshold process described in the previous post), you can work out exactly how the threshold needs to interact with the prior. In particular, we can see that if we’re trying to detect a true signal that is exactly zero (no components are active), then we know that \\(latex \\| \\mathbf{X} \\boldsymbol{\\beta} \\| = 0\\). This is not possible for these scale-mixture models, but we can require that in this case all of the components are at most \\(latex \\epsilon\\), in which case \\[\n\\| \\mathbf{X}\\boldsymbol{\\beta} \\| \\leq \\epsilon \\| \\mathbf{X} \\|,\n\\]\nwhich suggests we want \\(\\epsilon \\ll \\| \\mathbf{X} \\|_\\infty^{-1}\\). The calculation in the previous post shows that if we want this sort of almost zero signal to have any mass at all under the prior, we need to scale \\(\\lambda\\) using information about \\(\\mathbf{X}\\).\nOf course, this is a very very simple decision process. I have absolutely no idea how to repeat these arguments for actually good decision processes, like the predictive loss minimization favoured by Aki. But I’d still expect that we’d need to make sure there was a priori enough mass in the areas of the parameter space where the decision process is firmly one way or another (as well as mass in the indeterminate region). I doubt that the Bayesian Lasso would magically start to work under these more complex losses.\nModels specified through their full conditionals\n\nWhy won’t you cheat with me? You and I both know that he’s done the same. – Franklin Bruno\n\nSo we can view the design dependence of sparsity priors as preparation for the forthcoming decision process. (Those of you who just mentally broke into Prepare Ye The Way Of The Lord from Godspell, please come to the front of the class. You are my people.) Now let’s talk about a case where this isn’t true.\nTo do this, we need to cast our minds back to a time when people really did have the original cast recording of Godspell on their mind. In particular, we need to think about Julian Besag (who I’m sure was really into musicals about Jesus. I have no information to the contrary, so I’m just going to assume it’s true.) who wrote a series of important papers, one in 1974 and one in 1975 (and several before and after, but I can’t be arsed linking to them all. We all have google.) about specifying models through conditional independence relations.\nThese models have a special place in time series modelling (where we all know about discrete-time Markovian processes) and in spatial statistics. In particular, generalisations of Besag’s (Gaussian) conditional autoregressive (CAR) models are widely used in spatial epidemiology.\nMathematically, Gaussian CAR models (and more generally Gaussian Markov random fields on graphs) are defined through their precision matrix, that is the inverse of the covariance matrix as\\[\n\\mathbf{x} \\sim N(\\mathbf{0}, \\tau^{-1}\\mathbf{Q}^{-1}).\n\\]\nFor simple models, such as the popular CAR model, we assume \\(\\mathbf{Q}\\) is fixed, known, and sparse (i.e. it has a lot of zeros) and we typically interpret \\(\\tau\\) to be the inverse of the variance of \\(\\mathbf{x}\\).\nThis interpretation of \\(\\tau\\) could not be more wrong.\nWhy? Well, let’s look at the marginal distribution \\[\nx_j \\sim N\\left(0, \\tau^{-1}[Q^{-1}]_{ii}\\right).\n\\]\nTo interpet \\(\\tau\\) and the inverse variance, we need the diagonal elements of \\(\\mathbf{Q}^{-1}\\) to all be around 1. This is never the case.\nA simple, mathematically tractable example is the first order random walk on a one-dimensional lattice, which can be written in terms of the increment process as \\[\nx_{j+1} - x_j \\sim N(0, \\tau^{-1}), \\qquad j = 1, \\ldots J-1.\n\\]\nConditioned on a particular starting point, this process looks a lot like a discrete version of Brownian motion as you move the lattice points closer together. This is a useful model for rough non-linear random effects, such as the baseline hazard rate in a Cox proportional hazard model. A long and detailed (and quite general) discussion of these models can be found in Rue and Held’s book.\nI am bringing this case up because you can actually work out the size of the diagonal of \\(\\mathbf{Q}^{-1}\\). Sørbye and Rue talk about this in detail, but for this model maybe the easiest way to understand it is that if we had a fixed lattice with \\(n\\) points and we’d carefully worked out a sensible prior for \\(\\tau\\). Now imagine that we’ve gotten some new data and instead of only \\(n\\) points in the lattice, we got information at a finer scale, so now the same interval is covered by \\(nk\\) equally spaced nodes. We model this with the new first order random walk prior\\[\nx'_{j+1} - x'_j \\sim N(0,[\\tau']^{-1}).\n\\]\nIt turns out that we can relate the inverse variances of these two increment processes as \\(\\tau' = J \\tau\\).\nThis strongly suggests that we should not use the same prior for \\(\\tau\\) as we should for \\(\\tau'\\), but that the prior should actually know about how many nodes there are on the lattice. Concrete suggestions are in the Sørbye and Rue paper linked above.\nDesign dependence for Markov random fields\n\nNot to coin a phrase, but play it as it lays – Franklin Bruno in Nothing Painted Blue\n\nThis type of design dependence is a general problem for multivariate Gaussian models specified through their precision (so-called Gaussian Markov random fields). The critical thing here is that, unlike the sparsity case, the design dependence does not come from some type of decision process. It comes from the gap between the parameterisation (in terms of \\(\\tau\\) and \\(\\mathbf{Q}\\)) and the elicitable quantity (the scale of the random effect).\nThis is kinda a general lesson. When specifying multivariate priors, you must always check the implications of your prior on the one- and two-dimensional quantities of interest. Because weird things happen in multivariate land!\nGaussian process models\n\nAnd it’s not like we’re tearing down a house of more than gingerbread. It’s not like we’re calling down the wrath of heaven on our heads. – Jenny Toomey sings Franklin Bruno\n\nSo the design dependence doesn’t necessarily come in preparation for some kind of decision, it can also be because we have constructed (and therefore parameterised) our process in an inconvenient way. Let’s see if we can knock out another one before my bottle of wine dies.\nGaussian processes, the least exciting tool in the machine learner’s toolbox, are another example where your priors need to be design dependent. It will probably surprise you not a single sausage that in this case the need for design dependence comes from a completely different place.\nFor simplicity let’s consider a Gaussian process \\(f(t)\\) in one dimension with isotropic covariance function \\[\nc(s,t) =\\sigma^2 (\\kappa|s-t|)^\\nu K_\\nu(|\\kappa|s-t|).\n\\]\nThis is the commonly encountered Whittle-Matérn family of covariance functions. The distinguished members are the exponential covariance function when \\(\\nu = 0.5\\) and the squared exponential function \\[ c(s,t)= \\sigma^2\\exp\\left(\\kappa |s-t|^2 \\right),\n\\]\nwhich is the limit as \\(\\nu \\rightarrow \\infty\\).\nOne of the inconvenient features of Matérn models in 1-3 dimensions is that it is impossible to consistently recover all of the parameters by simply observing more and more of the random effect on a fixed interval. You need to see new replicates in order to properly pin these down10.\nSo one might expect that this non-identifiability would be the source of some problems.\nOne would be wrong.\nThe squared exponential covariance function does not have this pathology, but it’s still very very hard to fit. Why? Well the problem is that you can interpret \\(\\kappa\\) as an inverse-range parameter. Roughly, the interpretation is that if \\[\n|s - t | > \\frac{ \\sqrt{ 8 \\nu } }{\\kappa}\n\\]\nthen the value of \\(u(s)\\) is approximately independent of the value of \\(u(t)\\).\nThis means that a fixed data set provides no information about \\(\\kappa\\) in large parts of the parameter space. In particular if \\(\\kappa^{-1}\\) is bigger than the range of the measurement locations, then the data has almost no information about the parameter.\nSimilarly, if \\(\\kappa^{-1}\\) is smaller than the smallest distance between two data points (or for irregular data, this should be something like “smaller than some low quantile of the set of distances between points”), then the data will have nothing to say about the parameter.\nOf these two scenarios, it turns out that the inference is much less sensitive to the prior on small values of \\(\\kappa\\) (ie ranges longer than the data) than it is on small values of \\(\\kappa\\) (ie ranges shorter than the data).\nCurrently, we have two recommendations: one based around PC priors and a very similar one based around inverse gamma priors. But both of these require you to specify the design-dependent quantity of a “minimum length scale we expect this data set to be informative about”.\nDesign for Gaussian processes (I’d say “Designing Women”, but I’m aware of the demographics)\n\nI’m a disaster, you’re a disaster, we’re a disaster area. – Franklin Bruno in The Human Hearts (featuring alto extraordinaire and cabaret god Ms Molly Pope)\n\nSo in this final example we hit our ultimate goal. A case where design dependent priors are needed not because of a hacky decision process, or an awkward multivariate specification, but due to the limits of the data. In this case, priors that do not recognise the limitation of the design of the experiment will lead to poorly behaving posteriors. This manifests as the Gaussian processes severely over-fitting the data.\nThis is the ultimate expression of the point that we tried to make in the Entropy paper: The prior can often only be understood in the context of the likelihood.\nPrinciples can only get you so far\n\nI’m making scenes, you’re constructing dioramas – Franklin Bruno in Nothing Painted Blue\n\nJust to round this off, I guess I should mention that the strong likelihood principle really does suggest that certain details of the design are not relevant to a fully Bayesian analysis. In particular, if the design only pops up in the normalising constant of the likelihood, it should not be relevant to a Bayesian. This seems at odds with everything I’ve said so far.\nBut it’s not.\nIn each of these cases, the design was only invoked in order to deal with some external information. For sparsity, design was needed to properly infer a sparse signal and came in through the structure of the decision process.\nFor the CAR models, the external information was that the elicitable quantity was the marginal standard deviation, which was a complicated function of the design and the standard parameter.\nFor Gaussian processes, the same thing happened: the implicit decision criterion was that we wanted to make good predictions. The design told us which parts of the parameter space obstructed this goal, and a well specified prior removed the problem.\nThere are also any number of cases in real practice where the decision at hand is stochastically dependent on the data gathering mechanism. This is why things like MRP exist.\nI guess this is the tl;dr version of this post (because apparently I’m too wordy for some people. I suggest they read other things. Of course suggesting this in the final paragraph of such a wordy post is very me.):\nDesign matters even if you’re Bayesian. Especially if you want to do something with your posterior that’s more exciting than just sitting on it.\nEdited from an original blog, posted November 2017.\n\nImagine it’s November 2017.↩︎\nAgain, 2017.↩︎\n2021: I am still sad War Paint closed.↩︎\n2017↩︎\nI eventually did coincide with a Human Hearts concert and, to my extreme joy, Jenny Toomey did two songs with the band! They were supporting Gramercy Arms, who I’d never heard before that night but have several perfect albums.↩︎\nThis is like the standard deviation we’d use in an iid normal prior for a non-sparse model.↩︎\n2021: I did indeed type it all again. And a proof. Because why bother if you’re not going to do it well.↩︎\nYes. No open source for me!↩︎\nAbramowitz, M. and Stegun, I. (1972). Handbook of Mathematical Functions. Formula 5.1.20↩︎\nThere’s a recent paper (2021) in JRSSSB that says that these nuggets are identifiable under infill with a “nugget”, which is equivalent to observing with iid noise that magically stays independent as you observe locations closer and closer together. I will let you judge how relevant this case is to your practice. But regardless, for a finite set of data under any reasonable likelihood, you hit these identifiabiliy problems. And in my personal experience, they persevere even with a decent number of sites.↩︎\n",
    "preview": {},
    "last_modified": "2022-02-10T23:28:24+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-08-the-king-must-die-repost/",
    "title": "The king must die (repost)",
    "description": "A repost (with edits, revisions, and footnotes) from Andrew's blog about how much I hate the Bayesian Lasso. Originally published 2nd November, 2017.",
    "author": [
      {
        "name": "Dan Simpson",
        "url": "https://dpsimpson.github.io"
      }
    ],
    "date": "2021-12-08",
    "categories": [],
    "contents": "\n\nAnd then there was Yodeling Elaine, the Queen of the Air. She had a dollar sign medallion about as big as a dinner plate around her neck and a tiny bubble of spittle around her nostril and a little rusty tear, for she had lassoed and lost another tipsy sailor—Tom Waits\n\nIt turns out I turned thirty two1 and became unbearable. Some of you may feel, with an increasing sense of temporal dissonance, that I was already unbearable2. Others will wonder how I can look so good at my age3. None of that matters to me because all I want to do is talk about the evils of marketing like the 90s were a vaguely good idea4.\nThe thing is, I worry that the real problem in academic statistics in 2017 is not a reproducibility crisis, so much as that so many of our methods just don’t work. And to be honest, I don’t really know what to do about that, other than suggest that we tighten our standards and insist that people proposing new methods, models, and algorithms work harder to sketch out the boundaries of their creations. (What a suggestion. Really. Concrete proposals for concrete change. But it’s a blog. If ever there was a medium to be half-arsed in it’s this one. It’s like twitter for people who aren’t pithy.)\nBerätta för mig om det är sant att din hud är doppad i honung\nSo what is the object of my impotent ire today. Well nothing less storied than the Bayesian Lasso.\nIt should be the least controversial thing in this, the year of our lord two thousand and seventeen, to point out that this method bears no practical resemblance to the Lasso. Or, in the words of Law and Order: SVU, “The [Bayesian Lasso] is fictional and does not depict any actual person or event”.\nWho do you think you are?\nThe Bayesian Lasso is a good example of what’s commonly known as the Lupita Nyong’o fallacy5, which goes something like this: Lupita Nyong’o had a break out role in Twelve Years a Slave, she also had a heavily disguised role in one of ’ the Star Wars films (the specific Star Wars film is not important. I haven’t seen it and I don’t care). Hence Twelve Years a Slave exists in the extended Star Wars universe.6\nThe key point is that the (classical) Lasso plays a small part within the Bayesian Lasso (it’s the MAP estimate) in the same way that Lupita Nyong’o played a small role in that Star Wars film. But just as the presence of Ms Nyong’o does not turn Star Wars into Twelve Years a Slave, the fact that the classical Lasso can be recovered as the MAP estimate of the Bayesian Lasso does not make the Bayesian Lasso useful.\nAnd yet people still ask if they can be fit in Stan. In that case, Andrew answered the question that was asked, which is typically the best way to deal with software enquiries7. But I am brave and was not asked for my opinion, so I’m going to talk about why the Bayesian Lasso doesn’t work.\nHiding all away\nSo why would anyone not know that the Bayesian Lasso doesn’t work? Well, I don’t really know. But I will point out that all of the results that I’ve seen in this directions (not that I’ve been looking hard) have been published in the prestigious but obtuse places like Annals of Statistics, the journal we publish in when we either don’t want people without a graduate degree in mathematical statistics to understand us or when we want to get tenure.\nBy contrast, the original paper is very readable and published in JASA, where we put papers when we are ok with people who do not have a graduate degree in mathematical statistics being able to read them, or when we want to get tenure8.\nTo be fair to Park and Casella, they never really say that the Baysian Lasso should be used for sparsity. Except for one sentence in the introduction where they say the median gives approximately sparse estimators and the title which links it to the most prominent and popular method for estimating a sparse signal. Marketing eh. (See, I’m Canadian now9).\n##The devil has designed my death and is waiting to be sure\nSo what is the Bayesian LASSO (and why did I spend 600 words harping on about something before defining it? The answer will shock you. Actually the answer will not shock you, it’s because it’s kinda hard to do equations on this thing10.)\nFor data observed with Gaussian error, the Bayesian Lasso takes the form \\[\n\\mathbf{y} \\mid \\boldsymbol{\\beta} \\sim N( \\mathbf{X} \\boldsymbol{\\beta}, \\boldsymbol{\\Sigma})\n\\]\nwhere, instead of putting a Normal prior on \\(\\boldsymbol{\\beta}\\) as we would in a bog-standard Bayesian regression, we instead use independent Laplace priors \\[\np(\\beta_i) = \\frac{\\lambda}{2} \\exp(-\\lambda | \\beta_i|).\n\\]\nHere the tuning parameter11 \\(\\lambda = c(p,s_0,\\mathbf{X})\\tilde{\\lambda}\\) where \\(p\\) is the number of covariates, \\(s_0\\) is the number of “true” non-zero elements of \\(\\boldsymbol{\\beta}\\), \\(\\boldsymbol{\\Sigma}\\) is known, and \\(\\tilde{\\lambda}\\) is an unknown scaling parameter that should be \\(\\mathcal{O}(1)\\).\nImportant Side note: This isn’t the exact same model as Park and Castella used as they didn’t use the transformation \\[\n\\lambda = c(p,s_0,\\mathbf{X}) \\tilde{\\lambda}\n\\] but rather just dealt with \\(\\lambda\\) as the parameter. We will see below, and it’s born out by many papers in this field, that the best possible value of \\(\\lambda\\) will depend on this structural/design information\nIf we know how \\(\\lambda\\) varies as the structural/design information changes, it’s a much better idea to put a prior on \\(\\tilde{\\lambda}\\) than on \\(\\lambda\\) directly. Why? Because a prior on \\(\\lambda\\) needs to depend on p, \\(s_0\\), and X and hence needs to be changed for each problem, while a prior on \\(\\tilde{\\lambda}\\) can be used for many problems. One possible option is \\(c(p,s_0,\\mathbf{X}) = 2\\|\\mathbf{X}\\|\\sqrt{\\log p }\\), which is a rate optimal parameter for the (non-Bayesian) Lasso. Later, we’ll do a back-of-the-envelope calculation that suggests we might not need the square root around the logarithmic term.\nWhy do we scale priors\nThe critical idea behind the Bayesian Lasso is that we can use the i.i.d. Laplace priors to express the substantive belief that the most of the \\(\\beta_j\\) are (approximately) zero. The reason for scaling the prior is that the values of \\(\\lambda\\) that are consistent with this belief depend on \\(p\\), \\(s_0\\), and \\(X\\).\nFor example, \\(\\lambda = 1\\), the Bayesian Lasso will not give an approximately sparse signal.\nWhile we could just use a prior for \\(\\lambda\\) that has a very heavy right tail (something like an inverse gamma), this is at odds with a good practice principle of making sure all of thee parameters in your models are properly scaled to make them order 1. Why do we do this? Because it makes it much much easier to set sensible priors.\nSome of you may have noticed that the scaling \\(c(p,s_0,\\mathbf{X})\\) can depend on the unknown sparsity \\(s_0\\). This seems like cheating. People who do asymptotic theory call this sort of value for \\(\\lambda\\) an oracle value, mainly because people studying Bayesian asymptotics are really really into database software.\nThe idea is that this is the value of \\(\\lambda\\) that gives the model the best chance of working. When maths-ing, you work out the properties of the posterior with the oracle value of \\(\\lambda\\) and then you use some sort of smoothness argument to show that the actual method that is being used to select (or average over) the parameter gives almost the same answer.\nIt’s also worth noting that the scaling here doesn’t (directly12) depend on the number of observations, only the number of covariates. This is appropriate: it’s ok for priors to depend on things that should be known a priori (like the number of parameters) or things that can be worked with13 (like the scaling of \\(X\\)). It’s a bit weirder if it depends on the number of observations (that tends to break things like coherent Bayesian updating, while the other dependencies don’t).\nOnly once in Sheboygan. Only once.\nSo what’s wrong with the Bayesian Lasso? Well the short version is that the Laplace prior doesn’t have enough mass near zero relative to the mass in the tails to allow for a posterior that has a lot of entries that are almost zero and some entries that are emphatically not zero.\nBecause the Bayesian Lasso prior does not have a spike at zero, none of the entries will be a priori exactly zero, so we need some sort of rule to separate the “zero” entries from the “non-zero” entries. The way that we’re going to do this is to choose a cutoff \\(\\epsilon\\) where we assume that if \\(|\\beta_j| <\\epsilon\\), then \\(\\beta_j =0\\).\nSo how do we know that the Lasso prior doesn’t put enough mass in important parts of the parameter space? Well there are two ways. I learnt it during the exciting process of writing a paper that the reviewers insisted should have an extended section about sparsity (although this was at best tangential to the rest of the paper), so I suddenly needed to know about Bayesian models of sparsity. So I read those Annals of Stats papers. (That’s why I know I should be scaling \\(\\lambda\\)!).\nWhat are the key references? Well all the knowledge that you seek is here and here.\nBut a much easier way to work out that the Bayesian Lasso is bad is to do some simple maths.\nBecause the \\(\\beta_j\\) are a priori independent, we get a prior on the effective sparsity \\(s_\\epsilon = \\#\\{j : |\\beta_j| > \\epsilon\\}\\) \\[\ns_\\epsilon \\sim \\text{Bin}(p, \\Pr(|\\beta_j| > \\epsilon)).\n\\] For the Bayesian Lasso, that probability can be computed as \\[\n\\Pr ( | \\beta_j | > \\epsilon ) = e^{- \\lambda \\epsilon},\n\\] so \\[\ns_\\epsilon \\sim \\text{Bin}\\left(p, e^{-\\lambda \\epsilon}\\right).\n\\]\nIdeally, the distribution of this effective sparsity would be centred on the true sparsity.\nSo we’d like to choose \\(\\lambda\\) so that \\[\n\\mathbb{E}(s_\\epsilon)= p e^{- \\lambda \\epsilon}= s_0.\n\\]\nA quick re-arrangement suggests that\\[\n\\lambda = \\epsilon^{-1} \\log(p) - \\epsilon^{-1} \\log(s_0).\n\\]\nNow, we are interested in signals with \\(s_0 = o(p)\\), i.e. where only a very small number of the \\(\\beta_j\\) are non-zero. This suggests we can safely ignore the second term as it will be much smaller than the first term.\nTo choose \\(\\epsilon\\), we can work from the general principle that we want to choose it so that the effect of the “almost zero” \\(\\beta_j\\) \\[\n\\sum_{j:|\\beta_j| < \\epsilon} \\beta_j X_{:j}\n\\] is small. (here \\(X_{:j}\\) is the \\(j\\)th column of the matrix \\(X\\).)\nFrom this, it’s pretty clear that \\(\\epsilon\\) is going to have to depend on \\(p\\), \\(s_0\\), and \\(X\\) as well! But how?\nWell, first we note that \\[\n\\sum_{j:|\\beta_j| < \\epsilon} \\beta_j X_{:j} \\leq \\epsilon \\max_{i =1,\\ldots, n}\\sum_{j=1}^p |X_{ij}| = \\epsilon \\|X\\|_\\infty.\n\\] Hence we can make this asymptotically small (as \\(p\\rightarrow \\infty\\)) if \\[\n\\epsilon = o\\left(\\|X\\|_\\infty^{-1}\\right).\n\\] Critically, if we have scaled the design matrix so that each covariate is at most \\(1\\), ie \\[ \n\\max_{i=1,\\ldots,n} |X_{ij}| \\leq 1, \\qquad \\text{for all } j = 1,\\ldots, p,\n\\] then this reduces to the much more warm and fuzzy \\[\n\\epsilon = o\\left(p^{-1}\\right).\n\\]\nThis means that we need to take \\(\\lambda = \\mathcal{O}(p \\log(p))\\) in order to ensure that we have our prior centred on sparse vectors (in the sense that the prior mean for the number of non-zero components is always much less than \\(p\\)).\nShow some emotion\n\n\n\nSo for the Bayesian Lasso, a sensible parameter is \\(\\lambda = p\\log p\\), which will usually have a large number of components less than the threshold \\(\\epsilon\\) and a small number that are larger.\nBut this is still a bad prior.\nTo see this, let’s consider the prior probability of seeing a \\(\\beta_j\\) larger than one\\[\n\\Pr ( | \\beta_j | > 1) = p^{-p} \\downarrow \\downarrow \\downarrow 0.\n\\]\nThis is the problem with the Bayesian Lasso: in order to have a lot of zeros in the signal, you are also forcing the non-zero elements to be very small. A plot of this function is above, and it’s clear that even for very small values of \\(p\\) the probability of seeing a coefficient bigger than one is crushingly small.\nBasically, the Bayesian Lasso can’t give enough mass to both small and large signals simultaneously. Other Bayesian models (such as the horseshoe and the Finnish horseshoe) can support both simultaneously and this type of calculation can show that (although it’s harder. See Theorem 6 here).\n(The scaling that I derived in the previous section is a little different to the standard Lasso scaling of \\(\\lambda = \\mathcal{O} (p \\sqrt{\\log p})\\), but the same result holds: for large \\(p\\) the probability of seeing a large signal is vanishingly small.)\nMaybe I was mean, but I really don’t think so\nThis analysis is all very back of the envelope, but it contains a solid grain of truth14.\nIf you fit a Bayesian Lasso in Stan with an unknown scaling parameter \\(\\lambda\\), you will not see estimates that are all zero, like this analysis suggests. This is because the posterior for \\(\\lambda\\) tries to find the values of the parameters that best fit the data and not the values that give an \\(\\epsilon\\)-sparse signal.\nIn order to fit the data, it is important that the useful covariates have large \\(\\beta\\)s, which, in turn, forces the \\(\\beta\\)s that should be zero to be larger than our dreamt of \\(\\epsilon\\).\nAnd so you see posteriors constructed with the Bayesian Lasso exisiting in some sort of eternal tension: the small \\(\\beta\\)s are too big, and the large \\(\\beta\\)s are typically shrunken towards zero.\nIt’s the sort of compromise that leaves everyone unhappy.\nLet’s close it out with the title song.\n\nAnd I’m so afraid your courtiers\nCannot be called best friends\n\nLightly re-touched from the original, posted on Andrew’s blog. Orignal verison, 2 November, 2017.\n\n(2021 edit) I am no longer 32. I am still unbearable.↩︎\nFair point↩︎\nAnswer: Black Metal↩︎\nThey were not. The concept of authenticity is just another way for the dominant culture to suppress more interesting ones.↩︎\n(2021 edit): Really, Daniel? Really?↩︎\n(2021): Ok. That ended better than I feared.↩︎\nIt’s usually a fool’s game to try to guess why people are asking particular questions. It probably wouldn’t be hard for someone to catalogue the number of times I’ve not followed my advice on this, but in life as in statistics, consistency is really only a concern if everything else is going well.↩︎\n2021: Look at me trying to land a parallel construction.↩︎\n2021: The other week someone asked if I was Canadian, which is a sure sign that my accent is just broken.↩︎\n2021: Prophetic words↩︎\nCould we put a prior on this? Sure. And in practice this is what we should probably do. But for today, we are going to keep it fixed.↩︎\nIt depends on \\(\\|X\\|\\) which could depend on the number of observations.↩︎\nThere’s a lot of space for interesting questions here.↩︎\nIt’ws also fully justified by people who have written far more mathematically sophisticated papers on this topic!↩︎\n",
    "preview": "posts/2021-12-08-the-king-must-die-repost/the-king-must-die-repost_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-02-10T23:28:15+11:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-11-24-getting-into-the-subspace/",
    "title": "Getting into the subspace; or what happens when you approximate a Gaussian process",
    "description": "Fuck man, I don't know about this one. A lot of stuff happens. At some point there's a lot of PDEs. There are proofs. Back away.",
    "author": [
      {
        "name": "Dan Simpson",
        "url": "https://dpsimpson.github.io"
      }
    ],
    "date": "2021-11-24",
    "categories": [],
    "contents": "\nSo. Gaussian processes, eh.\nNow that we know what they are, I guess we should do something with a Gaussian process. But we immediately hit a problem. You see, Gaussian processes are charming things, sweet and caring. But they have a dark side. Used naively1, they’re computationally expensive when you’ve got a lot of data.\nStab of dramatic music\nYeah. So. What’s the problem here? Well, the first problem is people seem to really like having a lot of data. Fuck knows why. Most of it is rubbish2. But they do.\nThis is a problem for our poor little Gaussian processes because of how the data tends to come.\nA fairly robust model for data is that it comes like \\[\n(y_i, s_i, x_i),\n\\] where \\(y_i\\) is our measurement of choice (which might be a continuous, discrete or weird3), \\(s_i\\) is our location4 in the index set5 \\(\\Omega\\) (usually \\(\\Omega \\subset \\mathbb{R}^d\\)) of the Gaussian process, and \\(x_i\\) is whatever other information we have6. If we want to be really saucy, we could also assume these things are iid samples from some unknown distribution and then pretend like that isn’t a wildly strong structural assumption. But I’m not like that. I’ll assume the joint distribution of the samples is exchangeable7 8 9. Or something. I’m writing this sequentially, so I have no idea where this is going to end up.\nSo where is the problem? The problem is that, if we use the most immediately computational definition of a Gaussian process, then we need to build \\[\n\\begin{pmatrix} u(s_1)\\\\ u(s_2) \\\\ \\vdots \\\\ u(s_n)\\end{pmatrix} \\sim N\\left(\n\\begin{pmatrix} \\mu(s_1)\\\\ \\mu(s_2) \\\\ \\vdots \\\\ \\mu(s_n)\\end{pmatrix}, \n\\begin{pmatrix} c(s_1, s_1) & c(s_1, s_2) & \\cdots & c(s_1, s_n)  \\\\ c(s_2, s_1) & c(s_2, s_2) & \\cdots & c(s_2, s_n) \\\\\\vdots &\\vdots  &\\ddots &\\vdots \\\\ c(s_n, s_1) & c(s_n, s_2) & \\cdots & c(s_n, s_n)\\end{pmatrix}\\right).\n\\] Where \\(s_1,\\ldots, s_n\\) are all of the distinct values of \\(s_i\\) in the dataset. If there are a lot of these, the covariance matrix is very large and this becomes a problem. First, we must construct it. Then we must solve it. Then we must do actual computations with it. The storage scales quadratically in \\(n\\). The computation scales cubically in \\(n\\). This is too much storage and too much computation if the data set has a lot of distinct GP evaluations, it will simply be too expensive to do the matrix work that we need to do in order to make this run.\nSo we need to do something else.\nA tangent; or Can we just be smarter?\nOn a tangent, because straight lines are for poor souls who don’t know about Gaussian processes, there’s a body of work on trying to circumvent this problem by being good at maths. The idea is to try to find some cases where we don’t need to explicitly form the covariance matrix in order to do all of the calculations. There’s a somewhat under-cooked10 literature on this. It dances around an idea that traces back to fast multipole methods for integral equations: We know that correlations decay as points get further apart, so we do not need to calculate the correlations between points that are far apart as well as we need to calculate the correlations between points that are close together. For a fixed covariance kernel that decays in a certain way, you can modify the fast multipole method, however it’s more fruitful to use an algebraic11 method. H-matrices was the first real version of this, and there’s a paper from 2008 using them to approximate GPs. A solid chunk of time later, there have been two good papers recently on this stuff. Paper 1 Paper 2. These methods really only provide gradient descent type methods for maximum likelihood estimation and it’s not clear to me that you’d be able to extend these ideas easily to a Bayesian setting (particularly when you need to infer some parameters in the covariance function)12.\nI think this sort of stuff is cool for a variety of reasons, but I also don’t think it’s the entire solution. (There was also a 2019 NeurIPS paper that scales a GP fit to a million observations as if that’s a good idea. It is technically impressive, however.) But I think the main possibility of the H-matrix work is that it allows us to focus on the modelling and not have to make premature trade offs with the computation.\nThe problem with modelling a large dataset using a GP is that GPs are usually fit with a bunch of structural assumptions (like stationarity and isotropy) that are great simplifying assumptions for moderate data sizes but emphatically do not capture the complex dependency structures when there is a large amount of data. As you get more data, your model should become correspondingly more complex13 and stationary, and/or isotropic Gaussian processes emphatically do not do this.\nThis isn’t to say that you shouldn’t use GPs on a large data set (I am very much on record as thinking you should), but that it needs to be a part of your modelling arsenal and probably not the whole thing. The real glory of GPs is that they are a flexible enough structure to play well with other modelling techniques. Even if you end up modelling a large data set with a single GP, that GP will most likely be anisotropic, non-stationary, and built up from multiple scales. Which is a different way to say that it likely does not have a squared exponential kernel with different length scales for each feature.\n(It’s probably worth making the disclaimer at this point, but when I’m thinking about GPs, I’m typically thinking about them in 1-4 dimensions. My background is in spatial statistics, so that makes sense. Some of my reasoning doesn’t apply in more typical machine learning applications where \\(s_i\\) might be quite high-dimensional. That said, you simply get a different end of the same problem. In that case you need to balance the smoothness needed to interpolate in high dimensions with the structure needed to allow your variables to be a) scaled differently and b) correlated. Life is pain either way.)\nSo can we make things better?\nThe problem with Gaussian processes, at least from a computational point of view, is that they’re just too damn complicated. Because they are supported on some infinite dimensional Banach space \\(B\\), the more we need to see of them (for instance because we have a lot of unique \\(s_i\\)s) the more computational power they require. So the obvious solution is to somehow make Gaussian processes less complex.\nThis somehow has occupied a lot of people’s time over the last 20 years and there are many many many many possible options. But for the moment, I just want to focus on one of the generic classes of solutions: You can make Gaussian processes less computationally taxing by making them less expressive.\nOr to put it another way, if you choose an \\(m\\) dimensional subspace \\(V_m \\subset B\\) and rep;ace the GP \\(u\\), which is supported on the whole of \\(B\\), with a different Gaussian process \\(u_m\\) supported on \\(V_m\\), then all of your problems go away.\nWhy? Well because the Gaussian process on \\(V_m\\) can be represented in terms of an \\(m\\)-dimensional Gaussian random vector. Just take \\(\\phi_j\\), \\(j=1,\\ldots, m\\) to be a basis for \\(V_m\\), then the GP \\(u_m\\) can be written as \\[\nu_m = \\sum_{j=1}^m w_j \\phi_j,\n\\] where \\(w \\sim N(\\mu, \\Sigma)\\), for some \\(\\mu\\) and \\(\\Sigma\\). (The critical thing here is that the \\(\\phi_j\\) are functions so \\(u_m\\) is still a random function! That link between the multivariate Gaussian \\(w\\) and the function \\(u_m\\) that can be evaluated at any \\(s_i\\) is really important!)\nThis means that I can express my Gaussian process prior in terms of the multivariate Gaussian prior on \\(w\\), and I only need \\(\\mathcal{O}(m^3)\\) operations to evaluate its log-density.\nIf our observation model is such that \\(p(y_i \\mid u) = p(y_i \\mid u(s_i))\\), and we assume conditional14 independence, then we can eval the log-likelihood term \\[\n\\sum_{i=1}^n p(y \\mid u_m(s_i)) = \\sum_{i=1}^n p(y \\mid a_i^Tw)\n\\] in \\(\\mathcal{O}(m^2 n)\\) operations. Here \\([a_i]_j = \\phi_j(s_i)\\) is the vector that links the basis in \\(u_n\\) that we use to define \\(w\\) to the observation locations15.\nMany have been tempted to look at the previous paragraph and conclude that a single evaluation of the log-posterior (or its gradient) will be \\(\\mathcal{O}(n)\\), as if that \\(m^2\\) multiplier were just a piece of fluff to be flicked away into oblivion.\nThis is, of course, sparkly bullshit.\nThe subspace size \\(m\\) controls the trade off between bias and computational cost and, if we want that bias to be reasonably small, we need \\(m\\) to be quite large. In a lot of cases, it needs to grow with \\(n\\). A nice paper by David Burt, Carl Rasmussen, and Mark van der Wilk suggests that \\(m(n)\\) needs to depend on the covariance function16. In the best case (when you assume your function is so spectacularly smooth that a squared-exponential covariance function is ok), you need something like \\(m = \\mathcal{O}(\\log(n)^d)\\), while if you’re willing to make a more reasonable assumption that your function has \\(\\nu\\) continuous17 derivatives, then you need something like \\(m = \\mathcal{O}(n^\\frac{2d}{2\\nu-d})\\).\nYou might look at those two options for \\(m\\) and say to yourself “well shit. I’m gonna use a squared exponential from now on”. But it is never as simple as that. You see, if you assume a function is so smooth it is analytic18, then you’re assuming that it lacks the derring-do to be particularly interesting between its observed values19. This translates to relatively narrow uncertainty bands. Whereas a function with \\(\\nu\\) derivatives has more freedom to move around the smaller \\(\\nu\\) is. This naturally results in wider uncertainty bands.\nI think20 in every paper I’ve seen that compares a squared exponential covariance function to a Matérn-type covariance function (aka the ones that let you have \\(\\nu\\)-times differentiable sample paths), the Matérn family has performed better (in my mind this is also in terms of squared error, but it’s definitely the case when you’re also evaluating the uncertainty of the prediction intervals). So I guess the lesson is that cheap isn’t always good?\nAnyway. The point of all of this is that if we can somehow restrict our considerations to an \\(m\\)-dimensional subspace of \\(B\\), then we can get some decent (if not perfect) computational savings.\nBut what are the costs?\nSome notation that rapidly degenerates into a story that’s probably not interesting\nSo I guess the key question we need to answer before we commit to any particular approximation of our Gaussian process is what does it cost? That is, how does the approximation affect the posterior distribution?\nTo quantify this, we need a way to describe the posterior of a Gaussian process in general. As happens so often when dealing with Gaussian processes, shit is about to get wild.\nA real challenge with working with Gaussian processes theoretically is that they are objects that naturally live on some (separable21) Banach space \\(B\\). One of the consequences of this is that we cannot just write the density of \\(u\\) as \\[\np(u) \\propto  \\exp\\left(-\\frac{1}{2}C_u(u, u)\\right)\n\\] because there is no measure22 on \\(B\\) such that \\[\n\\Pr(u \\in A) = \\int_A p(u)\\,du.\n\\]\nThis means that we can’t just work with densities to do all of our Bayesian stuff. We need to work with posterior probabilities properly.\nUgh. Measures.\nSo let’s do this. We are going to need a prior probability associated with the Gaussian process \\(u\\), which we will write as \\[\n\\mu_0(A) = \\Pr(u \\in A),\n\\] where \\(A\\) is a nice23 set in \\(B\\). We can then use this as a base for our posterior, which we define as \\[\n\\mu^y(A) = \\Pr(u \\in A \\mid y) = \\frac{1}{Z}\\mathrm{e}^{-\\Phi(u;y)},\n\\] where \\(\\Phi(u;y)\\) is the negative log-likelihood function. Here \\(Z\\) is the normalising constant \\[\nZ = \\mathbb{E}_\\mu\\left( \\mathrm{e}^{-\\Phi(u;y)}\\right),\n\\] which is finite as long as \\(\\exp(-\\Phi(u;y)) \\leq C(\\epsilon)\\exp(\\epsilon\\|u\\|^2)\\) for all \\(\\epsilon > 0\\), where \\(C(\\epsilon)\\geq 0\\) is a constant. This is a very light condition.\nThis way of looking at posteriors resulting Gaussian process priors was popularised in the inverse problems literature24. It very much comes from a numerical analysis lens: the work is framed as here is an object, how do we approximate it?.\nThese questions are different to the traditional ones answered by a theoretical statistics papers, which are almost always riffs on “what happens in asymptopia?”.\nI came across this work for two reasons: one is because I have been low-key fascinated by Gaussian measures ever since I saw a talk about them during my PhD; and secondly my PhD was in numerical analysis, so I was reading the journals when these papers came out.\nThat’s not why I explored these questions, though. That is a longer story. The tl;dr is\n\nI had to learn this so I could show a particular point process model converges, and so now the whole rest of this blog post is contained in a technical appendix that no one has ever read in this paper.\n\nHere comes the anecdote. Just skip to the next bit. I know you’re like “but Daniel just delete the bullshit text” but that is clearly not how this works.\n\nExpand at your peril\nI know these papers pretty much backwards for the usual academic reason: out of absolute spite. One of my postdocs involved developing some approximation methods for Markovian Gaussian processes25, which allowed for fast computation, especially when combined with INLA26, which is a fabulous method for approximating posterior distributions when a big chunk of the unobserved parameters have a joint Gaussian prior27.\nOne of the things that INLA was already pretty good at doing was fitting log-Gaussian Cox processes (LGCP), which are a type of model for point patterns28 that can be approximated over a regular grid by a Poisson regression with a log-mean given by (covariates +) a Gaussian process defined on the grid. If that process is Markov, you can get full posterior inference quickly and accurately using INLA. This compared very favourably with pre-INLA methods, which gave you full posterior inference laboriously using a truncated gradient MALA scheme in about the same amount of time it would take the US to get a high-speed rail system.\nAnyway. I was in Trondheim working on INLA and the, at that stage, very new SPDE29 stuff (the 2011 JRSSSB read paper had not been written yet, let alone been read). Janine Illian, who is a very excellent statistician and an all round fabulous person, had been working on the grided LGCP stuff in INLA and came to Trondheim to work with Håvard30 and she happened to give a seminar on using these new LGCP methods to do species distribution mapping. I was strongly encouraged to work with Janine to extend her grid methods to the new shiny SPDE methods, which did not need a grid.\nJanine had to tell me what a Poisson distribution was.\nAnyway. A little while later31 we had a method that worked and we32 wrote it up. We submitted it to Series B and they desk rejected it. We then, for obscure reasons33, submitted it to Biometrika. Due to the glory of arXiv, I can link to the original version.\nAlthough it was completely unlike anything else that Biometrika publishes, we got some quite nice reviews and either major revisions or a revise and resubmit. But one of the reviewer comments pissed me off: they said that we hadn’t demonstrated that our method converges. Now, I was young at the time and new to the field and kinda shocked by all of the shonky numerics that was all over statistics at the time. So this comment34 pissed me off. More than that, though, I was fragile and I hated the comment because I was new to this and had absolutely no fucking idea how to prove this method would converge. Rasmus Waagepetersen had proven convergence of the grid approximation but a) I didn’t understand the proof and b) our situation was so far away there was no chance of piggybacking off it.\nIt was also very hard to use other existing statistics literature, because, far from being an iid situation, the negative log-likelihood for a Poisson process35 on an observation window \\(\\Omega\\) is \\[\n\\Phi(u;y) = \\int_\\Omega e^{u(s)}\\,ds - \\sum_{s_i \\in y}e^{u(s_i)} - |\\Omega|,\n\\] where the point pattern \\(y\\) is a (random) collection of points \\(s_j\\) and \\(|\\Omega|\\) is the area/volume of the observation window. This is fundamentally not like a standard GP regression.\nSo, long story short36, I was very insecure and rather than admit that it was difficult to show that these approximations converged, I worked on and off for like 2 years trying to work out how to do this37 and eventually came up with a fairly full convergence theory for posteriors derived from approximate likelihoods and finite dimensional approximations to Gaussian processes38. Which I then put into the appendix of a paper that was essentially about something completely different.\nI don’t have all that many professional regrets (which is surprising because I’ve made a lot of questionable choices), but I do regret not just making that appendix its own paper. Because it was really good work.\nBut anyway, I took the inverse problems39 work of Andrew Stuart and Masoumeh Dashti and extended it out to meet my needs. And to that end, I’m going to bring out a small corner of that appendix because it tells us what happens to a posterior when we replace a Gaussian process by a finite dimensional approximation.\nHow do we measure if a posterior approximation is good?\nPart of the struggle when you’re working with Gaussian processes as actual objects rather than as a way to generate a single finite-dimensional Gaussian distribution that you use for analysis is that, to quote Cosma Shalizi40, “the topology of such spaces is somewhat odd, and irritatingly abrupt”. Or to put it less mathematically, it is hard to quantify which Gaussian processes are close together.\nWe actually saw this in the last blog where we noted that the distribution of \\(v = cu\\) has no common support with the distribution of \\(u\\) if \\(|c| \\neq 1\\). This means, for instance, that the total variation between \\(u\\) and \\(v\\) is 2 (which is it’s largest possible value) even if \\(c = 1 + \\epsilon\\) for some tiny \\(\\epsilon\\).\nMore generally, if you’re allowed to choose what you mean by “these distributions are close” you can get a whole range of theoretical results for the posteriors of infinite dimensional parameters, ranging from this will never work and Bayes in bullshit to everything is wonderful and you never have to worry.\nSo this is not a neutral choice.\nIn the absence of a neutral choice, we should try to make a meaningful one! An ok option for that is to try to find functions \\(G(u)\\) that we may be interested in. Classically, we would choose \\(G\\) to be bounded (weak41 convergence / convergence in distribution) or bounded Lipschitz42. This is good but it precludes things like means and variances, which we would quite like to converge!\nThe nice thing about everything being based off a Gaussian process is that we know43 that there is some \\(\\epsilon > 0\\) (which may be very small) such that \\[\n\\mathbb{E}_{\\mu_0}\\left(\\mathrm{e}^{\\epsilon \\|u\\|_B^2}\\right) < \\infty.\n\\] This suggests that as long as the likelihood isn’t too evil, the posterior will also have a whole arseload of moments.\nThis is great because it suggests that we can be more ambitious than just looking at bounded Lipschitz functions. It turns out that we can consider convergence over the class of functionals \\(G\\) such that \\[\n|G(u) - G(v)| \\leq L(u) \\|u - v\\|_B,\n\\] where \\(\\mathbb{E}_{\\mu_0}(L(u)) < \\infty\\). Critically this includes functions like moments of \\(\\|u\\|_B\\) and, assuming all of the functions in \\(B\\) are continuous, moments of \\(u(s)\\). These are the functions we tend to care about!\nConvergence of finite dimensional Gaussian processes\nIn order to discuss the convergence of finite dimensional Gaussian processes, we need to define them and, in particular, we need to link them to some Gaussian process on \\(B\\) that they are approximating.\nLet \\(u\\) be a Gaussian process supported on a Banach space \\(B\\). We define a finite dimensional Gaussian process to be a Gaussian process supported on some space \\(V_m \\subset B\\) that satisfies \\[\nu_n = R_m u,\n\\] where \\(R_m: B \\rightarrow V_m\\) is some operator. (For this to be practical we want this to be a family of operators indexed by \\(m\\).)\nIt will turn out that how this restriction is made is important. In particular, we are going to need to see how stable this restriction is. This can be quantified by examining \\[\n\\sup_{\\|f\\|_V = 1} \\|R_m f\\|_{B} \\leq A_m \\|f\\|_V,\n\\] where \\(A_m > 0\\) is a constant that could vary with \\(m\\) and \\(V \\subseteq B\\) is some space we will talk about later. (Confusingly, I have set up the notation so that it’s not necessarily true that \\(V_m \\subset V\\). Don’t hate me because I’m pretty, hate me because I do stupid shit like that.)\nExample 1: An orthogonal truncation\nThere is a prototypical example of \\(R_m\\). Every Gaussian process on a separable Banach space admits a Karhunen-Loève representation \\[\nu = \\sum_{k = 0}^\\infty \\lambda_k^{1/2} z_k \\phi_k,\n\\] \\(z_k\\) are iid standard normal random variables and \\((\\lambda_k, \\phi_k)\\) are the eigenpairs44 of the covariance operator \\(C_u\\). The natural restriction operator is then \\[\nR_m f = \\sum_{j=0}^m \\langle f, \\phi_j\\rangle_{L^2}\\phi_j.\n\\] This was the case considered by Dashti and Stuart in their 2011 paper. Although it’s prototypical, we typically do not work with the Karhunen-Loève basis directly, as it tends to commit us to a domain \\(\\Omega\\). (Also because we almost45 never know what the \\(\\phi_j\\) are.)\nBecause this truncation is an orthogonal projection, it follows that we have the stability bound with \\(A_m = 1\\) for all \\(m\\).\nExample 2: Subset of regressors\nMaybe a more interesting example is the subset of regressors46. In this case, there are a set of inducing points \\(s_1, \\ldots, s_m\\) and \\[\nR_m f = \\sum_{j=1}^m w_j r_u(\\cdot, s_j),\n\\] where the weights solve \\[\nK_m w = b,\n\\] \\([K_m]_{ij} = r_u(s_i, s_j)\\) and \\(b_j = f(s_j)\\).\nIt’s a bit harder to get the stability result in this case. But if we let \\(V_m\\) have the RKHS47 norm, then \\[\\begin{align*}\n\\|R_m f\\|^2_{H_u} &= w^TK_m w \\\\\n&= b^T K^{-1} b \\\\\n&\\leq \\|K^{-1}\\|_2 \\|b\\|_2^2\n\\end{align*}\\]\nAssuming that \\(B\\) contains continuous functions, then \\(\\|b\\|_2 \\leq C\\sqrt{m} \\|f\\|_B\\). I’m pretty lazy so I’m choosing not to give a shit about that \\(\\sqrt{m}\\) but I doubt it’s unimprovable48. To be honest, I haven’t thought deeply about these bounds, I am doing them live, on my couch, after a couple of red wines. If you want a good complexity analysis of subset of regressors, google.\nMore interestingly, \\(\\|K_m^{-1}\\|_2\\) can be bounded, under mild conditions on the locations of the \\(s_j\\) by the \\(m\\)-th largest eigenvalue of the operator \\(Kf = \\int_\\Omega r_u(s,t)f(t)\\,dt\\). This eigenvalue is controlled by how differentiable \\(u\\) is, and is roughly \\(\\mathcal{O}\\left(m^{-\\alpha - d/2}\\right)\\) if \\(u\\) has a version with \\(\\alpha\\)-almost sure (Hölder) derivatives. In the (common) case where \\(u\\) is analytic (eg if you used the squared exponential covariance function), then this bound increases exponentially (or squared exponentially for the squared exponential) in \\(m\\).\nThis means that the stability constant \\(A_m \\geq \\|K_m^{-1}\\|\\) will increase with \\(m\\), sometimes quite alarmingly. Wing argues that it is always at least \\(\\mathcal{O}(m^2)\\). Wathan and Zhu have a good discussion for the one-dimensional case and a lot of references to the more general situation.\nExample 3: The SPDE method\nMy personal favourite way to approximate Gaussian processes works when they are Markovian. The Markov property, in general, says that if, for every49 set of disjoint open domains \\(S_1\\) and \\(S_2 = \\Omega \\backslash \\bar S_1\\) such that \\(S_1 \\cup \\Gamma \\cup S_2\\), where \\(\\Gamma\\) is the boundary between \\(S_1\\) and \\(S_2\\), then \\[\n\\Pr(A_1 \\cup A_2 \\mid B_\\epsilon) = \\Pr(A_1 \\mid B_\\epsilon) \\Pr(A_2 \\mid B_\\epsilon),\n\\] where \\(A_j \\in \\sigma\\left(\\{u(s), s \\in S_j\\}\\right)\\) and50 \\(B_\\epsilon \\in \\sigma\\left(\\{u(s); d(s, \\Gamma) < \\epsilon\\}\\right)\\) and \\(\\epsilon>0\\).\nWhich is to say that it’s the normal Markov property, but you may need to fatten out the boundary between disjoint domains infinitesimally for it to work.\nIn this case, we51 know that the reproducing kernel Hilbert space has the property that the inner product is local. That means that if \\(f\\) and \\(g\\) are in \\(H_u\\) and have disjoint support52 then \\[\n\\langle f, g\\rangle_{H_u} = 0,\n\\] which, if you squint, implies that the precision operator \\(\\mathcal{Q}\\) is a differential operator. (That the RKHS inner product being local basically defines the Markov property.)\nWe are going to consider a special case53, where \\(u\\) solves the partial differential equation \\[\nL u = W,\n\\] where \\(L\\) is some differential operator and \\(W\\) is white noise54.\nWe make sense of this equation by saying a Gaussian process \\(u\\) solves it if \\[\n\\int_\\Omega \\left(L^*\\phi(s)\\right)\\left( u(s)\\right)\\,ds  \\sim N\\left(0, \\int_\\Omega \\phi^2(s)\\,ds\\right),\n\\] for every smooth function \\(\\phi\\), where \\(L^*\\) is the adjoint of \\(L\\) (we need to do this because, in general, the derivatives of \\(u\\) could be a bit funky).\nIf we are willing to believe this exists (it does—it’s a linear filter of white noise, electrical engineers would die if it didn’t) then \\(u\\) is a Gaussian process with zero mean and covariance operator \\[\n\\mathcal{C} = (L^*L)^{-1},\n\\] where \\(L^*\\) is the adjoint of \\(L\\).\nThis all seems like an awful lot of work, but it’s the basis of one of the more powerful methods for approximating Gaussian processes on low-dimensional spaces (or low-dimensional manifolds). In particular in 1-3 dimensions55 or in (1-3)+1 dimensions56 (as in space-time), Gaussian processes that are built this way can be extremely efficient.\nThis representation was probably first found by Peter Whittle and Finn Lindgren, Johan Lindström and Håvard Rue combined it with the finite element method to produce the SPDE method57 A good review of the work that’s been done can be found here. There’s also a whole literature on linear filters and stochastic processes.\nWe can use this SDPE representation of \\(u\\) to construct a finite-dimensional Gaussian process and a restriction operator \\(R_m\\). To do this, we define \\(L_m\\) as the operator defined implicitly through the equation \\[\n\\langle \\phi, L\\psi\\rangle_{L^2} = \\langle \\phi, L_m\\psi\\rangle_{L^2}, \\quad \\forall \\phi,\\psi \\in V_m.\n\\] This is often called the Galerkin projection58. It is at the heart of the finite element method for solving elliptic partial differential equations.\nWe can use \\(L_m\\) to construct a Gaussian process with covariance function \\[\n\\mathcal{C}_m = (L_m^*L_m)^\\dagger,\n\\] where \\(^\\dagger\\) is a pseudo-inverse59.\nIt follows that \\[\n\\mathcal{C}_m  =(L_m)^\\dagger L L^{-1}(L^*)^{-1}L^*(L_m^*)^\\dagger = R_m \\mathcal{C} R_m^*,\n\\] where \\(R_m = L_m^\\dagger L\\).\nBefore we can get a stability estimate, we definitely need to choose our space \\(V_m\\). In general, the space will depend on the order of the PDE60, so to make things concrete we will work with second-order elliptic61 PDE \\[\nLu = -\\sum_{i,j = 1}^d\\frac{\\partial}{\\partial s_j}\\left(a_{ij}(s) \\frac{\\partial u}{\\partial s_i} \\right) +\\sum_{i=1}^d b_i\\frac{\\partial u}{\\partial s_i} + b_0(s)u(s),\n\\] where all of the \\(a_{ij}(s)\\) and \\(b_j(s)\\) are \\(L^\\infty(\\Omega)\\) and the uniform ellipticity condition62 holds.\nThese operators induce (potentially non-stationary) Gaussian processes that have continuous versions as long as63 \\(d \\leq 3\\).\nWith this fixed, the natural finite element space to use is the space of continuous piecewise linear functions. Traditionally, this is done using combinations of tent functions on a triangular mesh.\nA piecewise linear approximation. SourceWith this basis, we can get stability estimates by defining \\(v\\) and \\(v_m\\) by \\(Lv = f\\) and \\(L_m v_m = f_m\\), from which we get \\[\n\\|R_m v\\|_B = \\|v_m\\|_{V_m} \\leq A\\|f\\|_{L_2}\n\\] which holds, in particular, when the \\(L\\) has no first order derivatives64.\nAn oddity about this structure is that functions in \\(V_m\\) are not not continuously differentiable, while the sample paths of \\(u\\) almost surely are65. This means that \\(V_m\\) isn’t necessarily a subset of \\(B\\) as we would naturally define it. In this case, we need to inflate \\(B\\) to be big enough to contain the \\(V_m\\). So instead of taking \\(B = C^1(\\Omega)\\), we need to take \\(B = C(\\Omega)\\) or \\(B = L^2(\\Omega)\\).\nThis has implications on the smoothness assumptions on \\(\\Phi(u;y)\\), which will need to hold uniformly over \\(B\\) and \\(V_m\\) if \\(V_m \\not \\subset B\\) and on the set of functionals \\(G(u)\\) that we use to measure convergence.\nA bit of perspective\nThe critical difference between the SPDE method and the subset-of-regressors approximation is that for the SPDE method, the stability constant \\(A_m = A\\) is independent of \\(m\\). This will be important, as this constant pops up somewhere important when we are trying to quantify the error in the finite dimensional approximation.\nOn the other hand, the SPDE method only works in three and fewer dimensions and while it allows for quite flexible covariance structures66, it is can only directly construct Gaussian processes with integer numbers of continuous derivatives. Is this a problem? The asymptotics say yes, but they only hold if we are working with the exact Gaussian process (or, I guess, if we let the dimension of \\(V_m\\) hurtle off towards infinity as we get more and more data).\nIn practice, the Gaussian processes constructed via SPDE methods perform very well on real data67. I suspect part of this is that the stable set of basis functions are very good at approximating functions and the misspecification error plays off against the approximation error.\nBounding the approximation error\nWith all of this setup, we are finally ready to bound the error between the posterior we would get with the full Gaussian process prior and the posterior we would get using the finite dimensional Gaussian process prior.\nWe are going to deal with a simpler scenario than the paper we are (sort of) following, because in that situation, I was forced to deal with simultaneously approximating the likelihood and honestly who needs that trouble.\nTo remind ourselves, we have two priors: the full fat Gaussian process prior, the law of which we denote \\(\\mu_0\\) and the one we could possibly work with \\(\\mu_0^m\\). These lead to two different posteriors \\(\\mu_y\\) and \\(\\mu_y^m\\) given by \\[\n\\frac{d\\mu_y}{d\\mu_0}(u) = \\frac{1}{Z}\\mathrm{e}^{-\\Phi(u;y)} \\quad \\text{and}\\quad \\frac{d\\mu_y^m}{d\\mu_0^m}(u) = \\frac{1}{Z_m}\\mathrm{e}^{-\\Phi(u;y)} ,\n\\] where \\(Z_1\\) and \\(Z_m\\) are normalising constants.\nWe assume that the Gaussian process \\(u\\) is supported on some Banach space \\(V \\subseteq B\\) and the approximating spaces \\(V_m \\subset B\\). This covers the case where the approximating functions are rougher than the true realisations of the Gaussian process we are approximating. With this notation, we have the restriction operator \\(R_m\\) that satisfies \\[\n\\|R_mf\\|_{V_m} \\leq A_m \\|f\\|_V,\n\\] which is a slightly more targeted bound when \\(B\\) is larger than \\(V\\).\nWe will make the following assumptions about the negative log-likelihood (or potential function) \\(\\Phi\\): For every \\(\\epsilon > 0\\), \\(r> 0\\), and68 \\(\\|y\\| < r\\), there exist positive constants \\(C_1, C_2, C_3, C_4\\) that may depend on \\(\\epsilon\\) and \\(r\\) such that the following 4 conditions hold. (Note: when the norm isn’t specified, we want it to hold over both the \\(V\\) and \\(B\\) norms.)\nFor all \\(u \\in V \\cup \\left(\\bigcup_{m\\geq 1} V_m\\right)\\) \\[\n\\Phi(u;y) \\geq C_1 - \\epsilon \\|u\\|^2\n\\]\nFor every \\(u\\in B\\), \\(y \\in Y\\) with \\(\\max \\{\\|u\\|, \\|y\\|_Y\\} < r\\), \\[\n\\Phi(u;y) \\leq C_2\n\\]\nFor every \\(\\max \\{\\|u_1\\|_V, \\|u_2\\|_B, \\|y\\|_Y\\} < r\\), \\[\n|\\Phi(u_1; y) - \\Phi(u_2; y )| \\leq \\exp\\left(\\epsilon\\max\\{\\|u_1\\|_V^2, \\|u_2\\|_B^2\\} - C_3\\right) \\|u_1 - u_2\\|_B\n\\]\nFor every \\(u\\in B\\) and \\(\\max \\{\\|y_1\\|_Y, \\|y_2\\|_Y\\} < r\\), \\[\n|\\Phi(u; y_1) - \\Phi(u; y_2) | \\leq  \\exp\\left(\\epsilon \\|u\\|^2 + C_4\\right)\\|y_1 - y_2\\|_Y\n\\]\nThese restrictions are pretty light and are basically what are needed to make sure the posteriors exist. The first one say “don’t grow too fast” to the likelihood and is best explained while humming ABBA’s Slipping Through My Fingers. The second one makes sure the likelihood isn’t zero. The third and fourth are Lipschitz conditions that basically make sure that a small change in \\(u\\) (or \\(y\\)) doesn’t make a big change in the likelihood. It should be pretty clear that if that could happen, the two posteriors wouldn’t be close.\nWe are also going to need some conditions on our test functions. Once again, we need them to apply over \\(V\\) and \\(B\\) when no space is specified for the norm.\nFor all \\(u \\in V\\), \\(G(u) = \\exp(\\epsilon \\|u\\|^2_V+ C_5)\\)\nFor all \\(u_1 \\in V\\), \\(u_2 \\in V_m\\), \\[\n|G(u_1) - G(u_2)| \\leq \\exp(\\epsilon\\max\\{\\|u_1\\|^2_V, \\|u_2\\|^2_B\\})\\|u_1 - u_2\\|_B.\n\\]\nUnder these conditions, we get the following theorem, which is a simplified version of Theorem A2 here.\n\nTheorem Under the above assumptions, \\[\n\\left|\\mathbb{E}_{\\mu_y}(G(u)) - \\mathbb{E}_{\\mu^m_y}(G(u_m))\\right| \\leq C_m \\sup_{f \\in V}\\left(\\frac{\\|f - R_m f\\|_B}{\\|f\\|_V}\\right),\n\\] where \\(C_m\\) only depends on \\(m\\) through \\(A_m\\).\n\nI seriously doubt that the dependence on \\(A_m\\) is exponential, as it is in the proof, but I’m not going to try to track that down. That said, I’m also quite sure that the dependence \\(C_m\\) is not uniform in \\(m\\) unless \\(A_m\\) is constant.\nIt’s also worth noting that there’s nothing special about \\(G\\) being real-valued. In general it can take values in any Banach space \\(E\\). Just replace all those absolute values with norms. That means that the result covers convergence of approximations to things like covariance matrices.\n\nProof, if you’re interested\nWe are interested in approximating \\[\ne_G = \\left|\\mathbb{E}_{\\mu_y}(G(u)) - \\mathbb{E}_{\\mu^m_y}(G(u_m))\\right|.\n\\] We can expand this to get \\[\\begin{align*}\ne_G \\leq & \\frac{1}{Z}\\left|\\mathbb{E}_{\\mu_0}\\left(G(u)\\exp(-\\Phi(u;y))\\right)\n- \\mathbb{E}_{\\mu_0^m}\\left(G(u_m)\\exp(-\\Phi(u_mm;y))\\right)\\right| \\\\\n&\\quad +\n\\left|\\frac{1}{Z} \n - \\frac{1}{Z_m}\\right|\\mathbb{E}_{\\mu_0^m}\\left(|G(u_m)|\\exp(-\\Phi(u_m;y))\\right). \\\\\n &= B_1 + B_2.\n\\end{align*}\\]\nIt follows from Andrew Stuart’s work that the normalising constants \\(Z\\) and \\(Z_m\\) can be bounded above and below independently of \\(m\\), so the above expression makes sense.\nWe will now attack \\(B_1\\) and \\(B_2\\) separately. To do this, we need to consider the joint prior \\(\\lambda_0(u, u_m)\\) that is the joint law of the Gaussian process \\(u\\) and its finite dimensional approximation \\(u_m = R_m u\\).\nFor \\(B_1\\) we basically use the same trick again. \\[\\begin{align*}\nZB_1 \\leq & \\mathbb{E}_{\\lambda_0}\\left(|G(u)|\\left|\\exp(-\\Phi(u;y)) -\\exp(\\Phi(u;y))\\right| \\right) \\\\\n&\\quad + \\mathbb{E}_{\\lambda_0}\\left(\\exp(-\\Phi(u_m;y)) | G(u) - G(u_m)|\\right) \\\\\n&\\leq  \\mathbb{E}_{\\lambda_0}\\left(\\mathrm{e}^{C_5 + \\epsilon \\|u\\|_V^2}\\mathrm{e}^{\\epsilon\\max\\{1,A_m\\}\\|u\\|_v^2 - C_1} \\mathrm{e}^{\\epsilon\\max\\{1,A_m\\}\\|u\\|_V^2 + C_3}\\|u - u_m\\|_B\\right) \\\\\n& \\quad +\\mathbb{E}_{\\lambda_0}\\left(\\mathrm{e}^{\\epsilon A_m\\|u\\|_V^2 - C_1}\\mathrm{e}^{\\epsilon\\max\\{1,A_m\\}\\|u\\|_V^2 + C_6}\\|u - u_m\\|_V\\right) \\\\\n&\\leq \\sup_{f \\in V}\\left(\\frac{\\|f - R_m f\\|_B}{\\|f\\|_V}\\right) \n\\mathrm{e}^{C_3 + C_5 + C_6 -2 C_1}\\mathbb{E}_{\\mu_0}\\left(\\|u\\|_V\\mathrm{e}^{(1+3\\max\\{1,A_m\\} + A_m)\\epsilon \\|u\\|_V^2}\\right)\\\\\n&\\leq C_7 \\sup_{f \\in V}\\left(\\frac{\\|f - R_m f\\|_B}{\\|f\\|_V}\\right),\n\\end{align*}\\] where the second inequality comes from using all of the assumptions on \\(\\Phi\\) and \\(G\\) and noting that \\(\\left|e^{-x} - e^{-y}\\right| \\leq e^{-\\min\\{x,y\\}}|x-y|\\); and the final inequality comes from Fernique’s theorem, which implies that expectation is finite.\nWe can also bound \\(B_2\\) by noting that \\[\\begin{align*}\n\\left|Z^{-1} - Z_m^{-1} \\right| & \\leq \\max \\{Z^{-2}, Z_m^{-2}\\}\\mathbb{E}_{\\lambda_0}\\left(|\\exp(-\\Phi(u;y)) - \\exp(-\\Phi(u_m;z))\\right) \\\\\n&\\leq C_8 \\sup_{f \\in V}\\left(\\frac{\\|f - R_m f\\|_B}{\\|f\\|_V}\\right) \n\\end{align*}\\] by the same reasoning as above.\nDealing with the approximation error\nThe theorem above shows that the worst-case error in posterior functionals caused by replacing a Gaussian process \\(u\\) with it’s approximation \\(u_m = R_m u\\) is driven entirely by how well a general function from \\(V\\) can be approximated by a function in \\(V_m\\). This is not really a surprising result: if the approximation \\(u_m\\) is unable to approximate the sample paths of \\(u\\) it is very unlikely it will do a good job with all functionals.\nThankfully, approximation error is one of the better studied things in this world. Especially in the case where \\(V = B\\).\nFor instance, it’s pretty easy to show69 that if \\(u\\) has \\(\\nu\\) derivatives, then \\(e_G \\leq Cm^{-\\frac{\\nu}{d} + \\epsilon}\\) for all \\(\\epsilon>0\\).\nIf you dive deep enough into the literature, you can get similar results for the type of approximation underneath the subset of regressors approximation.\nFor the SPDE approximation, it’s all a little bit more tricky as \\(V_m \\not \\subset V\\). But ultimately, you get that, for any \\(\\epsilon >0\\), \\(e_G \\leq C h^{1-\\epsilon}\\), where \\(h\\) is a measure of the mesh size70. This is roughly what you’d expect, there’s a loss of \\(\\epsilon\\) from the ordinary interpolation rate which may or may not be a result of me being a bit shit at maths.\nThe argument that gets us here is really cute so I’ll sketch it below. This is here for two reasons: firstly, because I think it’s cool and secondly because the paper is so compressed it’s hard to completely follow the argument, so I thought it would be nice to put on here. (It also took me a whole afternoon to decipher the proof in the paper, which is usually a sign that it could do with a bit of a re-write. How successfully I clarified it is something I will leave up to others to decide.)\n\nFinite element shit\nSetup. Gird yourselves!\nWe are going to bound that error rate in a way that’s relevant for the finite element method. The natural choices for the function spaces are \\(V = H^{1-\\epsilon}(\\Omega)\\) for some fixed \\(0 < \\epsilon < 1/2\\) (close to zero is what we want). and \\(B = L^2(\\Omega)\\). (To be honest the domain \\(\\Omega\\) isn’t changing so I’m gonna forget it sometimes.)\nOnce again, we’re going to assume that \\(L\\) is a second order uniformly elliptic PDE with no first-order terms (aka \\(b_1 = \\cdots = b_d = 0\\)) and that \\(b_0(s) >0\\) on some subset of \\(\\Omega\\). We will use the symmetric, coercive bilinear form associated71 with \\(L\\), which we can define, for any \\(u,v \\in H^1\\), as \\[\na(u, v) = \\int_\\Omega (A(s)\\nabla u(s))\\cdot \\nabla v(s)\\,ds + \\int_\\Omega b_0(s) u(s)v(s)\\,ds\n\\]\nRemembering that \\(R_m = LL_m^\\dagger\\), we have \\[\n\\sup_{v \\in V}\\frac{ \\left\\|v - R_m v\\right\\|_B}{\\|v\\|_V} =\\sup_{f\\in LV}\\frac{ \\left\\|L^{-1}f - L_n^{\\dagger}f\\right\\|_B}{\\|L^{-1}f\\|_V}.\n\\]\nThe set of functions \\(f \\in LV\\) is the set of all functions \\(f = Lv\\) for some \\(v \\in V\\). It can be shown that \\(LV = H^{-1-\\epsilon}\\), where the negative index indicates a dual Sobolev space (aka the space of continuous linear functionals on \\(H^{1+ \\epsilon}\\)).\nThis means that we are looking at the difference between the solution to \\(Lu = f\\) and \\(L_m u_m = f_m\\), where \\(f_m\\) is the \\(L^2\\)-orthogonal projection of \\(f\\) onto \\(V_m\\), which is the space of piecewise linear functions on some72 triangular mesh \\(\\mathcal{T}_m\\).\nWe define the projection of the function \\(f \\in H^{-1-\\epsilon}(\\Omega)\\) onto \\(V_m\\) as the unique function \\(f_m \\in V_m\\) such that73 \\[\n\\int_\\Omega f_n(s) v_n(s)\\,ds = \\int f(s) v_n(s)\\,ds, \\quad \\forall v_n \\in V_n.\n\\]\nNow let’s do this!\nWith all of this in place, we can actually do something. We want to bound \\[\n\\frac{\\|u - u_m\\|_{L^2}}{\\|u\\|_{H^{1+\\epsilon}}},\n\\] where74 \\(a(u, \\phi) = \\int_\\Omega f(s) \\phi(s)\\,ds\\) for all \\(\\phi \\in H^{1+\\epsilon}\\) and \\(a(u_m, \\phi_m) = \\int_\\Omega f(s) \\phi_m(s)\\,ds\\) for all \\(\\phi_m \\in V_m \\subset H^{1+\\epsilon}\\).\nThe key observation is that \\[\n\\int_\\Omega f(s) \\phi_m(s)\\,ds = \\int_\\Omega f_m(s) \\phi_m(s)\\,ds,\n\\] which suggests that \\(u_m(s)\\) is an approximation to two different problems!\nLet’s write this second problem down! We want to find \\(z^{(m)}\\) such that \\[\na({z}^{(m)}, \\phi) = \\int_\\Omega f_n(s) \\phi(s)\\,ds \\quad \\forall \\phi \\in H^{1} ,\n\\] where the \\(m\\) superscript indicates that it depends on \\(m\\) through it’s right hand side. The projection \\(f_n \\in L^2\\), which means that we are in the realm of usual PDEs and (assuming some regularity) \\(z^{(m)} \\in H^2\\).\nHence, we can write \\[\n\\|u - u_m\\|_{L^2}\\leq \\|u - z^{(m)}\\|_{L^2} + \\|z^{(m)} - u_m\\|_{L^2}.\n\\]\nWe can bound the second term almost immediately from standard finite element theory, which says that \\[\n\\|z^{(m)} - u_m\\|_{L^2} \\leq Ch^2 \\|f_n\\|_{L^2}.\n\\]\nTo estimate \\(\\|f_m\\|\\) we use the inverse estimates of Ben Belgacem and Brenner to show that, for any \\(v\\in L^2(\\Omega)\\), \\[\n\\int_\\Omega f_m(s) v(s) \\,ds = \\int_\\Omega f(s)v_m(s)  \\,ds\\leq\\|f\\|_{H^{-1-\\epsilon}}\\|v_m\\|_{H^{1+\\epsilon}} \\leq Ch^{-1-\\epsilon} \\|f\\|_{H^{-1-\\epsilon}} \\|v\\|_{L^2},\n\\] where \\(v_m\\) is the orthogonal projection of \\(v\\) onto \\(V_m\\).\nIf we set \\(v = f_m\\) in the above equation, we get \\(\\|f_m\\|_{L^2} \\leq Ch^{-1-\\epsilon} \\|f\\|_{H^{-1-\\epsilon}}\\), which combines with our previous estimate to give \\[\n\\|z^{(m)} - u_m\\|_{L^2} \\leq Ch^{1-\\epsilon} \\|f_n\\|_{L^2}.\n\\]\nFinally, to bound \\(\\|u - z^{(m)}\\|_{L^2}\\) we are going to use one of my75 favourite arguments. Fix \\(w \\in L^2\\) and let \\(W\\) be the solution of the dual equation \\(a(\\phi, W) = \\int_\\Omega \\phi(s)w(s)\\,ds\\). It then follows that, for any \\(v_m \\in V_m\\), \\[\\begin{align*}\n\\left|\\int_\\Omega (u(s) - z^{(m)}(s))w(s)\\,ds\\right| &= \\left|a(u - z^{(m)}, W)\\right| \\\\\n&= \\left|\\int_\\Omega (f(s) - f_m(s))W(s)\\,ds\\right|\\\\\n&= \\left|\\int_\\Omega (f(s) - f_m(s))(W(s) - v_m(s))\\,ds\\right|\\\\\n&\\leq\\left|\\int_\\Omega f(s)(W(s) - v_m(s))\\,ds\\right|+  \\left|\\int_\\Omega f_m(s)(W(s) - v_m(s))\\,ds\\right| \\\\\n&\\leq \\|f\\|_{H^{-1-\\epsilon}}\\|W - v_m\\|_{H^{1+\\epsilon}} + Ch^{-1-\\epsilon} \\|f\\|_{H^{-1-\\epsilon}} \\|W - v_m\\|_{L^2} \\\\\n&\\leq C \\|f\\|_{H^{-1-\\epsilon}} h^{-1 -\\epsilon}\\left(h^{1+\\epsilon}\\|W - v_m\\|_{H^{1+\\epsilon}} +  \\|W - v_m\\|_{L^2} \\right),\n\\end{align*}\\] where the first line uses the definition of \\(W\\); the second uses the definition of \\(u\\) and \\(z^{(m)}\\); the third uses the fact that \\((f - f_m) \\perp V_m\\) so subtracting off \\(v_m\\) doesn’t change anything; the fourth is the triangle inequality; the fifth is the Hölder inequality on the left and the estimate from half a screen up on the right; and the sixth line is clean up.\nBecause the above bound holds for any \\(v_m \\in V_m\\), we can choose the one that makes the bound the smallest. This leads to \\[\\begin{align*}\n\\left|\\int_\\Omega (u(s) - z^{(m)}(s))w(s)\\,ds\\right| &\\leq  C \\|f\\|_{H^{-1-\\epsilon}} h^{-1 -\\epsilon}\\inf_{v \\in V_m}\\left(h^{1+\\epsilon}\\|W - v_m\\|_{H^{1+\\epsilon}} +  \\|W - v_m\\|_{L^2} \\right) \\\\\n& \\leq C\\|f\\|_{H^{-1-\\epsilon}} h^{-1 -\\epsilon} h^2 \\|W\\|_{H^2}\\\\\n&\\leq C h^{1-\\epsilon} \\|w\\|_{L^2},\n\\end{align*}\\] where the last two inequalities are Theorem 14.4.2 from Brenner and Scott and a standard estimate of the solution to an elliptic PDE by it’s RHS.\nPutting this all together we get the result. Phew.\nThis whole argument was a journey, but I think it’s quite pretty. It’s clobbered together from a lot of sleepless nights and an argument inspired by strip-mining76 a Ridgeway Scott paper from 1976. Anyway, I think it’s nifty.\nWrapping it up\nSo. That was quite a lot. I enjoyed it, but I’m weird like that. This has mostly been me trying to remember what I did in 2015. Why? Because I felt like it.\nI also think that there’s some value in this way of thinking about Gaussian processes and it’s nice to show off some ways to use all of that weird shit in the last post.\nAll of these words can be boiled down to this take away:\n\nIf your finite dimensional GP \\(u_m\\) is linked to a GP \\(u\\) by some (potentially non-linear relationship) \\(u_m= R_m u\\), then the posterior error will be controlled by how well you can approximate a function \\(v\\) that could be a realisation of the GP by \\(R_m v\\).\n\nThis is a very intuitive result if you are already thinking of GP approximation as approximating a random function. But a lot of the literature takes a view that we are approximating a covariance matrix or a multivariate normal. This might be enough to approximate a maximum likelihood estimator, but it’s insufficient for approximating a posterior77\nFurthermore, because most of the constants in the bounds don’t depend too heavily on the specific finite dimensional approximation (except through \\(A_m\\)), we can roughly say that if we have two methods for approximating a GP, the one that does a better job at approximating functions will be the better choice.\nAs long as it was, this isn’t a complete discussion of the problem. We have not considered hyper-parameters! This is a little bit tricky because if \\(\\mu_0\\) depends on parameters \\(\\theta\\), then \\(R_m\\) will also depend on parameters (and for subset of regressors, \\(V_m\\) also depends on the parameters).\nIn theory, we could use this to bound the error in the posterior \\(p(\\theta \\mid y)\\). To see how we would do that, let’s consider the case where we have Gaussian observations.\nThen we get \\[\\begin{align*}\np(\\theta \\mid y) & \\frac{\\exp(-\\Phi(u;y))}{p(y)} \\left[\\frac{d\\mu_y}{d\\mu_0}\\right]^{-1} p(\\theta) \\\\\n&= \\frac{Z(\\theta) p(\\theta)}{\\int_\\Theta Z(\\theta)p(\\theta)\\,d\\theta},\n\\end{align*}\\] where \\(Z(\\theta) = \\mathbb{E}_{\\mu_0}\\left(e^{-\\Phi(u;y)}\\right)\\).\nWe could undoubtedly bound the error in this using similar techniques to the ones we’ve already covered (in fact, we’ve already got a bound on \\(|Z - Z_m|\\)). And then it would just be a matter of piecing it all together.\nBut I’m tired and I just want to cry for me.\n\nNaively: a condescending way to say “the way you were told to use them”↩︎\nIs it better to have a large amount of crappy data or a small amount of decent data? Depends on if you’re trying to impress people by being right or by being flashy.↩︎\nWho doesn’t love a good shape. Or my personal favourite: a point pattern.↩︎\nOr, hell, this is our information about how to query the Gaussian process to get the information we need for this observation. Because, again, this does not have to be as simple as evaluating the function at a point!↩︎\nThis could be time, space, space-time, covariate space, a function space, a lattice, a graph, an orthogonal frame, a manifold, a perversion, whatever. It doesn’t matter. It’s all just Gaussian processes. Don’t let people try to tell you this shit is fancy.↩︎\nThis could be covariate information, group information, hierarchy information, causal information, survey information, or really anything else you want it to be. Take a deep breath. Locate your inner peace. Add whatever you need to the model to make it go boop.↩︎\nI will never use this assumption. Think of it like the probability space at the top of a annals of stats paper.↩︎\nSo the thing is that this is here because it was funny to me when I wrote it, but real talk: just being like “it’s iid” is some real optimism (optimism, like hope, has no place in statistics.) and pretending that this is a light or inconsequential assumption is putting some bad energy out into the world. But that said, I was once a bit drunk at a bar with a subjective Bayesian (if you want to pick your drinking Bayesian, that’s not a bad choice. They’re all from The North) and he was screaming at me for thinking about what would happen if I had more data, and I was asking him quietly and politely how the data could possibly inform models as complex as he seemed to be proposing. And he said to me: what you do is you look for structures within your data that are exchangeable in some sense (probably after conditioning) and you use those as weak replicates. And, of course, I knew that but I’d never thought about it that way. Modelling, eh. Do it properly.↩︎\nThese (and the associated parenthetical girls) were supposed to be nested footnotes but Markdown is homophobic and doesn’t allow them. I am being oppressed.↩︎\nIt’s an interesting area, but the tooling isn’t there for people who don’t want to devote a year of their lives to this to experiment.↩︎\nThis is what matrix nerds say when they mean “I love you”. Or when they mean that it’s all derived from the structure of a matrix rather than from some structural principles stolen from the underlying problem. The matrix people are complicated.↩︎\nThe reason for this is that, while there are clever methods for getting determinants of H-matrices, they don’t actually scale all that well. So Geoga, Anitescu, and Stein paper use a Hutchinson estimator of the log-determinant. This has ok relative accuracy, but unfortunately, we need it to have excellent absolute accuracy to use it in a Bayesian procedure (believe me, I have tried). On the other hand, the Hutchinson estimator of the gradient of the log-determinant is pretty stable and gives a really nice approximate gradient. This is why MLE type methods for learning the hyper-parameters of a GP can be made scalable with H-matrix techniques.↩︎\nOtherwise, why bother. Just sub-sample and get on the beers. Or the bears. Or both. Whatever floats your boat.↩︎\non \\(u\\) and probably other parameters in the model↩︎\nDual spaces, y’all. This vector was inevitable because \\(m\\)-dimensional row vectors are the dual space of \\(\\mathbb{R}^m\\), while \\(s_i \\rightarrow u(s_i)\\) is in \\(B^*\\).↩︎\nThis is not surprising if you’re familiar with the sketching-type bounds that Yang, Pilanci and Wainwright did a while back (or, for that matter, with any non-asymptotic bounds involving the the complexity of the RKHS). Isn’t maths fun.↩︎\nHölder↩︎\nThink “infinitely differentiable but more so”.↩︎\nAn analytic function is one that you know will walk straight home from the pub, whereas a \\(\\nu\\)-differentiable function might just go around the corner, hop on grindr, and get in a uber. Like he’s not going to go to the other side of the city, but he might pop over to a nearby suburb. A generalised function texts you a photo of a doorway covered by a bin bag with a conveniently placed hole at 2am with no accompanying message other than an address↩︎\nI mean, I cannot be sure, but I’m pretty sure.↩︎\nAgain, not strictly necessary but it removes a tranche of really annoying technicalities and isn’t an enormous restriction in practice.↩︎\nThe result is that there is no non-trivial translation invariant measure on a separable Banach space (aka there is no analogue of the Lebesgue measure). You can prove this by using separability to make a disjoint cover of equally sized balls, realise that they would all have to have the same measure, and then say “Fuck. I’ve got too many balls”.↩︎\nBorel. Because we have assumed \\(B\\) is separable, the cylindrical \\(\\sigma\\)-algebra is identical to the Borel \\(\\sigma\\)-algebra and \\(\\mu_0\\) is a Radon measure. Party.↩︎\nSee Andrew Stuart’s long article on formulating Bayesian problems in this context and Masoumeh Dashti and Andrew Stuart’s paper paper on (simple) finite dimensional approximations.↩︎\nThe SPDE approach. Read on Macduff.↩︎\nthe Irish National Liberation Army↩︎\nThis covers GP models, GAMs, lots of spatial models, and a bunch of other stuff.↩︎\nLike, the data is a single observation of a point pattern. Or, to put it a different way, a list of (x,y) coordinates of (a priori) unknown length.↩︎\nApproximate Markovian GPs in 2-4 dimensions. See here for some info↩︎\nRue. The king of INLA. Another all round fabulous person. And a person foolish enough to hire me twice even though I was very very useless.↩︎\nIn the interest of accuracy, Janine and I were giving back to back talks at a conference that we decided for some reason to give as a joint talk and I remember her getting more and more agitated as I was sitting in the back row of the conference desperately trying to contort the innards of INLA to the form I needed to make the damn thing work. It worked and we had results to present. We also used the INLA software in any number of ways it had not been used before that conference. The talk was pretty well received and I was very relieved. It was also my first real data analysis and I didn’t know to do things like “look at the data” to check assumptions, so it was a bit of a clusterfuck and again Janine was very patient. I was a very useless 25 year old and a truly shit statistician. But we get better if we practice and now I’m a perfectly ok statistician.↩︎\nJanine and I, with Finn Lindgren, Sigrunn Sørbye and Håvard Rue, who were all heavily involved throughout but I’m sure I’ve already exhausted people’s patience.↩︎\nIIRC, Sigrunn’s university has one of those stupid lists where venue matters more than quality. Australia is also obsessed with this. It’s dumb.↩︎\nIn hindsight, the reviewer was asking for a simulation study, which is a perfectly reasonable thing to ask for but at the time I couldn’t work out how to do that because, in my naive numerical analyst ways, I thought we would need to compare our answer to a ground truth and I didn’t know how to do that. Now I know that the statistician way is to compute the same thing two different ways on exactly one problem that’s chosen pretty carefully and saying “it looks similar”.↩︎\nConditional on the log-intensity surface, a LGCP is a Poisson process↩︎\nis it, though↩︎\nMy co-authors are all very patient.↩︎\nwith fixed hyper-parameters↩︎\nThe thing about inverse problems is that they assume \\(\\Phi(u;y)\\) is the solution of some PDE or integral equation, so they don’t make any convenient simplifying assumptions that make their results inapplicable to LGCPs!↩︎\nhttps://arxiv.org/pdf/0901.1342.pdf↩︎\nstar↩︎\nAlso weak convergence but metrized by the Wasserstein-1 distance.↩︎\nFernique’s Theorem. I am using “we” very liberally here. Fernique knew and said so in French a while back. Probably the Soviet probabilists knew too but, like, I’m not going to write a history of exponential moments.↩︎\nOn \\(L^2\\), which is a Hilbert space so the basis really is countable. The result is a shit-tonne easier to parse if we make \\(B\\) a separable Hilbert space but I’m feeling perverse. If you want the most gloriously psychotic expression of this theorem, check out Theorem 7.3 here↩︎\nThere are tonnes of examples where people do actually use the Karhunen-Loève basis or some other orthogonal basis expansion. Obviously all of this theory holds over there.↩︎\nThis has many names throughout the literature. I cannae be arsed listing them. But Quiñonero-Candela, Rasmussen, and Williams attribute it to Wahba’s book in 1990.↩︎\nFunctions of the form \\(\\sum_{i=1}^m a_j r_u(\\cdot, s_j)\\) are in the RKHS corresponding to covariance function \\(r_u\\). In fact, you can characterise the whole space as limits of sums that look like that.↩︎\nI mean, we are not going to be using the \\(A_m\\) to do anything except grow with \\(m\\), so the specifics aren’t super important. Because this is a blog post.↩︎\nNot every. You do this for nice sets. See Rozanov’s book on Markov random fields if you care.↩︎\n\\(d(s, A) = \\inf_{s'\\in A} \\|s - s'\\|\\)↩︎\nRozanov↩︎\nThe sets on which they are non-zero are different↩︎\nFor general Markov random fields, this representation still exists, but \\(L\\) is no longer a differential operator (although \\(L^*L\\) must be!). All of the stuff below follows, probably with some amount of hard work to get the theory right.↩︎\nWhat is white noise? It is emphatically not a stochastic process that has the delta function as it’s covariance function. That thing is just ugly. In order to make any of this work, we need to be able to integrate deterministic functions with respect to white noise. Hence, we view it as an independently scattered random measure that satisfies \\(W(A) \\sim N(0, |A|)\\) and \\(\\int_A f(s)W(ds) \\sim N(0, \\int_A f(s)^2\\, ds)\\). Section 5.2 of Adler and Taylor’s book Random Fields and Geometry is one place to learn more.↩︎\nThis paper is a solid review↩︎\nThis paper↩︎\nFinite element methods had been used before, especially in the splines community, with people like Tim Ramsay doing some interesting work. The key insight of Finn’s paper was to link this all to corresponding infinite dimensional Gaussian processes.↩︎\nWe’re assuming \\(V_m\\subset L^2(\\Omega)\\), which is not a big deal.↩︎\nSee the paper for details of exactly which pseudo-inverse. It doesn’t really matter tbh, it’s just we’ve got to do something with the other degrees of freedom.↩︎\nConsult your favourite finite element book and then get pissed off it doesn’t cover higher-order PDEs in any detail.↩︎\nIt looks like this is vital, but it isn’t. The main thing that changes if your PDE is hyperbolic or parabolic or hypo-elliptic is how you do the discretisation. As long as the PDE is linear, this whole thing works in principle.↩︎\nFor some \\(\\alpha>0\\), \\(\\sum_{i,j=1}^d w_iw_ja_{ij}(s) \\geq \\alpha \\sum_{i=1}^d w_i^2\\) holds for all \\(s \\in \\Omega\\).↩︎\nFor this construction to work in higher dimensions, you need to use a higher-order differential operator. In particular, if you want a continuous field on some subset of \\(\\mathbb{R}^d\\), you need \\(L\\) to be a differential operator of order \\(>d/2\\) or higher. So in 4 dimensions, we need the highest order derivative to be at least 4th order (technically \\(L\\) could be the square root of a 6th order operator, but that gets hairy).↩︎\nIt holds in general, but if the linear terms are dominant (a so-called advection-driven diffusion), then you will need a different numerical method to get a stable estimate.↩︎\nModulo some smoothness requirements on \\(\\Omega\\) and \\(a_{ij}(s)\\).↩︎\nIt’s very easy to model weird anisotropies and to work on manifolds↩︎\neg this comparison↩︎\nLet’s not let any of the data fly off to infinity!↩︎\nCorollary A2 in the paper we’re following↩︎\nThink of it as the triangle diameter if you want.↩︎\nIntegration by parts gives us \\(\\int_\\Omega (Lu(s))v(s)\\,ds = a(u,v)\\) if everything is smooth enough. We do this to confuse people and because it makes all of the maths work.↩︎\nnot weird↩︎\n\\(f\\) is a generalised function so we are interpreting the integrals as duality pairings. This makes sense because \\(V_m \\subset H^{1+\\epsilon}\\) if we allow for a mesh-dependent embedding constant (this is why we don’t use \\(B = H^{1+\\epsilon}\\))↩︎\nThis is how fancy people define solutions to PDEs. We’re fancy.↩︎\nAlso everyone else’s, but it’s so elegantly deployed here. This is what I stole from Scott 1976)↩︎\nReal talk. I can sorta see where this argument is in the Scott paper, but I must’ve been really in the pocket when I wrote this because phew it is not an obvious transposition.↩︎\nUnless the approximation is very, very good. If we want to be pedantic, we’re approximating everything by floating point arithmetic. But we’re usually doing a good job.↩︎\n",
    "preview": {},
    "last_modified": "2022-02-10T23:27:13+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/",
    "title": "Yes but what is a Gaussian process? or, Once, twice, three times a definition; or A descent into madness",
    "description": "Gaussian processes. As narrated by an increasingly deranged man during a day of torrential rain.",
    "author": [
      {
        "name": "Dan Simpson",
        "url": "https://dpsimpson.github.io"
      }
    ],
    "date": "2021-11-03",
    "categories": [],
    "contents": "\nI guess I’m going to talk about Gaussian processes now. This wasn’t\nthe plan but who really expected a) there to be a plan or b) me to stick\nto the plan. I feel like writing about Gaussian processes and so I\nshall! It will be grand.\nWhat is a Gaussian process?\nWell I could tell you that a Gaussian process is defined by its joint\ndistribution \\[\nu \\sim N(\\mu, \\Sigma),\n\\] where \\(u_i = u(s_i)\\), \\(\\mu_i = \\mu(s_i)\\) and \\(\\Sigma_{ij} = c(s_i, s_j)\\) for some\npositive definite covariance (or kernel) function \\(c(\\cdot, \\cdot)\\).\nBut that would be about as useful as presenting you with a dog that\ncan bark “she’s a grand old flag”: perhaps good enough for a novelty\nhit, but there’s just no longevity in it.\nTo understand a Gaussian process you need to feel it deep\ndown within you where the fear and the detailed mathematical concepts\nlive.\nSo let’s try again.\nWe’re\ngonna have a … you know what. I’m not gonna do that. But I am going to\ndefine this stuff three times. Once for mum, once for dad, and once for\nthe country.\nYou’ve got to wonder why anyone would introduce something three ways.\nThere are some reasons. The first is, of course, that each definition\ngives you a different insight into different aspects of Gaussian\nprocesses (the operational, the boundless generality, the functional).\nAnd the second is because I’ve had to use all three of these ideas (and\nseveral more) over the years in order to understand how Gaussian\nprocesses work.\nI learnt about GPs from several sources (listed not in order):\nA\nSwede1 (so I will rant about random fields\nin the footnotes eventually);\nA book2 that was introducing GPs in a very\ngeneral way because they needed the concept in outrageous generality to\nanswer questions about the distribution of the maximum of a Gaussian\nprocess;\nA book3 written by a Russian who’s really\nonly into measure theory and doesn’t believe anything is real if it\nisn’t at least happening on a Frechet space;\nAnd a book4 by a different Russian who’s really\nonly into generalised Markov properties and needed to work with Gaussian\nprocesses that are defined over functions.\nOf these, the most relevant is probably the first one. I was\nprimarily taught this stuff by Finn\nLindgren, who had the misfortune of having the office next to mine\nwhen we worked together in Trondheim a very long time ago. (We both had\na lot more hair then.)\nOne of the things that I learnt from him is that Gaussian processes\ncan appear in all kinds of contexts, which means you need to understand\nthem as a model for an unknown function rather than as a tool\nto be used in a specific context (like for Gaussian process regression\nor Gaussian process classification).\nIt’s some effort to really get a good grip on the whole “Gaussian\nprocesses as a model for an unknown function” thing but once you relax\ninto it5, it stops being alarming to see\nmodels where you are observing things that aren’t just \\(u(s_k)\\). It is not alarming when you are\nobserving integrals of the GP over regions, or derivatives. And you (or\nyour methods) don’t fall apart when presented with complex non-linear\nfunctions on the GP (as happens if you look at\nBayesian inverse problems literature6).\nWhat is a Gaussian\nprocess? (Version 1)\nI’m going to start with the most common definition of a Gaussian\nprocess7. This is the definition that was\nalluded to in the first section and it’s also the definition\noperationalised in books like Rasmussen and Williams’8,\nwhich is a bread and butter reference for most machine learners\ninterested in GPs, use.\nThe idea is pretty straightforward: I need to define a stochastic\nmodel for an unknown function \\(u(s)\\)\nand I want it to be, in some sense, Gaussian. So how do I go about doing\nthis?\nFirstly, I probably don’t care too much about the function as an\nabstract object. For example, if I’m using the Gaussian process to model\nsomething like temperature, I am only going to observe it at a fairly\nsmall number of places (even though I could choose any set of places I\nwant). This means that for some arbitrary set set of \\(K\\) locations \\(s_1, s_2, \\ldots, s_K\\), I am most\ninterested9 in understanding the joint\ndistribution10\\[\n(u(s_1), \\dots, u(s_K))^T.\n\\]\nSo how would we model the joint distribution? If we want the model to\nbe tractable, we probably want a nice distribution. This is where the\nGaussian part comes in. The Gaussian distribution is an\nextremely tractable11 distribution in medium-to-high\ndimensions. So the choice to model our joint distribution (which could\nbe any size \\(K\\)) as \\[\n(u(s_1), \\dots, u(s_K))^T \\sim N\\left(\\mu_{s_1, \\ldots, s_K},\n\\Sigma_{s_1, \\ldots, s_K}\\right),\n\\] makes sense from a purely mercenary position12.\nSo how do we choose the mean and the covariance function? We will see\nthat the mean can be selected as \\([\\mu_{s_1,\n\\ldots, s_K}]_{k} = \\mu(s_k)\\) for pretty much any function13 \\(\\mu(\\cdot)\\), but, when we come to write\n\\[\n[\\Sigma_{s_1, \\ldots, s_K}]_{ij} = c(s_i, s_j),\n\\] there will be some very strong restrictions on the\ncovariance function \\(c(\\cdot,\n\\cdot)\\).\nSo where do these restrictions come from?\nOh those (gay) Russians!\nAs with all things in probability, all the good shit comes from the\nSoviets. Kolmogorov14 was a leading light in the Soviet\npush to formalise probability and one of his many many many\ncontributions is something called the Kolmogorov extension\ntheorem, which gives the exact conditions under which we can go\nfrom declaring that the distributions of \\((u(s_1), \\ldots, u(s_K))^T\\) (these are\ncalled finite dimensional distributions) are Gaussian to describing a\nlegitimate random function \\(u(s)\\).\nThere are essentially two conditions:\nThe order of the observations doesn’t matter in a material way. In\nour case changing the order just permutes the rows and columns of the\nmean vector and covariance matrix, which is perfectly ok.\nThere is a consistent way to map between the distributions of \\((u(s_1), \\ldots, u(s_K), u(s_{K+1}))^T\\)\nand \\((u(s_1), \\ldots, u(s_K))^T\\).\nThis is the condition that puts a strong restriction on the covariance\nfunction.\nEssentially, we need to make sure that we have a consistent way to\nadd rows and columns to our covariance matrix while ensuring that stays\npositive definite (that is, while all of the eigenvalues stay\nnon-negative, which is the condition required for a multivariate normal\ndistribution15). The condition—which is really\ngross—is that for every positive integer \\(K\\) and every set of points \\(s_1, \\ldots, s_k\\), and for every \\(a_1, \\ldots, a_K\\) not all equal to zero,\nwe require that \\[\n\\sum_{i=1}^K \\sum_{j = 1}^K a_ia_j c(s_i, s_j) \\geq 0.\n\\]\nThis condition is obviously very difficult to check. This is why\npeople typically choose their covariance function from a very short\nlist16 that is typically found in a book\non Gaussian processes.\nBut Kolmogorov said a\nlittle bit more\nThere’s a weird thing in grad school in North America where they\ninsist on teaching measure theoretic probability theory and then never\never ever ever ever using any of the subtleties. But Gaussian processes\n(and, in general, stochastic processes on uncountable index spaces) are\na great example of when you need these details.\nWhy? Because unlike discrete probability (where the set of events\nthat we can compute the probability of is obvious) or even continuous\nrandom variables (where the events that we can’t compute the probability\nof are so weird we can truly just ignore them unless we are doing\nsomething truly exotic), for Gaussian processes,17\nthe set of allowable events is considerably smaller than the set of all\nthings you might want probabilities of.\nThe gist of it is that we have built up a random function \\(u(s)\\) from a bunch of finite random\nvectors. This means that we can only assign probabilities to events that\ncan be built up from events on finite random vectors. The resulting set\nof events (or \\(\\sigma\\)-algebra to use\nthe adult term) is called the cylindrical18\n\\(\\sigma\\)-algebra and can be roughly19 thought of as the set of all events\nthat can be evaluated by evaluating \\(u(\\cdot)\\) at most a countable number of\ntimes.\nThings that aren’t measurable\nThis will potentially become a problem if, for instance, you are\nworking with a Gaussian process in a model that uses a Gaussian process\nin a weird way. When this happens, it is not guaranteed that,\nfor instance, your likelihood is a measurable function, which would mean\nthat you can’t normalise your probability distribution! (I mean, don’t\nworry. Unless you’re doing something fairly wild it will be, but it has\ncome up especially in the inverse problems literature!)\nThis limited set of measurable events even seems to preclude well\nstudied “events” like “\\(u\\) is\ncontinuous” or “\\(u\\) is twice\ncontinuously differentiable” or “\\(u\\)\nhas a finite supremum”. All things that we a) want to know about a\nGaussian process and b) things people frequently say about Gaussian\nprocesses. It is common for people to say that “Brownian motion is\ncontinuous” and similar things.\nAs with all of mathematics, there are a lot of work arounds that we\ncan use. For those three statements in particular, there is some really\nelegant mathematical work (due, again, to Kolmogorov and extended\ngreatly by others). The idea is that we can build another function \\(\\tilde u(s)\\) such that \\(\\Pr(u(s) = \\tilde u(s)) = 1\\) for\nall20 \\(s\\) such that \\(\\tilde u(s)\\) is continuous (or\ndifferentiable or bounded).\nIn the language of stochastic processes, \\(\\tilde u(s)\\) is called a version\nof \\(u(s)\\) and the more correct,\ntemperate language (aka the one least likely to find in the literature)\nis that \\(u(s)\\) has a\ncontinuous/differentiable/bounded version.\nIf you’re interested in seeing how a differentiable version of a\nGaussian process is constructed, you basically have to dick around with\ndyads for a while. Martin Hairer’s lecture notes21\nis a nice clear example.\nWhere are the\nlimitations of this definition?\nThere are a few. These are, of course, in the eye of the beer holder.\nThe definition is workable in a lot of situations and, with some\nexplanation can be broadened out a bit more. It’s less of a great\ndefinition when you’re trying to manipulate Gaussian processes as\nmathematical objects, but that’s what the next one is for.\nThe first limitation is maybe not so much a limit of the definition\nas a bit of work you have to do to make it applicable. And that is: what\nhappens if I am observing (or my likelihood depends on) averages like\n\\[\n\\left(\\int_S \\ell_1(s) u(s)\\,ds, \\ldots, \\int_S \\ell_K(s)\nu(s)\\,ds\\right)^T\n\\] instead of simple point evaluations22.\nThis might seem like a massively different problem, until we remember\nthat integrals are just sums dressed up for Halloween, so we can\napproximate the integrals arbitrarily well by sums23.\nIn fact, if we squint24 a bit, we can see that the above\nvector will also be multivariate Gaussian with mean vector \\[\n[\\mu]_k = \\int_S \\ell_k(s) \\mu(s)\\,ds\n\\] and covariance matrix with entries \\[\n[\\Sigma]_{ij} = \\int_{S \\times S} \\ell_i(s) \\ell_j(s')c(s,\ns')\\,dsds'.\n\\] Similar formulas hold for derivative observations.\nProbably the bigger limitation is that in this way of seeing things,\nyour view is tied very tightly to the covariance function. While it is a\nnatural object for defining Gaussian processes, it is fucking\ninconvenient if you want to understand things like how well approximate\nGaussian processes work.\nAnd let’s face it, a big chunk of Gaussian processes we see in\npractice are approximate because the computational burden on large data\nsets is too big to do anything but approximate.\n(Fun fact, when I was much much younger I wrote a paper that was a\nbetter title than a paper25 called26\nIn\norder to make spatial statistics computationally feasible, we need to\nabandon the covariance function. I copped a lot of shit for it\nat the time [partly because the title was better than the paper, but\npartly because some people are dicks], but I think the subsequent 10\nyears largely proved me (or at least my title) right27.)\nThe focus on the covariance function also hides the strong similarity\nbetween Gaussian process literature and the smoothing splines literature\nstarting from Grace Wahba in the 1970s. It’s not that nobody notices\nthis, but it’s work to get there!\nIn a similar way, it hides the fundamental role the reproducing\nkernel Hilbert space (or Cameron-Martin space) is doing and the\nways that Gaussian process regression is (and is not) like kernel\nsmoothing in RKHSs. This, again, isn’t a secret per se—you can\nfind this information if you want it—but it’s confusing to people and\nthe lack of clarity leads to people missing useful connections (or\nsometimes leads to them drawing mistaken parallels).\nHow many times have you seen someone say that realisations of a\nGaussian process are in the RKHS associated with the covariance\nfunction? They are not. In fact, every realisation of a Gaussian process\nis rougher than any function in the RKHS (with probability 1)!\nUnfortunately, this means that your reason for choosing the kernel in a\nRKHS regression and for choosing the covariance function in a Gaussian\nprocess prior need to be subtly different. Or, to put it differently, a\npenalty is not a log-prior and interpreting the maximum a penalised\nlikelihood is, in high dimensions, a very distant activity from\ninterpreting a posterior distribution (even when the penalty is the log\nof the prior).\nWhat is a Gaussian\nprocess? (Version 2)\nOk. Let’s do this again. This definition lives in a considerably more\nmathematical space and while I’m gonna try to explain the key terms, I\nwill fail. But hey. Who doesn’t like googling weird terms?\nA Gaussian process is a collection of random variables \\(u(s)\\), where \\(s\n\\in S\\) and \\(S\\) is some set of\nthings that isn’t too topologically disastrous28.\nBut what makes it Gaussian? Here’s the general definition.\n\nA stochastic process/random field is Gaussian if and only if\nevery continuous linear functional has a univariate Gaussian\ndistribution.\n\nWell\nthat’s very useful Daniel. What the hell is a linear functional?\nGreat question angry man who lives inside my head! It is any function\n\\(\\ell(\\cdot)\\) that takes the Gaussian\nprocess \\(u(s)\\) and an input and spits\nout a real number that is is\nLinear. Aka \\(\\alpha \\ell(u) +\n\\beta\\ell(v) = \\ell(\\alpha u + \\beta v)\\)\nBounded29.\nGreat. Love a definition. Shall we try something more\nconcrete?\nPoint evaluation \\(u(s_j)\\) (aka\nevaluating the function at a point) is a linear functional (\\((u + v)(s)_j = u(s_j) + v(s_j)\\)). As is a\ndefinite integral over a set \\(\\int_A\nu(s)\\,ds\\).\nIt’s a fun little exercise to convince yourself that this all implies\nthat for any collection \\(\\ell_1(\\cdot),\n\\ldots, \\ell_J(\\cdot)\\) of continuous linear functionals, then\n\\(u(s)\\) is a Gaussian process means\nthat the vector \\[\n(\\ell_1(u), \\ldots \\ell_J(u))^T\n\\] is multivariate Gaussian.\nYour idea of fun is not my idea of fun. Anyway. Keep\ntalking.\nIf \\(u\\) lives in a Banach space30 \\(B\\), then the set of all continuous/bounded\nlinear functionals on \\(B\\) is called\nthe dual space and is denoted \\(B^*\\).\n\n\n\n\n\n\n\n\n\nI\nmean, cool I guess but where the merry hell is the covariance\nfunction\nIn this context, the most important thing about \\(B^*\\) is it does double duty: it is both a\nspace of linear functionals and a space that can be identified\nwith random variables.\nHow the fuck do you do that?\nWell, the trick is to remember the definition! If \\(\\ell \\in B^*\\), then \\(\\ell(u)\\) is a Gaussian. Similarly, if we\nhave two functionals \\(\\ell, \\ell' \\in\nB^*\\) we consider the covariance of their associated random\nvariables \\[\nC_u(\\ell, \\ell') = \\mathbb{E}(\\ell(u)\\ell'(u)).\n\\]\n\\(C_u(\\ell, \\ell')\\) is a\nsymmetric, positive definite bilinear form (aka good candidate for an\ninner product)!\nWe can use this to add more functions to \\(B^*\\), particularly for any sequence \\(b_n \\in B^*\\) that is Cauchy with respect\nto the norm \\(\\|\\ell\\|_{R_u} = \\sqrt{C_u(\\ell,\n\\ell)}\\) we append the limit to \\(B^*\\) to complete the space. Once we take\nequivalence classes, we end up with a Hilbert space \\(R_u\\) that, very unfortunately,\nprobabilists have a tendency to call the reproducing kernel Hilbert\nspace associated with \\(u\\).\nWhy is this unfortunate? Well primarily because it’s not the exact\nsame space that machine learners call the reproducing kernel Hilbert\nspace, which is, to put it mildly, confusing. But we can build the\nmachine learner’s RKHS (known to probabilists as the Cameron-Martin\nspace).\nWhy are you even telling me this? Is this a digression?\nHonestly. Yes. But regardless the space \\(R_u\\) is quite useful to understand what’s\ngoing on. To start off, let’s do one example that shows just how\ndifferent a Gaussian process is from a multivariate normal random\nvector. We will show that if we multiply a GP by a constant, we\ncompletely change its support31! Many a computational\nand inferential ship have come to grief on these sharp rocks.\nTo do this, though, we need32 to make an assumption\non \\(B\\): We assume that \\(B\\) is separable33.\nThis isn’t an vacuous assumption, but in a lot of cases of practical\ninterest, this is basically the same thing as assuming the set \\(S\\) is a nice bounded domain or a friendly\ncompact manifold (and not something like \\(\\mathbb{R}^d\\))34.\nSo. How do we use \\(R_u\\) to show\nthat Gaussian processes are evil? Well we begin by noting that \\(R_u\\) is a separable35\nHilbert space it contains an orthonormal basis \\(e_n\\), \\(n=1,\n\\ldots, \\infty\\) (that is \\(\\|e_n\\|_{R_u} = 1\\) and \\(\\langle e_n, e_m\\rangle_{R_u} = 0\\) if\n\\(n\\neq m\\)). We can use this basis to\nshow some really really weird stuff about \\(u(s)\\).\nIn particular, consider another Gaussian process \\(v(s) = c u(s)\\), where \\(c\\) is a non-zero constant. For this\nprocess we can build \\(R_v\\) in an\nanalogous way. The \\(e_n\\) are still\northogonal in \\(R_v\\) but now \\(\\|e_n\\|_{R_v} = c^2\\).\nNow consider the functional \\(X_K(\\cdot) =\nK^{-1}\\sum_{k = 1}^Ke_i(\\cdot)^2\\). We are going to use this\nfunction to break stuff! To do this, we are going to define two disjoint\nsets of functions \\(A_1 = \\{u:\n\\lim_{K\\rightarrow \\infty} X_K(u) = 1\\}\\) and \\(A_2 = \\{u: \\lim_{K\\rightarrow \\infty} X_K(u) =\nc^2\\}\\). Clearly \\(A_1\\) and\n\\(A_2\\) are disjoint if \\(|c|\\neq 1\\).\nBecause \\(e_n(\\cdot)\\) are\northonormal in \\(R_u\\), it follows that\nthat \\(u_n = e_n(u) \\sim N(0,1)\\) are\niid. Similarly, \\(v_n = e_n(v) \\sim N(0,\nc^2)\\) are also independent. Hence it follows from the properties\nof \\(\\chi^2\\) random variables (aka the\nmean plus the strong law of large numbers) that \\(X_K(u) \\rightarrow 1\\) and hence \\(\\Pr(u \\in A_1) = 1\\). On the other hand,\n\\(X_K(v) \\rightarrow c^2\\), so \\(\\Pr(v \\in A_2) = 1\\). As \\(A_1\\) and \\(A_2\\) are disjoint, this means that unless\n\\(|c|=1\\), the processes \\(u\\) and \\(v\\) are mutually singular (aka they have no\noverlapping support).\nWhat does this mean? This means the distributions of \\(u\\) and \\(v\\) (which remember is just \\(u\\) multiplied by a constant) are as\ndifferent from each other as a normal distribution truncated to \\((-\\infty, 1)\\) and another normal\ndistribution truncated to \\((1,\n\\infty)\\)! Or, more realistically36,\nas disjoint as a distribution over \\(2\\mathbb{Z}\\) and \\((2\\mathbb{Z} - 1)\\).\nThis is an example of the most annoying phenomena in Gaussian\nprocesses37: the slightest change in a Gaussian\nprocess can lead to a mutually singular process. In fact, this is not a\nparticularly strange example. It can be shown that Gaussian processes\nover uncountable index spaces are either absolutely continuous or\nmutually singular. There is no half-arsing it!\nThis has a lot of implications when it comes to computing38, setting priors on the parameters\nthat control the properties of the covariance function39,\nand just generally inference40.\nYes but\nwhere’s our reproducing kernel Hilbert space\nWe just saw that if \\(u\\) is a\nGaussian process than \\(c u\\) will be a\nsingular GP if \\(|c| \\neq 1\\). What\nhappens if we add things? Well, a result known as the Cameron-Martin\ntheorem says that, for a deterministic \\(h(s)\n\\in B\\), \\(u(s) + h(s)\\) is\nabsolutely continuous wrt \\(u(s)\\) if\nand only if \\(h(s)\\) is in the\nCameron-Martin space \\(H_u\\) (this\nis the one that machine learners call the RKHS!).\nBut how do we find this mythical space? I find this quite\nstressful!\nLike, honey I do not know. But when a probabilist is in distress, we\ncan calm them by screaming characteristic function at the top\nof our lungs right into their ear. Try it. It definitely works. You\nwon’t be arrested.\nSo let’s do that. The characteristic function of a univariate random\nvariable \\(X\\) is \\[\n\\phi_X(t) = \\mathbb{E}\\left(e^{itX}\\right),\n\\] which doesn’t seem like it’s going to be an amazingly\nuseful thing, but it actually is. It’s how you prove the central limit\ntheorem41, and a few other shiny things.\nWhen we are dealing with more complex random things, like random\nvectors and Gaussian processes, we can use characteristic functions, but\nwe need to extend beyond the fact that they’re currently only defined\nfor univariate random variables. Conveniently, we have some lying\naround. In particular, if \\(\\ell \\in\nB^*\\), we have the associated random variable \\(\\ell(u)\\) and we can compute its\ncharacteristic function42, which leads to the definition of a\ncharacteristic function of a stochastic process on \\(B\\) \\[\n\\phi_u(\\ell) = \\mathbb{E}(e^{i\\ell(u)}), \\quad \\ell \\in B^*.\n\\]\nNow this feels quite different. It’s no longer a function of\nsome real number \\(t\\) but is instead a\nfunction of a linear functional \\(\\ell\\), which feels weird but isn’t.\nCharacteristic functions are immensely useful because if two Gaussian\nprocesses have same characteristic function they have the same\ndistribution43.\nBecause \\(u(s)\\) is a Gaussian\nprocess, we can compute its characteristic function! We know that \\(\\ell(u)\\) is Gaussian so we can look up its\ncharacteristic function on Wikipedia and get that \\[\n\\mathbb{E}(e^{i\\ell(u)}) = \\exp\\left[{i \\mu(\\ell) -\n\\frac{\\sigma^2(\\ell)}{2}}\\right],\n\\] where \\(\\mu(\\ell) =\n\\mathbb{E}(\\ell(u))\\) and \\(\\sigma^2(\\ell) = \\mathbb{E}(\\ell(u) -\n\\mu(\\ell))^2\\).\nWe know that \\[\n\\mu(\\ell) = \\mathbb{E}(\\ell(u))\n\\] and \\[\n\\sigma^2(\\ell) = \\mathbb{E}\\left[(\\ell(u) - \\mu(\\ell)^2\\right],\n\\] the latter of which can be extended naturally to the\naforementioned positive definite quadratic form \\[\nC_u(\\ell, \\ell') = \\mathbb{E}\\left[(\\ell(u) - \\mu(\\ell)(\\ell'(u)\n- \\mu(\\ell'))\\right], \\quad \\ell, \\ell' \\in B^*.\n\\]\nThis leads to the exact form of the characteristic function and to\nthis theorem, which is true.\n\nTheorem: A stochastic process \\(u(\\cdot)\\) is a Gaussian process if and\nonly if \\[\n\\phi_u(\\ell) = \\exp\\left[i\\mu(\\ell) - \\frac{1}{2}C_u(\\ell, \\ell)\\right].\n\\]\n\nSo Alf is back. In pog form.\nYes.\nIn this case, we can define the covariance operator \\(C_u: B^* \\rightarrow B\\) as44\n\\[\n(C_u \\ell) (\\ell') = \\mathbb{E}\\left[(\\ell(u) -\n\\mu(\\ell)(\\ell'(u) - \\mu(\\ell'))\\right].\n\\] The definition is cleaner when \\(\\mu(\\ell) = 0\\) (which is why people tend\nto assume that when writing this shit down45),\nin which case we get \\[\nC_u\\ell = \\mathbb{E}(u\\ell(u))\n\\] and \\[\nC_u(\\ell, \\ell') = \\ell'(C_u\\ell)\n\\]\nGreat gowns, beautiful gowns.\nWow. Shady.\nAnyway, the whole reason to introduce this is the following:\n\nTheorem: Let \\(v = x +\nh\\). Then \\[\n\\phi_v(\\ell) = e^{i\\ell(h)}\\phi_u(\\ell)\n\\]\n\nThis does not not help us answer the question of whether or not \\(v\\) has the same support as \\(u\\). To do this, we construct a variable\nthat is absolutely continuous with respect to \\(u\\) (we guarantee this because we specify\nits density46 wrt \\(u\\)).\nTo this end, take some \\(g \\in R_u\\)\nand define a stochastic process \\(w\\)\nwith density wrt47 u \\[\n\\rho(u) = \\exp\\left[iC_u(g, u) - \\frac{1}{2}C_u(g,g)\\right].\n\\]\nFrom this, we can compute48 the characteristic\nfunction of \\(w\\) \\[\\begin{align*}\n\\phi_w(\\ell) &= \\mathbb{E}_w\\left(e^{i\\ell(w)}\\right) \\\\\n&= \\mathbb{E}_u\\left(\\rho(u) e^{i\\ell(u)}\\right) \\\\\n&= \\exp\\left[iC_u(g,\\ell) + i \\mu(\\ell)  - \\frac{1}{2}C_u(\\ell,\n\\ell)\\right]\n\\end{align*}\\]\nSo we are fine if we can find some \\(h \\in\nB\\) such that \\[\nC_u(g, \\ell) = \\ell(h).\n\\] To do this, we note that \\[\nC_u(g, \\ell) = \\ell(C_u g),\n\\] so for any \\(g\\) we can find\na \\(h \\in B\\) such that \\(h = C_ug\\) and for such a \\(h\\) \\(v(s) = u(s)\n+ h(s)\\) is absolutely continuous with respect to \\(u(s)\\).\nThis gives us our definition of the Cameron-Martin space (aka the\nRKHS) associated with \\(u\\).\n\nDefinition:  The Cameron-Martin space (or\nreproducing kernel Hilbert space, if you must) associated with a\nGaussian process \\(u\\) is the Hilbert\nspace \\(H_u = \\{h\\in B: h = C_uh^* \\text{ for\nsome } h^* \\in R_u\\}\\) equipped with the inner product \\[\n\\langle h, h'\\rangle_{H_u} = C_u(h^*, (h')^*)\n\\]\n\nA fun note is that the reason the probabilists don’t call\nthe Cameron-Martin space the reproducing kernel Hilbert space is that\nthere is no earthly reason to think that point evaluation will be\nbounded in general. So it become a problematique name. (And no, I don’t\nknow why they’re ok with calling \\(R_u\\) that some things are just\nmysterious.)\nLord in heaven. Any chance of being a bit more concrete?\nSure! Let’s consider the case where \\(u \\in\n\\mathbb{R}^n\\) is a Gaussian random vector \\[\nu \\sim N(\\mu, \\Sigma).\n\\] While all of this is horribly over-powered for this case, it\ndoes help get a grip on what the inner product on \\(H_u\\) is.\nIn this case, \\(B^*\\) is row vectors\nlike \\(f^T\\), \\(f\\in \\mathbb{R}^n\\) and \\[\nC_u(f^T, g^T) = \\operatorname{Cov}(f^Tu, g^Tu) = f^T\\Sigma g.\n\\]\nFurthermore,\n\n\n\n\nthe operator \\(C_u = \\Sigma f\\)\nsatisfies \\(g^T(\\Sigma f) =\nC_u(f^T,g^T)\\).\nSo what is \\(H_u\\)? Well,\nevery \\(n\\) dimensional vector\nspace can be represented as an \\(n\\)-dimensional vector, so what we really\nneed to do is identify \\(h^*\\) from\n\\(h\\). To do this, we use the\nrelationship \\(C(h^*, \\ell) = \\ell(h)\\)\nfor all \\(\\ell \\in B^*\\). Translating\nthat to our finite dimensional case we get that \\[\n(h^*)^T\\Sigma g = h^T g,\\qquad g \\in \\mathbb{R}^n,\n\\] from which it follows that \\(h^* =\n\\Sigma^{-1}h\\). Hence we get the inner product between \\(h, k \\in H_u\\) \\[\\begin{align*}\n\\langle h, k\\rangle_{H_u} &= \\langle h^*, k^*\\rangle_{R_h} \\\\\n&= (\\Sigma^{-1} h)^T \\Sigma (\\Sigma^{-1 k}) \\\\\n&= h^T \\Sigma^{-1} k.\n\\end{align*}\\]\nOk! That’s cool!\nYes! And the same thing holds in general, if you squint49. Just replace the covariance matrix\n\\(\\Sigma\\) with the covariance operator\n\\[\n(\\mathcal{C}f)(s) = \\int_S c(s, s') f(s') \\, ds'.\n\\]\nThis operator has (in a suitable sense) a symmetric50\nnon-negative definite (left) (closed) inverse operator \\(\\mathcal{Q}\\), which defines the RKHS inner\nproduct by \\[\n\\langle f, g \\rangle_{H_u} = \\int_{S} f(s) (\\mathcal{Q} g)(s) \\,ds,\n\\] where \\(f\\) and \\(g\\) are smooth enough functions for this to\nmake sense. In general, \\(\\mathcal{Q}\\)\nwill be a (very) singular integral operator, but when \\(u(s)\\) has the Markov property, \\(\\mathcal{Q}\\) is a differential operator.\nIn all of these cases the RKHS is the set of functions that are smooth\nenough that \\(\\langle f, f \\rangle_{H_u} <\n\\infty\\).\nWe sometimes call the operator \\(\\mathcal{Q}\\) the precision\noperator and it’s fundamental to thin plate spline theory as well\nas some nice ways to approximate GPs in 1-4 dimensions. I will blog\nabout this later, probably, but for now if you’re interested Finn Lindgren, Håvard Rue, and\nDavid Bolin just released a really nice survey paper about the\ntechnique.\nTell me some\nthings about the Cameron-Martin space\nNow that we’ve gone to the effort of finding it, I should probably\ntell you why it’s so important. So here are a collection of facts!\nFact 1: The Cameron-Martin space (the set of\nfunctions and the inner product) determines a51\nGaussian process, in that if two Gaussian processes have the same mean\nand the the same Cameron-Martin space, they have the same distribution.\nIn fact, the next definition of a Gaussian process is going to show this\nconstructively.\nThis is nice because it means you can define a Gaussian process\nwithout needing to specify its covariance function. You just (just!)\nneed to specify a Hilbert space. It turns out that this is a\nconsiderably easier task than trying to find a positive\ndefinite covariance function if the domain \\(S\\) is weird.\nFact 2: \\(u(s)\\) is\nnever in the RKHS. That is, \\(\\Pr(u\n\\in H_u) = 0\\). But52 if, for any \\(\\epsilon>0\\), \\(A_\\epsilon \\subset B\\) is any measurable\nset of functions with \\(\\Pr(u \\in A) =\n\\epsilon\\), then \\(\\Pr(u \\in A_\\epsilon\n+ H_u) = 1\\), where \\(A_\\epsilon+H_u =\n\\{a + h \\in B: a\\in A_\\epsilon, h \\in H_u\\}\\). Or to say it in\nwords, although \\(u\\) is never in \\(H_u\\), if you find a set \\(A_\\epsilon\\) that \\(u\\) could be in (even if it’s extremely\nunlikely to be there), then \\(u\\) is\nalmost surely made up of a function in \\(A_\\epsilon\\) plus a function in \\(H_u\\).\nThis is wild. It means that while \\(u(\\cdot)\\) is never in the RKHS,\nall you need to do is add a bit of rough to get all of the stuff out.\nAnother characterisation of the RKHS that are related to this is that it\nis the intersection of all subsets of \\(B\\) that have full measure under \\(u\\) (aka all sets \\(A\\subset B\\) such that \\(\\Pr(u \\in A) = 1\\)).\nFact 3: If we observe some data \\(y = N(Tu, \\Sigma_y)\\), where \\(Tu = (\\ell_1(u),\\ldots, \\ell_n(u))^T\\) is\nsome observation vector, then the posterior mean \\(\\mathbb{E}(u \\mid y)\\) is in the RKHS and\nthat posterior distribution of \\(u\\mid\ny\\) is a Gaussian process that’s absolutely continuous with\nrespect to the prior GP u(s). This means that the posterior mean, which\nis our best point prediction under squared error loss, is\nalways smoother than any of the posterior draws.\nThis kinda makes sense: averaging things smooths out the\nrough edges. And so when we average a Gaussian process in this way, we\nmake it smoother. But this is a thing that we need to be aware of! Our\nalgorithms, our reasoning for choosing a kernel, and our interpretations\nof the posterior need to be aware that the space of posterior\nrealizations \\(B\\) is rougher than the\nspace that contains the posterior mean.\nFrequentists / people who penalise likelihoods don’t have to worry\nabout this shit.\nSo what have we learnt?\nSo so so so so so so much notation and weird maths shit.\nBut there are three take aways here:\nThe importance of the Fourier transform (aka the characteristic\nfunction) when it comes to understanding Gaussian processes.\nThe maths buys us understanding of some of the more delicate\nproperties of a Gaussian process as a random object (in particular it’s\njoint properties)\nYou can define a Gaussian process exclusively using the RKHS inner\nproduct. (You can also do all of the computations that way too, but\nwe’ll cover that later). So you do not need to explicitly specify a\ncovariance function. Grace Wahba started doing this with thin plate\nsplines (and \\(L\\)-splines) in 1974 and\nit worked out pretty well for her.\nSo to finish off this post, let’s show one more way of constructing a\nGaussian process. This time we will explicitly start from the\nRKHS.\nWhat is a Gaussian\nprocess? (Version 3)\nOur final Gaussian process definition is going to centre the RKHS53 as the fundamental object.\nThis construction, which is known as an abstract Wiener space54 is less general55\nthan our previous definition, but it covers most of the processes we are\ngoing to encounter in applications.\nThis construction is by far the most abstract of the three (it is in\nthe name after all). So buckle up.\nThe jumping off point here is a separable Hilbert space \\(H\\). This has an inner-product \\(\\langle\\cdot, \\cdot \\rangle_H\\) on it, and\nthe associated notion of orthogonality and an orthogonal projector.\nConsider an \\(n\\)-dimensional subspace\n\\(V_n \\subset H_u\\). We can, without\nany trouble, define a Gaussian process on \\(V_n\\) \\(\\tilde\nu_n\\) with characteristic function \\[\n\\phi_{\\tilde u_n}(h) = \\exp\\left(-\\frac{1}{2}\\langle\nh,h\\rangle_H\\right).\n\\] We hit no mathematical problems because \\(V_n\\) is finite dimensional and nothing\nweird happens to Gaussians in finite dimensions.\nThe thing is, we can do this for any finite dimensional\nsubspace \\(V_n\\) and, in particular, if\nwe have a sequence of subspace \\(V_1 \\subset\nV_2 \\subset \\ldots\\), where \\(\\operatorname{dim}(V_n) =n\\), then we can\nbuild a sequence of finite dimensional Gaussian processes \\(\\tilde u_n\\) that are each supported in\ntheir respective \\(V_n\\).\nThe question is: can we construct a Gaussian process \\(\\tilde{u}\\) supported56\non \\(H\\) such that \\(P_n \\tilde u \\stackrel{d}{=} \\tilde u_n\\),\nwhere \\(P_n\\) is the orthogonal\nprojector from \\(H\\) to \\(V_n\\)?\nYou would think the answer is yes. It is not. In fact, Komolgorov’s\nextension theorem says that we can build a Gaussian process this way,\nbut it does not guarantee that the process will be supported on \\(H\\). And it is not.\nTo see why this is, we need to look a bit more carefully at the\ncovariance operator of a Gaussian process on a separable Hilbert space.\nThe key mathematical feature of a separable Hilbert space is that it has\nan57 orthonormal58\nbasis \\(e_n\\). We can use the\northonormal basis to do a tonne of things, but the one we need right now\nis the idea of a trace59 \\[\n\\operatorname{tr}(C_u) = \\sum_{n = 1}^\\infty C_u(e_i, e_i).\n\\]\nFor a (zero mean) Gaussian process \\(u\\) supported on \\(H\\), we can see that \\[\\begin{align*}\n\\operatorname{tr}(C_u) &= \\sum_{n = 1}^\\infty\n\\mathbb{E}\\left[(\\langle e_n, u\\rangle)^2\\right] \\\\\n&= \\mathbb{E}\\left[ \\sum_{n = 1}^\\infty\\langle e_n,\nu\\rangle_H^2\\right] \\\\\n&= \\mathbb{E}\\left[\\langle u, u\\rangle_H\\right] < \\infty,\n\\end{align*}\\] where the second line is just true because I say\nit is and the third line is Pythagoras’ theorem writ large (and is\nfinite because Gaussian processes have a lot of moments60!).\nIf we were to say this in words, we would say that the covariance\noperator of a Gaussian process supported on a separable Hilbert space is\na trace-class operator (or has a finite trace).\nAnd this is where we rejoin the main narrative. You see, if \\(\\tilde{u}\\) was a stochastic process on\n\\(H\\), then its characteristic function\nwould be \\[\n\\phi_{\\tilde u}(h) = \\exp\\left(-\\frac{1}{2}\\langle h, h\n\\rangle_H\\right).\n\\] But it can’t be! Because \\(H\\) is infinite dimensional and the\nproposed covariance operator is the identity on \\(H\\), which is not trace class (its trace is\nclearly infinite).\nSo whatever \\(\\tilde u\\) is61, it is emphatically not a\nGaussian process on \\(H\\).\nThat\ndoesn’t seem like a very useful trip through abstract land\nWell, while we did not successful make a Gaussian process on \\(H\\) we did actually build the guts of a\nGaussian process on a different space. The trick is to use the same idea\nin reverse. We showed that \\(\\tilde u\\)\nwas not a Gaussian process because its covariance operator wasn’t on\ntrace class. It turns out that the reverse also holds: if \\[\n\\phi_u(h) = \\exp\\left(-\\frac{1}{2}\\langle C_uh, h\\rangle_{H'}\\right)\n\\] and \\(C_u\\) is trace class on\n\\(H'\\), then \\(u\\) is a Gaussian process supported on\n\\(H'\\).\nThe hard part is going to be finding another Hilbert space \\(H' \\supset H\\).\nTo do this, we need to recall a definition of a separable Hilbert\nspace \\(H\\) with orthonormal basis\n\\(e_n\\), \\(n=1, \\ldots, \\infty\\): \\[\nH = \\left\\{\\sum_{n=1}^\\infty a_n e_n: \\sum_{n=1}^\\infty a_n^2 <\n\\infty\\right\\}.\n\\] From this, we can build a larger separable Hilbert space \\(H'\\) as \\[\nH' = \\left\\{\\sum_{n=1}^\\infty a_n e_n: \\sum_{n=1}^\\infty\n\\frac{a_n^2}{n^2} < \\infty\\right\\}.\n\\] This is larger because there are sequences of \\(a_n\\)s that are admissible for \\(H'\\) that aren’t admissible for \\(H\\) (for example62,\n\\(a_n = \\sqrt{n}\\)).\nWe let \\(j:H \\rightarrow H'\\) be\nthe linear embedding that we get by considering an element \\(h \\in H\\) as an element of \\(H'\\). If we let \\(e_n'\\) be an orthonormal basis on \\(H'\\) (note: this is not the\nsame as \\(e_n\\) as it needs to be\nre-scaled to have unit norm in \\(H'\\)), then we get \\[\nj\\left(\\sum_{n=1}^\\infty \\alpha_n e_n\\right) =\n\\sum_{n=1}^\\infty  \\frac{\\alpha_n}{n} e_n'.\n\\] Why? Because \\(\\|e_n\\|_{H'} =\nn^{-1}\\) which means that \\(e_n' =\nn e_n\\) is an orthonormal basis for \\(H'\\). This means we have to divide the\ncoefficients by \\(n\\) when we move from\n\\(H\\) to \\(H'\\), otherwise we wouldn’t be\nrepresenting the same function.\nWith this machinery set up, we can ask if \\(\\tilde u\\) is a Gaussian process on \\(H'\\). Or, more accurately, we can ask\nif \\(u = j(\\tilde u)\\) is a Gaussian\nprocess on \\(H\\).\nWell.\nLet’s compute its characteristic function. \\[\\begin{align*}\n\\phi_u(h') &= \\mathbb{E}\\left(e^{i\\left\\langle u, h'\n\\right\\rangle_{H'}}\\right) \\\\\n&= \\mathbb{E}\\left[\\exp\\left(i\\left\\langle \\sum_{n=1}^\\infty\n\\frac{\\langle \\tilde u, e_n\\rangle_H}{n}e_n', \\sum_{n=1}^\\infty\nh_n'e_n' \\right\\rangle_{H'}\\right) \\right] \\\\\n&= \\mathbb{E}\\left[\\exp\\left(i \\sum_{n=1}^\\infty \\frac{\\langle\n\\tilde u, e_n\\rangle}{n} h_n\\right) \\right] \\\\\n&= \\exp\\left(-\\frac{1}{2} \\sum_{n=1}^\\infty\n\\frac{h_n^2}{n^2}\\right).\n\\end{align*}\\] It follows that \\(\\phi_u(e_n') = e^{-1/(2n^2)}\\) and so63 \\[\n\\operatorname{tr}(C_u) = -2\\sum_{n=1}^\\infty \\log \\phi_u(e_n') =\n\\sum_{n=1}^\\infty \\frac{1}{n^2} < \\infty,\n\\] \\(C_u\\) is a trace class\noperator on \\(H'\\) and, therefore,\n\\(u\\) is a Gaussian process on \\(u\\).\nBut wait, there is more! To do the calculation above, we identified\nelements of \\(H'\\) as infinite\nsequences \\(h' = (h'_1, h'_2,\n\\ldots)\\) that satisfy \\(\\sum_{n=1}^\\infty n^{-2}h_n^2 <\n\\infty\\). In this case the covariance operator is \\(C_{u}\\) is diagonal, so the \\(n\\)th entry of \\(C_u h' = n^{-2}h'_n\\). From this,\nand the reasoning in the previous section, we see that the\nCameron-Martin space can be thought of as a subset of \\(H'\\). The Cameron-Martin inner product\ncan be constructed from the inverse of \\(C_u\\), which gives \\[\n\\langle a, b\\rangle_{H_u} = \\sum_{i=1}^\\infty n^2 a_n b_n.\n\\] Clearly, this will not be finite unless we put much much\nstronger restrictions on \\(a_n\\) and\n\\(b_n\\) than that \\(\\sum_{n\\geq 1} n^{-2}a_n^2 <\n\\infty\\).\nThe Cameron Marin space is the subspace of \\(H'\\) consisting of all functions \\(h' = \\sum_{n=1}^\\infty a_n e_n'\\)\nsuch that \\[\n\\sum_{n=1}^\\infty n^2a_n^2 < \\infty.\n\\] This is (isomorphic to) \\(H\\)!\nTo see this, we note that the condition is only going to hold if\n\\(a_n = n^{-1}\\alpha_n\\) for some\nsequence \\(\\alpha_n\\) such that \\(\\sum_{n\\geq 1} \\alpha_n^2 < \\infty\\).\nRemembering that \\(e_n' = n e_n\\),\nit follows that \\(h \\in H_u\\) if and\nonly if \\[\\begin{align*}\nh &= \\sum_{n=1}^n\\frac{\\alpha_n}{n} e_n' \\\\\n&=\\sum_{n=1}^n\\frac{\\alpha_n}{n} n e_n \\\\\n&=\\sum_{n=1}^n \\alpha_n e_n,\n\\end{align*}\\] which is exactly the definition of \\(H\\).\nAre you actually trying to\nkill me?\nYes.\nSo let’s recap what we just did: We took a separable Hilbert space\n\\(H\\) and used it to construct a\nGaussian process on a larger space \\(H'\\) with \\(H\\) as its Cameron-Martin space. And we did\nall of this without ever touching a covariance function. This is an\nabstract Wiener space construction of a Gaussian process.\nThe thing is that this construction is a lot more general\nthan this. The following is a (simplified64)\nversion of the abstract Wiener space theorem.\n\nTheorem: Let \\(H\\)\nbe a separable Hilbert space and let \\(B\\) be a separable Banach space.\nFurthermore, we assume that \\(H\\) is\ndense in \\(B\\). Then there is a unique\nGaussian process \\(u\\) with \\(\\Pr(u \\in B) = 1\\) and \\(H_u = H\\). It can be constructed from the\ncanonical cylindrical Gaussian process \\(\\tilde u\\) on \\(H\\) by \\(u =\nj(\\tilde u)\\), where \\(j:H \\rightarrow\nE\\) is the natural embedding.\n\nWas there any point to doing\nthat?\nI mean, probably not. The main thing we did here was see that you can\ntake the RKHS as the primal object when building a Gaussian process. Why\nthat may be a useful observation was not covered.\nWe also saw that there are some restrictions required on the\ncovariance operator to ensure that a Gaussian process is a proper\nstochastic process on a given space. (For the tech-heads, the problem\nwith \\(\\tilde u\\) is that it’s\nassociated probability measure is not countably additive. That is a bad\nthing, so we do not allow it.)\nThe restrictions are very clear for covariance operators on separable\nHilbert spaces (they must be trace class). Unfortunately, there isn’t\nany clean characterization of all allowable covariance operators on more\ncomplex spaces like Banach spaces65.\nWhere do we go now but\nnowhere\nAnd with that I have finished my task. I have defined Gaussian\nprocesses three different ways and if anyone is still reading at this\npoint: you’re a fucking champion.\nI probably want to talk about other stuff eventually:\nUsing all this technology to work out what happens to a posterior\nwhen we approximate a Gaussian process (which we usually do for\ncomputational reasons)\nUnderstanding how singularity/absolute continuity of Gaussian\nmeasures can help you set priors for the parameters in a covariance\nfunction\nThe Markov property in space: what is it and how do you use it\nShow how we can use methods for solving PDEs to approximate Gaussian\nprocesses.\nThe last one has gotten a lot less urgent because Finn, David and Håvard just\nreleased a lovely survey paper.\nMaybe by the time I am finished with these things (if that ever\nhappens, I don’t rate my chances), I will have justified all of this\ntechnicality. But for now, I am done.\n\nNot the root vegetable.↩︎\nThe first 5 chapters of Adler\nand Taylor’s masterpiece s are glorious↩︎\nNot gonna lie. Bogachev’s Gaussian\nMeasures is only recommended if you believe in intercessory\nprayer.↩︎\nRozanov’s Markov\nRandom Fields, which is freely available from that link and is so\nbeautiful you will cry when it turns the whole question into one about\nfunction space embeddings. It will be a moist old time. Bring tissues.↩︎\nI recommend some video head cleaner↩︎\nwhich spent an embarrassing amount of\ntime essentially divorced from the mainstream statistical literature↩︎\nThis is the nomenclature that machine\nlearners thrust upon us and it’s annoying and I hate it. Traditionally,\na stochastic process was indexed by time (so in this case it would be a\none-dimensional Gaussian process and when it was indexed by any other\nset it was referred to as a random field. So I would much\nrather be talking about Gaussian random fields. Why? Because there’s a\nbunch of shit that is only true in 1D and I’m not interested in talking\nabout that)↩︎\nGreat book. Great reference. No shade\nwhatsoever↩︎\nMaybe? But maybe I’m most interested\nin the average temperature over a region. This is why we are going to\nneed to think about things more general than just evaluating Gaussian\nprocesses at a location.↩︎\nWhy the joint? Well because it’s\nlikely that nearby temperature measurements will be similar, while\nmeasurements that are far apart are more likely to be (almost)\nindependent (maybe after adjusting for season, time of day, etc).↩︎\nin the sense that we have formulas\nfor almost everything we want to have formulas for↩︎\nWe can also play games with\nmultivariate Gaussians (like building deep Gaussian processes or putting\nstochastic models on the covariance structure) that markedly increase\ntheir flexibility.↩︎\nUsually this involves covariates!↩︎\nWikipedia edit war aside (have a\ngander, it’s a blast), there’s evidence that Kolmogorov had a long-term\nrelationship with Aleksandrov that was a) known at the time and b) used\nby the Soviets to blackmail them. So that’s fun.↩︎\nto be proper on some subspace. We\nare allowed zero eigenvalues for technical reasons and it actually turns\nout to be useful later, making things like thin plate splines a type of\nGaussian process. Grace Wahba had to do all this without Google.↩︎\nExponential, Mat'{e}rn, and\nsquared-exponential are the common ones on \\(\\mathbb{R}^d\\). After that shit gets\nexotic.↩︎\nand any other process built from\nKolmogorov’s extension theorem↩︎\nso named because a set \\(A = \\{u(\\cdot): u(s_1, \\ldots, u(s_K)) \\in B;\\;\nB\\in \\mathcal{B}(\\mathbb{R}^K)\\}\\) is called a cylinder\nset↩︎\nthe exact definition is the smallest\n\\(\\sigma\\)-algebra for which all\ncontinuous linear functionals are measurable↩︎\nThe all part is important here.\nConsider the function \\(u(s) =\n1_{A}(s)\\) where \\(A\\) is\nuniformly distributed on \\(S\\) and\n\\(1_A(s)\\) is 1 when \\(s=A\\) and zero otherwise. This function is\nequal to \\(0\\) for almost every \\(s\\) (rather than for every \\(s\\)), but the random function \\(u(s)\\) is definitely not the zero\nfunction (it is always non-zero at exactly one point).↩︎\nBottom of page 12 through\npage 14 here↩︎\nThis is a straight generalisation.\nIf \\(\\ell_k(s) = \\delta_{s_k}(s)\\) then\nit’s the exact situation we were in before.↩︎\nIn the technical language of the\nnext section, the set of delta functions is dense in the space of\nbounded linear functionals↩︎\naka replace integrals with sums,\ncompute the joint distribution of the sums, and then send everything to\ninfinity, which is ok when \\(\\ell_k\\)\nare bounded↩︎\nThe paper is pretty good and I think\nit’s a nice contribution. But the title was perfect.↩︎\nSorry for the pay wall. It’s from so\nlong ago it’s not on arXiv.↩︎\nYes. NN-GPs, Vecchia approximations,\nfixed-rank Kriging, variational GPs, and all of the methods I haven’t\nspecifically done work on, all abandon some or all of the covariance\nfunction. Whether the people who work on those methods think they’re\nabandoning the covariance function is between them an Cher.↩︎\nI say this, but you can make this\nwork over pretty bonkers spaces. If we want to be general, if \\(E\\) is a linear space and \\(F\\) is a space of functionals on \\(E\\) that separates the points of \\(F\\), then \\(u(s)\\) is defined as a Gaussian process\n(wrt the appropriate cylindrical \\(\\sigma\\)-algebra) if \\(f(u)\\) is Gaussian for all \\(f\\in F\\). Which is fairly general but also,\nlike, at this point I am just really showing off my maths degree.↩︎\nIt is very convenient that\ncontinuous linear functionals and bounded linear functionals are the\nsame thing.↩︎\nit’s the one with a norm↩︎\nThe support of \\(u\\) is a set \\(A\n\\subset B\\) such that \\(\\Pr(u \\in A) =\n1\\).↩︎\nneed is a big word here. We don’t\nneed to do this, but not doing it makes things more technical. The\nassumption we are about to make let’s us breeze past a lot of edge cases\nas we sail from the unfettered Chapter 2 of Bogachev to the more staid\nand calm Chapter 3 of Bogachev.↩︎\nThat it contains a countable dense\nset. Somewhat surprisingly, this implies that the Gaussian process is\nseparable (or alternatively that it’s law is a Radon measure),\nwhich is a wildly technical condition that just makes everything about\n80% less technical↩︎\nThere are separable spaces on the\nwhole space too, but, like, leave me alone.↩︎\nI’ve made a↩︎\nThese sets don’t overlap, but\nthey’re probably not very far apart from each other? Honestly I can’t be\narsed checking but this is my feeling.↩︎\nand continuously index stochastic\nprocesses/random fields in general↩︎\nsee Simon Cotter and\nFriends↩︎\nsee Geir-Arne Fuglstad and\nfriends↩︎\nZhang, H. (2004). Inconsistent\nestimation and asymptotically equal interpolations in model-based\ngeostatistics. Journal of the American Statistical Association,\n99(465):250–261.↩︎\nEveryone who’s ever suffered through\nthat inexplicable grad-level measure-valued probability course that\nbuilds up this really fucking intense mathematical system and then\nessentially never uses it to do anything interesting should be well\naware of the many many many many ways to prove the central limit\ntheorem.↩︎\nwell, the characteristic function\nwhen \\(t=1\\) because if \\(\\ell \\in B^*\\), \\(t\\ell \\in B^*\\).↩︎\nIn a locally convex space, this is\ntrue as measures over the cylindrical \\(\\sigma\\)-algebra, but for separable spaces\nit’s true over the Borel \\(\\sigma\\)-algebra (aka all open sets), which\nis an enormous improvement. (This happens because for separable spaces\nthese two \\(\\sigma\\)-algebras\ncoincide.) That we have to make these sorts of distinctions (between\nBaire and Borel measures) at all is an important example of when you\nreally need the measure theoretic machinery to do probability theory.\nUnfortunately, this is beyond the machinery that’s typically covered in\nthat useless fucking grad probability course.↩︎\nIt is not at all clear that\nthe range of this operator is contained in \\(B\\). It should be mapping to \\(B^{**}\\), but that separability really\nreally helps! Check out the Hairer notes.↩︎\nOr they define it on the set \\(\\{\\ell - \\mu(\\ell): \\ell \\in B^*\\}\\), the\ncompletion of which is the general definition of \\(R_u\\).↩︎\nRadon-Nikodym derivative↩︎\nA true pain in the arse when working\non infinite dimensional spaces is that there’s no natural equivalent of\na Lebesgue measure, so we don’t have a universal default measure to take\nthe density against. So we have to take it against an existing\nprobability measure. In this case, the most convenient one is the\ndistribution of \\(u\\). In finite\ndimensions, the density \\(\\rho(u)\\)\nwould satisfy \\(p_w(x) =\n\\rho(x)p_u(x)\\) where \\(p_w(\\cdot)\\) is the density of \\(w\\).↩︎\nI’m skipping the actual computation\nbecause I’m lazy.↩︎\nor if you’re working on a separable\nHilbert space↩︎\nself-adjoint↩︎\nseparable↩︎\nThis next thing is a consequence of\nBorel’s inequality: \\(\\mathbb{B}(t,\nH_u)\\) is the \\(t\\) ball in\n\\(H_u\\) and a \\(A\\) is any measurable subset of \\(B\\) with \\(\\Pr(u\n\\in A) = \\Phi(\\alpha)\\), then \\(\\Pr(u\n\\in A + \\mathbb{B}(t, H_u)) \\geq \\Phi(\\alpha + t)\\), where \\(\\Phi\\) is the CDF of the standard normal\ndistribution. Just take \\(t\\rightarrow\n\\infty\\).↩︎\nAt some point while writing this\nI’ve started using RKHS and Cameron-Martin space interchangeably for the\none that is a subset of \\(B\\). We’re\nall just gonna have to be ok with that.↩︎\nYou can get references for this from\nthe Bogachev book, but I actually quite like this\nsurvey from Jan van Neervaen, even though it’s almost comically\ngeneral.↩︎\nAlthough I made the big, ugly\nassumption that \\(B\\) was separable\nhalfway through the last definition, almost everything is true without\nthat. Just with more caveats. Whereas, the abstract Wiener space\nconstruction really fundamentally uses the separability of \\(H_u\\) and \\(B\\) as a place to start.↩︎\nie with \\(\\Pr(\\tilde u \\in H) = 1\\)↩︎\nIt has lots of them but everything\nwe’re about to talk about is independent of the choice of orthonormal\nbasis.↩︎\n\\(\\|e_n\\|_H = 1\\) and \\(\\langle e_n, e_m \\rangle = 0\\) for \\(m\\neq n\\).↩︎\nThis is the trace of the operator\n\\(C_u\\) and I would usually write this\nas \\(\\sum_{n\\geq 1} \\langle Ce_i,\ne_i\\rangle\\), but it makes no difference here.↩︎\nThere’s a result called Fernique’s\ntheorem that implies that Gaussian processes have all polynomial and\nexponential moments.↩︎\nIt’s called an iso-normal process\nand is strongly related to the idea of white noise and I’ll\nprobably talk about that at some point. But the key thing is it is\ndefinitely not a Gaussian process in the ordinary sense on\n\\(H\\). We typically call it a\ngeneralized Gaussian process or a Generalized Gaussian random field and\nit is a Gaussian process indexed by \\(H\\). Life is pain.↩︎\nchaos_reins.gif↩︎\nYou can convince yourself this is\ntrue. I’m not doing all the work for you↩︎\nIf you want more, read Bogachev or\nthat Radonification paper↩︎\nThe\nbest reference I have is this survey↩︎\n",
    "preview": {},
    "last_modified": "2022-03-31T15:53:10+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-15-priors3/",
    "title": "Priors: Fire With Fire (Track 3)",
    "description": "Objective priors? In finite dimensions? A confidence trick? Yes.",
    "author": [
      {
        "name": "Dan Simpson",
        "url": "https://dpsimpson.github.io"
      }
    ],
    "date": "2021-10-17",
    "categories": [],
    "contents": "\nIt is Friday night, I am in lockdown, and I have had a few drinks. So\nlet’s talk about objective priors.\nThe first and most obvious thing is that they are not fucking\nobjective. It is bad/unethical marketing from the 90s that has stuck. I\ndislike it. I think it’s unethical (and, personally, immoral) to\nproclaim a statistical method objective in any context, let\nalone one in which all you did was compute some derivatives and maybe\nsent something that isn’t going to fucking infinity to infinity. It’s\nfucking trash and I hate it.\nBut let’s talk about some objective priors.\nWhat is an objective prior\nFuck knows.\nWho uses objective priors\nNo one (see above). But otherwise, a lot of people who are being sold\na mislabeled bill of goods. People who tell you, unprompted, that they\nwent to Duke.\nShould I use objective\npriors\nNo.\nOk let’s try again.\nNo I do not think I will.\nI am willing to talk about finite dimensional priors that add minimal\ninformation or are otherwise related to MLEs.\nLater, I guess because I’m mathematically interested in it, I’ll talk\nabout the infinite dimensional case. But not today. Because I’m pissed\noff.\nAnyway.\nWhat is an objective prior\nHonestly, still a pretty vague and stupid concept. It is difficult to\ndefine for interesting (aka not univariate) cases, but maybe the most\npractical definition is priors that come from rules.\nBut that’s not a … great definition. Many priors that I will talk\nabout over the next little while could probably be shoved under the\nobjective banner. But in this post I’m going to talk about the\nOG concept of an objective prior: the type of priors that try\nto add minimal information beyond the data.\nNominally, the aim of these priors is to let the data speak for\nitself. And I’ve been doing this a while, and no matter how long\nI’ve listened to my .csv file, it has never said a word.\nBut if it weren’t for silly justifications, we wouldn’t have silly\nconcepts.\nThere are, essentially, three main types of priors that fall into\nthis traditionally objective category:\nJeffreys priors\nReference priors\nMatching priors\nJefferys priors argue, for a bunch of very sensible and solid\ngeometrical reasons, that the value of the parameter \\(\\theta\\) is less important than the way\nthat moving from \\(\\theta\\) to \\(\\theta + d\\theta\\) will change the\nlikelihood \\(p(y \\mid \\theta)\\). This\npush back against the Arianist\nnotion that the prior can be separated from its context is\nwelcome!\nThe actual prior itself comes from, I guess, the idea that the prior\nshould be invariant to reparameterisations and after some maths you get\n\\[\np(\\theta ) \\propto |I(\\theta)|^{1/2},\n\\] where \\(|I(\\theta)|\\) is the\ndeterminant of the Fisher1 information matrix \\[\nI(\\theta)_{ij} = \\frac{\\partial^2}{\\partial \\theta_i \\theta_j} \\log p(y\n\\mid \\theta).\n\\]\nThis immediately turns out to be a terrible idea for general models.\nWhen \\(\\theta\\) has more than one\ncomponent, the Jeffreys prior tends to concentrate in silly places in\nthe parameter space.\nBut when \\(\\theta\\) is one\ndimensional, it works fine. In fact, if you use it you will get the\nsampling distribution Maximum Likelihood estimator (or withing \\(\\mathcal{O}_p(n^{-1})\\) of it). So there’s\nvery little purpose pursing this line of reasoning.\nThere’s actually a bit of a theme that develops here: for models with\na single parameter a lot of things work perfectly. Sadly the\nintersection of one-dimensional statistical models that are regular\nenough for all this maths to work and interesting statistical problems\nis not exactly hefty.\nReference priors are an attempt to extend Jeffreys priors to multiple\nparameters while avoiding some of the more egregious problems of\nmultivariate Jeffreys priors. They were also the topic of the most\nboring talk I have ever seen at a conference2. It\nwas 45 minutes going through all of the different reference priors you\ncan make for inferring a bivariate normal (you see, to construct a\nreference prior you need to order your parameters and this ordering\nmatters). If I didn’t already think that reference priors were an\nimpractical waste of time, that certainly convinced me. A lot of people\nseem to mention reference priors, but it is rarer to see them in\nuse.\nMatching priors try to spin off Jeffreys priors in a different\ndirection. They are a mathematically very interesting idea asking if\nthere is a prior that will produce a posterior uncertainty interval that\nis exactly the same as (or very close to) the sampling distribution of\nthe MLE. It turns out that for one parameter models you can totally do\nthis (the Jeffreys prior does it! And you can get even closer). But when\nthere are nuisance parameters (aka parameters that aren’t of direct\ninferential interest but are important to modelling the data), the\nresulting prior tends to be data-dependent. A really nice example of the\nliterature is Reid,\nMukerjee, and Fraser’s 2003 paper. To some extent the matching\npriors literature is asking “should we even Bayes?”, which is not the\nworst question to ask3.\nThese three ideas have a number of weird bastard children. Most of\nthese are not recommended by anyone, but used prominently. These are the\nvague priors. The \\(N(0,100^2)\\)\npriors. The \\(\\text{Inverse-Gamma}(\\epsilon,\n\\epsilon)\\) priors. The Uniform over large interval priors. The\nmisinformed concept behind these priors is that wider prior = less\ninformation. This is, of course, bullshit. As many4\nmany5 many6\nexamples show.\nThe one big thing that I haven’t mentioned so far is that most of the\ntime the priors produced using these methods are not proper, which is to\nsay that you can’t integrate them. That isn’t a big deal mathematically\nas long as \\(\\int_\\Theta p(y \\mid\n\\theta)p(\\theta)\\,d\\theta\\) is finite for all7\ndata sets \\(y\\). This is a fairly\ndifficult thing to check for most models and if you want to really upset\na grad student at a Bayesian conference spend some time staring at their\nposter and then grimace and ask “are you sure that posterior is\nproper?”8 The frequent impropriety of these\nclasses of means you can’t simulate from them, can’t really consider\nthem a representation of prior information, and can’t easily transfer\nthem from one problem to another without at least a little bit of fear\nthat the whole house of cards is gonna come tumbling down.\nWho uses objective priors\nFrequentists. People who are obsessed with statistical bias of their\nestimators (the Venn diagram here isn’t a circle, but it’s also not the\nposter child for diversity of thought or modernity). People who read\nboring textbooks. People who write boring textbook. People who believe\nthat it’s the choice of prior and somehow not the choice of the\nlikelihood or, you know, their choice of data that will somehow lead to\nincorrect inferences9. People who tell you, without being\nasked10, that they went to Duke.\nShould I use objective\npriors\nIf you’ve more parameters than a clumsy butcher has fingers on their\nnon-dominant hand, you probably shouldn’t use objective priors. In these\ncases, you almost always need to inject some form of regularisation,\nprior information, or just plain hope into your model to make it behave\nsensibly11.\nBut if you have less, I mean, live your life I guess. But why go to\nthe effort. Just compute a maximum likelihood if you’re looking for\nsomething that is very very similar to a maximum likelihood estimate.\nIt’s faster, it’s cleaner, and it’s not pretending to be something it\nisn’t.\nI actually think you can usually make stronger, more explicitly\njustified choices using other things we can talk about later. But I’m\nnot the boss of statistics so you don’t have to listen to me.\n\newwwwwww↩︎\nThis is a very high bar. I have seen\n(and given) a lot of very very very dull talks. But this is the one that\nsticks in my mind.↩︎\nIf it seems like I like matching\npriors more than the other two, I do.↩︎\nhttps://projecteuclid.org/journals/bayesian-analysis/volume-1/issue-3/Prior-distributions-for-variance-parameters-in-hierarchical-models-comment-on/10.1214/06-BA117A.full↩︎\nPut a wide normal prior on the logit\nmean, simulate and back transform. Explain how that is uninformative.↩︎\nSome people love a wide uniform\ndistribution \\(\\text{Unif}(0,U)\\) on\nthe degrees of freedom of a student-t distribution. As \\(U\\) increases, you are putting more and\nmore prior mass on the \\(t\\)\ndistribution being very close to a normal distribution. Oops.↩︎\nor all after some minimal\nrestrictions↩︎\nAllegedly, Jim Berger’s wife, who is\nnot a statistician but was frequently at conferences, used to do this.↩︎\nLike seriously. I don’t want to\nrepeat that old canard that the choice of likelihood is as subjective as\nthe choice of prior because a) Arianism and b) the choice of likelihood\nis a waaaaaaaaaay more important subjective modelling choice than the\nchoice of prior in all but the most outre circumstances!↩︎\nI promise I have never asked and I\nwill never ask.↩︎\nThere are ideas of\nobjective priors in these cases, which we will talk about\nlater, but these usually take the form of priors that guarantee optimal\nfrequentist behaviour. And again, there is often another way to get\nthat.↩︎\n",
    "preview": {},
    "last_modified": "2022-06-24T14:32:51+10:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-14-priors2/",
    "title": "Priors: Whole New Way (Track 2)",
    "description": "Conjugate Priors? The crystal deoderant of Bayesian statistics",
    "author": [
      {
        "name": "Dan Simpson",
        "url": "https://dpsimpson.github.io"
      }
    ],
    "date": "2021-10-16",
    "categories": [],
    "contents": "\nIf we’re going to talk about priors, let’s talk about priors. And\nlet’s talk about the most prior-y priors in the whole damn prior\nuniverse. Let’s talk about conjugate priors.\nWhat is a conjugate prior?\nWho cares.\nWho uses conjugate priors?\n90s revivalists.\nShould I use conjugate\npriors?\nLive your life.\nOk. Maybe we should try\nagain\nDeep breaths. You soul is an island of positivity.\nWhat is a conjugate prior?\nConjugate priors are wild and fabulous beasts. They roam the strange,\nmathematic plains and live forever in our dreams of a better future.\nToo much?\nOK.\nConjugate priors are a mathematical curiosity that occasional turn\nout to be slightly useful.\nA prior distribution \\(p(\\theta)\\)\nis conjugate to the likelihood1\n\\(p(y \\mid \\theta)\\) if the\nposterior distribution \\(p(\\theta\n\\mid y)\\) is in the same distributional family as the prior.\nMoreover, there is an rule to update the parameters in the prior to\nget the parameters in the posterior based on some simple summaries of\nthe data. This means that you can simply write the posterior down as a\nspecific distribution that you can2 easily sample from and\nget on with your life.\nReally, it seems like a pretty good thing. But there is,\nunsurprisingly, a hitch: almost no likelihoods have conjugate priors.\nAnd if you happen to have a model with a nice3\nconjugate prior then good for you, but if you modify your model even\nslightly, you will no longer have one.\nThat is, the restriction to conjugate priors is a massive restriction\non your entire model.\nWho uses conjugate priors?\nConjugate priors are primarily used by two types of people:\nPeople who need to write exam questions for undergraduate Bayesian\nstatistics courses4,\nPeople who need to implement a Gibbs sampler and don’t want to live\nthrough the nightmare5 that is\nMetropolis-within-Gibbs.\nFor the most part, we can ignore the first group of people as a drain\non society.\nThe second group is made up of:\npeople who are using software that forces it on them. And like we\ndon’t all have time to learn new software6.\nLeave, as hero7 Chris Crocker, Britney\nalone.\npeople who are writing their own Gibbs sampler. Annie Lennox said\nit best: Why-y-y-y-y-y-y-y-y-y-y?\nFor a very large variety of problems, you do not have to do\nthis8. The exception is when you have a\ndiscrete parameter in your model that you can’t marginalise out9, like an exponential random graph\nmodel or something equally hideous. Thankfully, a lot of work in machine\nlearning has expanded the options for Bayesian and pseudo-semi-kinda\nBayesian10 estimation of these types of\nmodels. Anyway. Discrete parameters are disgusting. I am tremendously\nindiscrete.\nThe third type are the odd ducks who insist that because the\nposterior and the prior being in the same family means that the prior\ncan be interpreted as the outcome of Bayesian analysis on a previous\nexperiment. Instead of the much more realistic way of arriving at a\nconjugate prior where you find yourself waking up alone in a bathtub\nfull of ice and using an \\(\\text{Inverse-Gamma}(1/2, 0.0005)\\) prior\non the variance (which is conjugate for a Gaussian likelihood) because\nsome paper from 199511 told you it was a good choice.\nShould I use conjugate\npriors?\nThere is actually one situation where they can be pretty useful. If\nyour parameter space breaks down into \\(\\theta\n= (\\eta, \\phi)\\), where \\(\\eta\\)\nis a high-dimensional variable, then if \\(p(y\n\\mid \\theta) = p(y \\mid \\eta)\\) and \\(p(\\eta \\mid \\phi)\\) is conjugate for \\(p(y \\mid \\eta)\\), then a magical\nthing happens: you can compute \\(p(\\eta \\mid\ny, \\phi)\\) explicitly (using the conjugate property) and then you\ncan greatly simplify the posterior as \\(p(\\theta\\mid y ) = p(\\eta \\mid y, \\phi) p(\\phi\n\\mid y)\\), where12 \\[\np(\\phi \\mid y) = \\frac{p(y \\mid \\eta)p(\\eta \\mid \\phi)p(\\phi)}{p(y)\np(\\eta \\mid y, \\phi)} \\propto \\left.\\frac{p(y \\mid \\eta)p(\\eta \\mid\n\\phi)p(\\phi)}{p(\\eta \\mid y, \\phi)}\\right|_{\\eta = \\text{anything}},\n\\] where every term on the right hand side is able to be\ncalculated13. Even if this doesn’t have a known\ndistribution form, it is much much lower-dimensional than the original\nproblem and much more amenable to MCMC or possibly deterministic\nintegration methods.\nThis really does feel a bit abstract, so I will give you the one case\nwhere I know it’s used very commonly.This is the case where \\(y \\sim N(A\\eta, R)\\) and14\n\\(\\eta \\mid \\phi \\sim N(0,\n\\Sigma(\\phi))\\), where \\(\\Sigma(\\phi)\\) is a covariance matrix and\n\\(A\\) is a matrix (the dimension of\n\\(\\eta\\) is often higher than the\ndimension of \\(y\\)).\nThis is an example of a class of models that occur\nconstantly in statistics: Håvard Rue15\ncalls them Latent Gaussian models. They basically extend16\n(geostatistical)? linear|additive (mixed)? models. So for\nall of these models, we can explicitly integrate out the\nhigh-dimensional Gaussian component, which makes inference a\nbreeze17.\nIt gets slightly better than that because if you combine this\nobservation with a clever asymptotic approximation, you get an\napproximately conjugate model and can produce Laplace approximations,\nnested Laplace approximations18, and Integrated Nested\nLaplace approximations19, depending on how hard you are\nwilling to work.\nA conclusion, such as it is\nYes we have drifted somewhat from the topic, but that’s because the\ntopic is boring.\nConjugate priors are mostly a mathematical curiosity and their role\nin Bayesian statistics is inexplicably inflated20\nto make them seem like a core topic. If you never learn about conjugate\npriors your Bayesian education will not be lacking anything. It will not\nmeaningfully impact your practice. But even stopped clocks are right 2-3\ntimes a day21\n\nLike all areas of Bayesian\nstatistics, conjugate priors push back against the notion of Arianism.↩︎\noften, but not always↩︎\nChristian Robert’s book The Bayesian\nChoice has an example where a model has a conjugate prior but it doesn’t\nnormalise easily.↩︎\nOr, in my case, it’s explicitly\nlisted on the syllabus.↩︎\nNot a nightmare.↩︎\nI’m equally likely to learn Julia and\nStata. Which is is to say I’m tremendously unlikely to put the effort in\nto either. I wish them both well. Live your life and let me live mine.↩︎\nI have not fact checked this\nrecently, and we all know white gays sometimes go bad. But he started\nfrom a good place, so I’m sure it’s fine.↩︎\nThere is pedagogical value in\nlearning how MCMC methods work by implementing them yourself. But girl\nit is 2021. Go fuck with a bouncy particle sampler or something. Live\nyour live out loud! Young Bayesians run\nfree↩︎\nOften, like with mixture models or\nhidden markov models, you can eg https://mc-stan.org/docs/2_22/stan-users-guide/latent-discrete-chapter.html↩︎\nThe difference between these things\nis pretty slight in the usual situation where your MCMC scheme doesn’t\nexplore the space particularly well. I’m not of the opinion that you\neither explore the full posterior or you don’t use the model. Most of\nthe time you do perfectly fine with approximate exploration or, at\nleast, you do as well as anything else will.↩︎\nBERNARDINELLI, L., CLAYTON, D. and\nMONTOMOLI, C. (1995). Bayesian estimates of disease maps: How important\nare priors? Stat. Med. 14 2411–2431.↩︎\nMultiply both sides of the first\nequation by the denominator and it’s equivalent to \\(p(y, \\eta, \\phi) = p(y, \\eta, \\phi)\\),\nwhich is tautologically true.↩︎\nThe constant of proportionality does\nnot depend on \\(\\eta\\). All of the\n\\(\\eta\\) parts cancel!↩︎\nThe mean doesn’t have to be zero but\nyou can usually make it zero using … magic.↩︎\nFamous for INLA↩︎\napologies for the regexp.↩︎\nSee also Rasmussen and Williams\ndoing marginal inference with GPs. Exactly the same process.↩︎\nhttps://arxiv.org/abs/2004.12550↩︎\nhttps://www.r-inla.org↩︎\nI assume this is so people don’t\nneed to update their lecture notes.↩︎\ndaylight savings time fades the\ncurtains and wreaks havoc with metaphors.↩︎\n",
    "preview": {},
    "last_modified": "2022-06-24T14:33:14+10:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-14-priors1/",
    "title": "Priors: Night work (Track 1)",
    "description": "Priors? Defined. Questions? Outlined. Purpose? Declared.",
    "author": [
      {
        "name": "Dan Simpson",
        "url": "https://dpsimpson.github.io"
      }
    ],
    "date": "2021-10-15",
    "categories": [],
    "contents": "\nI have feelings. Too many feelings. And ninety six point seven three\npercent of them are about prior distributions1. So\nI am going to write a few blog posts about prior distributions.\nTo be very honest, this is mostly a writing exercise2 to\nget me out of a slump.\nSo let’s do this.\nNo love, deep web\nAs far as I am concerned it’s really fucking stupid to try to write\nabout priors on their own. They are meaningless outside of their\ncontext. But, you know, this is a blog. So I get to be stupid.\nSo what is a prior distribution? It is whatever you want it to be. It\nis a probability distribution3 that … I don’t know.\nExists4.\nOk. This is not going well. Let’s try again.\nA prior distribution is, most of the time, a probability distribution\non the parameters of a statistical model. For all practical purposes, we\ntend to work with its density, so if the parameter \\(\\theta\\), which could be a scalar but, in\nany interesting case, isn’t, has prior \\(p(\\theta)\\).\nCaptain fantastic\nand the brown dirt cowboy\nBut what does it all meeeeeeeean?\nWe have a prior distribution specified, gloriously, by it’s density.\nAnd unlike destiny, density is meaningless. It only makes sense when we\nintegrate it up to get a probability \\[\n\\Pr(A) = \\int_A p(\\theta)\\,d\\theta.\n\\]\nSo what does the prior probabilty \\(\\Pr(A)\\) of a set \\(A\\) actually mean in real life? The answer\nmay shock you: it means something between nothing and everything.\nScenario 1: Let’s imagine that we were trying to\nestimate the probability that someone in some relative homogeneous\nsubgroup of customers completed a purchase on our website. It’s a binary\nprocess, so the parameter of interest can probably just be the\nprobability that a sale is made. While we don’t know what the\nprobability of a sale is for the subgroup of interest, we know a lot\nsales on our website in general (in particular, we know that about 3% of\nvisits result in sales). So if I also believe that it would be wildly\nunlikely for 20% of visits to result in a sale, I could posit a prior\nlike a \\(\\text{Beta}(0.4,5)\\) prior\nthat captures (a version of) these two pieces of information.\n\n\nShow code\n\n  ## Step 1: \n  \nfn <- \\(x) (qbeta(0.5,x[1], x[2]) - 0.02)^2 + \n  (qbeta(0.9, x[1], x[2]) - 0.2)^2\n\nbest <- optim(c(1/2,1/2), fn)\n\n## Step 3: Profit.\n## (AKA round and check)\nqbeta(0.9, 0.4, 5)\nqbeta(0.5, 0.4, 5)\n\n\n\nScenario 2: Let’s imagine I want to do variable\nselection. I don’t know why. I was just told I want to do variable\nselection. So I fire up the Bayesian\nLasso5 and then threshold in some way. In\nthis case, the prior encode a hoped-for property of my posterior. (To\nparaphrase Lana, hope is a dangerous thing for a woman like you to have\nbecause the Bayeisan Lasso does not work to the point that the original\npaper doesn’t even suggest using it for variable selection6 it\njust, idk, liked the name. Statistics is wild.)\nScenario 3: I’m doing a regression with just one\nvariable (because why not) and I think that the relationship between the\nresponse \\(y\\) and the covariate \\(x\\) is non-linear. That is, I think there\nis some unknown to me function \\(f(x)\\)\nsuch that \\(\\mathbb{E}(y_i) = f(x_i)\\).\nSo I ask a friend and they tell me to use a Gaussian Process prior for\n\\(f(\\cdot)\\) with an exponential\ncovariance function.\nWhile I can write down the density for the joint prior of \\((f(x_1), f(x_2,), \\ldots, f(x_n))\\), I do\nnot know7 what this prior means in any\nsubstantive sense. But I can tell you, you’re gonna need that maths\ndegree to even try.\nAnd should you look deeper, you will find more and more scenarios\nwhere priors are doing different things for different reasons8. For each of these priors in each of\nthese scenarios, we will be able to compute the posterior (or a\nreasonable computational approximation to it) and then work with that\nposterior to answer our questions.\nDifferent people9 will use priors different ways even\nfor very similar problems10. This remains true\neven though they are nominally working under the same inferential\nframework.\nBayesians are chaotic.\nMapping out\na sky / What you feel like, planning a sky\nSondheim’s ode\nto pointillism feels relevant here. The reality of the prior\ndistribution—and the whole reason the concept is so slippery and\nchaotic—is that you are, dot by dot, constructing the world of your\ninference. This act of construction is fundamental to understanding how\nBayesian methods work, how to justify your choices, and how to use a Bayesian workflow to solve\ncomplex problems.\nTo torture the metaphor, our prior distribution is just our paint,\nunmixed, slowly congealing, possibly made of ground up mummys.\nIt is nothing without a painter and a brush.\nThe painter is the likelihood or, more generally, the generative link\nbetween the parameter values and the actual data, \\(p(y \\mid \\theta)\\). The brush is the\ncomputational engine you use to actually produce the posterior\npainting11.\nThis then speaks to the core challenge with writing about priors: it\ndepends on how you use them. It is a fallacy, or perhaps a foolishness,\nor perhaps a heresy12. Hell, when trying to understand a\nsingle inference The\nPrior Can Only Be Understood In The Context Of The Likelihood13. In the context of an entire\nworkflow, The\nExperiment is just as Important as the Likelihood in Understanding the\nPrior.\nFor instance, using independent Cauchy priors for the coefficients in\na linear regression model will result in a perfectly ok posterior.\nWhereas the same priors used in a logistic regression, you\nmay end up with posteriors with such heavy tails that they don’t have a\nmean! (Do we care? Well, yes. If we want reasonable uncertainty\nintervals we probably want 2 or so moments otherwise those large\ndeviations are gonna getcha!)\nSo what?\nAll of this is fascinating. And it is a lot less chaotic than it\ninitially sounds.\nThe reality is that while two Bayesians may use different priors and,\nhence, produce different posteriors for the same data set.This can be\nextreme. For example, if I am trying to estimate the mean of data\ngenerated by \\(y_i \\sim N(\\mu, 1)\\),\nthen I can choose a prior14 (that depends on the\ndata) so that the posterior mean \\(\\mathbb{E}(\\mu \\mid y) =1\\). Or, to put it\ndifferently, I can get any answer I want if I choose an prior carefully\n(and in a data-dependent manner).\nBut this isn’t necessarily a problem. This is because the posteriors\nproduced by two sensible priors for the same problem will\nproduce fairly similar results15. The prior I used to\ncheat in the previous example would not be considered sensible by anyone\nlooking at it16.\nBut what is a sensible prior? Can you tell if a prior is sensible or\nnot in its particular context? Well honey, how long have you got. The\nthing about starting a (potential) series of blog posts is that I don’t\nreally know how far I’m going to get, but I would really like to talk a\nlot about that over the next little while.\n\nThe rest are about the night I saw\nPatti LuPone trying to get through the big final scene in War Paint as\npart of her costume loudly disintegrated.↩︎\nI’m told it’s useful to warm up\nsometimes because this pandemic has me ice cold.↩︎\nSometimes.↩︎\nExcept, and I cannot stress this\nenough, when it doesn’t.↩︎\nPlease do not do this!↩︎\nExcept for once in the abstract in a\nsentence that is in no way shape or formed backed up in the text. Park\nand Casella (2008)↩︎\nI do know. I know a very large amount\nabout Gaussian processes. But lord in heaven I have seen the greatest\nminds of my generation subtly fuck up the interpretation of GP priors.\nBecause it’s increadibly hard. Maybe I’ll blog about it one day. Because\nthis is in markdown so I can haz equations.↩︎\nSome reasons are excellent. Some,\nlike the poor Bayesian Lasso, are simply misguided.↩︎\nor the same person in different\ncontexts↩︎\nAre any two statistical problems\never the same?↩︎\nYes. I have a lot of feelings about\nthis too, but meh. A good artist can make great art with minimal\nequipment (see When Doves Cry), but most people are not the genius\nPrince was so just use good tools and stress less!↩︎\nI have written extensively about\npriors in the context of the Arianist heresy because of course I fucking\nhave. Part\n1, Part\n2, Part\n3. Apologies for mathematics eaten by a recent formatting change!↩︎\nEditors forced the word often into\nthe published title and, like, who’s going to fight?↩︎\n\\(N(2-\\bar{y},n^{-1})\\)↩︎\nWhat does this even mean? Depends on\nyour context really. But a working definition is that the big picture\nfeatures of the posterior are similar enough that if you were to use it\nto make a decision, that decision doesn’t change very much.↩︎\nBut omg subtle high dimensional\nstuff and I guess I’ll talk about that later maybe too?↩︎\n",
    "preview": {},
    "last_modified": "2022-06-24T14:33:35+10:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-11-n-sane-in-the-membrane/",
    "title": "$(n-1)$-sane in the membrane",
    "description": "Windmills? Tilted. Topic? Boring. (n-1)? No.",
    "author": [
      {
        "name": "Dan Simpson",
        "url": "https://dpsimpson.github.io"
      }
    ],
    "date": "2021-10-14",
    "categories": [],
    "contents": "\nI’ve been teaching a lot lately. That’s no huge surprise. It is my job. Maybe the one slight oddity this year is that I shifted jobs and, in switching hemispheres, I landed 3 consecutive teaching semesters. So. I’ve been teaching a lot lately.\nAnd when you’re in a period of heavy teaching, every-fucking-thing is about teaching.\nSo this blogpost is about teaching.\nRight now, I’m coming to the end of a second year class called Statistical Thinking. It’s been fun to work out how to teach the material. It’s standard fare: sampling variation, tests, bootstraps1, regression, and just a hint of Bayes in the last 2 weeks that you incentivize by promising a bastard of an exam question. So you know, (arms up even though I’m Catholic) tradition!\nIf I were a rich man (Katrina Lenk with a violin)\nThe thing about teaching an intro stats class is that it brings screaming to mind that quote from Bennett’s The History Boys2: (paraphrasing) “How do I define [intro to Statistics]? It’s just one fucking thing after another”.\nConstructing twelve moderately sequential weeks from the whole mass of things that someone being introduced to statistics needs to know is not unlike being thrown in the middle of the lake with nothing but an ice-cream container and a desiccated whale penis: confusing, difficult, and rather damp.\nThe nice thing about building an intro stats course is you’re not alone. You’re adrift in a sea of shit ideas! (Also a lot of good ones3, but don’t ruin my flow!)\nThe trouble is that this sort of course is simultaneously teaching big concepts and complex details. And while it’s not toooooo hard to make the concepts build and reinforce as time inexorably marches on, the techniques and details needed to illuminate the big concepts are not quite as linear.\nThere are two routes through this conundrum: incantations inscribed onto books made of human skin using the blood of sacrificial virgins (aka gathered during engineering statistics service teaching) or computers.\nI went with computers because we are in lockdown and I couldn’t be bothered sourcing and bleeding virgins.\nThe downside is that you need the students to have a grip on R programming (and programmatic thinking). This only happens if the degree you are teaching in is built in such a way that these skills have already been taught. Otherwise you need to teach both (which is very possible, but you need to teach less statistical content).\nThis is not a postmortem on my teaching, but if it were, it would be about that last point.\nI saw Goody Proctor with the devil!\nA tweet from Sanjay SrivastavaThis is a very long way to say I saw a tweet an had feelings.\nBecause I’m thinking about this stuff pretty hard right now, I am (as Hedwig would say) fully dilated.\nAnd my question is what is the use of teaching this distinction? Should anyone bother dividing by \\((n-1)\\) instead of \\(n\\) in their variance estimates?\nWell I guess the first question is is there a difference in this distinction? Let’s do the sort of R experiment I want my students to do!\n\n\n# Independent samples for a qq-plot!\n# Thanks to Rob Trangucci for catching this!\nlibrary(tidyverse)\nn_sim <- 100000\nn <- 10\nexperiments <- tibble(exp = rep(1:n_sim, each = n),\n                      sample = rnorm(n * n_sim),\n                      sample2 = rnorm(n * n_sim))\n\ncompare <- experiments %>%\n  group_by(exp) %>%\n  summarise(m = mean(sample),\n            m2 = mean(sample2),\n            var_bias = mean((sample - m)^2),\n            z_bias = m / sqrt(mean(var_bias)),\n            z = m2 / sd(sample2))\n\n\ncompare %>% \n  ggplot(aes(sort(z), sort(z_bias))) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0) +\n  theme_bw() + \n  coord_fixed(xlim = c(-2,2), y = c(-2,2))\n\n\n\n\nWell that is clear. There is not.\nOr, well, there is a small difference.\nBut to see it, you need a lot of samples! Why? Well the easy answer is maths.\nFor one thing, when \\(n=10\\), \\[\n\\frac{1}{n} - \\frac{1}{n-1} = \\frac{1}{90} = 0.01.\n\\] This does not compare well against the sampling variance, which (assuming \\(\\sigma^2\\approx 1\\), which is usual if you’ve scaled your problem correctly) is about \\(0.3\\).\nBut we could choose to do it properly. The bias in the MLE (aka the divide by \\(n\\)) variance estimate is \\[\n-\\frac{\\sigma^2}{n}.\n\\] This is a lot smaller than the sampling variability of the estimate (aka how much uncertainty you have because of the finite sample), which is \\[\n\\frac{\\sigma}{\\sqrt{n}}.\n\\]\nAnd that’s the whole story. Dividing by \\(n\\) instead of \\((n-1)\\) leaves you with a slightly biased estimate. But the bias if fucking tiny. It is possibly moving your second decimal place by about 1 number (assume our population variance is one). The sampling variably is moving the first decimal place by several digits.\nTruly. What is the point. The old guys4 who went wild about bias are now mostly dead. Or they’ve changed their minds (which is, you know, a reasonable thing to do as information about best practice is updated). The war against bias was lost before your undergraduates were born.\nEven in crisis, I maintain\nBut nevertheless, this whole DIVIDE BY N-1 OR THE BIAS MONSTER IS GONNA GET YA bullshit continues.\nAnd to some extent, maybe I shouldn’t care. I definitely shouldn’t care this many words about it.\nBut I do. And I do for a couple of reasons.\nReason One: What is the point teaching students about uncertainty and that you can’t just say “this number is different” because the estimate on a single sample is different. If I am to say that I need things to be at least5 \\(\\mathcal{O}(n^{-1/2})\\) apart before I’m willing to say they are maybe different, then why am I harping on about the much smaller difference?\nReason Two: It’s a shitty example. Bias and bias corrections have a role to play in statistics6. But if this is your first introduction to bias correction, you are going to teach either:\nBias is always bad, regardless of context / sampling variance / etc\nBias can be corrected, but it’s trivial and small.\nBoth of those things are bullshit. Just teach them how to bootstrap and teach the damn thing properly. You do not have to go very far to show bias actually making a difference!\nMaybe the only place the difference will be noticed is if you compare against the in-build var or sd functions. This is not the use case I would build my class around, but it is a thing you would need to be aware of.\nThe worlds is a question, this room is an answer. And the answer is no.\nIf you are going to teach statistics as more than just stale incantations and over-done fear-mongering, you need to construct the types of stakes that are simply not present in the \\(n\\) vs \\(n-1\\) bullshit.\nIt is present when you are teaching the normal vs t distribution. You are teaching that the design of your experiment changes the possible extreme behaviour and sometimes it can change a lot.\nThe \\(n\\) vs \\((n-1)\\) denominator for a variance estimator is a curiosity. It is the source of thrilling7 exercises or exam questions. But it is not interesting.\nIt could maybe set up the idea that MLEs are not unbiased. But even then, the useless correction term is not needed. Just let it be slightly biased and move on with your life.\nBecause if that is the biggest bias in your analysis, you are truly blessed.\nIn real life, bias is the price you pay for being good at statistics. And like any market, if you pay too much you’re maybe not that good. But if you pay nothing at all, you don’t get to play.\n\nTo paraphrase Jimmy Somerville, tell me whyyyyyyyyy about 90% of the bootstrap material on the web is … misguided. And why tidymodels only has the shit bootstrap in it?↩︎\nOk. Straight up, “[Intro to statistics] is a commentary on the various and continuing incapabilities of men” would’ve also worked.↩︎\nThis course stands on the shoulders of giants: Di Cook and Catherine Forbes gave me a great base. And of course every single textbook (shout out to the OpenIntro crew!), blog post, weird subsection of some other book, paper from 1987 on some weird bootstrap, etc that I have used to make a course!↩︎\nYes. I used the word on purpose.↩︎\n\\(n\\) is the size of the sample.↩︎\nI spend most of my time doing Bayes shit, and we play this game somewhat differently. But the gist is the same.↩︎\nNot thrilling.↩︎\n",
    "preview": "posts/2021-10-11-n-sane-in-the-membrane/n-sane-in-the-membrane_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-02-10T23:24:11+11:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
