[
  {
    "path": "posts/2021-11-24-getting-into-the-subspace/",
    "title": "Getting into the subspace; or what happens when you approximate a Gaussian process",
    "description": "Fuck man, I don't know about this one. A lot of stuff happens. At some point there's a lot of PDEs. There are proofs. Back away.",
    "author": [],
    "date": "2021-11-24",
    "categories": [],
    "contents": "\nSo. Gaussian processes, eh.\nNow that we know what they are, I guess we should do something with a Gaussian process. But we immediately hit a problem. You see, Gaussian processes are charming things, sweet and caring. But they have a dark side. Used naively1, they’re computationally expensive when you’ve got a lot of data.\nStab of dramatic music\nYeah. So. What’s the problem here? Well, the first problem is people seem to really like having a lot of data. Fuck knows why. Most of it is rubbish2. But they do.\nThis is a problem for our poor little Gaussian processes because of how the data tends to come.\nA fairly robust model for data is that it comes like \\[\n(y_i, s_i, x_i),\n\\] where \\(y_i\\) is our measurement of choice (which might be a continuous, discrete or weird3), \\(s_i\\) is our location4 in the index set5 \\(\\Omega\\) (usually \\(\\Omega \\subset \\mathbb{R}^d\\)) of the Gaussian process, and \\(x_i\\) is whatever other information we have6. If we want to be really saucy, we could also assume these things are iid samples from some unknown distribution and then pretend like that isn’t a wildly strong structural assumption. But I’m not like that. I’ll assume the joint distribution of the samples is exchangeable7 8 9. Or something. I’m writing this sequentially, so I have no idea where this is going to end up.\nSo where is the problem? The problem is that, if we use the most immediately computational definition of a Gaussian process, then we need to build \\[\n\\begin{pmatrix} u(s_1)\\\\ u(s_2) \\\\ \\vdots \\\\ u(s_n)\\end{pmatrix} \\sim N\\left(\n\\begin{pmatrix} \\mu(s_1)\\\\ \\mu(s_2) \\\\ \\vdots \\\\ \\mu(s_n)\\end{pmatrix}, \n\\begin{pmatrix} c(s_1, s_1) & c(s_1, s_2) & \\cdots & c(s_1, s_n)  \\\\ c(s_2, s_1) & c(s_2, s_2) & \\cdots & c(s_2, s_n) \\\\\\vdots &\\vdots  &\\ddots &\\vdots \\\\ c(s_n, s_1) & c(s_n, s_2) & \\cdots & c(s_n, s_n)\\end{pmatrix}\\right).\n\\] Where \\(s_1,\\ldots, s_n\\) are all of the distinct values of \\(s_i\\) in the dataset. If there are a lot of these, the covariance matrix is very large and this becomes a problem. First, we must construct it. Then we must solve it. Then we must do actual computations with it. The storage scales quadratically in \\(n\\). The computation scales cubically in \\(n\\). This is too much storage and too much computation if the data set has a lot of distinct GP evaluations, it will simply be too expensive to do the matrix work that we need to do in order to make this run.\nSo we need to do something else.\nA tangent; or Can we just be smarter?\nOn a tangent, because straight lines are for poor souls who don’t know about Gaussian processes, there’s a body of work on trying to circumvent this problem by being good at maths. The idea is to try to find some cases where we don’t need to explicitly form the covariance matrix in order to do all of the calculations. There’s a somewhat under-cooked10 literature on this. It dances around an idea that traces back to fast multipole methods for integral equations: We know that correlations decay as points get further apart, so we do not need to calculate the correlations between points that are far apart as well as we need to calculate the correlations between points that are close together. For a fixed covariance kernel that decays in a certain way, you can modify the fast multipole method, however it’s more fruitful to use an algebraic11 method. H-matrices was the first real version of this, and there’s a paper from 2008 using them to approximate GPs. A solid chunk of time later, there have been two good papers recently on this stuff. Paper 1 Paper 2. These methods really only provide gradient descent type methods for maximum likelihood estimation and it’s not clear to me that you’d be able to extend these ideas easily to a Bayesian setting (particularly when you need to infer some parameters in the covariance function)12.\nI think this sort of stuff is cool for a variety of reasons, but I also don’t think it’s the entire solution. (There was also a 2019 NeurIPS paper that scales a GP fit to a million observations as if that’s a good idea. It is technically impressive, however.) But I think the main possibility of the H-matrix work is that it allows us to focus on the modelling and not have to make premature trade offs with the computation.\nThe problem with modelling a large dataset using a GP is that GPs are usually fit with a bunch of structural assumptions (like stationarity and isotropy) that are great simplifying assumptions for moderate data sizes but emphatically do not capture the complex dependency structures when there is a large amount of data. As you get more data, your model should become correspondingly more complex13 and stationary, and/or isotropic Gaussian processes emphatically do not do this.\nThis isn’t to say that you shouldn’t use GPs on a large data set (I am very much on record as thinking you should), but that it needs to be a part of your modelling arsenal and probably not the whole thing. The real glory of GPs is that they are a flexible enough structure to play well with other modelling techniques. Even if you end up modelling a large data set with a single GP, that GP will most likely be anisotropic, non-stationary, and built up from multiple scales. Which is a different way to say that it likely does not have a squared exponential kernel with different length scales for each feature.\n(It’s probably worth making the disclaimer at this point, but when I’m thinking about GPs, I’m typically thinking about them in 1-4 dimensions. My background is in spatial statistics, so that makes sense. Some of my reasoning doesn’t apply in more typical machine learning applications where \\(s_i\\) might be quite high-dimensional. That said, you simply get a different end of the same problem. In that case you need to balance the smoothness needed to interpolate in high dimensions with the structure needed to allow your variables to be a) scaled differently and b) correlated. Life is pain either way.)\nSo can we make things better?\nThe problem with Gaussian processes, at least from a computational point of view, is that they’re just too damn complicated. Because they are supported on some infinite dimensional Banach space \\(B\\), the more we need to see of them (for instance because we have a lot of unique \\(s_i\\)s) the more computational power they require. So the obvious solution is to somehow make Gaussian processes less complex.\nThis somehow has occupied a lot of people’s time over the last 20 years and there are many many many many possible options. But for the moment, I just want to focus on one of the generic classes of solutions: You can make Gaussian processes less computationally taxing by making them less expressive.\nOr to put it another way, if you choose an \\(m\\) dimensional subspace \\(V_m \\subset B\\) and rep;ace the GP \\(u\\), which is supported on the whole of \\(B\\), with a different Gaussian process \\(u_m\\) supported on \\(V_m\\), then all of your problems go away.\nWhy? Well because the Gaussian process on \\(V_m\\) can be represented in terms of an \\(m\\)-dimensional Gaussian random vector. Just take \\(\\phi_j\\), \\(j=1,\\ldots, m\\) to be a basis for \\(V_m\\), then the GP \\(u_m\\) can be written as \\[\nu_m = \\sum_{j=1}^m w_j \\phi_j,\n\\] where \\(w \\sim N(\\mu, \\Sigma)\\), for some \\(\\mu\\) and \\(\\Sigma\\). (The critical thing here is that the \\(\\phi_j\\) are functions so \\(u_m\\) is still a random function! That link between the multivariate Gaussian \\(w\\) and the function \\(u_m\\) that can be evaluated at any \\(s_i\\) is really important!)\nThis means that I can express my Gaussian process prior in terms of the multivariate Gaussian prior on \\(w\\), and I only need \\(\\mathcal{O}(m^3)\\) operations to evaluate its log-density.\nIf our observation model is such that \\(p(y_i \\mid u) = p(y_i \\mid u(s_i))\\), and we assume conditional14 independence, then we can eval the log-likelihood term \\[\n\\sum_{i=1}^n p(y \\mid u_m(s_i)) = \\sum_{i=1}^n p(y \\mid a_i^Tw)\n\\] in \\(\\mathcal{O}(m^2 n)\\) operations. Here \\([a_i]_j = \\phi_j(s_i)\\) is the vector that links the basis in \\(u_n\\) that we use to define \\(w\\) to the observation locations15.\nMany have been tempted to look at the previous paragraph and conclude that a single evaluation of the log-posterior (or its gradient) will be \\(\\mathcal{O}(n)\\), as if that \\(m^2\\) multiplier were just a piece of fluff to be flicked away into oblivion.\nThis is, of course, sparkly bullshit.\nThe subspace size \\(m\\) controls the trade off between bias and computational cost and, if we want that bias to be reasonably small, we need \\(m\\) to be quite large. In a lot of cases, it needs to grow with \\(n\\). A nice paper by David Burt, Carl Rasmussen, and Mark van der Wilk suggests that \\(m(n)\\) needs to depend on the covariance function16. In the best case (when you assume your function is so spectacularly smooth that a squared-exponential covariance function is ok), you need something like \\(m = \\mathcal{O}(\\log(n)^d)\\), while if you’re willing to make a more reasonable assumption that your function has \\(\\nu\\) continuous17 derivatives, then you need something like \\(m = \\mathcal{O}(n^\\frac{2d}{2\\nu-d})\\).\nYou might look at those two options for \\(m\\) and say to yourself “well shit. I’m gonna use a squared exponential from now on”. But it is never as simple as that. You see, if you assume a function is so smooth it is analytic18, then you’re assuming that it lacks the derring-do to be particularly interesting between its observed values19. This translates to relatively narrow uncertainty bands. Whereas a function with \\(\\nu\\) derivatives has more freedom to move around the smaller \\(\\nu\\) is. This naturally results in wider uncertainty bands.\nI think20 in every paper I’ve seen that compares a squared exponential covariance function to a Matérn-type covariance function (aka the ones that let you have \\(\\nu\\)-times differentiable sample paths), the Matérn family has performed better (in my mind this is also in terms of squared error, but it’s definitely the case when you’re also evaluating the uncertainty of the prediction intervals). So I guess the lesson is that cheap isn’t always good?\nAnyway. The point of all of this is that if we can somehow restrict our considerations to an \\(m\\)-dimensional subspace of \\(B\\), then we can get some decent (if not perfect) computational savings.\nBut what are the costs?\nSome notation that rapidly degenerates into a story that’s probably not interesting\nSo I guess the key question we need to answer before we commit to any particular approximation of our Gaussian process is what does it cost? That is, how does the approximation affect the posterior distribution?\nTo quantify this, we need a way to describe the posterior of a Gaussian process in general. As happens so often when dealing with Gaussian processes, shit is about to get wild.\nA real challenge with working with Gaussian processes theoretically is that they are objects that naturally live on some (separable21) Banach space \\(B\\). One of the consequences of this is that we cannot just write the density of \\(u\\) as \\[\np(u) \\propto  \\exp\\left(-\\frac{1}{2}C_u(u, u)\\right)\n\\] because there is no measure22 on \\(B\\) such that \\[\n\\Pr(u \\in A) = \\int_A p(u)\\,du.\n\\]\nThis means that we can’t just work with densities to do all of our Bayesian stuff. We need to work with posterior probabilities properly.\nUgh. Measures.\nSo let’s do this. We are going to need a prior probability associated with the Gaussian process \\(u\\), which we will write as \\[\n\\mu_0(A) = \\Pr(u \\in A),\n\\] where \\(A\\) is a nice23 set in \\(B\\). We can then use this as a base for our posterior, which we define as \\[\n\\mu^y(A) = \\Pr(u \\in A \\mid y) = \\frac{1}{Z}\\mathrm{e}^{-\\Phi(u;y)},\n\\] where \\(\\Phi(u;y)\\) is the negative log-likelihood function. Here \\(Z\\) is the normalising constant \\[\nZ = \\mathbb{E}_\\mu\\left( \\mathrm{e}^{-\\Phi(u;y)}\\right),\n\\] which is finite as long as \\(\\exp(-\\Phi(u;y)) \\leq C(\\epsilon)\\exp(\\epsilon\\|u\\|^2)\\) for all \\(\\epsilon > 0\\), where \\(C(\\epsilon)\\geq 0\\) is a constant. This is a very light condition.\nThis way of looking at posteriors resulting Gaussian process priors was popularised in the inverse problems literature24. It very much comes from a numerical analysis lens: the work is framed as here is an object, how do we approximate it?.\nThese questions are different to the traditional ones answered by a theoretical statistics papers, which are almost always riffs on “what happens in asymptopia?”.\nI came across this work for two reasons: one is because I have been low-key fascinated by Gaussian measures ever since I saw a talk about them during my PhD; and secondly my PhD was in numerical analysis, so I was reading the journals when these papers came out.\nThat’s not why I explored these questions, though. That is a longer story. The tl;dr is\n\nI had to learn this so I could show a particular point process model converges, and so now the whole rest of this blog post is contained in a technical appendix that no one has ever read in this paper.\n\nHere comes the anecdote. Just skip to the next bit. I know you’re like “but Daniel just delete the bullshit text” but that is clearly not how this works.\n\nExpand at your peril\nI know these papers pretty much backwards for the usual academic reason: out of absolute spite. One of my postdocs involved developing some approximation methods for Markovian Gaussian processes25, which allowed for fast computation, especially when combined with INLA26, which is a fabulous method for approximating posterior distributions when a big chunk of the unobserved parameters have a joint Gaussian prior27.\nOne of the things that INLA was already pretty good at doing was fitting log-Gaussian Cox processes (LGCP), which are a type of model for point patterns28 that can be approximated over a regular grid by a Poisson regression with a log-mean given by (covariates +) a Gaussian process defined on the grid. If that process is Markov, you can get full posterior inference quickly and accurately using INLA. This compared very favourably with pre-INLA methods, which gave you full posterior inference laboriously using a truncated gradient MALA scheme in about the same amount of time it would take the US to get a high-speed rail system.\nAnyway. I was in Trondheim working on INLA and the, at that stage, very new SPDE29 stuff (the 2011 JRSSSB read paper had not been written yet, let alone been read). Janine Illian, who is a very excellent statistician and an all round fabulous person, had been working on the grided LGCP stuff in INLA and came to Trondheim to work with Håvard30 and she happened to give a seminar on using these new LGCP methods to do species distribution mapping. I was strongly encouraged to work with Janine to extend her grid methods to the new shiny SPDE methods, which did not need a grid.\nJanine had to tell me what a Poisson distribution was.\nAnyway. A little while later31 we had a method that worked and we32 wrote it up. We submitted it to Series B and they desk rejected it. We then, for obscure reasons33, submitted it to Biometrika. Due to the glory of arXiv, I can link to the original version.\nAlthough it was completely unlike anything else that Biometrika publishes, we got some quite nice reviews and either major revisions or a revise and resubmit. But one of the reviewer comments pissed me off: they said that we hadn’t demonstrated that our method converges. Now, I was young at the time and new to the field and kinda shocked by all of the shonky numerics that was all over statistics at the time. So this comment34 pissed me off. More than that, though, I was fragile and I hated the comment because I was new to this and had absolutely no fucking idea how to prove this method would converge. Rasmus Waagepetersen had proven convergence of the grid approximation but a) I didn’t understand the proof and b) our situation was so far away there was no chance of piggybacking off it.\nIt was also very hard to use other existing statistics literature, because, far from being an iid situation, the negative log-likelihood for a Poisson process35 on an observation window \\(\\Omega\\) is \\[\n\\Phi(u;y) = \\int_\\Omega e^{u(s)}\\,ds - \\sum_{s_i \\in y}e^{u(s_i)} - |\\Omega|,\n\\] where the point pattern \\(y\\) is a (random) collection of points \\(s_j\\) and \\(|\\Omega|\\) is the area/volume of the observation window. This is fundamentally not like a standard GP regression.\nSo, long story short36, I was very insecure and rather than admit that it was difficult to show that these approximations converged, I worked on and off for like 2 years trying to work out how to do this37 and eventually came up with a fairly full convergence theory for posteriors derived from approximate likelihoods and finite dimensional approximations to Gaussian processes38. Which I then put into the appendix of a paper that was essentially about something completely different.\nI don’t have all that many professional regrets (which is surprising because I’ve made a lot of questionable choices), but I do regret not just making that appendix its own paper. Because it was really good work.\nBut anyway, I took the inverse problems39 work of Andrew Stuart and Masoumeh Dashti and extended it out to meet my needs. And to that end, I’m going to bring out a small corner of that appendix because it tells us what happens to a posterior when we replace a Gaussian process by a finite dimensional approximation.\nHow do we measure if a posterior approximation is good?\nPart of the struggle when you’re working with Gaussian processes as actual objects rather than as a way to generate a single finite-dimensional Gaussian distribution that you use for analysis is that, to quote Cosma Shalizi40, “the topology of such spaces is somewhat odd, and irritatingly abrupt”. Or to put it less mathematically, it is hard to quantify which Gaussian processes are close together.\nWe actually saw this in the last blog where we noted that the distribution of \\(v = cu\\) has no common support with the distribution of \\(u\\) if \\(|c| \\neq 1\\). This means, for instance, that the total variation between \\(u\\) and \\(v\\) is 2 (which is it’s largest possible value) even if \\(c = 1 + \\epsilon\\) for some tiny \\(\\epsilon\\).\nMore generally, if you’re allowed to choose what you mean by “these distributions are close” you can get a whole range of theoretical results for the posteriors of infinite dimensional parameters, ranging from this will never work and Bayes in bullshit to everything is wonderful and you never have to worry.\nSo this is not a neutral choice.\nIn the absence of a neutral choice, we should try to make a meaningful one! An ok option for that is to try to find functions \\(G(u)\\) that we may be interested in. Classically, we would choose \\(G\\) to be bounded (weak41 convergence / convergence in distribution) or bounded Lipschitz42. This is good but it precludes things like means and variances, which we would quite like to converge!\nThe nice thing about everything being based off a Gaussian process is that we know43 that there is some \\(\\epsilon > 0\\) (which may be very small) such that \\[\n\\mathbb{E}_{\\mu_0}\\left(\\mathrm{e}^{\\epsilon \\|u\\|_B^2}\\right) < \\infty.\n\\] This suggests that as long as the likelihood isn’t too evil, the posterior will also have a whole arseload of moments.\nThis is great because it suggests that we can be more ambitious than just looking at bounded Lipschitz functions. It turns out that we can consider convergence over the class of functionals \\(G\\) such that \\[\n|G(u) - G(v)| \\leq L(u) \\|u - v\\|_B,\n\\] where \\(\\mathbb{E}_{\\mu_0}(L(u)) < \\infty\\). Critically this includes functions like moments of \\(\\|u\\|_B\\) and, assuming all of the functions in \\(B\\) are continuous, moments of \\(u(s)\\). These are the functions we tend to care about!\nConvergence of finite dimensional Gaussian processes\nIn order to discuss the convergence of finite dimensional Gaussian processes, we need to define them and, in particular, we need to link them to some Gaussian process on \\(B\\) that they are approximating.\nLet \\(u\\) be a Gaussian process supported on a Banach space \\(B\\). We define a finite dimensional Gaussian process to be a Gaussian process supported on some space \\(V_m \\subset B\\) that satisfies \\[\nu_n = R_m u,\n\\] where \\(R_m: B \\rightarrow V_m\\) is some operator. (For this to be practical we want this to be a family of operators indexed by \\(m\\).)\nIt will turn out that how this restriction is made is important. In particular, we are going to need to see how stable this restriction is. This can be quantified by examining \\[\n\\sup_{\\|f\\|_V = 1} \\|R_m f\\|_{B} \\leq A_m \\|f\\|_V,\n\\] where \\(A_m > 0\\) is a constant that could vary with \\(m\\) and \\(V \\subseteq B\\) is some space we will talk about later. (Confusingly, I have set up the notation so that it’s not necessarily true that \\(V_m \\subset V\\). Don’t hate me because I’m pretty, hate me because I do stupid shit like that.)\nExample 1: An orthogonal truncation\nThere is a prototypical example of \\(R_m\\). Every Gaussian process on a separable Banach space admits a Karhunen-Loève representation \\[\nu = \\sum_{k = 0}^\\infty \\lambda_k^{1/2} z_k \\phi_k,\n\\] \\(z_k\\) are iid standard normal random variables and \\((\\lambda_k, \\phi_k)\\) are the eigenpairs44 of the covariance operator \\(C_u\\). The natural restriction operator is then \\[\nR_m f = \\sum_{j=0}^m \\langle f, \\phi_j\\rangle_{L^2}\\phi_j.\n\\] This was the case considered by Dashti and Stuart in their 2011 paper. Although it’s prototypical, we typically do not work with the Karhunen-Loève basis directly, as it tends to commit us to a domain \\(\\Omega\\). (Also because we almost45 never know what the \\(\\phi_j\\) are.)\nBecause this truncation is an orthogonal projection, it follows that we have the stability bound with \\(A_m = 1\\) for all \\(m\\).\nExample 2: Subset of regressors\nMaybe a more interesting example is the subset of regressors46. In this case, there are a set of inducing points \\(s_1, \\ldots, s_m\\) and \\[\nR_m f = \\sum_{j=1}^m w_j r_u(\\cdot, s_j),\n\\] where the weights solve \\[\nK_m w = b,\n\\] \\([K_m]_{ij} = r_u(s_i, s_j)\\) and \\(b_j = f(s_j)\\).\nIt’s a bit harder to get the stability result in this case. But if we let \\(V_m\\) have the RKHS47 norm, then \\[\\begin{align*}\n\\|R_m f\\|^2_{H_u} &= w^TK_m w \\\\\n&= b^T K^{-1} b \\\\\n&\\leq \\|K^{-1}\\|_2 \\|b\\|_2^2\n\\end{align*}\\]\nAssuming that \\(B\\) contains continuous functions, then \\(\\|b\\|_2 \\leq C\\sqrt{m} \\|f\\|_B\\). I’m pretty lazy so I’m choosing not to give a shit about that \\(\\sqrt{m}\\) but I doubt it’s unimprovable48. To be honest, I haven’t thought deeply about these bounds, I am doing them live, on my couch, after a couple of red wines. If you want a good complexity analysis of subset of regressors, google.\nMore interestingly, \\(\\|K_m^{-1}\\|_2\\) can be bounded, under mild conditions on the locations of the \\(s_j\\) by the \\(m\\)-th largest eigenvalue of the operator \\(Kf = \\int_\\Omega r_u(s,t)f(t)\\,dt\\). This eigenvalue is controlled by how differentiable \\(u\\) is, and is roughly \\(\\mathcal{O}\\left(m^{-\\alpha - d/2}\\right)\\) if \\(u\\) has a version with \\(\\alpha\\)-almost sure (Hölder) derivatives. In the (common) case where \\(u\\) is analytic (eg if you used the squared exponential covariance function), then this bound increases exponentially (or squared exponentially for the squared exponential) in \\(m\\).\nThis means that the stability constant \\(A_m \\geq \\|K_m^{-1}\\|\\) will increase with \\(m\\), sometimes quite alarmingly. Wing argues that it is always at least \\(\\mathcal{O}(m^2)\\). Wathan and Zhu have a good discussion for the one-dimensional case and a lot of references to the more general situation.\nExample 3: The SPDE method\nMy personal favourite way to approximate Gaussian processes works when they are Markovian. The Markov property, in general, says that if, for every49 set of disjoint open domains \\(S_1\\) and \\(S_2 = \\Omega \\backslash \\bar S_1\\) such that \\(S_1 \\cup \\Gamma \\cup S_2\\), where \\(\\Gamma\\) is the boundary between \\(S_1\\) and \\(S_2\\), then \\[\n\\Pr(A_1 \\cup A_2 \\mid B_\\epsilon) = \\Pr(A_1 \\mid B_\\epsilon) \\Pr(A_2 \\mid B_\\epsilon),\n\\] where \\(A_j \\in \\sigma\\left(\\{u(s), s \\in S_j\\}\\right)\\) and50 \\(B_\\epsilon \\in \\sigma\\left(\\{u(s); d(s, \\Gamma) < \\epsilon\\}\\right)\\) and \\(\\epsilon>0\\).\nWhich is to say that it’s the normal Markov property, but you may need to fatten out the boundary between disjoint domains infinitesimally for it to work.\nIn this case, we51 know that the reproducing kernel Hilbert space has the property that the inner product is local. That means that if \\(f\\) and \\(g\\) are in \\(H_u\\) and have disjoint support52 then \\[\n\\langle f, g\\rangle_{H_u} = 0,\n\\] which, if you squint, implies that the precision operator \\(\\mathcal{Q}\\) is a differential operator. (That the RKHS inner product being local basically defines the Markov property.)\nWe are going to consider a special case53, where \\(u\\) solves the partial differential equation \\[\nL u = W,\n\\] where \\(L\\) is some differential operator and \\(W\\) is white noise54.\nWe make sense of this equation by saying a Gaussian process \\(u\\) solves it if \\[\n\\int_\\Omega \\left(L^*\\phi(s)\\right)\\left( u(s)\\right)\\,ds  \\sim N\\left(0, \\int_\\Omega \\phi^2(s)\\,ds\\right),\n\\] for every smooth function \\(\\phi\\), where \\(L^*\\) is the adjoint of \\(L\\) (we need to do this because, in general, the derivatives of \\(u\\) could be a bit funky).\nIf we are willing to believe this exists (it does—it’s a linear filter of white noise, electrical engineers would die if it didn’t) then \\(u\\) is a Gaussian process with zero mean and covariance operator \\[\n\\mathcal{C} = (L^*L)^{-1},\n\\] where \\(L^*\\) is the adjoint of \\(L\\).\nThis all seems like an awful lot of work, but it’s the basis of one of the more powerful methods for approximating Gaussian processes on low-dimensional spaces (or low-dimensional manifolds). In particular in 1-3 dimensions55 or in (1-3)+1 dimensions56 (as in space-time), Gaussian processes that are built this way can be extremely efficient.\nThis representation was probably first found by Peter Whittle and Finn Lindgren, Johan Lindström and Håvard Rue combined it with the finite element method to produce the SPDE method57 A good review of the work that’s been done can be found here. There’s also a whole literature on linear filters and stochastic processes.\nWe can use this SDPE representation of \\(u\\) to construct a finite-dimensional Gaussian process and a restriction operator \\(R_m\\). To do this, we define \\(L_m\\) as the operator defined implicitly through the equation \\[\n\\langle \\phi, L\\psi\\rangle_{L^2} = \\langle \\phi, L_m\\psi\\rangle_{L^2}, \\quad \\forall \\phi,\\psi \\in V_m.\n\\] This is often called the Galerkin projection58. It is at the heart of the finite element method for solving elliptic partial differential equations.\nWe can use \\(L_m\\) to construct a Gaussian process with covariance function \\[\n\\mathcal{C}_m = (L_m^*L_m)^\\dagger,\n\\] where \\(^\\dagger\\) is a pseudo-inverse59.\nIt follows that \\[\n\\mathcal{C}_m  =(L_m)^\\dagger L L^{-1}(L^*)^{-1}L^*(L_m^*)^\\dagger = R_m \\mathcal{C} R_m^*,\n\\] where \\(R_m = L_m^\\dagger L\\).\nBefore we can get a stability estimate, we definitely need to choose our space \\(V_m\\). In general, the space will depend on the order of the PDE60, so to make things concrete we will work with second-order elliptic61 PDE \\[\nLu = -\\sum_{i,j = 1}^d\\frac{\\partial}{\\partial s_j}\\left(a_{ij}(s) \\frac{\\partial u}{\\partial s_i} \\right) +\\sum_{i=1}^d b_i\\frac{\\partial u}{\\partial s_i} + b_0(s)u(s),\n\\] where all of the \\(a_{ij}(s)\\) and \\(b_j(s)\\) are \\(L^\\infty(\\Omega)\\) and the uniform ellipticity condition62 holds.\nThese operators induce (potentially non-stationary) Gaussian processes that have continuous versions as long as63 \\(d \\leq 3\\).\nWith this fixed, the natural finite element space to use is the space of continuous piecewise linear functions. Traditionally, this is done using combinations of tent functions on a triangular mesh.\nA piecewise linear approximation. SourceWith this basis, we can get stability estimates by defining \\(v\\) and \\(v_m\\) by \\(Lv = f\\) and \\(L_m v_m = f_m\\), from which we get \\[\n\\|R_m v\\|_B = \\|v_m\\|_{V_m} \\leq A\\|f\\|_{L_2}\n\\] which holds, in particular, when the \\(L\\) has no first order derivatives64.\nAn oddity about this structure is that functions in \\(V_m\\) are not not continuously differentiable, while the sample paths of \\(u\\) almost surely are65. This means that \\(V_m\\) isn’t necessarily a subset of \\(B\\) as we would naturally define it. In this case, we need to inflate \\(B\\) to be big enough to contain the \\(V_m\\). So instead of taking \\(B = C^1(\\Omega)\\), we need to take \\(B = C(\\Omega)\\) or \\(B = L^2(\\Omega)\\).\nThis has implications on the smoothness assumptions on \\(\\Phi(u;y)\\), which will need to hold uniformly over \\(B\\) and \\(V_m\\) if \\(V_m \\not \\subset B\\) and on the set of functionals \\(G(u)\\) that we use to measure convergence.\nA bit of perspective\nThe critical difference between the SPDE method and the subset-of-regressors approximation is that for the SPDE method, the stability constant \\(A_m = A\\) is independent of \\(m\\). This will be important, as this constant pops up somewhere important when we are trying to quantify the error in the finite dimensional approximation.\nOn the other hand, the SPDE method only works in three and fewer dimensions and while it allows for quite flexible covariance structures66, it is can only directly construct Gaussian processes with integer numbers of continuous derivatives. Is this a problem? The asymptotics say yes, but they only hold if we are working with the exact Gaussian process (or, I guess, if we let the dimension of \\(V_m\\) hurtle off towards infinity as we get more and more data).\nIn practice, the Gaussian processes constructed via SPDE methods perform very well on real data67. I suspect part of this is that the stable set of basis functions are very good at approximating functions and the misspecification error plays off against the approximation error.\nBounding the approximation error\nWith all of this setup, we are finally ready to bound the error between the posterior we would get with the full Gaussian process prior and the posterior we would get using the finite dimensional Gaussian process prior.\nWe are going to deal with a simpler scenario than the paper we are (sort of) following, because in that situation, I was forced to deal with simultaneously approximating the likelihood and honestly who needs that trouble.\nTo remind ourselves, we have two priors: the full fat Gaussian process prior, the law of which we denote \\(\\mu_0\\) and the one we could possibly work with \\(\\mu_0^m\\). These lead to two different posteriors \\(\\mu_y\\) and \\(\\mu_y^m\\) given by \\[\n\\frac{d\\mu_y}{d\\mu_0}(u) = \\frac{1}{Z}\\mathrm{e}^{-\\Phi(u;y)} \\quad \\text{and}\\quad \\frac{d\\mu_y^m}{d\\mu_0^m}(u) = \\frac{1}{Z_m}\\mathrm{e}^{-\\Phi(u;y)} ,\n\\] where \\(Z_1\\) and \\(Z_m\\) are normalising constants.\nWe assume that the Gaussian process \\(u\\) is supported on some Banach space \\(V \\subseteq B\\) and the approximating spaces \\(V_m \\subset B\\). This covers the case where the approximating functions are rougher than the true realisations of the Gaussian process we are approximating. With this notation, we have the restriction operator \\(R_m\\) that satisfies \\[\n\\|R_mf\\|_{V_m} \\leq A_m \\|f\\|_V,\n\\] which is a slightly more targeted bound when \\(B\\) is larger than \\(V\\).\nWe will make the following assumptions about the negative log-likelihood (or potential function) \\(\\Phi\\): For every \\(\\epsilon > 0\\), \\(r> 0\\), and68 \\(\\|y\\| < r\\), there exist positive constants \\(C_1, C_2, C_3, C_4\\) that may depend on \\(\\epsilon\\) and \\(r\\) such that the following 4 conditions hold. (Note: when the norm isn’t specified, we want it to hold over both the \\(V\\) and \\(B\\) norms.)\nFor all \\(u \\in V \\cup \\left(\\bigcup_{m\\geq 1} V_m\\right)\\) \\[\n\\Phi(u;y) \\geq C_1 - \\epsilon \\|u\\|^2\n\\]\nFor every \\(u\\in B\\), \\(y \\in Y\\) with \\(\\max \\{\\|u\\|, \\|y\\|_Y\\} < r\\), \\[\n\\Phi(u;y) \\leq C_2\n\\]\nFor every \\(\\max \\{\\|u_1\\|_V, \\|u_2\\|_B, \\|y\\|_Y\\} < r\\), \\[\n|\\Phi(u_1; y) - \\Phi(u_2; y )| \\leq \\exp\\left(\\epsilon\\max\\{\\|u_1\\|_V^2, \\|u_2\\|_B^2\\} - C_3\\right) \\|u_1 - u_2\\|_B\n\\]\nFor every \\(u\\in B\\) and \\(\\max \\{\\|y_1\\|_Y, \\|y_2\\|_Y\\} < r\\), \\[\n|\\Phi(u; y_1) - \\Phi(u; y_2) | \\leq  \\exp\\left(\\epsilon \\|u\\|^2 + C_4\\right)\\|y_1 - y_2\\|_Y\n\\]\nThese restrictions are pretty light and are basically what are needed to make sure the posteriors exist. The first one say “don’t grow too fast” to the likelihood and is best explained while humming ABBA’s Slipping Through My Fingers. The second one makes sure the likelihood isn’t zero. The third and fourth are Lipschitz conditions that basically make sure that a small change in \\(u\\) (or \\(y\\)) doesn’t make a big change in the likelihood. It should be pretty clear that if that could happen, the two posteriors wouldn’t be close.\nWe are also going to need some conditions on our test functions. Once again, we need them to apply over \\(V\\) and \\(B\\) when no space is specified for the norm.\nFor all \\(u \\in V\\), \\(G(u) = \\exp(\\epsilon \\|u\\|^2_V+ C_5)\\)\nFor all \\(u_1 \\in V\\), \\(u_2 \\in V_m\\), \\[\n|G(u_1) - G(u_2)| \\leq \\exp(\\epsilon\\max\\{\\|u_1\\|^2_V, \\|u_2\\|^2_B\\})\\|u_1 - u_2\\|_B.\n\\]\nUnder these conditions, we get the following theorem, which is a simplified version of Theorem A2 here.\n\nTheorem Under the above assumptions, \\[\n\\left|\\mathbb{E}_{\\mu_y}(G(u)) - \\mathbb{E}_{\\mu^m_y}(G(u_m))\\right| \\leq C_m \\sup_{f \\in V}\\left(\\frac{\\|f - R_m f\\|_B}{\\|f\\|_V}\\right),\n\\] where \\(C_m\\) only depends on \\(m\\) through \\(A_m\\).\n\nI seriously doubt that the dependence on \\(A_m\\) is exponential, as it is in the proof, but I’m not going to try to track that down. That said, I’m also quite sure that the dependence \\(C_m\\) is not uniform in \\(m\\) unless \\(A_m\\) is constant.\nIt’s also worth noting that there’s nothing special about \\(G\\) being real-valued. In general it can take values in any Banach space \\(E\\). Just replace all those absolute values with norms. That means that the result covers convergence of approximations to things like covariance matrices.\n\nProof, if you’re interested\nWe are interested in approximating \\[\ne_G = \\left|\\mathbb{E}_{\\mu_y}(G(u)) - \\mathbb{E}_{\\mu^m_y}(G(u_m))\\right|.\n\\] We can expand this to get \\[\\begin{align*}\ne_G \\leq & \\frac{1}{Z}\\left|\\mathbb{E}_{\\mu_0}\\left(G(u)\\exp(-\\Phi(u;y))\\right)\n- \\mathbb{E}_{\\mu_0^m}\\left(G(u_m)\\exp(-\\Phi(u_mm;y))\\right)\\right| \\\\\n&\\quad +\n\\left|\\frac{1}{Z} \n - \\frac{1}{Z_m}\\right|\\mathbb{E}_{\\mu_0^m}\\left(|G(u_m)|\\exp(-\\Phi(u_m;y))\\right). \\\\\n &= B_1 + B_2.\n\\end{align*}\\]\nIt follows from Andrew Stuart’s work that the normalising constants \\(Z\\) and \\(Z_m\\) can be bounded above and below independently of \\(m\\), so the above expression makes sense.\nWe will now attack \\(B_1\\) and \\(B_2\\) separately. To do this, we need to consider the joint prior \\(\\lambda_0(u, u_m)\\) that is the joint law of the Gaussian process \\(u\\) and its finite dimensional approximation \\(u_m = R_m u\\).\nFor \\(B_1\\) we basically use the same trick again. \\[\\begin{align*}\nZB_1 \\leq & \\mathbb{E}_{\\lambda_0}\\left(|G(u)|\\left|\\exp(-\\Phi(u;y)) -\\exp(\\Phi(u;y))\\right| \\right) \\\\\n&\\quad + \\mathbb{E}_{\\lambda_0}\\left(\\exp(-\\Phi(u_m;y)) | G(u) - G(u_m)|\\right) \\\\\n&\\leq  \\mathbb{E}_{\\lambda_0}\\left(\\mathrm{e}^{C_5 + \\epsilon \\|u\\|_V^2}\\mathrm{e}^{\\epsilon\\max\\{1,A_m\\}\\|u\\|_v^2 - C_1} \\mathrm{e}^{\\epsilon\\max\\{1,A_m\\}\\|u\\|_V^2 + C_3}\\|u - u_m\\|_B\\right) \\\\\n& \\quad +\\mathbb{E}_{\\lambda_0}\\left(\\mathrm{e}^{\\epsilon A_m\\|u\\|_V^2 - C_1}\\mathrm{e}^{\\epsilon\\max\\{1,A_m\\}\\|u\\|_V^2 + C_6}\\|u - u_m\\|_V\\right) \\\\\n&\\leq \\sup_{f \\in V}\\left(\\frac{\\|f - R_m f\\|_B}{\\|f\\|_V}\\right) \n\\mathrm{e}^{C_3 + C_5 + C_6 -2 C_1}\\mathbb{E}_{\\mu_0}\\left(\\|u\\|_V\\mathrm{e}^{(1+3\\max\\{1,A_m\\} + A_m)\\epsilon \\|u\\|_V^2}\\right)\\\\\n&\\leq C_7 \\sup_{f \\in V}\\left(\\frac{\\|f - R_m f\\|_B}{\\|f\\|_V}\\right),\n\\end{align*}\\] where the second inequality comes from using all of the assumptions on \\(\\Phi\\) and \\(G\\) and noting that \\(\\left|e^{-x} - e^{-y}\\right| \\leq e^{-\\min\\{x,y\\}}|x-y|\\); and the final inequality comes from Fernique’s theorem, which implies that expectation is finite.\nWe can also bound \\(B_2\\) by noting that \\[\\begin{align*}\n\\left|Z^{-1} - Z_m^{-1} \\right| & \\leq \\max \\{Z^{-2}, Z_m^{-2}\\}\\mathbb{E}_{\\lambda_0}\\left(|\\exp(-\\Phi(u;y)) - \\exp(-\\Phi(u_m;z))\\right) \\\\\n&\\leq C_8 \\sup_{f \\in V}\\left(\\frac{\\|f - R_m f\\|_B}{\\|f\\|_V}\\right) \n\\end{align*}\\] by the same reasoning as above.\nDealing with the approximation error\nThe theorem above shows that the worst-case error in posterior functionals caused by replacing a Gaussian process \\(u\\) with it’s approximation \\(u_m = R_m u\\) is driven entirely by how well a general function from \\(V\\) can be approximated by a function in \\(V_m\\). This is not really a surprising result: if the approximation \\(u_m\\) is unable to approximate the sample paths of \\(u\\) it is very unlikely it will do a good job with all functionals.\nThankfully, approximation error is one of the better studied things in this world. Especially in the case where \\(V = B\\).\nFor instance, it’s pretty easy to show69 that if \\(u\\) has \\(\\nu\\) derivatives, then \\(e_G \\leq Cm^{-\\frac{\\nu}{d} + \\epsilon}\\) for all \\(\\epsilon>0\\).\nIf you dive deep enough into the literature, you can get similar results for the type of approximation underneath the subset of regressors approximation.\nFor the SPDE approximation, it’s all a little bit more tricky as \\(V_m \\not \\subset V\\). But ultimately, you get that, for any \\(\\epsilon >0\\), \\(e_G \\leq C h^{1-\\epsilon}\\), where \\(h\\) is a measure of the mesh size70. This is roughly what you’d expect, there’s a loss of \\(\\epsilon\\) from the ordinary interpolation rate which may or may not be a result of me being a bit shit at maths.\nThe argument that gets us here is really cute so I’ll sketch it below. This is here for two reasons: firstly, because I think it’s cool and secondly because the paper is so compressed it’s hard to completely follow the argument, so I thought it would be nice to put on here. (It also took me a whole afternoon to decipher the proof in the paper, which is usually a sign that it could do with a bit of a re-write. How successfully I clarified it is something I will leave up to others to decide.)\n\nFinite element shit\nSetup. Gird yourselves!\nWe are going to bound that error rate in a way that’s relevant for the finite element method. The natural choices for the function spaces are \\(V = H^{1-\\epsilon}(\\Omega)\\) for some fixed \\(0 < \\epsilon < 1/2\\) (close to zero is what we want). and \\(B = L^2(\\Omega)\\). (To be honest the domain \\(\\Omega\\) isn’t changing so I’m gonna forget it sometimes.)\nOnce again, we’re going to assume that \\(L\\) is a second order uniformly elliptic PDE with no first-order terms (aka \\(b_1 = \\cdots = b_d = 0\\)) and that \\(b_0(s) >0\\) on some subset of \\(\\Omega\\). We will use the symmetric, coercive bilinear form associated71 with \\(L\\), which we can define, for any \\(u,v \\in H^1\\), as \\[\na(u, v) = \\int_\\Omega (A(s)\\nabla u(s))\\cdot \\nabla v(s)\\,ds + \\int_\\Omega b_0(s) u(s)v(s)\\,ds\n\\]\nRemembering that \\(R_m = LL_m^\\dagger\\), we have \\[\n\\sup_{v \\in V}\\frac{ \\left\\|v - R_m v\\right\\|_B}{\\|v\\|_V} =\\sup_{f\\in LV}\\frac{ \\left\\|L^{-1}f - L_n^{\\dagger}f\\right\\|_B}{\\|L^{-1}f\\|_V}.\n\\]\nThe set of functions \\(f \\in LV\\) is the set of all functions \\(f = Lv\\) for some \\(v \\in V\\). It can be shown that \\(LV = H^{-1-\\epsilon}\\), where the negative index indicates a dual Sobolev space (aka the space of continuous linear functionals on \\(H^{1+ \\epsilon}\\)).\nThis means that we are looking at the difference between the solution to \\(Lu = f\\) and \\(L_m u_m = f_m\\), where \\(f_m\\) is the \\(L^2\\)-orthogonal projection of \\(f\\) onto \\(V_m\\), which is the space of piecewise linear functions on some72 triangular mesh \\(\\mathcal{T}_m\\).\nWe define the projection of the function \\(f \\in H^{-1-\\epsilon}(\\Omega)\\) onto \\(V_m\\) as the unique function \\(f_m \\in V_m\\) such that73 \\[\n\\int_\\Omega f_n(s) v_n(s)\\,ds = \\int f(s) v_n(s)\\,ds, \\quad \\forall v_n \\in V_n.\n\\]\nNow let’s do this!\nWith all of this in place, we can actually do something. We want to bound \\[\n\\frac{\\|u - u_m\\|_{L^2}}{\\|u\\|_{H^{1+\\epsilon}}},\n\\] where74 \\(a(u, \\phi) = \\int_\\Omega f(s) \\phi(s)\\,ds\\) for all \\(\\phi \\in H^{1+\\epsilon}\\) and \\(a(u_m, \\phi_m) = \\int_\\Omega f(s) \\phi_m(s)\\,ds\\) for all \\(\\phi_m \\in V_m \\subset H^{1+\\epsilon}\\).\nThe key observation is that \\[\n\\int_\\Omega f(s) \\phi_m(s)\\,ds = \\int_\\Omega f_m(s) \\phi_m(s)\\,ds,\n\\] which suggests that \\(u_m(s)\\) is an approximation to two different problems!\nLet’s write this second problem down! We want to find \\(z^{(m)}\\) such that \\[\na({z}^{(m)}, \\phi) = \\int_\\Omega f_n(s) \\phi(s)\\,ds \\quad \\forall \\phi \\in H^{1} ,\n\\] where the \\(m\\) superscript indicates that it depends on \\(m\\) through it’s right hand side. The projection \\(f_n \\in L^2\\), which means that we are in the realm of usual PDEs and (assuming some regularity) \\(z^{(m)} \\in H^2\\).\nHence, we can write \\[\n\\|u - u_m\\|_{L^2}\\leq \\|u - z^{(m)}\\|_{L^2} + \\|z^{(m)} - u_m\\|_{L^2}.\n\\]\nWe can bound the second term almost immediately from standard finite element theory, which says that \\[\n\\|z^{(m)} - u_m\\|_{L^2} \\leq Ch^2 \\|f_n\\|_{L^2}.\n\\]\nTo estimate \\(\\|f_m\\|\\) we use the inverse estimates of Ben Belgacem and Brenner to show that, for any \\(v\\in L^2(\\Omega)\\), \\[\n\\int_\\Omega f_m(s) v(s) \\,ds = \\int_\\Omega f(s)v_m(s)  \\,ds\\leq\\|f\\|_{H^{-1-\\epsilon}}\\|v_m\\|_{H^{1+\\epsilon}} \\leq Ch^{-1-\\epsilon} \\|f\\|_{H^{-1-\\epsilon}} \\|v\\|_{L^2},\n\\] where \\(v_m\\) is the orthogonal projection of \\(v\\) onto \\(V_m\\).\nIf we set \\(v = f_m\\) in the above equation, we get \\(\\|f_m\\|_{L^2} \\leq Ch^{-1-\\epsilon} \\|f\\|_{H^{-1-\\epsilon}}\\), which combines with our previous estimate to give \\[\n\\|z^{(m)} - u_m\\|_{L^2} \\leq Ch^{1-\\epsilon} \\|f_n\\|_{L^2}.\n\\]\nFinally, to bound \\(\\|u - z^{(m)}\\|_{L^2}\\) we are going to use one of my75 favourite arguments. Fix \\(w \\in L^2\\) and let \\(W\\) be the solution of the dual equation \\(a(\\phi, W) = \\int_\\Omega \\phi(s)w(s)\\,ds\\). It then follows that, for any \\(v_m \\in V_m\\), \\[\\begin{align*}\n\\left|\\int_\\Omega (u(s) - z^{(m)}(s))w(s)\\,ds\\right| &= \\left|a(u - z^{(m)}, W)\\right| \\\\\n&= \\left|\\int_\\Omega (f(s) - f_m(s))W(s)\\,ds\\right|\\\\\n&= \\left|\\int_\\Omega (f(s) - f_m(s))(W(s) - v_m(s))\\,ds\\right|\\\\\n&\\leq\\left|\\int_\\Omega f(s)(W(s) - v_m(s))\\,ds\\right|+  \\left|\\int_\\Omega f_m(s)(W(s) - v_m(s))\\,ds\\right| \\\\\n&\\leq \\|f\\|_{H^{-1-\\epsilon}}\\|W - v_m\\|_{H^{1+\\epsilon}} + Ch^{-1-\\epsilon} \\|f\\|_{H^{-1-\\epsilon}} \\|W - v_m\\|_{L^2} \\\\\n&\\leq C \\|f\\|_{H^{-1-\\epsilon}} h^{-1 -\\epsilon}\\left(h^{1+\\epsilon}\\|W - v_m\\|_{H^{1+\\epsilon}} +  \\|W - v_m\\|_{L^2} \\right),\n\\end{align*}\\] where the first line uses the definition of \\(W\\); the second uses the definition of \\(u\\) and \\(z^{(m)}\\); the third uses the fact that \\((f - f_m) \\perp V_m\\) so subtracting off \\(v_m\\) doesn’t change anything; the fourth is the triangle inequality; the fifth is the Hölder inequality on the left and the estimate from half a screen up on the right; and the sixth line is clean up.\nBecause the above bound holds for any \\(v_m \\in V_m\\), we can choose the one that makes the bound the smallest. This leads to \\[\\begin{align*}\n\\left|\\int_\\Omega (u(s) - z^{(m)}(s))w(s)\\,ds\\right| &\\leq  C \\|f\\|_{H^{-1-\\epsilon}} h^{-1 -\\epsilon}\\inf_{v \\in V_m}\\left(h^{1+\\epsilon}\\|W - v_m\\|_{H^{1+\\epsilon}} +  \\|W - v_m\\|_{L^2} \\right) \\\\\n& \\leq C\\|f\\|_{H^{-1-\\epsilon}} h^{-1 -\\epsilon} h^2 \\|W\\|_{H^2}\\\\\n&\\leq C h^{1-\\epsilon} \\|w\\|_{L^2},\n\\end{align*}\\] where the last two inequalities are Theorem 14.4.2 from Brenner and Scott and a standard estimate of the solution to an elliptic PDE by it’s RHS.\nPutting this all together we get the result. Phew.\nThis whole argument was a journey, but I think it’s quite pretty. It’s clobbered together from a lot of sleepless nights and an argument inspired by strip-mining76 a Ridgeway Scott paper from 1976. Anyway, I think it’s nifty.\nWrapping it up\nSo. That was quite a lot. I enjoyed it, but I’m weird like that. This has mostly been me trying to remember what I did in 2015. Why? Because I felt like it.\nI also think that there’s some value in this way of thinking about Gaussian processes and it’s nice to show off some ways to use all of that weird shit in the last post.\nAll of these words can be boiled down to this take away:\n\nIf your finite dimensional GP \\(u_m\\) is linked to a GP \\(u\\) by some (potentially non-linear relationship) \\(u_m= R_m u\\), then the posterior error will be controlled by how well you can approximate a function \\(v\\) that could be a realisation of the GP by \\(R_m v\\).\n\nThis is a very intuitive result if you are already thinking of GP approximation as approximating a random function. But a lot of the literature takes a view that we are approximating a covariance matrix or a multivariate normal. This might be enough to approximate a maximum likelihood estimator, but it’s insufficient for approximating a posterior77\nFurthermore, because most of the constants in the bounds don’t depend too heavily on the specific finite dimensional approximation (except through \\(A_m\\)), we can roughly say that if we have two methods for approximating a GP, the one that does a better job at approximating functions will be the better choice.\nAs long as it was, this isn’t a complete discussion of the problem. We have not considered hyper-parameters! This is a little bit tricky because if \\(\\mu_0\\) depends on parameters \\(\\theta\\), then \\(R_m\\) will also depend on parameters (and for subset of regressors, \\(V_m\\) also depends on the parameters).\nIn theory, we could use this to bound the error in the posterior \\(p(\\theta \\mid y)\\). To see how we would do that, let’s consider the case where we have Gaussian observations.\nThen we get \\[\\begin{align*}\np(\\theta \\mid y) & \\frac{\\exp(-\\Phi(u;y))}{p(y)} \\left[\\frac{d\\mu_y}{d\\mu_0}\\right]^{-1} p(\\theta) \\\\\n&= \\frac{Z(\\theta) p(\\theta)}{\\int_\\Theta Z(\\theta)p(\\theta)\\,d\\theta},\n\\end{align*}\\] where \\(Z(\\theta) = \\mathbb{E}_{\\mu_0}\\left(e^{-\\Phi(u;y)}\\right)\\).\nWe could undoubtedly bound the error in this using similar techniques to the ones we’ve already covered (in fact, we’ve already got a bound on \\(|Z - Z_m|\\)). And then it would just be a matter of piecing it all together.\nBut I’m tired and I just want to cry for me.\n\nNaively: a condescending way to say “the way you were told to use them”↩︎\nIs it better to have a large amount of crappy data or a small amount of decent data? Depends on if you’re trying to impress people by being right or by being flashy.↩︎\nWho doesn’t love a good shape. Or my personal favourite: a point pattern.↩︎\nOr, hell, this is our information about how to query the Gaussian process to get the information we need for this observation. Because, again, this does not have to be as simple as evaluating the function at a point!↩︎\nThis could be time, space, space-time, covariate space, a function space, a lattice, a graph, an orthogonal frame, a manifold, a perversion, whatever. It doesn’t matter. It’s all just Gaussian processes. Don’t let people try to tell you this shit is fancy.↩︎\nThis could be covariate information, group information, hierarchy information, causal information, survey information, or really anything else you want it to be. Take a deep breath. Locate your inner peace. Add whatever you need to the model to make it go boop.↩︎\nI will never use this assumption. Think of it like the probability space at the top of a annals of stats paper.↩︎\nSo the thing is that this is here because it was funny to me when I wrote it, but real talk: just being like “it’s iid” is some real optimism (optimism, like hope, has no place in statistics.) and pretending that this is a light or inconsequential assumption is putting some bad energy out into the world. But that said, I was once a bit drunk at a bar with a subjective Bayesian (if you want to pick your drinking Bayesian, that’s not a bad choice. They’re all from The North) and he was screaming at me for thinking about what would happen if I had more data, and I was asking him quietly and politely how the data could possibly inform models as complex as he seemed to be proposing. And he said to me: what you do is you look for structures within your data that are exchangeable in some sense (probably after conditioning) and you use those as weak replicates. And, of course, I knew that but I’d never thought about it that way. Modelling, eh. Do it properly.↩︎\nThese (and the associated parenthetical girls) were supposed to be nested footnotes but Markdown is homophobic and doesn’t allow them. I am being oppressed.↩︎\nIt’s an interesting area, but the tooling isn’t there for people who don’t want to devote a year of their lives to this to experiment.↩︎\nThis is what matrix nerds say when they mean “I love you”. Or when they mean that it’s all derived from the structure of a matrix rather than from some structural principles stolen from the underlying problem. The matrix people are complicated.↩︎\nThe reason for this is that, while there are clever methods for getting determinants of H-matrices, they don’t actually scale all that well. So Geoga, Anitescu, and Stein paper use a Hutchinson estimator of the log-determinant. This has ok relative accuracy, but unfortunately, we need it to have excellent absolute accuracy to use it in a Bayesian procedure (believe me, I have tried). On the other hand, the Hutchinson estimator of the gradient of the log-determinant is pretty stable and gives a really nice approximate gradient. This is why MLE type methods for learning the hyper-parameters of a GP can be made scalable with H-matrix techniques.↩︎\nOtherwise, why bother. Just sub-sample and get on the beers. Or the bears. Or both. Whatever floats your boat.↩︎\non \\(u\\) and probably other parameters in the model↩︎\nDual spaces, y’all. This vector was inevitable because \\(m\\)-dimensional row vectors are the dual space of \\(\\mathbb{R}^m\\), while \\(s_i \\rightarrow u(s_i)\\) is in \\(B^*\\).↩︎\nThis is not surprising if you’re familiar with the sketching-type bounds that Yang, Pilanci and Wainwright did a while back (or, for that matter, with any non-asymptotic bounds involving the the complexity of the RKHS). Isn’t maths fun.↩︎\nHölder↩︎\nThink “infinitely differentiable but more so”.↩︎\nAn analytic function is one that you know will walk straight home from the pub, whereas a \\(\\nu\\)-differentiable function might just go around the corner, hop on grindr, and get in a uber. Like he’s not going to go to the other side of the city, but he might pop over to a nearby suburb. A generalised function texts you a photo of a doorway covered by a bin bag with a conveniently placed hole at 2am with no accompanying message other than an address↩︎\nI mean, I cannot be sure, but I’m pretty sure.↩︎\nAgain, not strictly necessary but it removes a tranche of really annoying technicalities and isn’t an enormous restriction in practice.↩︎\nThe result is that there is no non-trivial translation invariant measure on a separable Banach space (aka there is no analogue of the Lebesgue measure). You can prove this by using separability to make a disjoint cover of equally sized balls, realise that they would all have to have the same measure, and then say “Fuck. I’ve got too many balls”.↩︎\nBorel. Because we have assumed \\(B\\) is separable, the cylindrical \\(\\sigma\\)-algebra is identical to the Borel \\(\\sigma\\)-algebra and \\(\\mu_0\\) is a Radon measure. Party.↩︎\nSee Andrew Stuart’s long article on formulating Bayesian problems in this context and Masoumeh Dashti and Andrew Stuart’s paper paper on (simple) finite dimensional approximations.↩︎\nThe SPDE approach. Read on Macduff.↩︎\nthe Irish National Liberation Army↩︎\nThis covers GP models, GAMs, lots of spatial models, and a bunch of other stuff.↩︎\nLike, the data is a single observation of a point pattern. Or, to put it a different way, a list of (x,y) coordinates of (a priori) unknown length.↩︎\nApproximate Markovian GPs in 2-4 dimensions. See here for some info↩︎\nRue. The king of INLA. Another all round fabulous person. And a person foolish enough to hire me twice even though I was very very useless.↩︎\nIn the interest of accuracy, Janine and I were giving back to back talks at a conference that we decided for some reason to give as a joint talk and I remember her getting more and more agitated as I was sitting in the back row of the conference desperately trying to contort the innards of INLA to the form I needed to make the damn thing work. It worked and we had results to present. We also used the INLA software in any number of ways it had not been used before that conference. The talk was pretty well received and I was very relieved. It was also my first real data analysis and I didn’t know to do things like “look at the data” to check assumptions, so it was a bit of a clusterfuck and again Janine was very patient. I was a very useless 25 year old and a truly shit statistician. But we get better if we practice and now I’m a perfectly ok statistician.↩︎\nJanine and I, with Finn Lindgren, Sigrunn Sørbye and Håvard Rue, who were all heavily involved throughout but I’m sure I’ve already exhausted people’s patience.↩︎\nIIRC, Sigrunn’s university has one of those stupid lists where venue matters more than quality. Australia is also obsessed with this. It’s dumb.↩︎\nIn hindsight, the reviewer was asking for a simulation study, which is a perfectly reasonable thing to ask for but at the time I couldn’t work out how to do that because, in my naive numerical analyst ways, I thought we would need to compare our answer to a ground truth and I didn’t know how to do that. Now I know that the statistician way is to compute the same thing two different ways on exactly one problem that’s chosen pretty carefully and saying “it looks similar”.↩︎\nConditional on the log-intensity surface, a LGCP is a Poisson process↩︎\nis it, though↩︎\nMy co-authors are all very patient.↩︎\nwith fixed hyper-parameters↩︎\nThe thing about inverse problems is that they assume \\(\\Phi(u;y)\\) is the solution of some PDE or integral equation, so they don’t make any convenient simplifying assumptions that make their results inapplicable to LGCPs!↩︎\nhttps://arxiv.org/pdf/0901.1342.pdf↩︎\nstar↩︎\nAlso weak convergence but metrized by the Wasserstein-1 distance.↩︎\nFernique’s Theorem. I am using “we” very liberally here. Fernique knew and said so in French a while back. Probably the Soviet probabilists knew too but, like, I’m not going to write a history of exponential moments.↩︎\nOn \\(L^2\\), which is a Hilbert space so the basis really is countable. The result is a shit-tonne easier to parse if we make \\(B\\) a separable Hilbert space but I’m feeling perverse. If you want the most gloriously psychotic expression of this theorem, check out Theorem 7.3 here↩︎\nThere are tonnes of examples where people do actually use the Karhunen-Loève basis or some other orthogonal basis expansion. Obviously all of this theory holds over there.↩︎\nThis has many names throughout the literature. I cannae be arsed listing them. But Quiñonero-Candela, Rasmussen, and Williams attribute it to Wahba’s book in 1990.↩︎\nFunctions of the form \\(\\sum_{i=1}^m a_j r_u(\\cdot, s_j)\\) are in the RKHS corresponding to covariance function \\(r_u\\). In fact, you can characterise the whole space as limits of sums that look like that.↩︎\nI mean, we are not going to be using the \\(A_m\\) to do anything except grow with \\(m\\), so the specifics aren’t super important. Because this is a blog post.↩︎\nNot every. You do this for nice sets. See Rozanov’s book on Markov random fields if you care.↩︎\n\\(d(s, A) = \\inf_{s'\\in A} \\|s - s'\\|\\)↩︎\nRozanov↩︎\nThe sets on which they are non-zero are different↩︎\nFor general Markov random fields, this representation still exists, but \\(L\\) is no longer a differential operator (although \\(L^*L\\) must be!). All of the stuff below follows, probably with some amount of hard work to get the theory right.↩︎\nWhat is white noise? It is emphatically not a stochastic process that has the delta function as it’s covariance function. That thing is just ugly. In order to make any of this work, we need to be able to integrate deterministic functions with respect to white noise. Hence, we view it as an independently scattered random measure that satisfies \\(W(A) \\sim N(0, |A|)\\) and \\(\\int_A f(s)W(ds) \\sim N(0, \\int_A f(s)^2\\, ds)\\). Section 5.2 of Adler and Taylor’s book Random Fields and Geometry is one place to learn more.↩︎\nThis paper is a solid review↩︎\nThis paper↩︎\nFinite element methods had been used before, especially in the splines community, with people like Tim Ramsay doing some interesting work. The key insight of Finn’s paper was to link this all to corresponding infinite dimensional Gaussian processes.↩︎\nWe’re assuming \\(V_m\\subset L^2(\\Omega)\\), which is not a big deal.↩︎\nSee the paper for details of exactly which pseudo-inverse. It doesn’t really matter tbh, it’s just we’ve got to do something with the other degrees of freedom.↩︎\nConsult your favourite finite element book and then get pissed off it doesn’t cover higher-order PDEs in any detail.↩︎\nIt looks like this is vital, but it isn’t. The main thing that changes if your PDE is hyperbolic or parabolic or hypo-elliptic is how you do the discretisation. As long as the PDE is linear, this whole thing works in principle.↩︎\nFor some \\(\\alpha>0\\), \\(\\sum_{i,j=1}^d w_iw_ja_{ij}(s) \\geq \\alpha \\sum_{i=1}^d w_i^2\\) holds for all \\(s \\in \\Omega\\).↩︎\nFor this construction to work in higher dimensions, you need to use a higher-order differential operator. In particular, if you want a continuous field on some subset of \\(\\mathbb{R}^d\\), you need \\(L\\) to be a differential operator of order \\(>d/2\\) or higher. So in 4 dimensions, we need the highest order derivative to be at least 4th order (technically \\(L\\) could be the square root of a 6th order operator, but that gets hairy).↩︎\nIt holds in general, but if the linear terms are dominant (a so-called advection-driven diffusion), then you will need a different numerical method to get a stable estimate.↩︎\nModulo some smoothness requirements on \\(\\Omega\\) and \\(a_{ij}(s)\\).↩︎\nIt’s very easy to model weird anisotropies and to work on manifolds↩︎\neg this comparison↩︎\nLet’s not let any of the data fly off to infinity!↩︎\nCorollary A2 in the paper we’re following↩︎\nThink of it as the triangle diameter if you want.↩︎\nIntegration by parts gives us \\(\\int_\\Omega (Lu(s))v(s)\\,ds = a(u,v)\\) if everything is smooth enough. We do this to confuse people and because it makes all of the maths work.↩︎\nnot weird↩︎\n\\(f\\) is a generalised function so we are interpreting the integrals as duality pairings. This makes sense because \\(V_m \\subset H^{1+\\epsilon}\\) if we allow for a mesh-dependent embedding constant (this is why we don’t use \\(B = H^{1+\\epsilon}\\))↩︎\nThis is how fancy people define solutions to PDEs. We’re fancy.↩︎\nAlso everyone else’s, but it’s so elegantly deployed here. This is what I stole from Scott 1976)↩︎\nReal talk. I can sorta see where this argument is in the Scott paper, but I must’ve been really in the pocket when I wrote this because phew it is not an obvious transposition.↩︎\nUnless the approximation is very, very good. If we want to be pedantic, we’re approximating everything by floating point arithmetic. But we’re usually doing a good job.↩︎\n",
    "preview": {},
    "last_modified": "2021-11-24T22:30:30+11:00",
    "input_file": "getting-into-the-subspace.knit.md"
  },
  {
    "path": "posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/",
    "title": "Yes but what is a Gaussian process? or, Once, twice, three times a definition; or A descent into madness",
    "description": "Gaussian processes. As narrated by an increasingly deranged man during a day of torrential rain.",
    "author": [],
    "date": "2021-11-03",
    "categories": [],
    "contents": "\nI guess I’m going to talk about Gaussian processes now. This wasn’t the plan but who really expected a) there to be a plan or b) me to stick to the plan. I feel like writing about Gaussian processes and so I shall! It will be grand.\nWhat is a Gaussian process?\nWell I could tell you that a Gaussian process is defined by its joint distribution \\[\nu \\sim N(\\mu, \\Sigma),\n\\] where \\(u_i = u(s_i)\\), \\(\\mu_i = \\mu(s_i)\\) and \\(\\Sigma_{ij} = c(s_i, s_j)\\) for some positive definite covariance (or kernel) function \\(c(\\cdot, \\cdot)\\).\nBut that would be about as useful as presenting you with a dog that can bark “she’s a grand old flag”: perhaps good enough for a novelty hit, but there’s just no longevity in it.\nTo understand a Gaussian process you need to feel it deep down within you where the fear and the detailed mathematical concepts live.\nSo let’s try again.\nWe’re gonna have a … you know what. I’m not gonna do that. But I am going to define this stuff three times. Once for mum, once for dad, and once for the country.\nYou’ve got to wonder why anyone would introduce something three ways. There are some reasons. The first is, of course, that each definition gives you a different insight into different aspects of Gaussian processes (the operational, the boundless generality, the functional). And the second is because I’ve had to use all three of these ideas (and several more) over the years in order to understand how Gaussian processes work.\nI learnt about GPs from several sources (listed not in order):\nA Swede1 (so I will rant about random fields in the footnotes eventually);\nA book2 that was introducing GPs in a very general way because they needed the concept in outrageous generality to answer questions about the distribution of the maximum of a Gaussian process;\nA book3 written by a Russian who’s really only into measure theory and doesn’t believe anything is real if it isn’t at least happening on a Frechet space;\nAnd a book4 by a different Russian who’s really only into generalised Markov properties and needed to work with Gaussian processes that are defined over functions.\nOf these, the most relevant is probably the first one. I was primarily taught this stuff by Finn Lindgren, who had the misfortune of having the office next to mine when we worked together in Trondheim a very long time ago. (We both had a lot more hair then.)\nOne of the things that I learnt from him is that Gaussian processes can appear in all kinds of contexts, which means you need to understand them as a model for an unknown function rather than as a tool to be used in a specific context (like for Gaussian process regression or Gaussian process classification).\nIt’s some effort to really get a good grip on the whole “Gaussian processes as a model for an unknown function” thing but once you relax into it5, it stops being alarming to see models where you are observing things that aren’t just \\(u(s_k)\\). It is not alarming when you are observing integrals of the GP over regions, or derivatives. And you (or your methods) don’t fall apart when presented with complex non-linear functions on the GP (as happens if you look at\nBayesian inverse problems literature6).\nWhat is a Gaussian process? (Version 1)\nI’m going to start with the most common definition of a Gaussian process7. This is the definition that was alluded to in the first section and it’s also the definition operationalised in books like Rasmussen and Williams’8, which is a bread and butter reference for most machine learners interested in GPs, use.\nThe idea is pretty straightforward: I need to define a stochastic model for an unknown function \\(u(s)\\) and I want it to be, in some sense, Gaussian. So how do I go about doing this?\nFirstly, I probably don’t care too much about the function as an abstract object. For example, if I’m using the Gaussian process to model something like temperature, I am only going to observe it at a fairly small number of places (even though I could choose any set of places I want). This means that for some arbitrary set set of \\(K\\) locations \\(s_1, s_2, \\ldots, s_K\\), I am most interested9 in understanding the joint distribution10\\[\n(u(s_1), \\dots, u(s_K))^T. \n\\]\nSo how would we model the joint distribution? If we want the model to be tractable, we probably want a nice distribution. This is where the Gaussian part comes in. The Gaussian distribution is an extremely tractable11 distribution in medium-to-high dimensions. So the choice to model our joint distribution (which could be any size \\(K\\)) as \\[\n(u(s_1), \\dots, u(s_K))^T \\sim N\\left(\\mu_{s_1, \\ldots, s_K}, \\Sigma_{s_1, \\ldots, s_K}\\right),\n\\] makes sense from a purely mercenary position12.\nSo how do we choose the mean and the covariance function? We will see that the mean can be selected as \\([\\mu_{s_1, \\ldots, s_K}]_{k} = \\mu(s_k)\\) for pretty much any function13 \\(\\mu(\\cdot)\\), but, when we come to write \\[\n[\\Sigma_{s_1, \\ldots, s_K}]_{ij} = c(s_i, s_j),\n\\] there will be some very strong restrictions on the covariance function \\(c(\\cdot, \\cdot)\\).\nSo where do these restrictions come from?\nOh those (gay) Russians!\nAs with all things in probability, all the good shit comes from the Soviets. Kolmogorov14 was a leading light in the Soviet push to formalise probability and one of his many many many contributions is something called the Kolmogorov extension theorem, which gives the exact conditions under which we can go from declaring that the distributions of \\((u(s_1), \\ldots, u(s_K))^T\\) (these are called finite dimensional distributions) are Gaussian to describing a legitimate random function \\(u(s)\\).\nThere are essentially two conditions:\nThe order of the observations doesn’t matter in a material way. In our case changing the order just permutes the rows and columns of the mean vector and covariance matrix, which is perfectly ok.\nThere is a consistent way to map between the distributions of \\((u(s_1), \\ldots, u(s_K), u(s_{K+1}))^T\\) and \\((u(s_1), \\ldots, u(s_K))^T\\). This is the condition that puts a strong restriction on the covariance function.\nEssentially, we need to make sure that we have a consistent way to add rows and columns to our covariance matrix while ensuring that stays positive definite (that is, while all of the eigenvalues stay non-negative, which is the condition required for a multivariate normal distribution15). The condition—which is really gross—is that for every positive integer \\(K\\) and every set of points \\(s_1, \\ldots, s_k\\), and for every \\(a_1, \\ldots, a_K\\) not all equal to zero, we require that \\[\n\\sum_{i=1}^K \\sum_{j = 1}^K a_ia_j c(s_i, s_j) \\geq 0.\n\\]\nThis condition is obviously very difficult to check. This is why people typically choose their covariance function from a very short list16 that is typically found in a book on Gaussian processes.\nBut Kolmogorov said a little bit more\nThere’s a weird thing in grad school in North America where they insist on teaching measure theoretic probability theory and then never ever ever ever ever using any of the subtleties. But Gaussian processes (and, in general, stochastic processes on uncountable index spaces) are a great example of when you need these details.\nWhy? Because unlike discrete probability (where the set of events that we can compute the probability of is obvious) or even continuous random variables (where the events that we can’t compute the probability of are so weird we can truly just ignore them unless we are doing something truly exotic), for Gaussian processes,17 the set of allowable events is considerably smaller than the set of all things you might want probabilities of.\nThe gist of it is that we have built up a random function \\(u(s)\\) from a bunch of finite random vectors. This means that we can only assign probabilities to events that can be built up from events on finite random vectors. The resulting set of events (or \\(\\sigma\\)-algebra to use the adult term) is called the cylindrical18 \\(\\sigma\\)-algebra and can be roughly19 thought of as the set of all events that can be evaluated by evaluating \\(u(\\cdot)\\) at most a countable number of times.\nThings that aren’t measurable\nThis will potentially become a problem if, for instance, you are working with a Gaussian process in a model that uses a Gaussian process in a weird way. When this happens, it is not guaranteed that, for instance, your likelihood is a measurable function, which would mean that you can’t normalise your probability distribution! (I mean, don’t worry. Unless you’re doing something fairly wild it will be, but it has come up especially in the inverse problems literature!)\nThis limited set of measurable events even seems to preclude well studied “events” like “\\(u\\) is continuous” or “\\(u\\) is twice continuously differentiable” or “\\(u\\) has a finite supremum”. All things that we a) want to know about a Gaussian process and b) things people frequently say about Gaussian processes. It is common for people to say that “Brownian motion is continuous” and similar things.\nAs with all of mathematics, there are a lot of work arounds that we can use. For those three statements in particular, there is some really elegant mathematical work (due, again, to Kolmogorov and extended greatly by others). The idea is that we can build another function \\(\\tilde u(s)\\) such that \\(\\Pr(u(s) = \\tilde u(s)) = 1\\) for all20 \\(s\\) such that \\(\\tilde u(s)\\) is continuous (or differentiable or bounded).\nIn the language of stochastic processes, \\(\\tilde u(s)\\) is called a version of \\(u(s)\\) and the more correct, temperate language (aka the one least likely to find in the literature) is that \\(u(s)\\) has a continuous/differentiable/bounded version.\nIf you’re interested in seeing how a differentiable version of a Gaussian process is constructed, you basically have to dick around with dyads for a while. Martin Hairer’s lecture notes21 is a nice clear example.\nWhere are the limitations of this definition?\nThere are a few. These are, of course, in the eye of the beer holder. The definition is workable in a lot of situations and, with some explanation can be broadened out a bit more. It’s less of a great definition when you’re trying to manipulate Gaussian processes as mathematical objects, but that’s what the next one is for.\nThe first limitation is maybe not so much a limit of the definition as a bit of work you have to do to make it applicable. And that is: what happens if I am observing (or my likelihood depends on) averages like \\[\n\\left(\\int_S \\ell_1(s) u(s)\\,ds, \\ldots, \\int_S \\ell_K(s) u(s)\\,ds\\right)^T\n\\] instead of simple point evaluations22.\nThis might seem like a massively different problem, until we remember that integrals are just sums dressed up for Halloween, so we can approximate the integrals arbitrarily well by sums23. In fact, if we squint24 a bit, we can see that the above vector will also be multivariate Gaussian with mean vector \\[\n[\\mu]_k = \\int_S \\ell_k(s) \\mu(s)\\,ds\n\\] and covariance matrix with entries \\[\n[\\Sigma]_{ij} = \\int_{S \\times S} \\ell_i(s) \\ell_j(s')c(s, s')\\,dsds'.\n\\] Similar formulas hold for derivative observations.\nProbably the bigger limitation is that in this way of seeing things, your view is tied very tightly to the covariance function. While it is a natural object for defining Gaussian processes, it is fucking inconvenient if you want to understand things like how well approximate Gaussian processes work.\nAnd let’s face it, a big chunk of Gaussian processes we see in practice are approximate because the computational burden on large data sets is too big to do anything but approximate.\n(Fun fact, when I was much much younger I wrote a paper that was a better title than a paper25 called26 In order to make spatial statistics computationally feasible, we need to abandon the covariance function. I copped a lot of shit for it at the time [partly because the title was better than the paper, but partly because some people are dicks], but I think the subsequent 10 years largely proved me (or at least my title) right27.)\nThe focus on the covariance function also hides the strong similarity between Gaussian process literature and the smoothing splines literature starting from Grace Wahba in the 1970s. It’s not that nobody notices this, but it’s work to get there!\nIn a similar way, it hides the fundamental role the reproducing kernel Hilbert space (or Cameron-Martin space) is doing and the ways that Gaussian process regression is (and is not) like kernel smoothing in RKHSs. This, again, isn’t a secret per se—you can find this information if you want it—but it’s confusing to people and the lack of clarity leads to people missing useful connections (or sometimes leads to them drawing mistaken parallels).\nHow many times have you seen someone say that realisations of a Gaussian process are in the RKHS associated with the covariance function? They are not. In fact, every realisation of a Gaussian process is rougher than any function in the RKHS (with probability 1)! Unfortunately, this means that your reason for choosing the kernel in a RKHS regression and for choosing the covariance function in a Gaussian process prior need to be subtly different. Or, to put it differently, a penalty is not a log-prior and interpreting the maximum a penalised likelihood is, in high dimensions, a very distant activity from interpreting a posterior distribution (even when the penalty is the log of the prior).\nWhat is a Gaussian process? (Version 2)\nOk. Let’s do this again. This definition lives in a considerably more mathematical space and while I’m gonna try to explain the key terms, I will fail. But hey. Who doesn’t like googling weird terms?\nA Gaussian process is a collection of random variables \\(u(s)\\), where \\(s \\in S\\) and \\(S\\) is some set of things that isn’t too topologically disastrous28.\nBut what makes it Gaussian? Here’s the general definition.\n\nA stochastic process/random field is Gaussian if and only if every continuous linear functional has a univariate Gaussian distribution.\n\nWell that’s very useful Daniel. What the hell is a linear functional?\nGreat question angry man who lives inside my head! It is any function \\(\\ell(\\cdot)\\) that takes the Gaussian process \\(u(s)\\) and an input and spits out a real number that is is\nLinear. Aka \\(\\alpha \\ell(u) + \\beta\\ell(v) = \\ell(\\alpha u + \\beta v)\\)\nBounded29.\nGreat. Love a definition. Shall we try something more concrete?\nPoint evaluation \\(u(s_j)\\) (aka evaluating the function at a point) is a linear functional (\\((u + v)(s)_j = u(s_j) + v(s_j)\\)). As is a definite integral over a set \\(\\int_A u(s)\\,ds\\).\nIt’s a fun little exercise to convince yourself that this all implies that for any collection \\(\\ell_1(\\cdot), \\ldots, \\ell_J(\\cdot)\\) of continuous linear functionals, then \\(u(s)\\) is a Gaussian process means that the vector \\[\n(\\ell_1(u), \\ldots \\ell_J(u))^T\n\\] is multivariate Gaussian.\nYour idea of fun is not my idea of fun. Anyway. Keep talking.\nIf \\(u\\) lives in a Banach space30 \\(B\\), then the set of all continuous/bounded linear functionals on \\(B\\) is called the dual space and is denoted \\(B^*\\).         \nI mean, cool I guess but where the merry hell is the covariance function\nIn this context, the most important thing about \\(B^*\\) is it does double duty: it is both a space of linear functionals and a space that can be identified with random variables.\nHow the fuck do you do that?\nWell, the trick is to remember the definition! If \\(\\ell \\in B^*\\), then \\(\\ell(u)\\) is a Gaussian. Similarly, if we have two functionals \\(\\ell, \\ell' \\in B^*\\) we consider the covariance of their associated random variables \\[\nC_u(\\ell, \\ell') = \\mathbb{E}(\\ell(u)\\ell'(u)).\n\\]\n\\(C_u(\\ell, \\ell')\\) is a symmetric, positive definite bilinear form (aka good candidate for an inner product)!\nWe can use this to add more functions to \\(B^*\\), particularly for any sequence \\(b_n \\in B^*\\) that is Cauchy with respect to the norm \\(\\|\\ell\\|_{R_u} = \\sqrt{C_u(\\ell, \\ell)}\\) we append the limit to \\(B^*\\) to complete the space. Once we take equivalence classes, we end up with a Hilbert space \\(R_u\\) that, very unfortunately, probabilists have a tendency to call the reproducing kernel Hilbert space associated with \\(u\\).\nWhy is this unfortunate? Well primarily because it’s not the exact same space that machine learners call the reproducing kernel Hilbert space, which is, to put it mildly, confusing. But we can build the machine learner’s RKHS (known to probabilists as the Cameron-Martin space).\nWhy are you even telling me this? Is this a digression?\nHonestly. Yes. But regardless the space \\(R_u\\) is quite useful to understand what’s going on. To start off, let’s do one example that shows just how different a Gaussian process is from a multivariate normal random vector. We will show that if we multiply a GP by a constant, we completely change its support31! Many a computational and inferential ship have come to grief on these sharp rocks.\nTo do this, though, we need32 to make an assumption on \\(B\\): We assume that \\(B\\) is separable33. This isn’t an vacuous assumption, but in a lot of cases of practical interest, this is basically the same thing as assuming the set \\(S\\) is a nice bounded domain or a friendly compact manifold (and not something like \\(\\mathbb{R}^d\\))34.\nSo. How do we use \\(R_u\\) to show that Gaussian processes are evil? Well we begin by noting that \\(R_u\\) is a separable35 Hilbert space it contains an orthonormal basis \\(e_n\\), \\(n=1, \\ldots, \\infty\\) (that is \\(\\|e_n\\|_{R_u} = 1\\) and \\(\\langle e_n, e_m\\rangle_{R_u} = 0\\) if \\(n\\neq m\\)). We can use this basis to show some really really weird stuff about \\(u(s)\\).\nIn particular, consider another Gaussian process \\(v(s) = c u(s)\\), where \\(c\\) is a non-zero constant. For this process we can build \\(R_v\\) in an analogous way. The \\(e_n\\) are still orthogonal in \\(R_v\\) but now \\(\\|e_n\\|_{R_v} = c^2\\).\nNow consider the functional \\(X_K(\\cdot) = K^{-1}\\sum_{k = 1}^Ke_i(\\cdot)^2\\). We are going to use this function to break stuff! To do this, we are going to define two disjoint sets of functions \\(A_1 = \\{u: \\lim_{K\\rightarrow \\infty} X_K(u) = 1\\}\\) and \\(A_2 = \\{u: \\lim_{K\\rightarrow \\infty} X_K(u) = c^2\\}\\). Clearly \\(A_1\\) and \\(A_2\\) are disjoint if \\(|c|\\neq 1\\).\nBecause \\(e_n(\\cdot)\\) are orthonormal in \\(R_u\\), it follows that that \\(u_n = e_n(u) \\sim N(0,1)\\) are iid. Similarly, \\(v_n = e_n(v) \\sim N(0, c^2)\\) are also independent. Hence it follows from the properties of \\(\\chi^2\\) random variables (aka the mean plus the strong law of large numbers) that \\(X_K(u) \\rightarrow 1\\) and hence \\(\\Pr(u \\in A_1) = 1\\). On the other hand, \\(X_K(v) \\rightarrow c^2\\), so \\(\\Pr(v \\in A_2) = 1\\). As \\(A_1\\) and \\(A_2\\) are disjoint, this means that unless \\(|c|=1\\), the processes \\(u\\) and \\(v\\) are mutually singular (aka they have no overlapping support).\nWhat does this mean? This means the distributions of \\(u\\) and \\(v\\) (which remember is just \\(u\\) multiplied by a constant) are as different from each other as a normal distribution truncated to \\((-\\infty, 1)\\) and another normal distribution truncated to \\((1, \\infty)\\)! Or, more realistically36, as disjoint as a distribution over \\(2\\mathbb{Z}\\) and \\((2\\mathbb{Z} - 1)\\).\nThis is an example of the most annoying phenomena in Gaussian processes37: the slightest change in a Gaussian process can lead to a mutually singular process. In fact, this is not a particularly strange example. It can be shown that Gaussian processes over uncountable index spaces are either absolutely continuous or mutually singular. There is no half-arsing it!\nThis has a lot of implications when it comes to computing38, setting priors on the parameters that control the properties of the covariance function39, and just generally inference40.\nYes but where’s our reproducing kernel Hilbert space\nWe just saw that if \\(u\\) is a Gaussian process than \\(c u\\) will be a singular GP if \\(|c| \\neq 1\\). What happens if we add things? Well, a result known as the Cameron-Martin theorem says that, for a deterministic \\(h(s) \\in B\\), \\(u(s) + h(s)\\) is absolutely continuous wrt \\(u(s)\\) if and only if \\(h(s)\\) is in the Cameron-Martin space \\(H_u\\) (this is the one that machine learners call the RKHS!).\nBut how do we find this mythical space? I find this quite stressful!\nLike, honey I do not know. But when a probabilist is in distress, we can calm them by screaming characteristic function at the top of our lungs right into their ear. Try it. It definitely works. You won’t be arrested.\nSo let’s do that. The characteristic function of a univariate random variable \\(X\\) is \\[\n\\phi_X(t) = \\mathbb{E}\\left(e^{itX}\\right),\n\\] which doesn’t seem like it’s going to be an amazingly useful thing, but it actually is. It’s how you prove the central limit theorem41, and a few other shiny things.\nWhen we are dealing with more complex random things, like random vectors and Gaussian processes, we can use characteristic functions, but we need to extend beyond the fact that they’re currently only defined for univariate random variables. Conveniently, we have some lying around. In particular, if \\(\\ell \\in B^*\\), we have the associated random variable \\(\\ell(u)\\) and we can compute its characteristic function42, which leads to the definition of a characteristic function of a stochastic process on \\(B\\) \\[\n\\phi_u(\\ell) = \\mathbb{E}(e^{i\\ell(u)}), \\quad \\ell \\in B^*.\n\\]\nNow this feels quite different. It’s no longer a function of some real number \\(t\\) but is instead a function of a linear functional \\(\\ell\\), which feels weird but isn’t.\nCharacteristic functions are immensely useful because if two Gaussian processes have same characteristic function they have the same distribution43.\nBecause \\(u(s)\\) is a Gaussian process, we can compute its characteristic function! We know that \\(\\ell(u)\\) is Gaussian so we can look up its characteristic function on Wikipedia and get that \\[\n\\mathbb{E}(e^{i\\ell(u)}) = \\exp\\left[{i \\mu(\\ell) - \\frac{\\sigma^2(\\ell)}{2}}\\right],\n\\] where \\(\\mu(\\ell) = \\mathbb{E}(\\ell(u))\\) and \\(\\sigma^2(\\ell) = \\mathbb{E}(\\ell(u) - \\mu(\\ell))^2\\).\nWe know that \\[\n\\mu(\\ell) = \\mathbb{E}(\\ell(u))\n\\] and \\[\n\\sigma^2(\\ell) = \\mathbb{E}\\left[(\\ell(u) - \\mu(\\ell)^2\\right],\n\\] the latter of which can be extended naturally to the aforementioned positive definite quadratic form \\[\nC_u(\\ell, \\ell') = \\mathbb{E}\\left[(\\ell(u) - \\mu(\\ell)(\\ell'(u) - \\mu(\\ell'))\\right], \\quad \\ell, \\ell' \\in B^*.\n\\]\nThis leads to the exact form of the characteristic function and to this theorem, which is true.\n\nTheorem: A stochastic process \\(u(\\cdot)\\) is a Gaussian process if and only if \\[\n\\phi_u(\\ell) = \\exp\\left[i\\mu(\\ell) - \\frac{1}{2}C_u(\\ell, \\ell)\\right].\n\\]\n\nSo Alf is back. In pog form.\nYes.\nIn this case, we can define the covariance operator \\(C_u: B^* \\rightarrow B\\) as44 \\[\n(C_u \\ell) (\\ell') = \\mathbb{E}\\left[(\\ell(u) - \\mu(\\ell)(\\ell'(u) - \\mu(\\ell'))\\right].\n\\] The definition is cleaner when \\(\\mu(\\ell) = 0\\) (which is why people tend to assume that when writing this shit down45), in which case we get \\[\nC_u\\ell = \\mathbb{E}(u\\ell(u)) \n\\] and \\[\nC_u(\\ell, \\ell') = \\ell'(C_u\\ell)\n\\]\nGreat gowns, beautiful gowns.\nWow. Shady.\nAnyway, the whole reason to introduce this is the following:\n\nTheorem: Let \\(v = x + h\\). Then \\[\n\\phi_v(\\ell) = e^{i\\ell(h)}\\phi_u(\\ell)\n\\]\n\nThis does not not help us answer the question of whether or not \\(v\\) has the same support as \\(u\\). To do this, we construct a variable that is absolutely continuous with respect to \\(u\\) (we guarantee this because we specify its density46 wrt \\(u\\)).\nTo this end, take some \\(g \\in R_u\\) and define a stochastic process \\(w\\) with density wrt47 u \\[\n\\rho(u) = \\exp\\left[iC_u(g, u) - \\frac{1}{2}C_u(g,g)\\right].\n\\]\nFrom this, we can compute48 the characteristic function of \\(w\\) \\[\\begin{align*}\n\\phi_w(\\ell) &= \\mathbb{E}_w\\left(e^{i\\ell(w)}\\right) \\\\\n&= \\mathbb{E}_u\\left(\\rho(u) e^{i\\ell(u)}\\right) \\\\\n&= \\exp\\left[iC_u(g,\\ell) + i \\mu(\\ell)  - \\frac{1}{2}C_u(\\ell, \\ell)\\right]\n\\end{align*}\\]\nSo we are fine if we can find some \\(h \\in B\\) such that \\[\nC_u(g, \\ell) = \\ell(h).\n\\] To do this, we note that \\[\nC_u(g, \\ell) = \\ell(C_u g), \n\\] so for any \\(g\\) we can find a \\(h \\in B\\) such that \\(h = C_ug\\) and for such a \\(h\\) \\(v(s) = u(s) + h(s)\\) is absolutely continuous with respect to \\(u(s)\\).\nThis gives us our definition of the Cameron-Martin space (aka the RKHS) associated with \\(u\\).\n\nDefinition:  The Cameron-Martin space (or reproducing kernel Hilbert space, if you must) associated with a Gaussian process \\(u\\) is the Hilbert space \\(H_u = \\{h\\in B: h = C_uh^* \\text{ for some } h^* \\in R_u\\}\\) equipped with the inner product \\[\n\\langle h, h'\\rangle_{H_u} = C_u(h^*, (h')^*)\n\\]\n\nA fun note is that the reason the probabilists don’t call the Cameron-Martin space the reproducing kernel Hilbert space is that there is no earthly reason to think that point evaluation will be bounded in general. So it become a problematique name. (And no, I don’t know why they’re ok with calling \\(R_u\\) that some things are just mysterious.)\nLord in heaven. Any chance of being a bit more concrete?\nSure! Let’s consider the case where \\(u \\in \\mathbb{R}^n\\) is a Gaussian random vector \\[\nu \\sim N(\\mu, \\Sigma).\n\\] While all of this is horribly over-powered for this case, it does help get a grip on what the inner product on \\(H_u\\) is.\nIn this case, \\(B^*\\) is row vectors like \\(f^T\\), \\(f\\in \\mathbb{R}^n\\) and \\[\nC_u(f^T, g^T) = \\operatorname{Cov}(f^Tu, g^Tu) = f^T\\Sigma g.\n\\]\nFurthermore,     the operator \\(C_u = \\Sigma f\\) satisfies \\(g^T(\\Sigma f) = C_u(f^T,g^T)\\).\nSo what is \\(H_u\\)? Well, every \\(n\\) dimensional vector space can be represented as an \\(n\\)-dimensional vector, so what we really need to do is identify \\(h^*\\) from \\(h\\). To do this, we use the relationship \\(C(h^*, \\ell) = \\ell(h)\\) for all \\(\\ell \\in B^*\\). Translating that to our finite dimensional case we get that \\[\n(h^*)^T\\Sigma g = h^T g,\\qquad g \\in \\mathbb{R}^n,\n\\] from which it follows that \\(h^* = \\Sigma^{-1}h\\). Hence we get the inner product between \\(h, k \\in H_u\\) \\[\\begin{align*}\n\\langle h, k\\rangle_{H_u} &= \\langle h^*, k^*\\rangle_{R_h} \\\\\n&= (\\Sigma^{-1} h)^T \\Sigma (\\Sigma^{-1 k}) \\\\\n&= h^T \\Sigma^{-1} k.\n\\end{align*}\\]\nOk! That’s cool!\nYes! And the same thing holds in general, if you squint49. Just replace the covariance matrix \\(\\Sigma\\) with the covariance operator \\[\n(\\mathcal{C}f)(s) = \\int_S c(s, s') f(s') \\, ds'.\n\\]\nThis operator has (in a suitable sense) a symmetric50 non-negative definite (left) (closed) inverse operator \\(\\mathcal{Q}\\), which defines the RKHS inner product by \\[\n\\langle f, g \\rangle_{H_u} = \\int_{S} f(s) (\\mathcal{Q} g)(s) \\,ds,\n\\] where \\(f\\) and \\(g\\) are smooth enough functions for this to make sense. In general, \\(\\mathcal{Q}\\) will be a (very) singular integral operator, but when \\(u(s)\\) has the Markov property, \\(\\mathcal{Q}\\) is a differential operator. In all of these cases the RKHS is the set of functions that are smooth enough that \\(\\langle f, f \\rangle_{H_u} < \\infty\\).\nWe sometimes call the operator \\(\\mathcal{Q}\\) the precision operator and it’s fundamental to thin plate spline theory as well as some nice ways to approximate GPs in 1-4 dimensions. I will blog about this later, probably, but for now if you’re interested Finn Lindgren, Håvard Rue, and David Bolin just released a really nice survey paper about the technique.\nTell me some things about the Cameron-Martin space\nNow that we’ve gone to the effort of finding it, I should probably tell you why it’s so important. So here are a collection of facts!\nFact 1: The Cameron-Martin space (the set of functions and the inner product) determines a51 Gaussian process, in that if two Gaussian processes have the same mean and the the same Cameron-Martin space, they have the same distribution. In fact, the next definition of a Gaussian process is going to show this constructively.\nThis is nice because it means you can define a Gaussian process without needing to specify its covariance function. You just (just!) need to specify a Hilbert space. It turns out that this is a considerably easier task than trying to find a positive definite covariance function if the domain \\(S\\) is weird.\nFact 2: \\(u(s)\\) is never in the RKHS. That is, \\(\\Pr(u \\in H_u) = 0\\). But52 if, for any \\(\\epsilon>0\\), \\(A_\\epsilon \\subset B\\) is any measurable set of functions with \\(\\Pr(u \\in A) = \\epsilon\\), then \\(\\Pr(u \\in A_\\epsilon + H_u) = 1\\), where \\(A_\\epsilon+H_u = \\{a + h \\in B: a\\in A_\\epsilon, h \\in H_u\\}\\). Or to say it in words, although \\(u\\) is never in \\(H_u\\), if you find a set \\(A_\\epsilon\\) that \\(u\\) could be in (even if it’s extremely unlikely to be there), then \\(u\\) is almost surely made up of a function in \\(A_\\epsilon\\) plus a function in \\(H_u\\).\nThis is wild. It means that while \\(u(\\cdot)\\) is never in the RKHS, all you need to do is add a bit of rough to get all of the stuff out. Another characterisation of the RKHS that are related to this is that it is the intersection of all subsets of \\(B\\) that have full measure under \\(u\\) (aka all sets \\(A\\subset B\\) such that \\(\\Pr(u \\in A) = 1\\)).\nFact 3: If we observe some data \\(y = N(Tu, \\Sigma_y)\\), where \\(Tu = (\\ell_1(u),\\ldots, \\ell_n(u))^T\\) is some observation vector, then the posterior mean \\(\\mathbb{E}(u \\mid y)\\) is in the RKHS and that posterior distribution of \\(u\\mid y\\) is a Gaussian process that’s absolutely continuous with respect to the prior GP u(s). This means that the posterior mean, which is our best point prediction under squared error loss, is always smoother than any of the posterior draws.\nThis kinda makes sense: averaging things smooths out the rough edges. And so when we average a Gaussian process in this way, we make it smoother. But this is a thing that we need to be aware of! Our algorithms, our reasoning for choosing a kernel, and our interpretations of the posterior need to be aware that the space of posterior realizations \\(B\\) is rougher than the space that contains the posterior mean.\nFrequentists / people who penalise likelihoods don’t have to worry about this shit.\nSo what have we learnt?\nSo so so so so so so much notation and weird maths shit.\nBut there are three take aways here:\nThe importance of the Fourier transform (aka the characteristic function) when it comes to understanding Gaussian processes.\nThe maths buys us understanding of some of the more delicate properties of a Gaussian process as a random object (in particular it’s joint properties)\nYou can define a Gaussian process exclusively using the RKHS inner product. (You can also do all of the computations that way too, but we’ll cover that later). So you do not need to explicitly specify a covariance function. Grace Wahba started doing this with thin plate splines (and \\(L\\)-splines) in 1974 and it worked out pretty well for her.\nSo to finish off this post, let’s show one more way of constructing a Gaussian process. This time we will explicitly start from the RKHS.\nWhat is a Gaussian process? (Version 3)\nOur final Gaussian process definition is going to centre the RKHS53 as the fundamental object. This construction, which is known as an abstract Wiener space54 is less general55 than our previous definition, but it covers most of the processes we are going to encounter in applications.\nThis construction is by far the most abstract of the three (it is in the name after all). So buckle up.\nThe jumping off point here is a separable Hilbert space \\(H\\). This has an inner-product \\(\\langle\\cdot, \\cdot \\rangle_H\\) on it, and the associated notion of orthogonality and an orthogonal projector. Consider an \\(n\\)-dimensional subspace \\(V_n \\subset H_u\\). We can, without any trouble, define a Gaussian process on \\(V_n\\) \\(\\tilde u_n\\) with characteristic function \\[\n\\phi_{\\tilde u_n}(h) = \\exp\\left(-\\frac{1}{2}\\langle h,h\\rangle_H\\right).\n\\] We hit no mathematical problems because \\(V_n\\) is finite dimensional and nothing weird happens to Gaussians in finite dimensions.\nThe thing is, we can do this for any finite dimensional subspace \\(V_n\\) and, in particular, if we have a sequence of subspace \\(V_1 \\subset V_2 \\subset \\ldots\\), where \\(\\operatorname{dim}(V_n) =n\\), then we can build a sequence of finite dimensional Gaussian processes \\(\\tilde u_n\\) that are each supported in their respective \\(V_n\\).\nThe question is: can we construct a Gaussian process \\(\\tilde{u}\\) supported56 on \\(H\\) such that \\(P_n \\tilde u \\stackrel{d}{=} \\tilde u_n\\), where \\(P_n\\) is the orthogonal projector from \\(H\\) to \\(V_n\\)?\nYou would think the answer is yes. It is not. In fact, Komolgorov’s extension theorem says that we can build a Gaussian process this way, but it does not guarantee that the process will be supported on \\(H\\). And it is not.\nTo see why this is, we need to look a bit more carefully at the covariance operator of a Gaussian process on a separable Hilbert space. The key mathematical feature of a separable Hilbert space is that it has an57 orthonormal58 basis \\(e_n\\). We can use the orthonormal basis to do a tonne of things, but the one we need right now is the idea of a trace59 \\[\n\\operatorname{tr}(C_u) = \\sum_{n = 1}^\\infty C_u(e_i, e_i).\n\\]\nFor a (zero mean) Gaussian process \\(u\\) supported on \\(H\\), we can see that \\[\\begin{align*}\n\\operatorname{tr}(C_u) &= \\sum_{n = 1}^\\infty \\mathbb{E}\\left[(\\langle e_n, u\\rangle)^2\\right] \\\\\n&= \\mathbb{E}\\left[ \\sum_{n = 1}^\\infty\\langle e_n, u\\rangle_H^2\\right] \\\\\n&= \\mathbb{E}\\left[\\langle u, u\\rangle_H\\right] < \\infty,\n\\end{align*}\\] where the second line is just true because I say it is and the third line is Pythagoras’ theorem writ large (and is finite because Gaussian processes have a lot of moments60!).\nIf we were to say this in words, we would say that the covariance operator of a Gaussian process supported on a separable Hilbert space is a trace-class operator (or has a finite trace).\nAnd this is where we rejoin the main narrative. You see, if \\(\\tilde{u}\\) was a stochastic process on \\(H\\), then its characteristic function would be \\[\n\\phi_{\\tilde u}(h) = \\exp\\left(-\\frac{1}{2}\\langle h, h \\rangle_H\\right).\n\\] But it can’t be! Because \\(H\\) is infinite dimensional and the proposed covariance operator is the identity on \\(H\\), which is not trace class (its trace is clearly infinite).\nSo whatever \\(\\tilde u\\) is61, it is emphatically not a Gaussian process on \\(H\\).\nThat doesn’t seem like a very useful trip through abstract land\nWell, while we did not successful make a Gaussian process on \\(H\\) we did actually build the guts of a Gaussian process on a different space. The trick is to use the same idea in reverse. We showed that \\(\\tilde u\\) was not a Gaussian process because its covariance operator wasn’t on trace class. It turns out that the reverse also holds: if \\[\n\\phi_u(h) = \\exp\\left(-\\frac{1}{2}\\langle C_uh, h\\rangle_{H'}\\right)\n\\] and \\(C_u\\) is trace class on \\(H'\\), then \\(u\\) is a Gaussian process supported on \\(H'\\).\nThe hard part is going to be finding another Hilbert space \\(H' \\supset H\\).\nTo do this, we need to recall a definition of a separable Hilbert space \\(H\\) with orthonormal basis \\(e_n\\), \\(n=1, \\ldots, \\infty\\): \\[\nH = \\left\\{\\sum_{n=1}^\\infty a_n e_n: \\sum_{n=1}^\\infty a_n^2 < \\infty\\right\\}.\n\\] From this, we can build a larger separable Hilbert space \\(H'\\) as \\[\nH' = \\left\\{\\sum_{n=1}^\\infty a_n e_n: \\sum_{n=1}^\\infty \\frac{a_n^2}{n^2} < \\infty\\right\\}.\n\\] This is larger because there are sequences of \\(a_n\\)s that are admissible for \\(H'\\) that aren’t admissible for \\(H\\) (for example62, \\(a_n = \\sqrt{n}\\)).\nWe let \\(j:H \\rightarrow H'\\) be the linear embedding that we get by considering an element \\(h \\in H\\) as an element of \\(H'\\). If we let \\(e_n'\\) be an orthonormal basis on \\(H'\\) (note: this is not the same as \\(e_n\\) as it needs to be re-scaled to have unit norm in \\(H'\\)), then we get \\[\nj\\left(\\sum_{n=1}^\\infty \\alpha_n e_n\\right) = \\sum_{n=1}^\\infty  \\frac{\\alpha_n}{n} e_n'.\n\\] Why? Because \\(\\|e_n\\|_{H'} = n^{-1}\\) which means that \\(e_n' = n e_n\\) is an orthonormal basis for \\(H'\\). This means we have to divide the coefficients by \\(n\\) when we move from \\(H\\) to \\(H'\\), otherwise we wouldn’t be representing the same function.\nWith this machinery set up, we can ask if \\(\\tilde u\\) is a Gaussian process on \\(H'\\). Or, more accurately, we can ask if \\(u = j(\\tilde u)\\) is a Gaussian process on \\(H\\).\nWell.\nLet’s compute its characteristic function. \\[\\begin{align*}\n\\phi_u(h') &= \\mathbb{E}\\left(e^{i\\left\\langle u, h' \\right\\rangle_{H'}}\\right) \\\\\n&= \\mathbb{E}\\left[\\exp\\left(i\\left\\langle \\sum_{n=1}^\\infty \\frac{\\langle \\tilde u, e_n\\rangle_H}{n}e_n', \\sum_{n=1}^\\infty h_n'e_n' \\right\\rangle_{H'}\\right) \\right] \\\\\n&= \\mathbb{E}\\left[\\exp\\left(i \\sum_{n=1}^\\infty \\frac{\\langle \\tilde u, e_n\\rangle}{n} h_n\\right) \\right] \\\\\n&= \\exp\\left(-\\frac{1}{2} \\sum_{n=1}^\\infty \\frac{h_n^2}{n^2}\\right).\n\\end{align*}\\] It follows that \\(\\phi_u(e_n') = e^{-1/(2n^2)}\\) and so63 \\[\n\\operatorname{tr}(C_u) = -2\\sum_{n=1}^\\infty \\log \\phi_u(e_n') = \\sum_{n=1}^\\infty \\frac{1}{n^2} < \\infty,\n\\] \\(C_u\\) is a trace class operator on \\(H'\\) and, therefore, \\(u\\) is a Gaussian process on \\(u\\).\nBut wait, there is more! To do the calculation above, we identified elements of \\(H'\\) as infinite sequences \\(h' = (h'_1, h'_2, \\ldots)\\) that satisfy \\(\\sum_{n=1}^\\infty n^{-2}h_n^2 < \\infty\\). In this case the covariance operator is \\(C_{u}\\) is diagonal, so the \\(n\\)th entry of \\(C_u h' = n^{-2}h'_n\\). From this, and the reasoning in the previous section, we see that the Cameron-Martin space can be thought of as a subset of \\(H'\\). The Cameron-Martin inner product can be constructed from the inverse of \\(C_u\\), which gives \\[\n\\langle a, b\\rangle_{H_u} = \\sum_{i=1}^\\infty n^2 a_n b_n.\n\\] Clearly, this will not be finite unless we put much much stronger restrictions on \\(a_n\\) and \\(b_n\\) than that \\(\\sum_{n\\geq 1} n^{-2}a_n^2 < \\infty\\).\nThe Cameron Marin space is the subspace of \\(H'\\) consisting of all functions \\(h' = \\sum_{n=1}^\\infty a_n e_n'\\) such that \\[\n \\sum_{n=1}^\\infty n^2a_n^2 < \\infty.\n\\] This is (isomorphic to) \\(H\\)!\nTo see this, we note that the condition is only going to hold if \\(a_n = n^{-1}\\alpha_n\\) for some sequence \\(\\alpha_n\\) such that \\(\\sum_{n\\geq 1} \\alpha_n^2 < \\infty\\). Remembering that \\(e_n' = n e_n\\), it follows that \\(h \\in H_u\\) if and only if \\[\\begin{align*}\nh &= \\sum_{n=1}^n\\frac{\\alpha_n}{n} e_n' \\\\\n&=\\sum_{n=1}^n\\frac{\\alpha_n}{n} n e_n \\\\\n&=\\sum_{n=1}^n \\alpha_n e_n,\n\\end{align*}\\] which is exactly the definition of \\(H\\).\nAre you actually trying to kill me?\nYes.\nSo let’s recap what we just did: We took a separable Hilbert space \\(H\\) and used it to construct a Gaussian process on a larger space \\(H'\\) with \\(H\\) as its Cameron-Martin space. And we did all of this without ever touching a covariance function. This is an abstract Wiener space construction of a Gaussian process.\nThe thing is that this construction is a lot more general than this. The following is a (simplified64) version of the abstract Wiener space theorem.\n\nTheorem: Let \\(H\\) be a separable Hilbert space and let \\(B\\) be a separable Banach space. Furthermore, we assume that \\(H\\) is dense in \\(B\\). Then there is a unique Gaussian process \\(u\\) with \\(\\Pr(u \\in B) = 1\\) and \\(H_u = H\\). It can be constructed from the canonical cylindrical Gaussian process \\(\\tilde u\\) on \\(H\\) by \\(u = j(\\tilde u)\\), where \\(j:H \\rightarrow E\\) is the natural embedding.\n\nWas there any point to doing that?\nI mean, probably not. The main thing we did here was see that you can take the RKHS as the primal object when building a Gaussian process. Why that may be a useful observation was not covered.\nWe also saw that there are some restrictions required on the covariance operator to ensure that a Gaussian process is a proper stochastic process on a given space. (For the tech-heads, the problem with \\(\\tilde u\\) is that it’s associated probability measure is not countably additive. That is a bad thing, so we do not allow it.)\nThe restrictions are very clear for covariance operators on separable Hilbert spaces (they must be trace class). Unfortunately, there isn’t any clean characterization of all allowable covariance operators on more complex spaces like Banach spaces65.\nWhere do we go now but nowhere\nAnd with that I have finished my task. I have defined Gaussian processes three different ways and if anyone is still reading at this point: you’re a fucking champion.\nI probably want to talk about other stuff eventually:\nUsing all this technology to work out what happens to a posterior when we approximate a Gaussian process (which we usually do for computational reasons)\nUnderstanding how singularity/absolute continuity of Gaussian measures can help you set priors for the parameters in a covariance function\nThe Markov property in space: what is it and how do you use it\nShow how we can use methods for solving PDEs to approximate Gaussian processes.\nThe last one has gotten a lot less urgent because Finn, David and Håvard just released a lovely survey paper.\nMaybe by the time I am finished with these things (if that ever happens, I don’t rate my chances), I will have justified all of this technicality. But for now, I am done.\n\nNot the root vegetable.↩︎\nThe first 5 chapters of Adler and Taylor’s masterpiece s are glorious↩︎\nNot gonna lie. Bogachev’s Gaussian Measures is only recommended if you believe in intercessory prayer.↩︎\nRozanov’s Markov Random Fields, which is freely available from that link and is so beautiful you will cry when it turns the whole question into one about function space embeddings. It will be a moist old time. Bring tissues.↩︎\nI recommend some video head cleaner↩︎\nwhich spent an embarrassing amount of time essentially divorced from the mainstream statistical literature↩︎\nThis is the nomenclature that machine learners thrust upon us and it’s annoying and I hate it. Traditionally, a stochastic process was indexed by time (so in this case it would be a one-dimensional Gaussian process and when it was indexed by any other set it was referred to as a random field. So I would much rather be talking about Gaussian random fields. Why? Because there’s a bunch of shit that is only true in 1D and I’m not interested in talking about that)↩︎\nGreat book. Great reference. No shade whatsoever↩︎\nMaybe? But maybe I’m most interested in the average temperature over a region. This is why we are going to need to think about things more general than just evaluating Gaussian processes at a location.↩︎\nWhy the joint? Well because it’s likely that nearby temperature measurements will be similar, while measurements that are far apart are more likely to be (almost) independent (maybe after adjusting for season, time of day, etc).↩︎\nin the sense that we have formulas for almost everything we want to have formulas for↩︎\nWe can also play games with multivariate Gaussians (like building deep Gaussian processes or putting stochastic models on the covariance structure) that markedly increase their flexibility.↩︎\nUsually this involves covariates!↩︎\nWikipedia edit war aside (have a gander, it’s a blast), there’s evidence that Kolmogorov had a long-term relationship with Aleksandrov that was a) known at the time and b) used by the Soviets to blackmail them. So that’s fun.↩︎\nto be proper on some subspace. We are allowed zero eigenvalues for technical reasons and it actually turns out to be useful later, making things like thin plate splines a type of Gaussian process. Grace Wahba had to do all this without Google.↩︎\nExponential, Mat'{e}rn, and squared-exponential are the common ones on \\(\\mathbb{R}^d\\). After that shit gets exotic.↩︎\nand any other process built from Kolmogorov’s extension theorem↩︎\nso named because a set \\(A = \\{u(\\cdot): u(s_1, \\ldots, u(s_K)) \\in B;\\; B\\in \\mathcal{B}(\\mathbb{R}^K)\\}\\) is called a cylinder set↩︎\nthe exact definition is the smallest \\(\\sigma\\)-algebra for which all continuous linear functionals are measurable↩︎\nThe all part is important here. Consider the function \\(u(s) = 1_{A}(s)\\) where \\(A\\) is uniformly distributed on \\(S\\) and \\(1_A(s)\\) is 1 when \\(s=A\\) and zero otherwise. This function is equal to \\(0\\) for almost every \\(s\\) (rather than for every \\(s\\)), but the random function \\(u(s)\\) is definitely not the zero function (it is always non-zero at exactly one point).↩︎\nBottom of page 12 through page 14 here↩︎\nThis is a straight generalisation. If \\(\\ell_k(s) = \\delta_{s_k}(s)\\) then it’s the exact situation we were in before.↩︎\nIn the technical language of the next section, the set of delta functions is dense in the space of bounded linear functionals↩︎\naka replace integrals with sums, compute the joint distribution of the sums, and then send everything to infinity, which is ok when \\(\\ell_k\\) are bounded↩︎\nThe paper is pretty good and I think it’s a nice contribution. But the title was perfect.↩︎\nSorry for the pay wall. It’s from so long ago it’s not on arXiv.↩︎\nYes. NN-GPs, Vecchia approximations, fixed-rank Kriging, variational GPs, and all of the methods I haven’t specifically done work on, all abandon some or all of the covariance function. Whether the people who work on those methods think they’re abandoning the covariance function is between them an Cher.↩︎\nI say this, but you can make this work over pretty bonkers spaces. If we want to be general, if \\(E\\) is a linear space and \\(F\\) is a space of functionals on \\(E\\) that separates the points of \\(F\\), then \\(u(s)\\) is defined as a Gaussian process (wrt the appropriate cylindrical \\(\\sigma\\)-algebra) if \\(f(u)\\) is Gaussian for all \\(f\\in F\\). Which is fairly general but also, like, at this point I am just really showing off my maths degree.↩︎\nIt is very convenient that continuous linear functionals and bounded linear functionals are the same thing.↩︎\nit’s the one with a norm↩︎\nThe support of \\(u\\) is a set \\(A \\subset B\\) such that \\(\\Pr(u \\in A) = 1\\).↩︎\nneed is a big word here. We don’t need to do this, but not doing it makes things more technical. The assumption we are about to make let’s us breeze past a lot of edge cases as we sail from the unfettered Chapter 2 of Bogachev to the more staid and calm Chapter 3 of Bogachev.↩︎\nThat it contains a countable dense set. Somewhat surprisingly, this implies that the Gaussian process is separable (or alternatively that it’s law is a Radon measure), which is a wildly technical condition that just makes everything about 80% less technical↩︎\nThere are separable spaces on the whole space too, but, like, leave me alone.↩︎\nI’ve made a↩︎\nThese sets don’t overlap, but they’re probably not very far apart from each other? Honestly I can’t be arsed checking but this is my feeling.↩︎\nand continuously index stochastic processes/random fields in general↩︎\nsee Simon Cotter and Friends↩︎\nsee Geir-Arne Fuglstad and friends↩︎\nZhang, H. (2004). Inconsistent estimation and asymptotically equal interpolations in model-based geostatistics. Journal of the American Statistical Association, 99(465):250–261.↩︎\nEveryone who’s ever suffered through that inexplicable grad-level measure-valued probability course that builds up this really fucking intense mathematical system and then essentially never uses it to do anything interesting should be well aware of the many many many many ways to prove the central limit theorem.↩︎\nwell, the characteristic function when \\(t=1\\) because if \\(\\ell \\in B^*\\), \\(t\\ell \\in B^*\\).↩︎\nIn a locally convex space, this is true as measures over the cylindrical \\(\\sigma\\)-algebra, but for separable spaces it’s true over the Borel \\(\\sigma\\)-algebra (aka all open sets), which is an enormous improvement. (This happens because for separable spaces these two \\(\\sigma\\)-algebras coincide.) That we have to make these sorts of distinctions (between Baire and Borel measures) at all is an important example of when you really need the measure theoretic machinery to do probability theory. Unfortunately, this is beyond the machinery that’s typically covered in that useless fucking grad probability course.↩︎\nIt is not at all clear that the range of this operator is contained in \\(B\\). It should be mapping to \\(B^{**}\\), but that separability really really helps! Check out the Hairer notes.↩︎\nOr they define it on the set \\(\\{\\ell - \\mu(\\ell): \\ell \\in B^*\\}\\), the completion of which is the general definition of \\(R_u\\).↩︎\nRadon-Nikodym derivative↩︎\nA true pain in the arse when working on infinite dimensional spaces is that there’s no natural equivalent of a Lebesgue measure, so we don’t have a universal default measure to take the density against. So we have to take it against an existing probability measure. In this case, the most convenient one is the distribution of \\(u\\). In finite dimensions, the density \\(\\rho(u)\\) would satisfy \\(p_w(x) = \\rho(x)p_u(x)\\) where \\(p_w(\\cdot)\\) is the density of \\(w\\).↩︎\nI’m skipping the actual computation because I’m lazy.↩︎\nor if you’re working on a separable Hilbert space↩︎\nself-adjoint↩︎\nseparable↩︎\nThis next thing is a consequence of Borel’s inequality: \\(\\mathbb{B}(t, H_u)\\) is the \\(t\\) ball in \\(H_u\\) and a \\(A\\) is any measurable subset of \\(B\\) with \\(\\Pr(u \\in A) = \\Phi(\\alpha)\\), then \\(\\Pr(u \\in A + \\mathbb{B}(t, H_u)) \\geq \\Phi(\\alpha + t)\\), where \\(\\Phi\\) is the CDF of the standard normal distribution. Just take \\(t\\rightarrow \\infty\\).↩︎\nAt some point while writing this I’ve started using RKHS and Cameron-Martin space interchangeably for the one that is a subset of \\(B\\). We’re all just gonna have to be ok with that.↩︎\nYou can get references for this from the Bogachev book, but I actually quite like this survey from Jan van Neervaen, even though it’s almost comically general.↩︎\nAlthough I made the big, ugly assumption that \\(B\\) was separable halfway through the last definition, almost everything is true without that. Just with more caveats. Whereas, the abstract Wiener space construction really fundamentally uses the separability of \\(H_u\\) and \\(B\\) as a place to start.↩︎\nie with \\(\\Pr(\\tilde u \\in H) = 1\\)↩︎\nIt has lots of them but everything we’re about to talk about is independent of the choice of orthonormal basis.↩︎\n\\(\\|e_n\\|_H = 1\\) and \\(\\langle e_n, e_m \\rangle = 0\\) for \\(m\\neq n\\).↩︎\nThis is the trace of the operator \\(C_u\\) and I would usually write this as \\(\\sum_{n\\geq 1} \\langle Ce_i, e_i\\rangle\\), but it makes no difference here.↩︎\nThere’s a result called Fernique’s theorem that implies that Gaussian processes have all polynomial and exponential moments.↩︎\nIt’s called an iso-normal process and is strongly related to the idea of white noise and I’ll probably talk about that at some point. But the key thing is it is definitely not a Gaussian process in the ordinary sense on \\(H\\). We typically call it a generalized Gaussian process or a Generalized Gaussian random field and it is a Gaussian process indexed by \\(H\\). Life is pain.↩︎\nchaos_reins.gif↩︎\nYou can convince yourself this is true. I’m not doing all the work for you↩︎\nIf you want more, read Bogachev or that Radonification paper↩︎\nThe best reference I have is this survey↩︎\n",
    "preview": {},
    "last_modified": "2021-11-07T16:02:44+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-15-priors3/",
    "title": "Priors: Fire With Fire (Track 3)",
    "description": "Objective priors? In finite dimensions? A confidence trick? Yes.",
    "author": [],
    "date": "2021-10-16",
    "categories": [],
    "contents": "\nIt is Friday night, I am in lockdown, and I have had a few drinks. So let’s talk about objective priors.\nThe first and most obvious thing is that they are not fucking objective. It is bad/unethical marketing from the 90s that has stuck. I dislike it. I think it’s unethical (and, personally, immoral) to proclaim a statistical method objective in any context, let alone one in which all you did was compute some derivatives and maybe sent something that isn’t going to fucking infinity to infinity. It’s fucking trash and I hate it.\nBut let’s talk about some objective priors.\nWhat is an objective prior\nFuck knows.\nWho uses objective priors\nNo one (see above). But otherwise, a lot of people who are being sold a mislabeled bill of goods. People who tell you, unprompted, that they went to Duke.\nShould I use objective priors\nNo.\nOk let’s try again.\nNo I do not think I will.\nI am willing to talk about finite dimensional priors that add minimal information or are otherwise related to MLEs.\nLater, I guess because I’m mathematically interested in it, I’ll talk about the infinite dimensional case. But not today. Because I’m pissed off.\nAnyway.\nWhat is an objective prior\nHonestly, still a pretty vague and stupid concept. It is difficult to define for interesting (aka not univariate) cases, but maybe the most practical definition is priors that come from rules.\nBut that’s not a … great definition. Many priors that I will talk about over the next little while could probably be shoved under the objective banner. But in this post I’m going to talk about the OG concept of an objective prior: the type of priors that try to add minimal information beyond the data.\nNominally, the aim of these priors is to let the data speak for itself. And I’ve been doing this a while, and no matter how long I’ve listened to my .csv file, it has never said a word. But if it weren’t for silly justifications, we wouldn’t have silly concepts.\nThere are, essentially, three main types of priors that fall into this traditionally objective category:\nJeffreys priors\nReference priors\nMatching priors\nJefferys priors argue, for a bunch of very sensible and solid geometrical reasons, that the value of the parameter \\(\\theta\\) is less important than the way that moving from \\(\\theta\\) to \\(\\theta + d\\theta\\) will change the likelihood \\(p(y \\mid \\theta)\\). This push back against the Arianist notion that the prior can be separated from its context is welcome!\nThe actual prior itself comes from, I guess, the idea that the prior should be invariant to reparameterisations and after some maths you get \\[\np(\\theta ) \\propto |I(\\theta)|^{1/2},\n\\] where \\(|I(\\theta)|\\) is the determinant of the Fisher1 information matrix \\[\nI(\\theta)_{ij} = \\frac{\\partial^2}{\\partial \\theta_i \\theta_j} \\log p(y \\mid \\theta).\n\\]\nThis immediately turns out to be a terrible idea for general models. When \\(\\theta\\) has more than one component, the Jeffreys prior tends to concentrate in silly places in the parameter space.\nBut when \\(\\theta\\) is one dimensional, it works fine. In fact, if you use it you will get the sampling distribution Maximum Likelihood estimator (or withing \\(\\mathcal{O}_p(n^{-1})\\) of it). So there’s very little purpose pursing this line of reasoning.\nThere’s actually a bit of a theme that develops here: for models with a single parameter a lot of things work perfectly. Sadly the intersection of one-dimensional statistical models that are regular enough for all this maths to work and interesting statistical problems is not exactly hefty.\nReference priors are an attempt to extend Jeffreys priors to multiple parameters while avoiding some of the more egregious problems of multivariate Jeffreys priors. They were also the topic of the most boring talk I have ever seen at a conference2. It was 45 minutes going through all of the different reference priors you can make for inferring a bivariate normal (you see, to construct a reference prior you need to order your parameters and this ordering matters). If I didn’t already think that reference priors were an impractical waste of time, that certainly convinced me. A lot of people seem to mention reference priors, but it is rarer to see them in use.\nMatching priors try to spin off Jeffreys priors in a different direction. They are a mathematically very interesting idea asking if there is a prior that will produce a posterior uncertainty interval that is exactly the same as (or very close to) the sampling distribution of the MLE. It turns out that for one parameter models you can totally do this (the Jeffreys prior does it! And you can get even closer). But when there are nuisance parameters (aka parameters that aren’t of direct inferential interest but are important to modelling the data), the resulting prior tends to be data-dependent. A really nice example of the literature is Reid, Mukerjee, and Fraser’s 2003 paper. To some extent the matching priors literature is asking “should we even Bayes?”, which is not the worst question to ask3.\nThese three ideas have a number of weird bastard children. Most of these are not recommended by anyone, but used prominently. These are the vague priors. The \\(N(0,100^2)\\) priors. The \\(\\text{Inverse-Gamma}(\\epsilon, \\epsilon)\\) priors. The Uniform over large interval priors. The misinformed concept behind these priors is that wider prior = less information. This is, of course, bullshit. As many4 many5 many6 examples show.\nThe one big thing that I haven’t mentioned so far is that most of the time the priors produced using these methods are not proper, which is to say that you can’t integrate them. That isn’t a big deal mathematically as long as \\(\\int_\\Theta p(y \\mid \\theta)p(\\theta)\\,d\\theta\\) is finite for all7 data sets \\(y\\). This is a fairly difficult thing to check for most models and if you want to really upset a grad student at a Bayesian conference spend some time staring at their poster and then grimace and ask “are you sure that posterior is proper?”8 The frequent impropriety of these classes of means you can’t simulate from them, can’t really consider them a representation of prior information, and can’t easily transfer them from one problem to another without at least a little bit of fear that the whole house of cards is gonna come tumbling down.\nWho uses objective priors\nFrequentists. People who are obsessed with statistical bias of their estimators (the Venn diagram here isn’t a circle, but it’s also not the poster child for diversity of thought or modernity). People who read boring textbooks. People who write boring textbook. People who believe that it’s the choice of prior and somehow not the choice of the likelihood or, you know, their choice of data that will somehow lead to incorrect inferences9. People who tell you, without being asked10, that they went to Duke.\nShould I use objective priors\nIf you’ve more parameters than a clumsy butcher has fingers on their non-dominant hand, you probably shouldn’t use objective priors. In these cases, you almost always need to inject some form of regularisation, prior information, or just plain hope into your model to make it behave sensibly11.\nBut if you have less, I mean, live your life I guess. But why go to the effort. Just compute a maximum likelihood if you’re looking for something that is very very similar to a maximum likelihood estimate. It’s faster, it’s cleaner, and it’s not pretending to be something it isn’t.\nI actually think you can usually make stronger, more explicitly justified choices using other things we can talk about later. But I’m not the boss of statistics so you don’t have to listen to me.\n\newwwwwww↩︎\nThis is a very high bar. I have seen (and given) a lot of very very very dull talks. But this is the one that sticks in my mind.↩︎\nIf it seems like I like matching priors more than the other two, I do.↩︎\nhttps://projecteuclid.org/journals/bayesian-analysis/volume-1/issue-3/Prior-distributions-for-variance-parameters-in-hierarchical-models-comment-on/10.1214/06-BA117A.full↩︎\nPut a wide normal prior on the logit mean, simulate and back transform. Explain how that is uninformative.↩︎\nSome people love a wide uniform distribution \\(\\text{Unif}(0,U)\\) on the degrees of freedom of a student-t distribution. As \\(U\\) increases, you are putting more and more prior mass on the \\(t\\) distribution being very close to a normal distribution. Oops.↩︎\nor all after some minimal restrictions↩︎\nAllegedly, Jim Berger’s wife, who is not a statistician but was frequently at conferences, used to do this.↩︎\nLike seriously. I don’t want to repeat that old canard that the choice of likelihood is as subjective as the choice of prior because a) Arianism and b) the choice of likelihood is a waaaaaaaaaay more important subjective modelling choice than the choice of prior in all but the most outre circumstances!↩︎\nI promise I have never asked and I will never ask.↩︎\nThere are ideas of objective priors in these cases, which we will talk about later, but these usually take the form of priors that guarantee optimal frequentist behaviour. And again, there is often another way to get that.↩︎\n",
    "preview": {},
    "last_modified": "2021-11-03T23:44:35+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-14-priors2/",
    "title": "Priors: Whole New Way (Track 2)",
    "description": "Conjugate Priors? The crystal deoderant of Bayesian statistics",
    "author": [],
    "date": "2021-10-15",
    "categories": [],
    "contents": "\nIf we’re going to talk about priors, let’s talk about priors. And let’s talk about the most prior-y priors in the whole damn prior universe. Let’s talk about conjugate priors.\nWhat is a conjugate prior?\nWho cares.\nWho uses conjugate priors?\n90s revivalists.\nShould I use conjugate priors?\nLive your life.\nOk. Maybe we should try again\nDeep breaths. You soul is an island of positivity.\nWhat is a conjugate prior?\nConjugate priors are wild and fabulous beasts. They roam the strange, mathematic plains and live forever in our dreams of a better future.\nToo much?\nOK.\nConjugate priors are a mathematical curiosity that occasional turn out to be slightly useful.\nA prior distribution \\(p(\\theta)\\) is conjugate to the likelihood1 \\(p(y \\mid \\theta)\\) if the posterior distribution \\(p(\\theta \\mid y)\\) is in the same distributional family as the prior.\nMoreover, there is an rule to update the parameters in the prior to get the parameters in the posterior based on some simple summaries of the data. This means that you can simply write the posterior down as a specific distribution that you can2 easily sample from and get on with your life.\nReally, it seems like a pretty good thing. But there is, unsurprisingly, a hitch: almost no likelihoods have conjugate priors. And if you happen to have a model with a nice3 conjugate prior then good for you, but if you modify your model even slightly, you will no longer have one.\nThat is, the restriction to conjugate priors is a massive restriction on your entire model.\nWho uses conjugate priors?\nConjugate priors are primarily used by two types of people:\nPeople who need to write exam questions for undergraduate Bayesian statistics courses4,\nPeople who need to implement a Gibbs sampler and don’t want to live through the nightmare5 that is Metropolis-within-Gibbs.\nFor the most part, we can ignore the first group of people as a drain on society.\nThe second group is made up of:\npeople who are using software that forces it on them. And like we don’t all have time to learn new software6. Leave, as hero7 Chris Crocker, Britney alone.\npeople who are writing their own Gibbs sampler. Annie Lennox said it best: Why-y-y-y-y-y-y-y-y-y-y? For a very large variety of problems, you do not have to do this8. The exception is when you have a discrete parameter in your model that you can’t marginalise out9, like an exponential random graph model or something equally hideous. Thankfully, a lot of work in machine learning has expanded the options for Bayesian and pseudo-semi-kinda Bayesian10 estimation of these types of models. Anyway. Discrete parameters are disgusting. I am tremendously indiscrete.\nThe third type are the odd ducks who insist that because the posterior and the prior being in the same family means that the prior can be interpreted as the outcome of Bayesian analysis on a previous experiment. Instead of the much more realistic way of arriving at a conjugate prior where you find yourself waking up alone in a bathtub full of ice and using an \\(\\text{Inverse-Gamma}(1/2, 0.0005)\\) prior on the variance (which is conjugate for a Gaussian likelihood) because some paper from 199511 told you it was a good choice.\nShould I use conjugate priors?\nThere is actually one situation where they can be pretty useful. If your parameter space breaks down into \\(\\theta = (\\eta, \\phi)\\), where \\(\\eta\\) is a high-dimensional variable, then if \\(p(y \\mid \\theta) = p(y \\mid \\eta)\\) and \\(p(\\eta \\mid \\phi)\\) is conjugate for \\(p(y \\mid \\eta)\\), then a magical thing happens: you can compute \\(p(\\eta \\mid y, \\phi)\\) explicitly (using the conjugate property) and then you can greatly simplify the posterior as \\(p(\\theta\\mid y ) = p(\\eta \\mid y, \\phi) p(\\phi \\mid y)\\), where12 \\[\np(\\phi \\mid y) = \\frac{p(y \\mid \\eta)p(\\eta \\mid \\phi)p(\\phi)}{p(y) p(\\eta \\mid y, \\phi)} \\propto \\left.\\frac{p(y \\mid \\eta)p(\\eta \\mid \\phi)p(\\phi)}{p(\\eta \\mid y, \\phi)}\\right|_{\\eta = \\text{anything}},\n\\] where every term on the right hand side is able to be calculated13. Even if this doesn’t have a known distribution form, it is much much lower-dimensional than the original problem and much more amenable to MCMC or possibly deterministic integration methods.\nThis really does feel a bit abstract, so I will give you the one case where I know it’s used very commonly.This is the case where \\(y \\sim N(A\\eta, R)\\) and14 \\(\\eta \\mid \\phi \\sim N(0, \\Sigma(\\phi))\\), where \\(\\Sigma(\\phi)\\) is a covariance matrix and \\(A\\) is a matrix (the dimension of \\(\\eta\\) is often higher than the dimension of \\(y\\)).\nThis is an example of a class of models that occur constantly in statistics: Håvard Rue15 calls them Latent Gaussian models. They basically extend16 (geostatistical)? linear|additive (mixed)? models. So for all of these models, we can explicitly integrate out the high-dimensional Gaussian component, which makes inference a breeze17.\nIt gets slightly better than that because if you combine this observation with a clever asymptotic approximation, you get an approximately conjugate model and can produce Laplace approximations, nested Laplace approximations18, and Integrated Nested Laplace approximations19, depending on how hard you are willing to work.\nA conclusion, such as it is\nYes we have drifted somewhat from the topic, but that’s because the topic is boring.\nConjugate priors are mostly a mathematical curiosity and their role in Bayesian statistics is inexplicably inflated20 to make them seem like a core topic. If you never learn about conjugate priors your Bayesian education will not be lacking anything. It will not meaningfully impact your practice. But even stopped clocks are right 2-3 times a day21\n\nLike all areas of Bayesian statistics, conjugate priors push back against the notion of Arianism.↩︎\noften, but not always↩︎\nChristian Robert’s book The Bayesian Choice has an example where a model has a conjugate prior but it doesn’t normalise easily.↩︎\nOr, in my case, it’s explicitly listed on the syllabus.↩︎\nNot a nightmare.↩︎\nI’m equally likely to learn Julia and Stata. Which is is to say I’m tremendously unlikely to put the effort in to either. I wish them both well. Live your life and let me live mine.↩︎\nI have not fact checked this recently, and we all know white gays sometimes go bad. But he started from a good place, so I’m sure it’s fine.↩︎\nThere is pedagogical value in learning how MCMC methods work by implementing them yourself. But girl it is 2021. Go fuck with a bouncy particle sampler or something. Live your live out loud! Young Bayesians run free↩︎\nOften, like with mixture models or hidden markov models, you can eg https://mc-stan.org/docs/2_22/stan-users-guide/latent-discrete-chapter.html↩︎\nThe difference between these things is pretty slight in the usual situation where your MCMC scheme doesn’t explore the space particularly well. I’m not of the opinion that you either explore the full posterior or you don’t use the model. Most of the time you do perfectly fine with approximate exploration or, at least, you do as well as anything else will.↩︎\nBERNARDINELLI, L., CLAYTON, D. and MONTOMOLI, C. (1995). Bayesian estimates of disease maps: How important are priors? Stat. Med. 14 2411–2431.↩︎\nMultiply both sides of the first equation by the denominator and it’s equivalent to \\(p(y, \\eta, \\phi) = p(y, \\eta, \\phi)\\), which is tautologically true.↩︎\nThe constant of proportionality does not depend on \\(\\eta\\). All of the \\(\\eta\\) parts cancel!↩︎\nThe mean doesn’t have to be zero but you can usually make it zero using … magic.↩︎\nFamous for INLA↩︎\napologies for the regexp.↩︎\nSee also Rasmussen and Williams doing marginal inference with GPs. Exactly the same process.↩︎\nhttps://arxiv.org/abs/2004.12550↩︎\nhttps://www.r-inla.org↩︎\nI assume this is so people don’t need to update their lecture notes.↩︎\ndaylight savings time fades the curtains and wreaks havoc with metaphors.↩︎\n",
    "preview": {},
    "last_modified": "2021-11-03T23:44:24+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-11-n-sane-in-the-membrane/",
    "title": "$(n-1)$-sane in the membrane",
    "description": "Windmills? Tilted. Topic? Boring. (n-1)? No.",
    "author": [
      {
        "name": "Dan Simpson",
        "url": "https://dpsimpson.github.io"
      }
    ],
    "date": "2021-10-14",
    "categories": [],
    "contents": "\nI’ve been teaching a lot lately. That’s no huge surprise. It is my job. Maybe the one slight oddity this year is that I shifted jobs and, in switching hemispheres, I landed 3 consecutive teaching semesters. So. I’ve been teaching a lot lately.\nAnd when you’re in a period of heavy teaching, every-fucking-thing is about teaching.\nSo this blogpost is about teaching.\nRight now, I’m coming to the end of a second year class called Statistical Thinking. It’s been fun to work out how to teach the material. It’s standard fare: sampling variation, tests, bootstraps1, regression, and just a hint of Bayes in the last 2 weeks that you incentivize by promising a bastard of an exam question. So you know, (arms up even though I’m Catholic) tradition!\nIf I were a rich man (Katrina Lenk with a violin)\nThe thing about teaching an intro stats class is that it brings screaming to mind that quote from Bennett’s The History Boys2: (paraphrasing) “How do I define [intro to Statistics]? It’s just one fucking thing after another”.\nConstructing twelve moderately sequential weeks from the whole mass of things that someone being introduced to statistics needs to know is not unlike being thrown in the middle of the lake with nothing but an ice-cream container and a desiccated whale penis: confusing, difficult, and rather damp.\nThe nice thing about building an intro stats course is you’re not alone. You’re adrift in a sea of shit ideas! (Also a lot of good ones3, but don’t ruin my flow!)\nThe trouble is that this sort of course is simultaneously teaching big concepts and complex details. And while it’s not toooooo hard to make the concepts build and reinforce as time inexorably marches on, the techniques and details needed to illuminate the big concepts are not quite as linear.\nThere are two routes through this conundrum: incantations inscribed onto books made of human skin using the blood of sacrificial virgins (aka gathered during engineering statistics service teaching) or computers.\nI went with computers because we are in lockdown and I couldn’t be bothered sourcing and bleeding virgins.\nThe downside is that you need the students to have a grip on R programming (and programmatic thinking). This only happens if the degree you are teaching in is built in such a way that these skills have already been taught. Otherwise you need to teach both (which is very possible, but you need to teach less statistical content).\nThis is not a postmortem on my teaching, but if it were, it would be about that last point.\nI saw Goody Proctor with the devil!\nA tweet from Sanjay SrivastavaThis is a very long way to say I saw a tweet an had feelings.\nBecause I’m thinking about this stuff pretty hard right now, I am (as Hedwig would say) fully dilated.\nAnd my question is what is the use of teaching this distinction? Should anyone bother dividing by \\((n-1)\\) instead of \\(n\\) in their variance estimates?\nWell I guess the first question is is there a difference in this distinction? Let’s do the sort of R experiment I want my students to do!\n\n\n# Independent samples for a qq-plot!\n# Thanks to Rob Trangucci for catching this!\nlibrary(tidyverse)\nn_sim <- 100000\nn <- 10\nexperiments <- tibble(exp = rep(1:n_sim, each = n),\n                      sample = rnorm(n * n_sim),\n                      sample2 = rnorm(n * n_sim))\n\ncompare <- experiments %>%\n  group_by(exp) %>%\n  summarise(m = mean(sample),\n            m2 = mean(sample2),\n            var_bias = mean((sample - m)^2),\n            z_bias = m / sqrt(mean(var_bias)),\n            z = m2 / sd(sample2))\n\n\ncompare %>% \n  ggplot(aes(sort(z), sort(z_bias))) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0) +\n  theme_bw() + \n  coord_fixed(xlim = c(-2,2), y = c(-2,2))\n\n\n\n\nWell that is clear. There is not.\nOr, well, there is a small difference.\nBut to see it, you need a lot of samples! Why? Well the easy answer is maths.\nFor one thing, when \\(n=10\\), \\[\n\\frac{1}{n} - \\frac{1}{n-1} = \\frac{1}{90} = 0.01.\n\\] This does not compare well against the sampling variance, which (assuming \\(\\sigma^2\\approx 1\\), which is usual if you’ve scaled your problem correctly) is about \\(0.3\\).\nBut we could choose to do it properly. The bias in the MLE (aka the divide by \\(n\\)) variance estimate is \\[\n-\\frac{\\sigma^2}{n}.\n\\] This is a lot smaller than the sampling variability of the estimate (aka how much uncertainty you have because of the finite sample), which is \\[\n\\frac{\\sigma}{\\sqrt{n}}.\n\\]\nAnd that’s the whole story. Dividing by \\(n\\) instead of \\((n-1)\\) leaves you with a slightly biased estimate. But the bias if fucking tiny. It is possibly moving your second decimal place by about 1 number (assume our population variance is one). The sampling variably is moving the first decimal place by several digits.\nTruly. What is the point. The old guys4 who went wild about bias are now mostly dead. Or they’ve changed their minds (which is, you know, a reasonable thing to do as information about best practice is updated). The war against bias was lost before your undergraduates were born.\nEven in crisis, I maintain\nBut nevertheless, this whole DIVIDE BY N-1 OR THE BIAS MONSTER IS GONNA GET YA bullshit continues.\nAnd to some extent, maybe I shouldn’t care. I definitely shouldn’t care this many words about it.\nBut I do. And I do for a couple of reasons.\nReason One: What is the point teaching students about uncertainty and that you can’t just say “this number is different” because the estimate on a single sample is different. If I am to say that I need things to be at least5 \\(\\mathcal{O}(n^{-1/2})\\) apart before I’m willing to say they are maybe different, then why am I harping on about the much smaller difference?\nReason Two: It’s a shitty example. Bias and bias corrections have a role to play in statistics6. But if this is your first introduction to bias correction, you are going to teach either:\nBias is always bad, regardless of context / sampling variance / etc\nBias can be corrected, but it’s trivial and small.\nBoth of those things are bullshit. Just teach them how to bootstrap and teach the damn thing properly. You do not have to go very far to show bias actually making a difference!\nMaybe the only place the difference will be noticed is if you compare against the in-build var or sd functions. This is not the use case I would build my class around, but it is a thing you would need to be aware of.\nThe worlds is a question, this room is an answer. And the answer is no.\nIf you are going to teach statistics as more than just stale incantations and over-done fear-mongering, you need to construct the types of stakes that are simply not present in the \\(n\\) vs \\(n-1\\) bullshit.\nIt is present when you are teaching the normal vs t distribution. You are teaching that the design of your experiment changes the possible extreme behaviour and sometimes it can change a lot.\nThe \\(n\\) vs \\((n-1)\\) denominator for a variance estimator is a curiosity. It is the source of thrilling7 exercises or exam questions. But it is not interesting.\nIt could maybe set up the idea that MLEs are not unbiased. But even then, the useless correction term is not needed. Just let it be slightly biased and move on with your life.\nBecause if that is the biggest bias in your analysis, you are truly blessed.\nIn real life, bias is the price you pay for being good at statistics. And like any market, if you pay too much you’re maybe not that good. But if you pay nothing at all, you don’t get to play.\n\nTo paraphrase Jimmy Somerville, tell me whyyyyyyyyy about 90% of the bootstrap material on the web is … misguided. And why tidymodels only has the shit bootstrap in it?↩︎\nOk. Straight up, “[Intro to statistics] is a commentary on the various and continuing incapabilities of men” would’ve also worked.↩︎\nThis course stands on the shoulders of giants: Di Cook and Catherine Forbes gave me a great base. And of course every single textbook (shout out to the OpenIntro crew!), blog post, weird subsection of some other book, paper from 1987 on some weird bootstrap, etc that I have used to make a course!↩︎\nYes. I used the word on purpose.↩︎\n\\(n\\) is the size of the sample.↩︎\nI spend most of my time doing Bayes shit, and we play this game somewhat differently. But the gist is the same.↩︎\nNot thrilling.↩︎\n",
    "preview": "posts/2021-10-11-n-sane-in-the-membrane/n-sane-in-the-membrane_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-11-03T23:44:00+11:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-10-14-priors1/",
    "title": "Priors: Night work (Track 1)",
    "description": "Priors? Defined. Questions? Outlined. Purpose? Declared.",
    "author": [],
    "date": "2021-10-14",
    "categories": [],
    "contents": "\nI have feelings. Too many feelings. And ninety six point seven three percent of them are about prior distributions1. So I am going to write a few blog posts about prior distributions.\nTo be very honest, this is mostly a writing exercise2 to get me out of a slump.\nSo let’s do this.\nNo love, deep web\nAs far as I am concerned it’s really fucking stupid to try to write about priors on their own. They are meaningless outside of their context. But, you know, this is a blog. So I get to be stupid.\nSo what is a prior distribution? It is whatever you want it to be. It is a probability distribution3 that … I don’t know. Exists4.\nOk. This is not going well. Let’s try again.\nA prior distribution is, most of the time, a probability distribution on the parameters of a statistical model. For all practical purposes, we tend to work with its density, so if the parameter \\(\\theta\\), which could be a scalar but, in any interesting case, isn’t, has prior \\(p(\\theta)\\).\nCaptain fantastic and the brown dirt cowboy\nBut what does it all meeeeeeeean?\nWe have a prior distribution specified, gloriously, by it’s density. And unlike destiny, density is meaningless. It only makes sense when we integrate it up to get a probability \\[\n\\Pr(A) = \\int_A p(\\theta)\\,d\\theta.\n\\]\nSo what does the prior probabilty \\(\\Pr(A)\\) of a set \\(A\\) actually mean in real life? The answer may shock you: it means something between nothing and everything.\nScenario 1: Let’s imagine that we were trying to estimate the probability that someone in some relative homogeneous subgroup of customers completed a purchase on our website. It’s a binary process, so the parameter of interest can probably just be the probability that a sale is made. While we don’t know what the probability of a sale is for the subgroup of interest, we know a lot sales on our website in general (in particular, we know that about 3% of visits result in sales). So if I also believe that it would be wildly unlikely for 20% of visits to result in a sale, I could posit a prior like a \\(\\text{Beta}(0.4,5)\\) prior that captures (a version of) these two pieces of information.\n\n\nShow code\n\n  ## Step 1: \n  \nfn <- \\(x) (qbeta(0.5,x[1], x[2]) - 0.02)^2 + \n  (qbeta(0.9, x[1], x[2]) - 0.2)^2\n\nbest <- optim(c(1/2,1/2), fn)\n\n## Step 3: Profit.\n## (AKA round and check)\nqbeta(0.9, 0.4, 5)\nqbeta(0.5, 0.4, 5)\n\n\n\nScenario 2: Let’s imagine I want to do variable selection. I don’t know why. I was just told I want to do variable selection. So I fire up the Bayesian Lasso5 and then threshold in some way. In this case, the prior encode a hoped-for property of my posterior. (To paraphrase Lana, hope is a dangerous thing for a woman like you to have because the Bayeisan Lasso does not work to the point that the original paper doesn’t even suggest using it for variable selection6 it just, idk, liked the name. Statistics is wild.)\nScenario 3: I’m doing a regression with just one variable (because why not) and I think that the relationship between the response \\(y\\) and the covariate \\(x\\) is non-linear. That is, I think there is some unknown to me function \\(f(x)\\) such that \\(\\mathbb{E}(y_i) = f(x_i)\\). So I ask a friend and they tell me to use a Gaussian Process prior for \\(f(\\cdot)\\) with an exponential covariance function.\nWhile I can write down the density for the joint prior of \\((f(x_1), f(x_2,), \\ldots, f(x_n))\\), I do not know7 what this prior means in any substantive sense. But I can tell you, you’re gonna need that maths degree to even try.\nAnd should you look deeper, you will find more and more scenarios where priors are doing different things for different reasons8. For each of these priors in each of these scenarios, we will be able to compute the posterior (or a reasonable computational approximation to it) and then work with that posterior to answer our questions.\nDifferent people9 will use priors different ways even for very similar problems10. This remains true even though they are nominally working under the same inferential framework.\nBayesians are chaotic.\nMapping out a sky / What you feel like, planning a sky\nSondheim’s ode to pointillism feels relevant here. The reality of the prior distribution—and the whole reason the concept is so slippery and chaotic—is that you are, dot by dot, constructing the world of your inference. This act of construction is fundamental to understanding how Bayesian methods work, how to justify your choices, and how to use a Bayesian workflow to solve complex problems.\nTo torture the metaphor, our prior distribution is just our paint, unmixed, slowly congealing, possibly made of ground up mummys. It is nothing without a painter and a brush.\nThe painter is the likelihood or, more generally, the generative link between the parameter values and the actual data, \\(p(y \\mid \\theta)\\). The brush is the computational engine you use to actually produce the posterior painting11.\nThis then speaks to the core challenge with writing about priors: it depends on how you use them. It is a fallacy, or perhaps a foolishness, or perhaps a heresy12. Hell, when trying to understand a single inference The Prior Can Only Be Understood In The Context Of The Likelihood13. In the context of an entire workflow, The Experiment is just as Important as the Likelihood in Understanding the Prior.\nFor instance, using independent Cauchy priors for the coefficients in a linear regression model will result in a perfectly ok posterior. Whereas the same priors used in a logistic regression, you may end up with posteriors with such heavy tails that they don’t have a mean! (Do we care? Well, yes. If we want reasonable uncertainty intervals we probably want 2 or so moments otherwise those large deviations are gonna getcha!)\nSo what?\nAll of this is fascinating. And it is a lot less chaotic than it initially sounds.\nThe reality is that while two Bayesians may use different priors and, hence, produce different posteriors for the same data set.This can be extreme. For example, if I am trying to estimate the mean of data generated by \\(y_i \\sim N(\\mu, 1)\\), then I can choose a prior14 (that depends on the data) so that the posterior mean \\(\\mathbb{E}(\\mu \\mid y) =1\\). Or, to put it differently, I can get any answer I want if I choose an prior carefully (and in a data-dependent manner).\nBut this isn’t necessarily a problem. This is because the posteriors produced by two sensible priors for the same problem will produce fairly similar results15. The prior I used to cheat in the previous example would not be considered sensible by anyone looking at it16.\nBut what is a sensible prior? Can you tell if a prior is sensible or not in its particular context? Well honey, how long have you got. The thing about starting a (potential) series of blog posts is that I don’t really know how far I’m going to get, but I would really like to talk a lot about that over the next little while.\n\nThe rest are about the night I saw Patti LuPone trying to get through the big final scene in War Paint as part of her costume loudly disintegrated.↩︎\nI’m told it’s useful to warm up sometimes because this pandemic has me ice cold.↩︎\nSometimes.↩︎\nExcept, and I cannot stress this enough, when it doesn’t.↩︎\nPlease do not do this!↩︎\nExcept for once in the abstract in a sentence that is in no way shape or formed backed up in the text. Park and Casella (2008)↩︎\nI do know. I know a very large amount about Gaussian processes. But lord in heaven I have seen the greatest minds of my generation subtly fuck up the interpretation of GP priors. Because it’s increadibly hard. Maybe I’ll blog about it one day. Because this is in markdown so I can haz equations.↩︎\nSome reasons are excellent. Some, like the poor Bayesian Lasso, are simply misguided.↩︎\nor the same person in different contexts↩︎\nAre any two statistical problems ever the same?↩︎\nYes. I have a lot of feelings about this too, but meh. A good artist can make great art with minimal equipment (see When Doves Cry), but most people are not the genius Prince was so just use good tools and stress less!↩︎\nI have written extensively about priors in the context of the Arianist heresy because of course I fucking have. Part 1, Part 2, Part 3. Apologies for mathematics eaten by a recent formatting change!↩︎\nEditors forced the word often into the published title and, like, who’s going to fight?↩︎\n\\(N(2-\\bar{y},n^{-1})\\)↩︎\nWhat does this even mean? Depends on your context really. But a working definition is that the big picture features of the posterior are similar enough that if you were to use it to make a decision, that decision doesn’t change very much.↩︎\nBut omg subtle high dimensional stuff and I guess I’ll talk about that later maybe too?↩︎\n",
    "preview": {},
    "last_modified": "2021-11-03T23:44:13+11:00",
    "input_file": {}
  }
]
