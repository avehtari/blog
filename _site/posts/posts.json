[
  {
    "path": "posts/2021-10-14-priors1/",
    "title": "Priors: Night work",
    "description": "Priors? Defined. Questions? Outlined. Purpose? Declared.",
    "author": [
      {
        "name": "Dan Simpson",
        "url": {}
      }
    ],
    "date": "2021-10-14",
    "categories": [],
    "contents": "\nI have feelings. Too many feelings. And ninety six point seven three percent of them are about prior distributions1. So I am going to write a few blog posts about prior distributions.\nTo be very honest, this is mostly a writing exercise2 to get me out of a slump.\nSo let’s do this.\nNo love, deep web\nAs far as I am concerned it’s really fucking stupid to try to write about priors on their own. They are meaningless outside of their context. But, you know, this is a blog. So I get to be stupid.\nSo what is a prior distribution? It is whatever you want it to be. It is a probability distribution3 that … I don’t know. Exists4.\nOk. This is not going well. Let’s try again.\nA prior distribution is, most of the time, a probability distribution on the parameters of a statistical model. For all practical purposes, we tend to work with its density, so if the parameter \\(\\theta\\), which could be a scalar but, in any interesting case, isn’t, has prior \\(p(\\theta)\\).\nCaptain fantastic and the brown dirt cowboy\nBut what does it all meeeeeeeean?\nWe have a prior distribution specified, gloriously, by it’s density. And unlike destiny, density is meaningless. It only makes sense when we integrate it up to get a probability \\[\n\\Pr(A) = \\int_A p(\\theta)\\,d\\theta.\n\\]\nSo what does the prior probabilty \\(\\Pr(A)\\) of a set \\(A\\) actually mean in real life? The answer may shock you: it means something between nothing and everything.\nScenario 1: Let’s imagine that we were trying to estimate the probability that someone in some relative homogeneous subgroup of customers completed a purchase on our website. It’s a binary process, so the parameter of interest can probably just be the probability that a sale is made. While we don’t know what the probability of a sale is for the subgroup of interest, we know a lot sales on our website in general (in particular, we know that about 3% of visits result in sales). So if I also believe that it would be wildly unlikely for 20% of visits to result in a sale, I could posit a prior like a \\(\\text{Beta}(0.4,5)\\) prior that captures (a version of) these two pieces of information.\n\n\nShow code\n\n  ## Step 1: \n  \nfn <- \\(x) (qbeta(0.5,x[1], x[2]) - 0.02)^2 + \n  (qbeta(0.9, x[1], x[2]) - 0.2)^2\n\nbest <- optim(c(1/2,1/2), fn)\n\n## Step 3: Profit.\n## (AKA round and check)\nqbeta(0.9, 0.4, 5)\nqbeta(0.5, 0.4, 5)\n\n\n\nScenario 2: Let’s imagine I want to do variable selection. I don’t know why. I was just told I want to do variable selection. So I fire up the Bayesian Lasso5 and then threshold in some way. In this case, the prior encode a hoped-for property of my posterior. (To paraphrase Lana, hope is a dangerous thing for a woman like you to have because the Bayeisan Lasso does not work to the point that the original paper doesn’t even suggest using it for variable selection6 it just, idk, liked the name. Statistics is wild.)\nScenario 3: I’m doing a regression with just one variable (because why not) and I think that the relationship between the response \\(y\\) and the covariate \\(x\\) is non-linear. That is, I think there is some unknown to me function \\(f(x)\\) such that \\(\\mathbb{E}(y_i) = f(x_i)\\). So I ask a friend and they tell me to use a Gaussian Process prior for \\(f(\\cdot)\\) with an exponential covariance function.\nWhile I can write down the density for the joint prior of \\((f(x_1), f(x_2,), \\ldots, f(x_n))\\), I do not know7 what this prior means in any substantive sense. But I can tell you, you’re gonna need that maths degree to even try.\nAnd should you look deeper, you will find more and more scenarios where priors are doing different things for different reasons8. For each of these priors in each of these scenarios, we will be able to compute the posterior (or a reasonable computational approximation to it) and then work with that posterior to answer our questions.\nDifferent people9 will use priors different ways even for very similar problems10. This remains true even though they are nominally working under the same inferential framework.\nBayesians are chaotic.\nMapping out a sky / What you feel like, planning a sky\nSondheim’s ode to pointillism feels relevant here. The reality of the prior distribution—and the whole reason the concept is so slippery and chaotic—is that you are, dot by dot, constructing the world of your inference. This act of construction is fundamental to understanding how Bayesian methods work, how to justify your choices, and how to use a Bayesian workflow to solve complex problems.\nTo torture the metaphor, our prior distribution is just our paint, unmixed, slowly congealing, possibly made of ground up mummys. It is nothing without a painter and a brush.\nThe painter is the likelihood or, more generally, the generative link between the parameter values and the actual data, \\(p(y \\mid \\theta)\\). The brush is the computational engine you use to actually produce the posterior painting11.\nThis then speaks to the core challenge with writing about priors: it depends on how you use them. It is a fallacy, or perhaps a foolishness, or perhaps a heresy12. Hell, when trying to understand a single inference The Prior Can Only Be Understood In The Context Of The Likelihood13. In the context of an entire workflow, The Experiment is just as Important as the Likelihood in Understanding the Prior.\nFor instance, using independent Cauchy priors for the coefficients in a linear regression model will result in a perfectly ok posterior. Whereas the same priors used in a logistic regression, you may end up with posteriors with such heavy tails that they don’t have a mean! (Do we care? Well, yes. If we want reasonable uncertainty intervals we probably want 2 or so moments otherwise those large deviations are gonna getcha!)\nSo what?\nAll of this is fascinating. And it is a lot less chaotic than it initially sounds.\nThe reality is that while two Bayesians may use different priors and, hence, produce different posteriors for the same data set.This can be extreme. For example, if I am trying to estimate the mean of data generated by \\(y_i \\sim N(\\mu, 1)\\), then I can choose a prior14 (that depends on the data) so that the posterior mean \\(\\mathbb{E}(\\mu \\mid y) =1\\). Or, to put it differently, I can get any answer I want if I choose an prior carefully (and in a data-dependent manner).\nBut this isn’t necessarily a problem. This is because the posteriors produced by two sensible priors for the same problem will produce fairly similar results15. The prior I used to cheat in the previous example would not be considered sensible by anyone looking at it16.\nBut what is a sensible prior? Can you tell if a prior is sensible or not in its particular context? Well honey, how long have you got. The thing about starting a (potential) series of blog posts is that I don’t really know how far I’m going to get, but I would really like to talk a lot about that over the next little while.\n\nThe rest are about the night I saw Patti LuPone trying to get through the big final scene in War Paint as part of her costume loudly disintegrated.↩︎\nI’m told it’s useful to warm up sometimes because this pandemic has me ice cold.↩︎\nSometimes.↩︎\nExcept, and I cannot stress this enough, when it doesn’t.↩︎\nPlease do not do this!↩︎\nExcept for once in the abstract in a sentence that is in no way shape or formed backed up in the text. Park and Casella (2008)↩︎\nI do know. I know a very large amount about Gaussian processes. But lord in heaven I have seen the greatest minds of my generation subtly fuck up the interpretation of GP priors. Because it’s increadibly hard. Maybe I’ll blog about it one day. Because this is in markdown so I can haz equations.↩︎\nSome reasons are excellent. Some, like the poor Bayesian Lasso, are simply misguided.↩︎\nor the same person in different contexts↩︎\nAre any two statistical problems ever the same?↩︎\nYes. I have a lot of feelings about this too, but meh. A good artist can make great art with minimal equipment (see When Doves Cry), but most people are not the genius Prince was so just use good tools and stress less!↩︎\nI have written extensively about priors in the context of the Arianist heresy because of course I fucking have. Part 1, Part 2, Part 3. Apologies for mathematics eaten by a recent formatting change!↩︎\nEditors forced the word often into the published title and, like, who’s going to fight?↩︎\n\\(N(2-\\bar{y},n^{-1})\\)↩︎\nWhat does this even mean? Depends on your context really. But a working definition is that the big picture features of the posterior are similar enough that if you were to use it to make a decision, that decision doesn’t change very much.↩︎\nBut omg subtle high dimensional stuff and I guess I’ll talk about that later maybe too?↩︎\n",
    "preview": {},
    "last_modified": "2021-10-14T18:06:40+11:00",
    "input_file": "priors1.knit.md"
  },
  {
    "path": "posts/2021-10-11-n-sane-in-the-membrane/",
    "title": "$(n-1)$-sane in the membrane",
    "description": "Windmills? Tilted. Topic? Boring. (n-1)? No.",
    "author": [
      {
        "name": "Dan Simpson",
        "url": {}
      }
    ],
    "date": "2021-10-10",
    "categories": [],
    "contents": "\nI’ve been teaching a lot lately. That’s no huge surprise. It is my job. Maybe the one slight oddity this year is that I shifted jobs and, in switching hemispheres, I landed 3 consecutive teaching semesters. So. I’ve been teaching a lot lately.\nAnd when you’re in a period of heavy teaching, every-fucking-thing is about teaching.\nSo this blogpost is about teaching.\nRight now, I’m coming to the end of a second year class called Statistical Thinking. It’s been fun to work out how to teach the material. It’s standard fare: sampling variation, tests, bootstraps1, regression, and just a hint of Bayes in the last 2 weeks that you incentivize by promising a bastard of an exam question. So you know, (arms up even though I’m Catholic) tradition!\nIf I were a rich man (Katrina Lenk with a violin)\nThe thing about teaching an intro stats class is that it brings screaming to mind that quote from Bennett’s The History Boys2: (paraphrasing) “How do I define [intro to Statistics]? It’s just one fucking thing after another”.\nConstructing twelve moderately sequential weeks from the whole mass of things that someone being introduced to statistics needs to know is not unlike being thrown in the middle of the lake with nothing but an ice-cream container and a desiccated whale penis: confusing, difficult, and rather damp.\nThe nice thing about building an intro stats course is you’re not alone. You’re adrift in a sea of shit ideas! (Also a lot of good ones3, but don’t ruin my flow!)\nThe trouble is that this sort of course is simultaneously teaching big concepts and complex details. And while it’s not toooooo hard to make the concepts build and reinforce as time inexorably marches on, the techniques and details needed to illuminate the big concepts are not quite as linear.\nThere are two routes through this conundrum: incantations inscribed onto books made of human skin using the blood of sacrificial virgins (aka gathered during engineering statistics service teaching) or computers.\nI went with computers because we are in lockdown and I couldn’t be bothered sourcing and bleeding virgins.\nThe downside is that you need the students to have a grip on R programming (and programmatic thinking). This only happens if the degree you are teaching in is built in such a way that these skills have already been taught. Otherwise you need to teach both (which is very possible, but you need to teach less statistical content).\nThis is not a postmortem on my teaching, but if it were, it would be about that last point.\nI saw Goody Proctor with the devil!\nA tweet from Sanjay SrivastavaThis is a very long way to say I saw a tweet an had feelings.\nBecause I’m thinking about this stuff pretty hard right now, I am (as Hedwig would say) fully dilated.\nAnd my question is what is the use of teaching this distinction? Should anyone bother dividing by \\((n-1)\\) instead of \\(n\\) in their variance estimates?\nWell I guess the first question is is there a difference in this distinction? Let’s do the sort of R experiment I want my students to do!\n\n\n# Independent samples for a qq-plot!\n# Thanks to Rob Trangucci for catching this!\nlibrary(tidyverse)\nn_sim <- 100000\nn <- 10\nexperiments <- tibble(exp = rep(1:n_sim, each = n),\n                      sample = rnorm(n * n_sim),\n                      sample2 = rnorm(n * n_sim))\n\ncompare <- experiments %>%\n  group_by(exp) %>%\n  summarise(m = mean(sample),\n            m2 = mean(sample2),\n            var_bias = mean((sample - m)^2),\n            z_bias = m / sqrt(mean(var_bias)),\n            z = m2 / sd(sample2))\n\n\ncompare %>% \n  ggplot(aes(sort(z), sort(z_bias))) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0) +\n  theme_bw() + \n  coord_fixed(xlim = c(-2,2), y = c(-2,2))\n\n\n\n\nWell that is clear. There is not.\nOr, well, there is a small difference.\nBut to see it, you need a lot of samples! Why? Well the easy answer is maths.\nFor one thing, when \\(n=10\\), \\[\n\\frac{1}{n} - \\frac{1}{n-1} = \\frac{1}{90} = 0.01.\n\\] This does not compare well against the sampling variance, which (assuming \\(\\sigma^2\\approx 1\\), which is usual if you’ve scaled your problem correctly) is about \\(0.3\\).\nBut we could choose to do it properly. The bias in the MLE (aka the divide by \\(n\\)) variance estimate is \\[\n-\\frac{\\sigma^2}{n}.\n\\] This is a lot smaller than the sampling variability of the estimate (aka how much uncertainty you have because of the finite sample), which is \\[\n\\frac{\\sigma}{\\sqrt{n}}.\n\\]\nAnd that’s the whole story. Dividing by \\(n\\) instead of \\((n-1)\\) leaves you with a slightly biased estimate. But the bias if fucking tiny. It is possibly moving your second decimal place by about 1 number (assume our population variance is one). The sampling variably is moving the first decimal place by several digits.\nTruly. What is the point. The old guys4 who went wild about bias are now mostly dead. Or they’ve changed their minds (which is, you know, a reasonable thing to do as information about best practice is updated). The war against bias was lost before your undergraduates were born.\nEven in crisis, I maintain\nBut nevertheless, this whole DIVIDE BY N-1 OR THE BIAS MONSTER IS GONNA GET YA bullshit continues.\nAnd to some extent, maybe I shouldn’t care. I definitely shouldn’t care this many words about it.\nBut I do. And I do for a couple of reasons.\nReason One: What is the point teaching students about uncertainty and that you can’t just say “this number is different” because the estimate on a single sample is different. If I am to say that I need things to be at least5 \\(\\mathcal{O}(n^{-1/2})\\) apart before I’m willing to say they are maybe different, then why am I harping on about the much smaller difference?\nReason Two: It’s a shitty example. Bias and bias corrections have a role to play in statistics6. But if this is your first introduction to bias correction, you are going to teach either:\nBias is always bad, regardless of context / sampling variance / etc\nBias can be corrected, but it’s trivial and small.\nBoth of those things are bullshit. Just teach them how to bootstrap and teach the damn thing properly. You do not have to go very far to show bias actually making a difference!\nMaybe the only place the difference will be noticed is if you compare against the in-build var or sd functions. This is not the use case I would build my class around, but it is a thing you would need to be aware of.\nThe worlds is a question, this room is an answer. And the answer is no.\nIf you are going to teach statistics as more than just stale incantations and over-done fear-mongering, you need to construct the types of stakes that are simply not present in the \\(n\\) vs \\(n-1\\) bullshit.\nIt is present when you are teaching the normal vs t distribution. You are teaching that the design of your experiment changes the possible extreme behaviour and sometimes it can change a lot.\nThe \\(n\\) vs \\((n-1)\\) denominator for a variance estimator is a curiosity. It is the source of thrilling7 exercises or exam questions. But it is not interesting.\nIt could maybe set up the idea that MLEs are not unbiased. But even then, the useless correction term is not needed. Just let it be slightly biased and move on with your life.\nBecause if that is the biggest bias in your analysis, you are truly blessed.\nIn real life, bias is the price you pay for being good at statistics. And like any market, if you pay too much you’re maybe not that good. But if you pay nothing at all, you don’t get to play.\n\nTo paraphrase Jimmy Somerville, tell me whyyyyyyyyy about 90% of the bootstrap material on the web is … misguided. And why tidymodels only has the shit bootstrap in it?↩︎\nOk. Straight up, “[Intro to statistics] is a commentary on the various and continuing incapabilities of men” would’ve also worked.↩︎\nThis course stands on the shoulders of giants: Di Cook and Catherine Forbes gave me a great base. And of course every single textbook (shout out to the OpenIntro crew!), blog post, weird subsection of some other book, paper from 1987 on some weird bootstrap, etc that I have used to make a course!↩︎\nYes. I used the word on purpose.↩︎\n\\(n\\) is the size of the sample.↩︎\nI spend most of my time doing Bayes shit, and we play this game somewhat differently. But the gist is the same.↩︎\nNot thrilling.↩︎\n",
    "preview": "posts/2021-10-11-n-sane-in-the-membrane/n-sane-in-the-membrane_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-10-11T10:11:32+11:00",
    "input_file": {}
  }
]
