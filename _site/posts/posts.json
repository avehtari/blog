[
  {
    "path": "posts/2021-10-11-n-sane-in-the-membrane/",
    "title": "$(n-1)$-sane in the membrane",
    "description": "Windmills? Tilted. Topic? Boring. (n-1)? No.",
    "author": [
      {
        "name": "Dan Simpson",
        "url": {}
      }
    ],
    "date": "2021-10-10",
    "categories": [],
    "contents": "\nI’ve been teaching a lot lately. That’s no huge surprise. It is my job. Maybe the one slight oddity this year is that I shifted jobs and, in switching hemispheres, I landed 3 consecutive teaching semesers. So. I’ve been teaching a lot lately.\nAnd when you’re in a period of heavy teaching, every-fucking-thing is about teaching.\nSo this blogpost is about teaching.\nRight now, I’m coming to the end of a second year class called Statistical Thinking. It’s been fun to work out how to teach the material. It’s standard fare: sampling variation, tests, bootstraps, regression, and just a hint of Bayes in the last 2 weeks that you incentivize by promising a bastard of an exam question. So you know, (arms up even though I’m Catholic) tradition!\nIf I were a rich man (Katrina Lenk with a violin)\nThe thing about teaching an intro stats class is that it brings screaming to mind that quote from Bennett’s The History Boys1: (paraphrasing) “How do I define [intro to Statistics]? It’s just one fucking thing after another”.\nConstructing 12 moderately sequential weeks from the whole mass of things that someone being introduced to statistics needs to know is not unlike being thrown in the middle of the lake with nothing but an ice-cream container and a desiccated whale penis: confusing, difficult, and rather damp.\nThe nice thing about building an intro stats course is you’re not alone. You’re adrift in a sea of shit ideas! (Also a lot of good ones2, but don’t ruin my flow!)\nThe trouble is that this sort of course is simultaneously teaching big concepts and complex details. And while it’s not toooooo hard to make the concepts build and reinforce as time inexorably marches on, the techniques and details needed to illuminate the big concepts are not quite as linear.\nThere are two routes through this conundrum: incantations inscribed onto books made of human skin using the blood of sacrificial virgins (aka the engineering statistics service teaching) or computers.\nI went with computers because we are in lockdown and I couldn’t be bothered sourcing and bleeding virgins.\nThe downside is that you need the students to have a grip on R programming (and programatic thinking). This only happens if your program is built for it. Otherwise you need to teach both (which is very possible, but you need to teach less statistical content).\nThis is not a postmortem on my teaching, but if it were, it would be about that last point.\nI saw Goody Proctor with the devil!\nA tweet from Sanjay SrivastavaThis is a very long way to say I saw a tweet an had feelings.\nBecause I’m thinking about this stuff pretty hard right now, I am (as Hedwig would say) fully dilated.\nAnd my question is _what is the use of teaching this distinction? Should anyone bother dividing by \\((n-1)\\) instead of \\(n\\) in their variance estimates?\nWell I guess the first question is is there a distinction? Let’s do the sort of R experient I want my students to do!\n\n\n\nThere is not.\nOr, well, there is a small difference.\nBut to see it, you need a lot of samples! Why? Well the easy answer is maths.\nFor one thing, when \\(n=10\\), \\[\n\\frac{1}{n} - \\frac{1}{n-1} = \\frac{1}{90} = 0.01.\n\\] This does not compare well against the sampling variance, which (assumeing \\(\\sigma^2=1\\)) is about \\(0.3\\).\nBut we could choose to do it propely. The bias in the MLE (aka the divide by \\(n\\)) variance estimate is \\[\n-\\frac{\\sigma^2}{n}.\n\\] This is a lot smaller than the sampling variability of the estimate (aka how much uncertainty you have becasue of the finite sample), which is \\[\n\\frac{\\sigma^2}{\\sqrt{n}}.\n\\]\nAnd that’s the whole story. Dividing by \\(n\\) instead of \\((n-1)\\) leaves you with a slightly biased estimate. But the bias if fucking tiny. It is possibly moving your second decimal place by about 1 number (assume our population variance is one). The sampling variably is moving the first decimal place by several digits.\nTruly. What is the point. The old guys3 who went wild about bias are now mostly dead. Or they’ve changed their minds (which is, you know, a reasonable thing to do). The war against bias was lost before your undergraduates were born.\nEven in crisis, I maintain\nBut nevertheless, this whole DIVIDE BY N-1 OR THE BIAS MONSTER IS GONNA GET YA bullshit continues.\nAnd to some extent, maybe I shouldn’t care. I defintiely shouldn’t care this many words about it.\nBut I do. And I do for a couple of reasions.\nReason One: What is the point teaching students about uncertainty and that you can’t just say “this number is different” because the estimate on a single sample is different. If I am to say that I need things to be at least4 \\(\\mathcal{O}(n^{-1/2})\\) apart before I’m willing to say they are maybe different, then why am I harping on about the much smaller difference?\nReason Two: It’s a shitty example. Bias and bias corrections have a role to play in statistics5. But if this is your first introduction to bias correction, you are going to teach either:\nBias is always bad, regardless of context / sampling variance / etc\nBias can be corrected, but it’s trivial and small.\nBoth of those things are bullshit. Just teach them how to bootstrap and teach the damn thing properly. You do not have to go very far to show bias actually making a difference!\nThe worlds is a question, this room is an answer. And the answer is no.\nIf you are going to teach statistics as more than just stale incantations and over-done fear-mongering, you need to constuct the types of stakes that are simply not present in the \\(n\\) vs \\(n-1\\) bullshit.\nIt is present when you are teaching the normal vs t distribution. You are teaching that the design of your experiment changes the possible extreme behaviour and sometimes it can change a lot.\nThe \\(n\\) vs \\((n-1)\\) denominator for a variance estimator is a curiosity. It is the source of thrilling6 exercises or exam questions. But it is not interesting.\nIt could maybe set up the idea that MLEs are not unbiased. But even then, the useless correciton term is not needed. Just let it be slightly biased and move on with your life.\nBecause if that is the biggest bias in your analysis, you are truly blessed.\n\nOk. Straight up, “[Intro to statistics] is a commentary on the various and continuing incapabilities of men” would’ve also worked.↩︎\nThis course stands on the shoulders of giants: Di Cook and Catherine Forbes gave me a great base. And of course every single textbook (shout out to the OpenIntro crew!), blog post, weird subsection of some other book, paper from 1987 on some weird bootstrap, etc that I have used to make a course!↩︎\nYes. I used the word on purpose.↩︎\n\\(n\\) is the size of the sample.↩︎\nI spend most of my time doing Bayes shit, and we play this game somewhat differently. But the gist is the same.↩︎\nNot thrilling.↩︎\n",
    "preview": "posts/2021-10-11-n-sane-in-the-membrane/n-sane-in-the-membrane_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-10-11T00:32:54+11:00",
    "input_file": "n-sane-in-the-membrane.knit.md"
  }
]
