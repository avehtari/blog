<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.15">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dan Simpson">
<meta name="dcterms.date" content="2021-12-08">
<meta name="description" content="A repost (with edits, revisions, and footnotes) from Andrew’s blog about how much I hate the Bayesian Lasso. Originally published 2nd November, 2017.">

<title>Un garçon pas comme les autres (Bayes) - The king must die (repost)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Un garçon pas comme les autres (Bayes) - The king must die (repost)">
<meta property="og:description" content="A repost (with edits, revisions, and footnotes) from Andrew’s blog about how much I hate the Bayesian Lasso. Originally published 2nd November, 2017.">
<meta property="og:image" content="https://dansblog.netlify.app/posts/2021-12-08-the-king-must-die-repost/sylvia.jpg">
<meta property="og:site-name" content="Un garçon pas comme les autres (Bayes)">
<meta name="twitter:title" content="The king must die (repost)">
<meta name="twitter:description" content="A repost (with edits, revisions, and footnotes) from Andrew’s blog about how much I hate the Bayesian Lasso. Originally published 2nd November, 2017.">
<meta name="twitter:image" content="https://dansblog.netlify.app/posts/2021-12-08-the-king-must-die-repost/sylvia.jpg">
<meta name="twitter:creator" content="@dan_p_simpson">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Un garçon pas comme les autres (Bayes)</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About this blog</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/dan_p_simpson"><i class="bi bi-twitter" role="img" aria-label="twitter">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/dpsimpson"><i class="bi bi-github" role="img" aria-label="github">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://dansblog.netlify.app"><i class="bi bi-person-circle" role="img" aria-label="website">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"><i class="bi bi-rss" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">The king must die (repost)</h1>
                  <div>
        <div class="description">
          <p>A repost (with edits, revisions, and footnotes) from Andrew’s blog about how much I hate the Bayesian Lasso. Originally published 2nd November, 2017.</p>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Bayesian Lasso</div>
                <div class="quarto-category">Variable Selection</div>
                <div class="quarto-category">Fundamentals</div>
                <div class="quarto-category">Theory</div>
                <div class="quarto-category">Things that don’t work</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Dan Simpson </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 8, 2021</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#berätta-för-mig-om-det-är-sant-att-din-hud-är-doppad-i-honung" id="toc-berätta-för-mig-om-det-är-sant-att-din-hud-är-doppad-i-honung" class="nav-link active" data-scroll-target="#berätta-för-mig-om-det-är-sant-att-din-hud-är-doppad-i-honung">Berätta för mig om det är sant att din hud är doppad i honung</a></li>
  <li><a href="#who-do-you-think-you-are" id="toc-who-do-you-think-you-are" class="nav-link" data-scroll-target="#who-do-you-think-you-are">Who do you think you are?</a></li>
  <li><a href="#hiding-all-away" id="toc-hiding-all-away" class="nav-link" data-scroll-target="#hiding-all-away">Hiding all away</a>
  <ul class="collapse">
  <li><a href="#why-do-we-scale-priors" id="toc-why-do-we-scale-priors" class="nav-link" data-scroll-target="#why-do-we-scale-priors">Why do we scale priors</a></li>
  </ul></li>
  <li><a href="#only-once-in-sheboygan.-only-once." id="toc-only-once-in-sheboygan.-only-once." class="nav-link" data-scroll-target="#only-once-in-sheboygan.-only-once.">Only once in Sheboygan. Only once.</a></li>
  <li><a href="#show-some-emotion" id="toc-show-some-emotion" class="nav-link" data-scroll-target="#show-some-emotion">Show some emotion</a></li>
  <li><a href="#maybe-i-was-mean-but-i-really-dont-think-so" id="toc-maybe-i-was-mean-but-i-really-dont-think-so" class="nav-link" data-scroll-target="#maybe-i-was-mean-but-i-really-dont-think-so">Maybe I was mean, but I really don’t think so</a></li>
  </ul>
</nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<blockquote class="blockquote">
<p>And then there was Yodeling Elaine, the Queen of the Air. She had a dollar sign medallion about as big as a dinner plate around her neck and a tiny bubble of spittle around her nostril and a little rusty tear, for she had lassoed and lost another tipsy sailor—<a href="https://www.youtube.com/watch?v=q6zAp0NvPSs">Tom Waits</a></p>
</blockquote>
<p>It turns out I turned thirty two<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> and became unbearable. Some of you may feel, with an increasing sense of temporal dissonance, that I was already unbearable<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. Others will wonder how I can look so good at my age<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. None of that matters to me because all I want to do is talk about the evils of marketing like the 90s were a vaguely good idea<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
<p>The thing is, I worry that the real problem in academic statistics in 2017 is not a reproducibility crisis, so much as that so many of our methods just don’t work. And to be honest, I don’t really know what to do about that, other than suggest that we tighten our standards and insist that people proposing new methods, models, and algorithms work harder to sketch out the boundaries of their creations. (What a suggestion. Really. Concrete proposals for concrete change. But it’s a blog. If ever there was a medium to be half-arsed in it’s this one. It’s like twitter for people who aren’t pithy.)</p>
<section id="berätta-för-mig-om-det-är-sant-att-din-hud-är-doppad-i-honung" class="level2">
<h2 class="anchored" data-anchor-id="berätta-för-mig-om-det-är-sant-att-din-hud-är-doppad-i-honung">Berätta för mig om det är sant att din hud är doppad i honung</h2>
<p>So what is the object of my impotent ire today. Well nothing less storied than the <a href="https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/park-casella.pdf">Bayesian Lasso</a>.</p>
<p>It should be the least controversial thing in this, the year of our lord two thousand and seventeen, to point out that this method bears no practical resemblance to the Lasso. Or, in the words of Law and Order: SVU, “The [Bayesian Lasso] is fictional and does not depict any actual person or event”.</p>
</section>
<section id="who-do-you-think-you-are" class="level2">
<h2 class="anchored" data-anchor-id="who-do-you-think-you-are">Who do you think you are?</h2>
<p>The Bayesian Lasso is a good example of what’s commonly known as the Lupita Nyong’o fallacy<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, which goes something like this: Lupita Nyong’o had a break out role in Twelve Years a Slave, she also had a heavily disguised role in one of ’ the Star Wars films (the specific Star Wars film is not important. I haven’t seen it and I don’t care). Hence Twelve Years a Slave exists in the extended Star Wars universe.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p>The key point is that the (classical) Lasso plays a small part within the Bayesian Lasso (it’s the MAP estimate) in the same way that Lupita Nyong’o played a small role in that Star Wars film. But just as the presence of Ms Nyong’o does not turn Star Wars into Twelve Years a Slave, the fact that the classical Lasso can be recovered as the MAP estimate of the Bayesian Lasso does not make the Bayesian Lasso useful.</p>
<p>And yet people still ask <a href="https://statmodeling.stat.columbia.edu/2017/02/14/lasso-regression-etc-stan/">if they can be fit in Stan</a>. In that case, Andrew answered the question that was asked, which is typically the best way to deal with software enquiries<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. But I am brave and was not asked for my opinion, so I’m going to talk about why the Bayesian Lasso doesn’t work.</p>
</section>
<section id="hiding-all-away" class="level2">
<h2 class="anchored" data-anchor-id="hiding-all-away">Hiding all away</h2>
<p>So why would anyone not know that the Bayesian Lasso doesn’t work? Well, I don’t really know. But I will point out that all of the results that I’ve seen in this directions (not that I’ve been looking hard) have been published in the prestigious but obtuse places like Annals of Statistics, the journal we publish in when we either don’t want people without a graduate degree in mathematical statistics to understand us or when we want to get tenure.</p>
<p>By contrast, the original paper is very readable and published in JASA, where we put papers when we are ok with people who do not have a graduate degree in mathematical statistics being able to read them, or when we want to get tenure<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>.</p>
<p>To be fair to Park and Casella, they never really say that the Baysian Lasso should be used for sparsity. Except for one sentence in the introduction where they say the median gives approximately sparse estimators and the title which links it to the most prominent and popular method for estimating a sparse signal. Marketing eh. (See, I’m Canadian now<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>).</p>
<p>##The devil has designed my death and is waiting to be sure</p>
<p>So what is the Bayesian LASSO (and why did I spend 600 words harping on about something before defining it? The answer will shock you. Actually the answer will not shock you, it’s because it’s kinda hard to do equations on this thing<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>.)</p>
<p>For data observed with Gaussian error, the Bayesian Lasso takes the form <span class="math display">\[
\mathbf{y} \mid \boldsymbol{\beta} \sim N( \mathbf{X} \boldsymbol{\beta}, \boldsymbol{\Sigma})
\]</span></p>
<p>where, instead of putting a Normal prior on <span class="math inline">\(\boldsymbol{\beta}\)</span> as we would in a bog-standard Bayesian regression, we instead use independent Laplace priors <span class="math display">\[
p(\beta_i) = \frac{\lambda}{2} \exp(-\lambda | \beta_i|).
\]</span></p>
<p>Here the tuning parameter<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> <span class="math inline">\(\lambda = c(p,s_0,\mathbf{X})\tilde{\lambda}\)</span> where <span class="math inline">\(p\)</span> is the number of covariates, <span class="math inline">\(s_0\)</span> is the number of “true” non-zero elements of <span class="math inline">\(\boldsymbol{\beta}\)</span>, <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is known, and <span class="math inline">\(\tilde{\lambda}\)</span> is an unknown scaling parameter that should be <span class="math inline">\(\mathcal{O}(1)\)</span>.</p>
<p><em>Important Side note</em>: This isn’t the exact same model as Park and Castella used as they didn’t use the transformation <span class="math display">\[
\lambda = c(p,s_0,\mathbf{X}) \tilde{\lambda}
\]</span> but rather just dealt with <span class="math inline">\(\lambda\)</span> as the parameter. We will see below, and it’s born out by many papers in this field, that the best possible value of <span class="math inline">\(\lambda\)</span> will depend on this structural/design information</p>
<p>If we know how <span class="math inline">\(\lambda\)</span> varies as the structural/design information changes, it’s a much better idea to put a prior on <span class="math inline">\(\tilde{\lambda}\)</span> than on <span class="math inline">\(\lambda\)</span> directly. Why? Because a prior on <span class="math inline">\(\lambda\)</span> needs to depend on p, <span class="math inline">\(s_0\)</span>, and X and hence needs to be changed for each problem, while a prior on <span class="math inline">\(\tilde{\lambda}\)</span> can be used for many problems. One possible option is <span class="math inline">\(c(p,s_0,\mathbf{X}) = 2\|\mathbf{X}\|\sqrt{\log p }\)</span>, which is a rate optimal parameter for the (non-Bayesian) Lasso. Later, we’ll do a back-of-the-envelope calculation that suggests we might not need the square root around the logarithmic term.</p>
<section id="why-do-we-scale-priors" class="level3">
<h3 class="anchored" data-anchor-id="why-do-we-scale-priors">Why do we scale priors</h3>
<p>The critical idea behind the Bayesian Lasso is that <em>we can use the i.i.d. Laplace priors to express the substantive belief that the most of the</em> <span class="math inline">\(\beta_j\)</span> <em>are (approximately) zero</em>. The reason for scaling the prior is that the values of <span class="math inline">\(\lambda\)</span> that are consistent with this belief depend on <span class="math inline">\(p\)</span>, <span class="math inline">\(s_0\)</span>, and <span class="math inline">\(X\)</span>.</p>
<p>For example, <span class="math inline">\(\lambda = 1\)</span>, the Bayesian Lasso will not give an approximately sparse signal.</p>
<p>While we could just use a prior for <span class="math inline">\(\lambda\)</span> that has a very heavy right tail (something like an inverse gamma), this is at odds with a good practice principle of making sure all of thee parameters in your models are properly scaled to make them order 1. Why do we do this? Because it makes it much much easier to set sensible priors.</p>
<p>Some of you may have noticed that the scaling <span class="math inline">\(c(p,s_0,\mathbf{X})\)</span> can depend on the unknown sparsity <span class="math inline">\(s_0\)</span>. This seems like cheating. People who do asymptotic theory call this sort of value for <span class="math inline">\(\lambda\)</span> an oracle value, mainly because people studying Bayesian asymptotics are really really into database software.</p>
<p>The idea is that this is the value of <span class="math inline">\(\lambda\)</span> that gives the model the best chance of working. When maths-ing, you work out the properties of the posterior with the oracle value of <span class="math inline">\(\lambda\)</span> and then you use some sort of smoothness argument to show that the actual method that is being used to select (or average over) the parameter gives almost the same answer.</p>
<p>It’s also worth noting that the scaling here doesn’t (directly<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>) depend on the number of observations, only the number of covariates. This is appropriate: it’s ok for priors to depend on things that should be known <em>a priori</em> (like the number of parameters) or things that can be worked with<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> (like the scaling of <span class="math inline">\(X\)</span>). It’s a bit weirder if it depends on the number of observations (that tends to break things like coherent Bayesian updating, while the other dependencies don’t).</p>
</section>
</section>
<section id="only-once-in-sheboygan.-only-once." class="level2">
<h2 class="anchored" data-anchor-id="only-once-in-sheboygan.-only-once.">Only once in Sheboygan. Only once.</h2>
<p>So what’s wrong with the Bayesian Lasso? Well the short version is that the Laplace prior doesn’t have enough mass near zero relative to the mass in the tails to allow for a posterior that has a lot of entries that are almost zero and some entries that are emphatically not zero.<br>
Because the Bayesian Lasso prior does not have a spike at zero, none of the entries will be a priori exactly zero, so we need some sort of rule to separate the “zero” entries from the “non-zero” entries. The way that we’re going to do this is to choose a cutoff <span class="math inline">\(\epsilon\)</span> where we assume that if <span class="math inline">\(|\beta_j| &lt;\epsilon\)</span>, then <span class="math inline">\(\beta_j =0\)</span>.</p>
<p>So how do we know that the Lasso prior doesn’t put enough mass in important parts of the parameter space? Well there are two ways. I learnt it during the exciting process of <a href="https://arxiv.org/abs/1403.4630">writing a paper</a> that the reviewers insisted should have an extended section about sparsity (although this was at best tangential to the rest of the paper), so I suddenly needed to know about Bayesian models of sparsity. So I read those Annals of Stats papers. (That’s why I know I should be scaling <span class="math inline">\(\lambda\)</span>!).</p>
<p>What are the key references? Well all the knowledge that you seek is <a href="https://arxiv.org/pdf/1403.0735.pdf">here</a> and <a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1460463652">here.</a></p>
<p>But a much easier way to work out that the Bayesian Lasso is bad is to do some simple maths.</p>
<p>Because the <span class="math inline">\(\beta_j\)</span> are a priori independent, we get a prior on the effective sparsity <span class="math inline">\(s_\epsilon = \#\{j : |\beta_j| &gt; \epsilon\}\)</span> <span class="math display">\[
s_\epsilon \sim \text{Bin}(p, \Pr(|\beta_j| &gt; \epsilon)).
\]</span> For the Bayesian Lasso, that probability can be computed as <span class="math display">\[
\Pr ( | \beta_j | &gt; \epsilon ) = e^{- \lambda \epsilon},
\]</span> so <span class="math display">\[
s_\epsilon \sim \text{Bin}\left(p, e^{-\lambda \epsilon}\right).
\]</span></p>
<p>Ideally, the distribution of this effective sparsity would be centred on the true sparsity.<br>
So we’d like to choose <span class="math inline">\(\lambda\)</span> so that <span class="math display">\[
\mathbb{E}(s_\epsilon)= p e^{- \lambda \epsilon}= s_0.
\]</span></p>
<p>A quick re-arrangement suggests that <span class="math display">\[
\lambda = \epsilon^{-1} \log(p) - \epsilon^{-1} \log(s_0).
\]</span></p>
<p>Now, we are interested in signals with <span class="math inline">\(s_0 = o(p)\)</span>, i.e.&nbsp;where only a very small number of the <span class="math inline">\(\beta_j\)</span> are non-zero. This suggests we can safely ignore the second term as it will be much smaller than the first term.</p>
<p>To choose <span class="math inline">\(\epsilon\)</span>, we can work from the general principle that we want to choose it so that the effect of the “almost zero” <span class="math inline">\(\beta_j\)</span> <span class="math display">\[
\sum_{j:|\beta_j| &lt; \epsilon} \beta_j X_{:j}
\]</span> is small. (here <span class="math inline">\(X_{:j}\)</span> is the <span class="math inline">\(j\)</span>th column of the matrix <span class="math inline">\(X\)</span>.)</p>
<p>From this, it’s pretty clear that <span class="math inline">\(\epsilon\)</span> is going to have to depend on <span class="math inline">\(p\)</span>, <span class="math inline">\(s_0\)</span>, and <span class="math inline">\(X\)</span> as well! But how?</p>
<p>Well, first we note that <span class="math display">\[
\sum_{j:|\beta_j| &lt; \epsilon} \beta_j X_{:j} \leq \epsilon \max_{i =1,\ldots, n}\sum_{j=1}^p |X_{ij}| = \epsilon \|X\|_\infty.
\]</span> Hence we can make this asymptotically small (as <span class="math inline">\(p\rightarrow \infty\)</span>) if <span class="math display">\[
\epsilon = o\left(\|X\|_\infty^{-1}\right).
\]</span> Critically, if we have scaled the design matrix so that each covariate is at most <span class="math inline">\(1\)</span>, ie <span class="math display">\[
\max_{i=1,\ldots,n} |X_{ij}| \leq 1, \qquad \text{for all } j = 1,\ldots, p,
\]</span> then this reduces to the much more warm and fuzzy <span class="math display">\[
\epsilon = o\left(p^{-1}\right).
\]</span></p>
<p>This means that we need to take <span class="math inline">\(\lambda = \mathcal{O}(p \log(p))\)</span> in order to ensure that we have our prior centred on sparse vectors (in the sense that the prior mean for the number of non-zero components is always much less than <span class="math inline">\(p\)</span>).</p>
</section>
<section id="show-some-emotion" class="level2">
<h2 class="anchored" data-anchor-id="show-some-emotion">Show some emotion</h2>
<div class="cell">
<div class="cell-output-display">
<p><img src="the-king-must-die-repost_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>So for the Bayesian Lasso, a sensible parameter is <span class="math inline">\(\lambda = p\log p\)</span>, which will usually have a large number of components less than the threshold <span class="math inline">\(\epsilon\)</span> and a small number that are larger.</p>
<p>But this is still a bad prior.</p>
<p>To see this, let’s consider the prior probability of seeing a <span class="math inline">\(\beta_j\)</span> larger than one <span class="math display">\[
\Pr ( | \beta_j | &gt; 1) = p^{-p} \downarrow \downarrow \downarrow 0.
\]</span></p>
<p>This is the problem with the Bayesian Lasso: in order to have a lot of zeros in the signal, you are also forcing the non-zero elements to be very small. A plot of this function is above, and it’s clear that even for very small values of <span class="math inline">\(p\)</span> the probability of seeing a coefficient bigger than one is crushingly small.</p>
<p>Basically, the Bayesian Lasso can’t give enough mass to both small and large signals simultaneously. Other Bayesian models (such as the horseshoe and the <a href="https://arxiv.org/abs/1707.01694">Finnish horseshoe</a>) can support both simultaneously and this type of calculation can show that (although it’s harder. See Theorem 6 <a href="https://arxiv.org/pdf/1403.4630.pdf">here</a>).</p>
<p>(The scaling that I derived in the previous section is a little different to the standard Lasso scaling of <span class="math inline">\(\lambda = \mathcal{O} (p \sqrt{\log p})\)</span>, but the same result holds: for large <span class="math inline">\(p\)</span> the probability of seeing a large signal is vanishingly small.)</p>
</section>
<section id="maybe-i-was-mean-but-i-really-dont-think-so" class="level2">
<h2 class="anchored" data-anchor-id="maybe-i-was-mean-but-i-really-dont-think-so">Maybe I was mean, but I really don’t think so</h2>
<p>This analysis is all very <em>back of the envelope</em>, but it contains a solid grain of truth<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>.</p>
<p>If you fit a Bayesian Lasso in Stan with an unknown scaling parameter <span class="math inline">\(\lambda\)</span>, you will not see estimates that are all zero, like this analysis suggests. This is because the posterior for <span class="math inline">\(\lambda\)</span> tries to find the values of the parameters that best fit the data <em>and not</em> the values that give an <span class="math inline">\(\epsilon\)</span>-sparse signal.</p>
<p>In order to fit the data, it is important that the useful covariates have large <span class="math inline">\(\beta\)</span>s, which, in turn, forces the <span class="math inline">\(\beta\)</span>s that should be zero to be larger than our dreamt of <span class="math inline">\(\epsilon\)</span>.</p>
<p>And so you see posteriors constructed with the Bayesian Lasso exisiting in some sort of eternal tension: the small <span class="math inline">\(\beta\)</span>s are too big, and the large <span class="math inline">\(\beta\)</span>s are typically shrunken towards zero.</p>
<p>It’s the sort of compromise that leaves everyone unhappy.</p>
<p>Let’s close it out with <a href="https://www.youtube.com/watch?v=wVw1wIj9nC8">the title song</a>.</p>
<blockquote class="blockquote">
<p>And I’m so afraid your courtiers<br>
Cannot be called best friends</p>
</blockquote>
<p><strong>Lightly re-touched from the original, posted on <a href="https://statmodeling.stat.columbia.edu/2017/11/02/king-must-die/">Andrew’s blog</a>. Orignal verison, 2 November, 2017.</strong></p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>(2021 edit) I am no longer 32. I am still unbearable.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><a href="https://www.youtube.com/watch?v=92IkddsjtAA">Fair point</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Answer: <a href="https://www.youtube.com/watch?v=cEC2hUHqEg4">Black Metal</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>They were not. The concept of authenticity is just another way for the dominant culture to suppress more interesting ones.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>(2021 edit): Really, Daniel? Really?<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>(2021): Ok. That ended better than I feared.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>It’s usually a fool’s game to try to guess why people are asking particular questions. It probably wouldn’t be hard for someone to catalogue the number of times I’ve not followed my advice on this, but in life as in statistics, consistency is really only a concern if everything else is going well.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>2021: Look at me trying to land a parallel construction.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>2021: The other week someone asked if I was Canadian, which is a sure sign that my accent is just broken.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>2021: Prophetic words<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>Could we put a prior on this? Sure. And in practice this is what we should probably do. But for today, we are going to keep it fixed.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>It depends on <span class="math inline">\(\|X\|\)</span> which could depend on the number of observations.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>There’s a lot of space for interesting questions here.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>It’ws also fully justified by people who have written far more mathematically sophisticated papers on this topic!<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{simpson2021,
  author = {Dan Simpson and Dan Simpson},
  editor = {},
  title = {The King Must Die (Repost)},
  date = {2021-12-08},
  url = {https://dansblog.netlify.app/2021-12-08-the-king-must-die-repost},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-simpson2021" class="csl-entry quarto-appendix-citeas" role="doc-biblioentry">
Dan Simpson, and Dan Simpson. 2021. <span>“The King Must Die
(Repost).”</span> December 8, 2021. <a href="https://dansblog.netlify.app/2021-12-08-the-king-must-die-repost">https://dansblog.netlify.app/2021-12-08-the-king-must-die-repost</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>