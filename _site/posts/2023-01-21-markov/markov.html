<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.15">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dan Simpson">
<meta name="dcterms.date" content="2023-01-10">
<meta name="description" content="Well this is gonna be technical.">

<title>Un garçon pas comme les autres (Bayes) - Markovian Gaussian processes: A lot of theory and some practical stuff</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Un garçon pas comme les autres (Bayes) - Markovian Gaussian processes: A lot of theory and some practical stuff">
<meta property="og:description" content="Well this is gonna be technical.">
<meta property="og:image" content="https://dansblog.netlify.app/posts/2023-01-21-markov/gays.png">
<meta property="og:site-name" content="Un garçon pas comme les autres (Bayes)">
<meta property="og:image:height" content="640">
<meta property="og:image:width" content="560">
<meta name="twitter:title" content="Markovian Gaussian processes: A lot of theory and some practical stuff">
<meta name="twitter:description" content="Well this is gonna be technical.">
<meta name="twitter:image" content="https://dansblog.netlify.app/posts/2023-01-21-markov/gays.png">
<meta name="twitter:creator" content="@dan_p_simpson">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image-height" content="640">
<meta name="twitter:image-width" content="560">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Un garçon pas comme les autres (Bayes)</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About this blog</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/dan_p_simpson"><i class="bi bi-twitter" role="img" aria-label="twitter">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/dpsimpson"><i class="bi bi-github" role="img" aria-label="github">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://dansblog.netlify.app"><i class="bi bi-person-circle" role="img" aria-label="website">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"><i class="bi bi-rss" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Markovian Gaussian processes: A lot of theory and some practical stuff</h1>
                  <div>
        <div class="description">
          <p>Well this is gonna be technical.</p>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Gaussian processes</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Dan Simpson </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 10, 2023</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-most-relevant-definition-of-a-gaussian-process" id="toc-the-most-relevant-definition-of-a-gaussian-process" class="nav-link active" data-scroll-target="#the-most-relevant-definition-of-a-gaussian-process">The most relevant definition of a Gaussian process</a></li>
  <li><a href="#gaussian-processes-via-the-covariance-operator" id="toc-gaussian-processes-via-the-covariance-operator" class="nav-link" data-scroll-target="#gaussian-processes-via-the-covariance-operator">Gaussian processes via the covariance operator</a></li>
  <li><a href="#white-noise-and-its-associated-things" id="toc-white-noise-and-its-associated-things" class="nav-link" data-scroll-target="#white-noise-and-its-associated-things">White noise and its associated things</a></li>
  <li><a href="#the-generalised-gaussian-process-eta-mathcalc12w" id="toc-the-generalised-gaussian-process-eta-mathcalc12w" class="nav-link" data-scroll-target="#the-generalised-gaussian-process-eta-mathcalc12w">The generalised Gaussian process <span class="math inline">\(\eta = \mathcal{C}^{1/2}W\)</span></a></li>
  <li><a href="#the-markov-property-for-on-abstract-spaces" id="toc-the-markov-property-for-on-abstract-spaces" class="nav-link" data-scroll-target="#the-markov-property-for-on-abstract-spaces">The Markov property for on abstract spaces</a>
  <ul class="collapse">
  <li><a href="#rewriting-the-markov-property-i-splitting-spaces" id="toc-rewriting-the-markov-property-i-splitting-spaces" class="nav-link" data-scroll-target="#rewriting-the-markov-property-i-splitting-spaces">Rewriting the Markov property I: Splitting spaces</a></li>
  <li><a href="#rewriting-the-markov-property-ii-the-dual-random-field-ha" id="toc-rewriting-the-markov-property-ii-the-dual-random-field-ha" class="nav-link" data-scroll-target="#rewriting-the-markov-property-ii-the-dual-random-field-ha">Rewriting the Markov property II: The dual random field <span class="math inline">\(H^*(A)\)</span></a></li>
  <li><a href="#biorthogonal-gaussian-processes" id="toc-biorthogonal-gaussian-processes" class="nav-link" data-scroll-target="#biorthogonal-gaussian-processes">Biorthogonal Gaussian processes</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Gaussian processes are lovely things. I’m a big fan. They are, however, thirsty. They will take your memory, your time, and anything else they can. Basically, the art of fitting Gaussian process models is the fine art of reducing the GP model until it’s simple enough to fit while still being flexible enough to be useful.</p>
<p>There’s a long literature on effective approximation to Gaussian Processes that don’t turn out to be computational nightmares. I’m definitely not going to summarise them here, but I’ll point to an <a href="https://dansblog.netlify.app/posts/2021-11-24-getting-into-the-subspace/getting-into-the-subspace.html">earlier (quite technical) post</a> that mentioned some of them. The particular computational approximation that I am most fond of makes use of the Markov property and efficient sparse matrix computations to reduce memory use and make the linear algebra operations significantly faster.</p>
<p>One of the odder challenges with Markov models is that information about how Markov structures work in more than one dimension can be quite difficult to find. So in this post I am going to lay out some of the theory.</p>
<p>A much more practical (and readable) introduction to this topic can be found in this <a href="https://arxiv.org/abs/2111.01084">lovely paper by Finn, David, and Håvard</a>. So don’t feel the burning urge to read this post if you don’t want to. I’m approaching the material from a different viewpoint and, to be very frank with you, I was writing something else and this section just became extremely long so I decided to pull it out into a blog post.</p>
<p>So please enjoy today’s entry in <em>Dan writes about the weird corners of Gaussian processes</em>. I promise that even though this post doesn’t make it seem like this stuff is useful, it really is.</p>
<section id="the-most-relevant-definition-of-a-gaussian-process" class="level2">
<h2 class="anchored" data-anchor-id="the-most-relevant-definition-of-a-gaussian-process">The most relevant definition of a Gaussian process</h2>
<p>Before we talk about how to fix boundary issues for general Markovian GPs, we probably should explain what a Markovian GP is. In order to do this, we need to use one of the less common definitions of a Gaussian process. <a href="https://dansblog.netlify.app/posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness.html">Thankfully, I wrote a whole damn blog post about this type of stuff</a>. Do you need to read that post to understand this one? Not really. But if you happen to have read it, we are going to be using the third definition of a Gaussian process, the weirdest one.</p>
<p>While Gaussian processes<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> are usually characterised by their covariance function, that is not the only option. A particularly powerful object that is associated with a Gaussian process is the Reproducing Kernel Hilbert Space (RKHS) (also known as the Cameron-Martin space) associated with the covariance function <span class="math inline">\(c(\cdot, \cdot)\)</span>. This is a space <span class="math inline">\(H(c)\)</span> of functions equipped with the unique inner product with the property that for any continuous function <span class="math inline">\(f\)</span>, <span class="math display">\[
\langle f(\cdot), c(s, \cdot) \rangle_{H(c)} = f(s).
\]</span> A function <span class="math inline">\(f \in H(c)\)</span> if and only if <span class="math inline">\(\langle f, f \rangle_{H(c)} &lt; \infty\)</span>.</p>
<p>While every covariance function has an associated RKHS (and vice versa), it is usually pretty difficult to derive one from the other. So what we typically do is either choose to work with the covariance function and use that as the basis of our computations or we choose to work with the RKHS and use that as the basis of our computations.</p>
<p>It turns out that just as the covariance function specifies the covariance structure of a GP, the RKHS controls the <em>conditional independence</em> structure of the Gaussian process. This makes it the natural object to consider when talking about Markovian processes.</p>
<p>But first, a brief word about computation.</p>
</section>
<section id="gaussian-processes-via-the-covariance-operator" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-processes-via-the-covariance-operator">Gaussian processes via the covariance operator</h2>
<p>The problem with basing our computations off a RKHS is that it is not immediately obvious how we will do that. This is in contrast to a covariance function approach, where it is quite easy<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> to work out how to convert the model specification to something you can attack with a computer.</p>
<p>The extra complexity of the RKHS pays off in modelling flexibility, both in terms of the types of model that can be build and the spaces<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> you can build them on. I am telling you this now because things are about to get a little mathematical.</p>
<p>To motivate the technique, let’s consider the covariance operator <span class="math display">\[
[\mathcal{C}f](s) = \int_T c(s, s') f(s') \, ds',
\]</span> where <span class="math inline">\(T\)</span> is the domain over which the GP is defined (usually <span class="math inline">\(\mathbb{R}^d\)</span> but maybe you’re feeling frisky). One of the defining properties of the RKHS inner product is that <span class="math display">\[
\langle \mathcal{C}^{1/2} f, \mathcal{C}^{1/2} g \rangle_{H(c)} = \int_T f(s') g(s')\,ds',
\]</span> where <span class="math inline">\(C^{1/2}\)</span> is the square root<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> of the covariance operator, which can be (mostly) defined as the symmetric operator that satisfies <span class="math display">\[
\langle \mathcal{C}^{1/2} f, \mathcal{C}^{1/2} g \rangle_{L^2(T)} = \langle f, \mathcal{C} g \rangle_{L^2(T)}  .
\]</span></p>
<p>To see how this could be useful, we are going to need to think a little bit about how we can simulate a multivariate Gaussian random variable <span class="math inline">\(N(0, \Sigma)\)</span>. To do this, we first compute the square root<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> <span class="math inline">\(L = \Sigma^{1/2}\)</span> and sample a vector of iid standard normal variables <span class="math inline">\(z \sim N(0,I)\)</span>. Then <span class="math inline">\(u = Lz \sim N(0, \Sigma)\)</span>. You can check it by checking the covariance. (it’s ok. I’ll wait.)</p>
<p>While the square root of the covariance operator is a fairly straightforward mathematical object^{Albeit a bit advanced. It’s straightforward in the sense that for an infinite-dimensional operaotr it happens to work a whole like a symmetric positive semi-definite matrix. It is not straightforward in the sense that your three year old could do it. Your three year old can’t do it. But it will keep them quiet in the back seat of the car while you pop into the store for some fags. It’s ok. The window’s down.}, the analogue of the iid vector of standard normal random variables is a bit more complex.</p>
</section>
<section id="white-noise-and-its-associated-things" class="level2">
<h2 class="anchored" data-anchor-id="white-noise-and-its-associated-things">White noise and its associated things</h2>
<p>Thankfully I’ve covered this <a href="file:///Users/danielsimpson/Documents/blog/_site/posts/2022-09-07-priors5/priors5.html#spectral-representations-and-the-simplest-of-the-many-many-versions-of-a-stochastic-integral">in a previous blog</a>. The engineering definition of white noise as a GP <span class="math inline">\(w(\cdot)\)</span> such that for every <span class="math inline">\(s\)</span>, <span class="math inline">\(w(s)\)</span> is an iid <span class="math inline">\(N(0,1)\)</span> random variable is not good enough for our purposes. Such a process is hauntingly irregular<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> and it’s fairly difficult to actually do anything wiht it. Instead, we consider white noise as a random function defined on the subsets of our domain. This feels like it’s just needless technicality, but it turns out to actually be very very useful.</p>
<div id="def-white-noise" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 (White noise) </strong></span>A (complex) Gaussian white noise is a random measure<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> <span class="math inline">\(W(\cdot)\)</span> such that, for every<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> disjoint<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> pair of sets <span class="math inline">\(A, B\)</span> satisfies the following properties</p>
<ol type="1">
<li><span class="math inline">\(W(A) \sim N(0, |A|)\)</span></li>
<li>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are disjoint then <span class="math inline">\(W(A\cup B) = W(A) + W(B)\)</span></li>
<li>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are disjoint then <span class="math inline">\(W(A)\)</span> and <span class="math inline">\(W(B)\)</span> are uncorrelated<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>, ie <span class="math inline">\(\mathbb{E}(W(A) \overline{W(B)}) = 0\)</span>.</li>
</ol>
</div>
<p>This doesn’t feel like we are helping very much because how on <em>earth</em> am I going to define the product <span class="math inline">\(\mathcal{C}^{1/2} W\)</span>? Well the answer, you may be shocked to discover, requires a little bit more maths. We need to define an integral, which turns out to not be <em>shockingly</em> difficult to do. The trick is to realise that if I have an indicator function <span class="math display">\[
1_A(s) = \begin{cases} 1, \qquad &amp;s \in A \\ 0, &amp; s \not \in A \end{cases}
\]</span> then<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> <span class="math display">\[
\int_T 1_A(s)\, dW(s) = \int_A dW(s) = W(A) \sim N(0, |A|).
\]</span> In that calculation, I just treated <span class="math inline">\(W(s)\)</span> like I would any other measure. (If you’re more of a probability type of girl, it’s the same thing as noticing <span class="math inline">\(\mathbb{E}(1_A(X) = \Pr(X \in A)\)</span>.)</p>
<p>We can extend the above by taking the sum of two indicator function <span class="math display">\[
f(s) = f_11_{A_1}(s) + f_2 1_{A_2}(s),
\]</span> where <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span> are disjoint and <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span> are any real numbers. By the same reasoning above, and using the linearity of the integral, we get that <span class="math display">\[\begin{align*}
\int_T f(s) \, dW(s) &amp;= f_1 \int_{A_1} \,d W(s) + f_2 \int_{A_2} \,d W(s) \\
&amp;= N(0, f_1^2 |A_1| + f_2^2 |A_2|) \\
&amp;= N\left(0, \int_T f(s)^2 \,ds\right),
\end{align*}\]</span> where the last line follows by doing the ordinary<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> integral of <span class="math inline">\(f(s)\)</span>.</p>
<p>It turns out that every interesting function can be written as the limit of piecewise constant functions<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> and we can therefore <em>define</em> for any function<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> <span class="math inline">\(f\in L^2(T)\)</span> <span class="math display">\[
\int f(s) \, dW(s) \sim N\left(0, \int_T f(s)^2 \,ds\right).
\]</span></p>
<p>With this notion in hand, we can finally define the action of an operator on white noise.</p>
<div id="def-operator-on-noise" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 (The action of an operator on white noise) </strong></span>Let <span class="math inline">\(\mathcal{A}\)</span> be an operator on some Hilbert space of functions <span class="math inline">\(H\)</span> with adjoint <span class="math inline">\(\mathcal{A}^*\)</span>, then we define <span class="math inline">\(\mathcal{A}W\)</span> to be the random measure that satisfies, for every <span class="math inline">\(f \in \operatorname{Dom}(\mathcal{A^*})\)</span>, <span class="math display">\[
\int_T f(s) \, d (\mathcal{A}W)(s) = \int_T \mathcal{A}^*f(s) \, dW(s).
\]</span></p>
</div>
</section>
<section id="the-generalised-gaussian-process-eta-mathcalc12w" class="level2">
<h2 class="anchored" data-anchor-id="the-generalised-gaussian-process-eta-mathcalc12w">The generalised Gaussian process <span class="math inline">\(\eta = \mathcal{C}^{1/2}W\)</span></h2>
<p>One of those inconvenient things that you may have noticed from above is that <span class="math inline">\(\mathcal{C}^{1/2}W\)</span> is <em>not</em> going to be a function. It is going to be a measure or, as it is more commonly known, a <em>generalised Gaussian process</em>. This is the GP analogue of a generalised function and, as such, only gives an actual value when you integrate it against some sufficiently smooth function.</p>
<div id="def-generalised-gp" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 (Generalised Gauusian Process) </strong></span>A generalised Gaussian process <span class="math inline">\(\xi\)</span> is a random signed measure (or a random generalised function) that, for any <span class="math inline">\(f \in C^\infty_0(T)\)</span>, <span class="math inline">\(\int_T f(s)\,d\xi(s)\)</span> is Gaussian. We will often write <span class="math display">\[
\xi(f) = \int_T f(s)\,d\xi(s),
\]</span> which helps us understand that a generalised GP is indexed by functions.</p>
</div>
<p>In order to separate this out from the ordinary GP <span class="math inline">\(u(s)\)</span>, we will write it as <span class="math display">\[
\eta = \mathcal{C}^{1/2}W.
\]</span> These two ideas coincide in the special case where <span class="math display">\[
\eta = u(s)\,ds,
\]</span> which will occur when <span class="math inline">\(\mathcal{C}^{1/2}\)</span> smooths the white noise sufficiently. In all of the cases we really care about today, this happens. But there are plenty of Gaussian processes that can only be considered as generalised GPs<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a></p>
<p>With this in hand, we are ready to understand the Markov property.</p>
</section>
<section id="the-markov-property-for-on-abstract-spaces" class="level2">
<h2 class="anchored" data-anchor-id="the-markov-property-for-on-abstract-spaces">The Markov property for on abstract spaces</h2>
<p>Part of the reason why I introduced the notion of a generalised Gaussian process is that it is useful in the definition of the Markov process. Intuitively, we know what this definition is going to be: if I split my space into three disjoint sets <span class="math inline">\(A\)</span>, <span class="math inline">\(\Gamma\)</span> and <span class="math inline">\(B\)</span> in such a way that you can’t get from <span class="math inline">\(A\)</span> to <span class="math inline">\(B\)</span> without passing through <span class="math inline">\(\Gamma\)</span>, then the Markov property should say, roughly, that every random variable <span class="math inline">\(\{x(s): s\in A\}\)</span> is conditionally independent of every random variable <span class="math inline">\(\{x(s): s \in B\}\)</span> <em>given</em> (or conditional on) knowing the values of the entire set ${x(s): s }.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="markov.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">A graphical illustration of the three sets used above Markov property.</figcaption><p></p>
</figure>
</div>
<p>That definition is all well and good for a hand-wavey approach, but unfortunately it doesn’t quite hold up to mathematics. In particular, if we try to make <span class="math inline">\(\Gamma\)</span> a line<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>, we will hit a few problems. So instead let’s do this properly.</p>
<p>All of the material here is covered in Rozanov’s excellent but unimaginatively named book <em>Markov Random Fields</em>.</p>
<p>To set us up. we should consider the types of sets we have. There are three main sets that we are going to be using: the open<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> set <span class="math inline">\(S_1 \subset T\)</span>, its boundary<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> <span class="math inline">\(\Gamma \supseteq \partial S\)</span>. For example, if <span class="math inline">\(T = \mathbb{R}^2\)</span> and <span class="math inline">\(S\)</span> is the interior of the unit circle, and its open complement <span class="math inline">\(S_2 = S_1^C \backslash \partial S_1\)</span>. For a 2D example, if <span class="math inline">\(S_1\)</span> is the <em>interior</em> of the unit circle, then <span class="math inline">\(\Gamma\)</span> could be the unit circle, and <span class="math inline">\(S_2\)</span> would be the _exterior of the unit circle.</p>
<p>One problem with these sets, is that while <span class="math inline">\(S_1\)</span> will be a 2D set, <span class="math inline">\(\Gamma\)</span> is only one dimensional (it’s a circle, so it’s a line!). This causes some troubles mathematically, which we need to get around by using the <span class="math inline">\(\epsilon\)</span> fattening of <span class="math inline">\(\Gamma\)</span>, which is the set <span class="math display">\[
\Gamma^\epsilon = \{s \in T : d(s, \Gamma) &lt; \epsilon\},
\]</span> where <span class="math inline">\(d(s, \Gamma)\)</span> is the distance from <span class="math inline">\(s\)</span> to the nearest point in <span class="math inline">\(\Gamma\)</span>.</p>
<p>With all of this in hand we can now give a general definition of the Markov property.</p>
<div id="def-markov" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4 (The Markov property for a generalised Gaussian process) </strong></span>Consider a zero mean generalised GP <span class="math inline">\(\xi\)</span>. For any<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> subset <span class="math inline">\(A \subset T\)</span>, we define the collection of random variables<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a> <span class="math display">\[
H(A) = \operatorname{span}\{\xi(f): \operatorname{supp}(f) \subseteq A\}.
\]</span> We will call <span class="math inline">\(\{H(A); A \subseteq T\}\)</span> the <em>random field</em><a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a> associated with <span class="math inline">\(\xi\)</span>.</p>
<p>Let <span class="math inline">\(\mathcal{G}\)</span> be a system of domains<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a> in <span class="math inline">\(T\)</span>. We say that <span class="math inline">\(\xi\)</span> has the Markov<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a> property (with respect to <span class="math inline">\(\mathcal{G}\)</span>) if, for all <span class="math inline">\(S_1 \in \mathcal{G}\)</span> and for any sufficiently small <span class="math inline">\(\epsilon &gt; 0\)</span>, <span class="math display">\[
\mathbb{E}(xy \mid H(\Gamma^\epsilon)) = 0, \qquad x \in H(S_1), y \in H(S_2),
\]</span> where <span class="math inline">\(\Gamma = \partial S_1\)</span> and <span class="math inline">\(S_2 = S_1^C \backslash \Gamma\)</span>.</p>
</div>
<section id="rewriting-the-markov-property-i-splitting-spaces" class="level3">
<h3 class="anchored" data-anchor-id="rewriting-the-markov-property-i-splitting-spaces">Rewriting the Markov property I: Splitting spaces</h3>
<p>The Markov property defined above is great and everything, but in order to manipulate it, we need to think carefully about the how the domains <span class="math inline">\(S_1\)</span>, <span class="math inline">\(\Gamma^\epsilon\)</span> and <span class="math inline">\(S_2\)</span> can be used to divide up the space <span class="math inline">\(H(T)\)</span>. To do this, we need to basically locallise the Markov property to one set of <span class="math inline">\(S_1\)</span>, <span class="math inline">\(\Gamma\)</span>, <span class="math inline">\(S_2\)</span>. This concept is called a <em>splitting</em><a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a> of <span class="math inline">\(H(S_1)\)</span> and <span class="math inline">\(H(S_2)\)</span> by <span class="math inline">\(H(\Gamma^\epsilon)\)</span></p>
<div id="def-splitting" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5 </strong></span>For some domain <span class="math inline">\(S_1\)</span> and <span class="math inline">\(\Gamma \supseteq \partial S_1\)</span>, set <span class="math inline">\(S_2 = (S_1 \cup \Gamma)^c\)</span>. The space <span class="math inline">\(H(\Gamma^\epsilon)\)</span> splits <span class="math inline">\(H(S_1)\)</span> and <span class="math inline">\(H(S_2)\)</span> if <span class="math display">\[
H(T) = H(S_1 \ominus \Gamma^\epsilon) \oplus H(\Gamma^\epsilon) \oplus H(S_2 \ominus \Gamma^\epsilon),
\]</span> where <span class="math inline">\(\oplus\)</span> is the sum of orthogonal components<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a> and <span class="math inline">\(x\in H(S \ominus \Gamma^\epsilon)\)</span> if and only if there is some <span class="math inline">\(y \in H(S)\)</span> such that<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a> <span class="math display">\[
x = y - \mathbb{E}(y \mid H(\Gamma^\epsilon)).
\]</span></p>
</div>
<p>This emphasizes that we can split our space into three separate components: inside <span class="math inline">\(S_1\)</span>, outside <span class="math inline">\(S_1\)</span> and on the boundary of <span class="math inline">\(S_1\)</span> and the ability to do that for any<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a> domain is the key part of the Markov<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a> property.</p>
<p>A slightly more convenient way to deal with spiting spaces is the case where the we have overlapping sets <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> that cover the domain (ie <span class="math inline">\(A \cup B = T\)</span>) and the splitting set is their intersection <span class="math inline">\(S = A \cap B\)</span>. In this case, the splitting equation becomes <span class="math display">\[
H(A)^\perp \perp H(B)^\perp.
\]</span> I shan’t lie: that looks wild. But it makes sense when you take <span class="math inline">\(A = S_1 \cup \Gamma^\epsilon\)</span> and <span class="math inline">\(B = S_2 \cup \Gamma^\epsilon\)</span>, in which case <span class="math inline">\(H(A)^\perp = H(S_2)\)</span> and <span class="math inline">\(H(B)^\perp = H(S_1)\)</span>.</p>
<p>Ths final thing to add before we can get to business is a way to get rid of all of the annoying <span class="math inline">\(\epsilon\)</span>s. The idea is to take the intersection of all of the <span class="math inline">\(H(\Gamma^\epsilon)\)</span> as the splitting space. If we define <span class="math display">\[
H_+(\Gamma) = \bigcap_{\epsilon&gt;0} H(\Gamma^\epsilon)
\]</span> we can re-write<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a> the splitting equation as <span class="math display">\[\begin{align*}
&amp;H_+(\Gamma) = H_+(S_1 \cup \Gamma) \cap H_+(S_1 \cup \Gamma) \\
&amp; H_+(S_1 \cup \Gamma)^\perp \perp H_+(S_2 \cup \Gamma)^\perp.
\end{align*}\]</span></p>
<p>This gives the following statement of the Markov property.</p>
<div id="def-markov2" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6 </strong></span>Let <span class="math inline">\(\mathcal{G}\)</span> be a system of domains<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a> in <span class="math inline">\(T\)</span>. We say that <span class="math inline">\(\xi\)</span> has the Markov property (with respect to <span class="math inline">\(\mathcal{G}\)</span>) if, for all <span class="math inline">\(S_1 \in \mathcal{G}\)</span>, <span class="math inline">\(\Gamma\supseteq \partial S_1\)</span> ,<span class="math inline">\(S_2 = S_1^C \backslash \Gamma\)</span>, we have, for some <span class="math inline">\(\epsilon &gt; 0\)</span> <span class="math display">\[
H_+(\Gamma^\epsilon) = H_+(S_1 \cup \Gamma^\epsilon) \cap H_+(S_1 \cup \Gamma^\epsilon)
\]</span> and <span class="math display">\[
H_+(S_1 \cup \Gamma)^\perp \perp H_+(S_2 \cup \Gamma)^\perp.
\]</span></p>
</div>
</section>
<section id="rewriting-the-markov-property-ii-the-dual-random-field-ha" class="level3">
<h3 class="anchored" data-anchor-id="rewriting-the-markov-property-ii-the-dual-random-field-ha">Rewriting the Markov property II: The dual random field <span class="math inline">\(H^*(A)\)</span></h3>
<p>We are going to fall further down the abstraction rabbit hole in the hope of ending up somewhere useful. In this case, we are going to invent an object that has no reason to exist and we will show that it can be used to compactly restate the Markov property. It will turn out in the next section that it is actually a useful characterization that will lead (finally) to an operational characterisation of a Markovian Gaussian process.</p>
<div id="def-dual-field" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7 (Dual random field) </strong></span>Let <span class="math inline">\(\xi\)</span> be a generalised Gaussian process with an associated random field <span class="math inline">\(H(A)\)</span>, <span class="math inline">\(A \subseteq T\)</span> and let <span class="math inline">\(\mathcal{G}\)</span> be a complete system of open domains in <span class="math inline">\(T\)</span>. The <em>dual</em> to the random field <span class="math inline">\(H(A)\)</span>, <span class="math inline">\(A \subseteq T\)</span> on the system <span class="math inline">\(\mathcal{G}\)</span> is the random field <span class="math inline">\(H^*(A)\)</span>, <span class="math inline">\(A \subseteq T\)</span> that satisfies <span class="math display">\[
H^*(T) = H(T)
\]</span> and <span class="math display">\[
H^*(A) = H_+(A^c)^\perp, \qquad A \in \mathcal{G}.
\]</span></p>
</div>
<p>This definition looks frankly a bit wild, but I promise you, we will use it.</p>
<p>The reason for its structure is that it directly relates to the Markov property. In particular, the existance of a dual field implies that, if we have any <span class="math inline">\(S_1 \in \mathcal{G}\)</span>, then <span class="math display">\[\begin{align*}
H_+(S_1 \cup \bar{\Gamma^\epsilon}) \cap H_+(S_1 \cup \bar{\Gamma^\epsilon}) &amp;= H^*((S_1 \cup \bar{\Gamma^\epsilon})^c)^\perp \cap H^*((S_2 \cup \bar{\Gamma^\epsilon})^c)^\perp \\
H^*((S_1 \cup \bar{\Gamma^\epsilon})^c \cup (S_2 \cup \bar{\Gamma^\epsilon})^c) \\
&amp;= H_+((S_1 \cup \bar{\Gamma^\epsilon}) \cap (S_2 \cup \bar{\Gamma^\epsilon})) \\
&amp;= H_+(\Gamma^\epsilon).
\end{align*}\]</span> That’s the first thing we need to show to demonstrate the Markov property.</p>
<p>The second part is much easier. If we note that <span class="math inline">\((S_2 \cup \Gamma)^c = S_1 \backslash \Gamma\)</span>, it follows that <span class="math display">\[
H_+(S_1 \cup \Gamma)^\perp = H^*(S_2 \backslash \Gamma).
\]</span></p>
<p>This gives us our third (and final) characterisation of the (second-order) Markov property.</p>
<div id="def-markov2" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8 </strong></span>Let <span class="math inline">\(\mathcal{G}\)</span> be a system of domains<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a> in <span class="math inline">\(T\)</span>. Assume that the random field <span class="math inline">\(H(\cdot)\)</span> has an associated dual random field <span class="math inline">\(H^*(\cdot)\)</span>.</p>
<p>We say that <span class="math inline">\(H(A)\)</span>, <span class="math inline">\(A \in \mathcal{G}\)</span> has the Markov property (with respect to<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a> <span class="math inline">\(\mathcal{G}\)</span>) if for all <span class="math inline">\(S_1 \in \mathcal{G}\)</span>, <span class="math display">\[
H^*(S_1 \backslash \Gamma) \perp H^*(S_2 \backslash \Gamma).
\]</span> When this holds, we say that the dual field is <em>orthogonal</em> with respect to <span class="math inline">\(\mathcal{G}\)</span>.</p>
</div>
<p>There is probably more to say about dual fields. For instance, the dual of the dual field is the original field. Neat, huh. But really, all we need to do is know that an orthogonal dual field implies a the Markov property. Because next we are going to construct a dual field, which will give us an actually useful characterisation of Markovian GPs.</p>
</section>
<section id="biorthogonal-gaussian-processes" class="level3">
<h3 class="anchored" data-anchor-id="biorthogonal-gaussian-processes">Biorthogonal Gaussian processes</h3>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Here there and everywhere my GPs always have zero means. Why? Is it because once someone said to me that statisticians are bad in bed because they are “mean lovers”? Well it’s not not because of that. I worry that I’m not a good statistician because I am merely an indifferent lover.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Step 1: Open Rasmussen and Williams.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>For example, the process I’m about to describe is not meaningfully different for a process on a sphere. Whereas if you want to use a covariance function on a sphere you are stuck trying to find a whole new class of positive definite functions. It’s frankly very annoying. Although if you want to build a career out of characterising positive definite functions on increasingly exotic spaces, you probably don’t find it annoying.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Because <span class="math inline">\(\mathcal{C}\)</span> is a compact operator (and some other maths-y stuff), it has a discrete spectrum that accumulates at zero. That means, basically, that we can just pretend it’s a big fucking symmetric postitive semi-definite matrix and so its square root is well definied (just take the square root of the eigenvalues).<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Or the Cholesky factor if you add a bunch of transposes in the right places, but let’s not kid ourselves this is not a practical discussion of how to do it<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>For any subset <span class="math inline">\(B\)</span>, <span class="math inline">\(\sup_{s\in B} w(s) = \infty\)</span> <em>and</em> <span class="math inline">\(\inf_{s \in B} w(s) = -\infty\)</span><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Countably additive set-valued function taking any value in <span class="math inline">\(\mathbb{C}\)</span><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>measurable<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p><span class="math inline">\(A \cap B = \emptyset\)</span><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>If <span class="math inline">\(W(A)\)</span> is also Gaussian then this is the same as them being independent<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>Recall that <span class="math inline">\(T\)</span> is our whole space. Usually <span class="math inline">\(\mathbb{R}^d\)</span>, but it doesn’t matter here.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>A bit of a let down really.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>like <span class="math inline">\(f(s)\)</span> but with more subsets<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p><span class="math inline">\(L^2(T)\)</span> is the space of functions with the property that <span class="math inline">\(\int_T f(s)^2\,ds &lt; \infty\)</span>.<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>eg the Gaussian free field in physics, or the de Wijs process.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>(<span class="math inline">\(d-1\)</span>)-dimensional submanifold<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>This set does not include its boundary<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p>This is defined as the set <span class="math inline">\(\partial S_1 = \bar{S_1} \backslash S_1\)</span>, where <span class="math inline">\(\bar{S_1}\)</span> is the closure of <span class="math inline">\(S_1\)</span>. But let’s face it. It’s the fucking boundary. It means what you think it means.<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p>measurable<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p>Here <span class="math inline">\(\operatorname{supp}(f)\)</span> is the support of <span class="math inline">\(f\)</span>, that is the values of <span class="math inline">\(s\)</span> such that <span class="math inline">\(f(s) \neq 0\)</span>.<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p>This is the terminology of Rozanov. Random Field is also another term for stochastic process. Why only let words mean one thing?<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22"><p>non-empty connected open sets<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23"><p>Strictly, this is the <em>weak</em> or <em>second-order</em> Markov property<a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24"><p>If you’re curious, this is basically the same thing as a splitting <span class="math inline">\(\sigma\)</span>-algebra. But, you know, sans the <span class="math inline">\(\sigma\)</span>-algebra bullshit.<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25"><p>That is, any <span class="math inline">\(x \in H(T)\)</span> can be written as the sum <span class="math inline">\(x = x_1 + x_2 + x_3\)</span>, where $x_1 H(S_1 ^) $, <span class="math inline">\(x_2 \in H(\Gamma^\epsilon)\)</span>, and <span class="math inline">\(x_3 \in H(S_2 \ominus \Gamma^\epsilon)\)</span> are <em>mutually orthogonal</em> (ie <span class="math inline">\(\mathbb{E}(x_1x_2) = \mathbb{E}(x_1x_3) = \mathbb{E}(x_2x_3) =0\)</span>!).<a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26"><p>This is using the idea that the conditional expectation is a projection.<a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27"><p>Typically any open set, or any open connected set, or any open, bounded set. A subtlety that I don’t really want to dwell on is that it is possible to have a GP that is Markov with respect to one system of domains but not another.<a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn28"><p>The Markov property can be restated in this language as for every system of complementary domains and boundary <span class="math inline">\(S_1\)</span>, <span class="math inline">\(\Gamma\)</span>, <span class="math inline">\(S_2\)</span>, there exists a small enough <span class="math inline">\(\epislon &gt; 0\)</span> such that <span class="math inline">\(\Gamma^\epsilon\)</span> splits <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span><a href="#fnref28" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn29"><p>Technically we are assuming that for small enough <span class="math inline">\(\epsilon\)</span> <span class="math inline">\(H(\Gamma^\epsilon) = \operatorname{span}\left(H(\Gamma^\epsilon \cap S_1) \cup H_+(\Gamma) \cup H(\Gamma^\epsilon \cap S_2)\right)\)</span>. This is not a particularly onerous assumption.<a href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn30"><p>non-empty connected open sets<a href="#fnref30" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn31"><p>non-empty connected open sets<a href="#fnref31" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn32"><p>The result works with some subsystem <span class="math inline">\(\mathcal{G_0}\)</span>. To prove it for <span class="math inline">\(\mathcal{G}\)</span> it’s enough to prove it for some subset <span class="math inline">\(\mathcal{G}_0\)</span> that separates points of <span class="math inline">\(T\)</span>. This is a wildly technical aside and if it makes no sense to you, that’s very much ok. Frankly I’m impressed you’ve hung in this long.<a href="#fnref32" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{simpson2023,
  author = {Dan Simpson},
  editor = {},
  title = {Markovian {Gaussian} Processes: {A} Lot of Theory and Some
    Practical Stuff},
  date = {2023-01-10},
  url = {https://dansblog.netlify.app/posts/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-simpson2023" class="csl-entry quarto-appendix-citeas" role="doc-biblioentry">
Dan Simpson. 2023. <span>“Markovian Gaussian Processes: A Lot of Theory
and Some Practical Stuff.”</span> January 10, 2023. <a href="https://dansblog.netlify.app/posts/">https://dansblog.netlify.app/posts/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>