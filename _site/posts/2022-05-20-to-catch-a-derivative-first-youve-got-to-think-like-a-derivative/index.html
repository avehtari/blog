<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


  <!--radix_placeholder_meta_tags-->
<title>Un garçon pas comme les autres (Bayes): Sparse matrices part 6: To catch a derivative, first you've got to think like a derivative</title>

<meta property="description" itemprop="description" content="Open up the kennels, Kenneth. Mamma’s coming home tonight."/>

<link rel="canonical" href="https://dansblog.netlify.app/posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/"/>
<link rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/"/>

<!--  https://schema.org/Article -->
<meta property="article:published" itemprop="datePublished" content="2022-05-30"/>
<meta property="article:created" itemprop="dateCreated" content="2022-05-30"/>
<meta name="article:author" content="Dan Simpson"/>

<!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
<meta property="og:title" content="Un garçon pas comme les autres (Bayes): Sparse matrices part 6: To catch a derivative, first you&#39;ve got to think like a derivative"/>
<meta property="og:type" content="article"/>
<meta property="og:description" content="Open up the kennels, Kenneth. Mamma’s coming home tonight."/>
<meta property="og:url" content="https://dansblog.netlify.app/posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:site_name" content="Un garçon pas comme les autres (Bayes)"/>

<!--  https://dev.twitter.com/cards/types/summary -->
<meta property="twitter:card" content="summary"/>
<meta property="twitter:title" content="Un garçon pas comme les autres (Bayes): Sparse matrices part 6: To catch a derivative, first you&#39;ve got to think like a derivative"/>
<meta property="twitter:description" content="Open up the kennels, Kenneth. Mamma’s coming home tonight."/>
<meta property="twitter:url" content="https://dansblog.netlify.app/posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/"/>
<meta property="twitter:creator" content="@dan_p_simpson"/>

<!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
<meta name="citation_title" content="Un garçon pas comme les autres (Bayes): Sparse matrices part 6: To catch a derivative, first you&#39;ve got to think like a derivative"/>
<meta name="citation_fulltext_html_url" content="https://dansblog.netlify.app/posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/"/>
<meta name="citation_fulltext_world_readable" content=""/>
<meta name="citation_online_date" content="2022/05/30"/>
<meta name="citation_publication_date" content="2022/05/30"/>
<meta name="citation_author" content="Dan Simpson"/>
<!--/radix_placeholder_meta_tags-->
  <!--radix_placeholder_rmarkdown_metadata-->

<script type="text/json" id="radix-rmarkdown-metadata">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","output","twitter","creative_commons","repository_url","draft","citation_url","canonical_url"]}},"value":[{"type":"character","attributes":{},"value":["Sparse matrices part 6: To catch a derivative, first you've got to think like a derivative"]},{"type":"character","attributes":{},"value":["Open up the kennels, Kenneth. Mamma’s coming home tonight."]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url"]}},"value":[{"type":"character","attributes":{},"value":["Dan Simpson"]},{"type":"character","attributes":{},"value":["https://dpsimpson.github.io"]}]}]},{"type":"character","attributes":{},"value":["2022-05-30"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained","highlight","pandoc_args"]}},"value":[{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["pygments"]},{"type":"character","attributes":{},"value":["--lua-filter","/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/rmdfiltr/wordcount.lua"]}]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["creator"]}},"value":[{"type":"character","attributes":{},"value":["@dan_p_simpson"]}]},{"type":"character","attributes":{},"value":["CC BY-NC"]},{"type":"character","attributes":{},"value":["https://github.com/dpsimpson/blog/tree/master/_posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative"]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["https://dansblog.netlify.app/posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/"]},{"type":"character","attributes":{},"value":["https://dansblog.netlify.app/posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/"]}]}
</script>
<!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["python/linalg.py","python/scratch.py","to-catch-a-derivative-first-youve-got-to-think-like-a-derivative_files/anchor-4.2.2/anchor.min.js","to-catch-a-derivative-first-youve-got-to-think-like-a-derivative_files/bowser-1.9.3/bowser.min.js","to-catch-a-derivative-first-youve-got-to-think-like-a-derivative_files/distill-2.2.21/template.v2.js","to-catch-a-derivative-first-youve-got-to-think-like-a-derivative_files/header-attrs-2.11/header-attrs.js","to-catch-a-derivative-first-youve-got-to-think-like-a-derivative_files/jquery-3.6.0/jquery-3.6.0.js","to-catch-a-derivative-first-youve-got-to-think-like-a-derivative_files/jquery-3.6.0/jquery-3.6.0.min.js","to-catch-a-derivative-first-youve-got-to-think-like-a-derivative_files/jquery-3.6.0/jquery-3.6.0.min.map","to-catch-a-derivative-first-youve-got-to-think-like-a-derivative_files/popper-2.6.0/popper.min.js","to-catch-a-derivative-first-youve-got-to-think-like-a-derivative_files/tippy-6.2.7/tippy-bundle.umd.min.js","to-catch-a-derivative-first-youve-got-to-think-like-a-derivative_files/tippy-6.2.7/tippy-light-border.css","to-catch-a-derivative-first-youve-got-to-think-like-a-derivative_files/tippy-6.2.7/tippy.css","to-catch-a-derivative-first-youve-got-to-think-like-a-derivative_files/tippy-6.2.7/tippy.umd.min.js","to-catch-a-derivative-first-youve-got-to-think-like-a-derivative_files/webcomponents-2.0.0/webcomponents.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
<meta name="distill:offset" content="../.."/>

<script type="application/javascript">

  window.headroom_prevent_pin = false;

  window.document.addEventListener("DOMContentLoaded", function (event) {

    // initialize headroom for banner
    var header = $('header').get(0);
    var headerHeight = header.offsetHeight;
    var headroom = new Headroom(header, {
      tolerance: 5,
      onPin : function() {
        if (window.headroom_prevent_pin) {
          window.headroom_prevent_pin = false;
          headroom.unpin();
        }
      }
    });
    headroom.init();
    if(window.location.hash)
      headroom.unpin();
    $(header).addClass('headroom--transition');

    // offset scroll location for banner on hash change
    // (see: https://github.com/WickyNilliams/headroom.js/issues/38)
    window.addEventListener("hashchange", function(event) {
      window.scrollTo(0, window.pageYOffset - (headerHeight + 25));
    });

    // responsive menu
    $('.distill-site-header').each(function(i, val) {
      var topnav = $(this);
      var toggle = topnav.find('.nav-toggle');
      toggle.on('click', function() {
        topnav.toggleClass('responsive');
      });
    });

    // nav dropdowns
    $('.nav-dropbtn').click(function(e) {
      $(this).next('.nav-dropdown-content').toggleClass('nav-dropdown-active');
      $(this).parent().siblings('.nav-dropdown')
         .children('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $("body").click(function(e){
      $('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $(".nav-dropdown").click(function(e){
      e.stopPropagation();
    });
  });
</script>

<style type="text/css">

/* Theme (user-documented overrideables for nav appearance) */

.distill-site-nav {
  color: rgba(255, 255, 255, 0.8);
  background-color: #0F2E3D;
  font-size: 15px;
  font-weight: 300;
}

.distill-site-nav a {
  color: inherit;
  text-decoration: none;
}

.distill-site-nav a:hover {
  color: white;
}

@media print {
  .distill-site-nav {
    display: none;
  }
}

.distill-site-header {

}

.distill-site-footer {

}


/* Site Header */

.distill-site-header {
  width: 100%;
  box-sizing: border-box;
  z-index: 3;
}

.distill-site-header .nav-left {
  display: inline-block;
  margin-left: 8px;
}

@media screen and (max-width: 768px) {
  .distill-site-header .nav-left {
    margin-left: 0;
  }
}


.distill-site-header .nav-right {
  float: right;
  margin-right: 8px;
}

.distill-site-header a,
.distill-site-header .title {
  display: inline-block;
  text-align: center;
  padding: 14px 10px 14px 10px;
}

.distill-site-header .title {
  font-size: 18px;
  min-width: 150px;
}

.distill-site-header .logo {
  padding: 0;
}

.distill-site-header .logo img {
  display: none;
  max-height: 20px;
  width: auto;
  margin-bottom: -4px;
}

.distill-site-header .nav-image img {
  max-height: 18px;
  width: auto;
  display: inline-block;
  margin-bottom: -3px;
}



@media screen and (min-width: 1000px) {
  .distill-site-header .logo img {
    display: inline-block;
  }
  .distill-site-header .nav-left {
    margin-left: 20px;
  }
  .distill-site-header .nav-right {
    margin-right: 20px;
  }
  .distill-site-header .title {
    padding-left: 12px;
  }
}


.distill-site-header .nav-toggle {
  display: none;
}

.nav-dropdown {
  display: inline-block;
  position: relative;
}

.nav-dropdown .nav-dropbtn {
  border: none;
  outline: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 16px 10px;
  background-color: transparent;
  font-family: inherit;
  font-size: inherit;
  font-weight: inherit;
  margin: 0;
  margin-top: 1px;
  z-index: 2;
}

.nav-dropdown-content {
  display: none;
  position: absolute;
  background-color: white;
  min-width: 200px;
  border: 1px solid rgba(0,0,0,0.15);
  border-radius: 4px;
  box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.1);
  z-index: 1;
  margin-top: 2px;
  white-space: nowrap;
  padding-top: 4px;
  padding-bottom: 4px;
}

.nav-dropdown-content hr {
  margin-top: 4px;
  margin-bottom: 4px;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.nav-dropdown-active {
  display: block;
}

.nav-dropdown-content a, .nav-dropdown-content .nav-dropdown-header {
  color: black;
  padding: 6px 24px;
  text-decoration: none;
  display: block;
  text-align: left;
}

.nav-dropdown-content .nav-dropdown-header {
  display: block;
  padding: 5px 24px;
  padding-bottom: 0;
  text-transform: uppercase;
  font-size: 14px;
  color: #999999;
  white-space: nowrap;
}

.nav-dropdown:hover .nav-dropbtn {
  color: white;
}

.nav-dropdown-content a:hover {
  background-color: #ddd;
  color: black;
}

.nav-right .nav-dropdown-content {
  margin-left: -45%;
  right: 0;
}

@media screen and (max-width: 768px) {
  .distill-site-header a, .distill-site-header .nav-dropdown  {display: none;}
  .distill-site-header a.nav-toggle {
    float: right;
    display: block;
  }
  .distill-site-header .title {
    margin-left: 0;
  }
  .distill-site-header .nav-right {
    margin-right: 0;
  }
  .distill-site-header {
    overflow: hidden;
  }
  .nav-right .nav-dropdown-content {
    margin-left: 0;
  }
}


@media screen and (max-width: 768px) {
  .distill-site-header.responsive {position: relative; min-height: 500px; }
  .distill-site-header.responsive a.nav-toggle {
    position: absolute;
    right: 0;
    top: 0;
  }
  .distill-site-header.responsive a,
  .distill-site-header.responsive .nav-dropdown {
    display: block;
    text-align: left;
  }
  .distill-site-header.responsive .nav-left,
  .distill-site-header.responsive .nav-right {
    width: 100%;
  }
  .distill-site-header.responsive .nav-dropdown {float: none;}
  .distill-site-header.responsive .nav-dropdown-content {position: relative;}
  .distill-site-header.responsive .nav-dropdown .nav-dropbtn {
    display: block;
    width: 100%;
    text-align: left;
  }
}

/* Site Footer */

.distill-site-footer {
  width: 100%;
  overflow: hidden;
  box-sizing: border-box;
  z-index: 3;
  margin-top: 30px;
  padding-top: 30px;
  padding-bottom: 30px;
  text-align: center;
}

/* Headroom */

d-title {
  padding-top: 6rem;
}

@media print {
  d-title {
    padding-top: 4rem;
  }
}

.headroom {
  z-index: 1000;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
}

.headroom--transition {
  transition: all .4s ease-in-out;
}

.headroom--unpinned {
  top: -100px;
}

.headroom--pinned {
  top: 0;
}

/* adjust viewport for navbar height */
/* helps vertically center bootstrap (non-distill) content */
.min-vh-100 {
  min-height: calc(100vh - 100px) !important;
}

</style>

<script src="../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="../../site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet"/>
<link href="../../site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet"/>
<script src="../../site_libs/headroom-0.9.4/headroom.min.js"></script>
<script src="../../site_libs/autocomplete-0.37.1/autocomplete.min.js"></script>
<script src="../../site_libs/fuse-6.4.1/fuse.min.js"></script>

<script type="application/javascript">

function getMeta(metaName) {
  var metas = document.getElementsByTagName('meta');
  for (let i = 0; i < metas.length; i++) {
    if (metas[i].getAttribute('name') === metaName) {
      return metas[i].getAttribute('content');
    }
  }
  return '';
}

function offsetURL(url) {
  var offset = getMeta('distill:offset');
  return offset ? offset + '/' + url : url;
}

function createFuseIndex() {

  // create fuse index
  var options = {
    keys: [
      { name: 'title', weight: 20 },
      { name: 'categories', weight: 15 },
      { name: 'description', weight: 10 },
      { name: 'contents', weight: 5 },
    ],
    ignoreLocation: true,
    threshold: 0
  };
  var fuse = new window.Fuse([], options);

  // fetch the main search.json
  return fetch(offsetURL('search.json'))
    .then(function(response) {
      if (response.status == 200) {
        return response.json().then(function(json) {
          // index main articles
          json.articles.forEach(function(article) {
            fuse.add(article);
          });
          // download collections and index their articles
          return Promise.all(json.collections.map(function(collection) {
            return fetch(offsetURL(collection)).then(function(response) {
              if (response.status === 200) {
                return response.json().then(function(articles) {
                  articles.forEach(function(article) {
                    fuse.add(article);
                  });
                })
              } else {
                return Promise.reject(
                  new Error('Unexpected status from search index request: ' +
                            response.status)
                );
              }
            });
          })).then(function() {
            return fuse;
          });
        });

      } else {
        return Promise.reject(
          new Error('Unexpected status from search index request: ' +
                      response.status)
        );
      }
    });
}

window.document.addEventListener("DOMContentLoaded", function (event) {

  // get search element (bail if we don't have one)
  var searchEl = window.document.getElementById('distill-search');
  if (!searchEl)
    return;

  createFuseIndex()
    .then(function(fuse) {

      // make search box visible
      searchEl.classList.remove('hidden');

      // initialize autocomplete
      var options = {
        autoselect: true,
        hint: false,
        minLength: 2,
      };
      window.autocomplete(searchEl, options, [{
        source: function(query, callback) {
          const searchOptions = {
            isCaseSensitive: false,
            shouldSort: true,
            minMatchCharLength: 2,
            limit: 10,
          };
          var results = fuse.search(query, searchOptions);
          callback(results
            .map(function(result) { return result.item; })
          );
        },
        templates: {
          suggestion: function(suggestion) {
            var img = suggestion.preview && Object.keys(suggestion.preview).length > 0
              ? `<img src="${offsetURL(suggestion.preview)}"</img>`
              : '';
            var html = `
              <div class="search-item">
                <h3>${suggestion.title}</h3>
                <div class="search-item-description">
                  ${suggestion.description || ''}
                </div>
                <div class="search-item-preview">
                  ${img}
                </div>
              </div>
            `;
            return html;
          }
        }
      }]).on('autocomplete:selected', function(event, suggestion) {
        window.location.href = offsetURL(suggestion.path);
      });
      // remove inline display style on autocompleter (we want to
      // manage responsive display via css)
      $('.algolia-autocomplete').css("display", "");
    })
    .catch(function(error) {
      console.log(error);
    });

});

</script>

<style type="text/css">

.nav-search {
  font-size: x-small;
}

/* Algolioa Autocomplete */

.algolia-autocomplete {
  display: inline-block;
  margin-left: 10px;
  vertical-align: sub;
  background-color: white;
  color: black;
  padding: 6px;
  padding-top: 8px;
  padding-bottom: 0;
  border-radius: 6px;
  border: 1px #0F2E3D solid;
  width: 180px;
}


@media screen and (max-width: 768px) {
  .distill-site-nav .algolia-autocomplete {
    display: none;
    visibility: hidden;
  }
  .distill-site-nav.responsive .algolia-autocomplete {
    display: inline-block;
    visibility: visible;
  }
  .distill-site-nav.responsive .algolia-autocomplete .aa-dropdown-menu {
    margin-left: 0;
    width: 400px;
    max-height: 400px;
  }
}

.algolia-autocomplete .aa-input, .algolia-autocomplete .aa-hint {
  width: 90%;
  outline: none;
  border: none;
}

.algolia-autocomplete .aa-hint {
  color: #999;
}
.algolia-autocomplete .aa-dropdown-menu {
  width: 550px;
  max-height: 70vh;
  overflow-x: visible;
  overflow-y: scroll;
  padding: 5px;
  margin-top: 3px;
  margin-left: -150px;
  background-color: #fff;
  border-radius: 5px;
  border: 1px solid #999;
  border-top: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion {
  cursor: pointer;
  padding: 5px 4px;
  border-bottom: 1px solid #eee;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion:last-of-type {
  border-bottom: none;
  margin-bottom: 2px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item {
  overflow: hidden;
  font-size: 0.8em;
  line-height: 1.4em;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item h3 {
  font-size: 1rem;
  margin-block-start: 0;
  margin-block-end: 5px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-description {
  display: inline-block;
  overflow: hidden;
  height: 2.8em;
  width: 80%;
  margin-right: 4%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview {
  display: inline-block;
  width: 15%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img {
  height: 3em;
  width: auto;
  display: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img[src] {
  display: initial;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion.aa-cursor {
  background-color: #eee;
}
.algolia-autocomplete .aa-dropdown-menu .aa-suggestion em {
  font-weight: bold;
  font-style: normal;
}

</style>


<!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

<style type="text/css">

body {
  background-color: white;
}

.pandoc-table {
  width: 100%;
}

.pandoc-table>caption {
  margin-bottom: 10px;
}

.pandoc-table th:not([align]) {
  text-align: left;
}

.pagedtable-footer {
  font-size: 15px;
}

d-byline .byline {
  grid-template-columns: 2fr 2fr;
}

d-byline .byline h3 {
  margin-block-start: 1.5em;
}

d-byline .byline .authors-affiliations h3 {
  margin-block-start: 0.5em;
}

.authors-affiliations .orcid-id {
  width: 16px;
  height:16px;
  margin-left: 4px;
  margin-right: 4px;
  vertical-align: middle;
  padding-bottom: 2px;
}

d-title .dt-tags {
  margin-top: 1em;
  grid-column: text;
}

.dt-tags .dt-tag {
  text-decoration: none;
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0em 0.4em;
  margin-right: 0.5em;
  margin-bottom: 0.4em;
  font-size: 70%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

d-article table.gt_table td,
d-article table.gt_table th {
  border-bottom: none;
}

.html-widget {
  margin-bottom: 2.0em;
}

.l-screen-inset {
  padding-right: 16px;
}

.l-screen .caption {
  margin-left: 10px;
}

.shaded {
  background: rgb(247, 247, 247);
  padding-top: 20px;
  padding-bottom: 20px;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .html-widget {
  margin-bottom: 0;
  border: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .shaded-content {
  background: white;
}

.text-output {
  margin-top: 0;
  line-height: 1.5em;
}

.hidden {
  display: none !important;
}

d-article {
  padding-top: 2.5rem;
  padding-bottom: 30px;
}

d-appendix {
  padding-top: 30px;
}

d-article>p>img {
  width: 100%;
}

d-article h2 {
  margin: 1rem 0 1.5rem 0;
}

d-article h3 {
  margin-top: 1.5rem;
}

d-article iframe {
  border: 1px solid rgba(0, 0, 0, 0.1);
  margin-bottom: 2.0em;
  width: 100%;
}

/* Tweak code blocks */

d-article div.sourceCode code,
d-article pre code {
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
}

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: auto;
}

d-article div.sourceCode {
  background-color: white;
}

d-article div.sourceCode pre {
  padding-left: 10px;
  font-size: 12px;
  border-left: 2px solid rgba(0,0,0,0.1);
}

d-article pre {
  font-size: 12px;
  color: black;
  background: none;
  margin-top: 0;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

d-article pre a {
  border-bottom: none;
}

d-article pre a:hover {
  border-bottom: none;
  text-decoration: underline;
}

d-article details {
  grid-column: text;
  margin-bottom: 0.8em;
}

@media(min-width: 768px) {

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: visible !important;
}

d-article div.sourceCode pre {
  padding-left: 18px;
  font-size: 14px;
}

d-article pre {
  font-size: 14px;
}

}

figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

/* CSS for d-contents */

.d-contents {
  grid-column: text;
  color: rgba(0,0,0,0.8);
  font-size: 0.9em;
  padding-bottom: 1em;
  margin-bottom: 1em;
  padding-bottom: 0.5em;
  margin-bottom: 1em;
  padding-left: 0.25em;
  justify-self: start;
}

@media(min-width: 1000px) {
  .d-contents.d-contents-float {
    height: 0;
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: center;
    padding-right: 3em;
    padding-left: 2em;
  }
}

.d-contents nav h3 {
  font-size: 18px;
  margin-top: 0;
  margin-bottom: 1em;
}

.d-contents li {
  list-style-type: none
}

.d-contents nav > ul {
  padding-left: 0;
}

.d-contents ul {
  padding-left: 1em
}

.d-contents nav ul li {
  margin-top: 0.6em;
  margin-bottom: 0.2em;
}

.d-contents nav a {
  font-size: 13px;
  border-bottom: none;
  text-decoration: none
  color: rgba(0, 0, 0, 0.8);
}

.d-contents nav a:hover {
  text-decoration: underline solid rgba(0, 0, 0, 0.6)
}

.d-contents nav > ul > li > a {
  font-weight: 600;
}

.d-contents nav > ul > li > ul {
  font-weight: inherit;
}

.d-contents nav > ul > li > ul > li {
  margin-top: 0.2em;
}


.d-contents nav ul {
  margin-top: 0;
  margin-bottom: 0.25em;
}

.d-article-with-toc h2:nth-child(2) {
  margin-top: 0;
}


/* Figure */

.figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

.figure img {
  width: 100%;
}

.figure .caption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

.figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

.figure .caption a {
  color: rgba(0, 0, 0, 0.6);
}

.figure .caption b,
.figure .caption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

/* Citations */

d-article .citation {
  color: inherit;
  cursor: inherit;
}

div.hanging-indent{
  margin-left: 1em; text-indent: -1em;
}

/* Citation hover box */

.tippy-box[data-theme~=light-border] {
  background-color: rgba(250, 250, 250, 0.95);
}

.tippy-content > p {
  margin-bottom: 0;
  padding: 2px;
}


/* Tweak 1000px media break to show more text */

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }

  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
  figure .caption, .figure .caption, figure figcaption {
    font-size: 13px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}


/* Get the citation styles for the appendix (not auto-injected on render since
   we do our own rendering of the citation appendix) */

d-appendix .citation-appendix,
.d-appendix .citation-appendix {
  font-size: 11px;
  line-height: 15px;
  border-left: 1px solid rgba(0, 0, 0, 0.1);
  padding-left: 18px;
  border: 1px solid rgba(0,0,0,0.1);
  background: rgba(0, 0, 0, 0.02);
  padding: 10px 18px;
  border-radius: 3px;
  color: rgba(150, 150, 150, 1);
  overflow: hidden;
  margin-top: -12px;
  white-space: pre-wrap;
  word-wrap: break-word;
}

/* Include appendix styles here so they can be overridden */

d-appendix {
  contain: layout style;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-top: 60px;
  margin-bottom: 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  color: rgba(0,0,0,0.5);
  padding-top: 60px;
  padding-bottom: 48px;
}

d-appendix h3 {
  grid-column: page-start / text-start;
  font-size: 15px;
  font-weight: 500;
  margin-top: 1em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.65);
}

d-appendix h3 + * {
  margin-top: 1em;
}

d-appendix ol {
  padding: 0 0 0 15px;
}

@media (min-width: 768px) {
  d-appendix ol {
    padding: 0 0 0 30px;
    margin-left: -30px;
  }
}

d-appendix li {
  margin-bottom: 1em;
}

d-appendix a {
  color: rgba(0, 0, 0, 0.6);
}

d-appendix > * {
  grid-column: text;
}

d-appendix > d-footnote-list,
d-appendix > d-citation-list,
d-appendix > distill-appendix {
  grid-column: screen;
}

/* Include footnote styles here so they can be overridden */

d-footnote-list {
  contain: layout style;
}

d-footnote-list > * {
  grid-column: text;
}

d-footnote-list a.footnote-backlink {
  color: rgba(0,0,0,0.3);
  padding-left: 0.5em;
}



/* Anchor.js */

.anchorjs-link {
  /*transition: all .25s linear; */
  text-decoration: none;
  border-bottom: none;
}
*:hover > .anchorjs-link {
  margin-left: -1.125em !important;
  text-decoration: none;
  border-bottom: none;
}

/* Social footer */

.social_footer {
  margin-top: 30px;
  margin-bottom: 0;
  color: rgba(0,0,0,0.67);
}

.disqus-comments {
  margin-right: 30px;
}

.disqus-comment-count {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  cursor: pointer;
}

#disqus_thread {
  margin-top: 30px;
}

.article-sharing a {
  border-bottom: none;
  margin-right: 8px;
}

.article-sharing a:hover {
  border-bottom: none;
}

.sidebar-section.subscribe {
  font-size: 12px;
  line-height: 1.6em;
}

.subscribe p {
  margin-bottom: 0.5em;
}


.article-footer .subscribe {
  font-size: 15px;
  margin-top: 45px;
}


.sidebar-section.custom {
  font-size: 12px;
  line-height: 1.6em;
}

.custom p {
  margin-bottom: 0.5em;
}

/* Styles for listing layout (hide title) */
.layout-listing d-title, .layout-listing .d-title {
  display: none;
}

/* Styles for posts lists (not auto-injected) */


.posts-with-sidebar {
  padding-left: 45px;
  padding-right: 45px;
}

.posts-list .description h2,
.posts-list .description p {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
}

.posts-list .description h2 {
  font-weight: 700;
  border-bottom: none;
  padding-bottom: 0;
}

.posts-list h2.post-tag {
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  padding-bottom: 12px;
}
.posts-list {
  margin-top: 60px;
  margin-bottom: 24px;
}

.posts-list .post-preview {
  text-decoration: none;
  overflow: hidden;
  display: block;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  padding: 24px 0;
}

.post-preview-last {
  border-bottom: none !important;
}

.posts-list .posts-list-caption {
  grid-column: screen;
  font-weight: 400;
}

.posts-list .post-preview h2 {
  margin: 0 0 6px 0;
  line-height: 1.2em;
  font-style: normal;
  font-size: 24px;
}

.posts-list .post-preview p {
  margin: 0 0 12px 0;
  line-height: 1.4em;
  font-size: 16px;
}

.posts-list .post-preview .thumbnail {
  box-sizing: border-box;
  margin-bottom: 24px;
  position: relative;
  max-width: 500px;
}
.posts-list .post-preview img {
  width: 100%;
  display: block;
}

.posts-list .metadata {
  font-size: 12px;
  line-height: 1.4em;
  margin-bottom: 18px;
}

.posts-list .metadata > * {
  display: inline-block;
}

.posts-list .metadata .publishedDate {
  margin-right: 2em;
}

.posts-list .metadata .dt-authors {
  display: block;
  margin-top: 0.3em;
  margin-right: 2em;
}

.posts-list .dt-tags {
  display: block;
  line-height: 1em;
}

.posts-list .dt-tags .dt-tag {
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0.3em 0.4em;
  margin-right: 0.2em;
  margin-bottom: 0.4em;
  font-size: 60%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

.posts-list img {
  opacity: 1;
}

.posts-list img[data-src] {
  opacity: 0;
}

.posts-more {
  clear: both;
}


.posts-sidebar {
  font-size: 16px;
}

.posts-sidebar h3 {
  font-size: 16px;
  margin-top: 0;
  margin-bottom: 0.5em;
  font-weight: 400;
  text-transform: uppercase;
}

.sidebar-section {
  margin-bottom: 30px;
}

.categories ul {
  list-style-type: none;
  margin: 0;
  padding: 0;
}

.categories li {
  color: rgba(0, 0, 0, 0.8);
  margin-bottom: 0;
}

.categories li>a {
  border-bottom: none;
}

.categories li>a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
}

.categories .active {
  font-weight: 600;
}

.categories .category-count {
  color: rgba(0, 0, 0, 0.4);
}


@media(min-width: 768px) {
  .posts-list .post-preview h2 {
    font-size: 26px;
  }
  .posts-list .post-preview .thumbnail {
    float: right;
    width: 30%;
    margin-bottom: 0;
  }
  .posts-list .post-preview .description {
    float: left;
    width: 45%;
  }
  .posts-list .post-preview .metadata {
    float: left;
    width: 20%;
    margin-top: 8px;
  }
  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.5em;
    font-size: 16px;
  }
  .posts-with-sidebar .posts-list {
    float: left;
    width: 75%;
  }
  .posts-with-sidebar .posts-sidebar {
    float: right;
    width: 20%;
    margin-top: 60px;
    padding-top: 24px;
    padding-bottom: 24px;
  }
}


/* Improve display for browsers without grid (IE/Edge <= 15) */

.downlevel {
  line-height: 1.6em;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  margin: 0;
}

.downlevel .d-title {
  padding-top: 6rem;
  padding-bottom: 1.5rem;
}

.downlevel .d-title h1 {
  font-size: 50px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

.downlevel .d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  margin-top: 0;
}

.downlevel .d-byline {
  padding-top: 0.8em;
  padding-bottom: 0.8em;
  font-size: 0.8rem;
  line-height: 1.8em;
}

.downlevel .section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
}

.downlevel .d-article {
  font-size: 1.06rem;
  line-height: 1.7em;
  padding-top: 1rem;
  padding-bottom: 2rem;
}


.downlevel .d-appendix {
  padding-left: 0;
  padding-right: 0;
  max-width: none;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.5);
  padding-top: 40px;
  padding-bottom: 48px;
}

.downlevel .footnotes ol {
  padding-left: 13px;
}

.downlevel .base-grid,
.downlevel .distill-header,
.downlevel .d-title,
.downlevel .d-abstract,
.downlevel .d-article,
.downlevel .d-appendix,
.downlevel .distill-appendix,
.downlevel .d-byline,
.downlevel .d-footnote-list,
.downlevel .d-citation-list,
.downlevel .distill-footer,
.downlevel .appendix-bottom,
.downlevel .posts-container {
  padding-left: 40px;
  padding-right: 40px;
}

@media(min-width: 768px) {
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
  padding-left: 150px;
  padding-right: 150px;
  max-width: 900px;
}
}

.downlevel pre code {
  display: block;
  border-left: 2px solid rgba(0, 0, 0, .1);
  padding: 0 0 0 20px;
  font-size: 14px;
}

.downlevel code, .downlevel pre {
  color: black;
  background: none;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

.downlevel .posts-list .post-preview {
  color: inherit;
}



</style>

<script type="application/javascript">

function is_downlevel_browser() {
  if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                 window.navigator.userAgent)) {
    return true;
  } else {
    return window.load_distill_framework === undefined;
  }
}

// show body when load is complete
function on_load_complete() {

  // add anchors
  if (window.anchors) {
    window.anchors.options.placement = 'left';
    window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
  }


  // set body to visible
  document.body.style.visibility = 'visible';

  // force redraw for leaflet widgets
  if (window.HTMLWidgets) {
    var maps = window.HTMLWidgets.findAll(".leaflet");
    $.each(maps, function(i, el) {
      var map = this.getMap();
      map.invalidateSize();
      map.eachLayer(function(layer) {
        if (layer instanceof L.TileLayer)
          layer.redraw();
      });
    });
  }

  // trigger 'shown' so htmlwidgets resize
  $('d-article').trigger('shown');
}

function init_distill() {

  init_common();

  // create front matter
  var front_matter = $('<d-front-matter></d-front-matter>');
  $('#distill-front-matter').wrap(front_matter);

  // create d-title
  $('.d-title').changeElementType('d-title');

  // create d-byline
  var byline = $('<d-byline></d-byline>');
  $('.d-byline').replaceWith(byline);

  // create d-article
  var article = $('<d-article></d-article>');
  $('.d-article').wrap(article).children().unwrap();

  // move posts container into article
  $('.posts-container').appendTo($('d-article'));

  // create d-appendix
  $('.d-appendix').changeElementType('d-appendix');

  // flag indicating that we have appendix items
  var appendix = $('.appendix-bottom').children('h3').length > 0;

  // replace footnotes with <d-footnote>
  $('.footnote-ref').each(function(i, val) {
    appendix = true;
    var href = $(this).attr('href');
    var id = href.replace('#', '');
    var fn = $('#' + id);
    var fn_p = $('#' + id + '>p');
    fn_p.find('.footnote-back').remove();
    var text = fn_p.html();
    var dtfn = $('<d-footnote></d-footnote>');
    dtfn.html(text);
    $(this).replaceWith(dtfn);
  });
  // remove footnotes
  $('.footnotes').remove();

  // move refs into #references-listing
  $('#references-listing').replaceWith($('#refs'));

  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    var id = $(this).attr('id');
    $('.d-contents a[href="#' + id + '"]').parent().remove();
    appendix = true;
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
  });

  // show d-appendix if we have appendix content
  $("d-appendix").css('display', appendix ? 'grid' : 'none');

  // localize layout chunks to just output
  $('.layout-chunk').each(function(i, val) {

    // capture layout
    var layout = $(this).attr('data-layout');

    // apply layout to markdown level block elements
    var elements = $(this).children().not('details, div.sourceCode, pre, script');
    elements.each(function(i, el) {
      var layout_div = $('<div class="' + layout + '"></div>');
      if (layout_div.hasClass('shaded')) {
        var shaded_content = $('<div class="shaded-content"></div>');
        $(this).wrap(shaded_content);
        $(this).parent().wrap(layout_div);
      } else {
        $(this).wrap(layout_div);
      }
    });


    // unwrap the layout-chunk div
    $(this).children().unwrap();
  });

  // remove code block used to force  highlighting css
  $('.distill-force-highlighting-css').parent().remove();

  // remove empty line numbers inserted by pandoc when using a
  // custom syntax highlighting theme
  $('code.sourceCode a:empty').remove();

  // load distill framework
  load_distill_framework();

  // wait for window.distillRunlevel == 4 to do post processing
  function distill_post_process() {

    if (!window.distillRunlevel || window.distillRunlevel < 4)
      return;

    // hide author/affiliations entirely if we have no authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
    if (!have_authors)
      $('d-byline').addClass('hidden');

    // article with toc class
    $('.d-contents').parent().addClass('d-article-with-toc');

    // strip links that point to #
    $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

    // add orcid ids
    $('.authors-affiliations').find('.author').each(function(i, el) {
      var orcid_id = front_matter.authors[i].orcidID;
      if (orcid_id) {
        var a = $('<a></a>');
        a.attr('href', 'https://orcid.org/' + orcid_id);
        var img = $('<img></img>');
        img.addClass('orcid-id');
        img.attr('alt', 'ORCID ID');
        img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
        a.append(img);
        $(this).append(a);
      }
    });

    // hide elements of author/affiliations grid that have no value
    function hide_byline_column(caption) {
      $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
    }

    // affiliations
    var have_affiliations = false;
    for (var i = 0; i<front_matter.authors.length; ++i) {
      var author = front_matter.authors[i];
      if (author.affiliation !== "&nbsp;") {
        have_affiliations = true;
        break;
      }
    }
    if (!have_affiliations)
      $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

    // published date
    if (!front_matter.publishedDate)
      hide_byline_column("Published");

    // document object identifier
    var doi = $('d-byline').find('h3:contains("DOI")');
    var doi_p = doi.next().empty();
    if (!front_matter.doi) {
      // if we have a citation and valid citationText then link to that
      if ($('#citation').length > 0 && front_matter.citationText) {
        doi.html('Citation');
        $('<a href="#citation"></a>')
          .text(front_matter.citationText)
          .appendTo(doi_p);
      } else {
        hide_byline_column("DOI");
      }
    } else {
      $('<a></a>')
         .attr('href', "https://doi.org/" + front_matter.doi)
         .html(front_matter.doi)
         .appendTo(doi_p);
    }

     // change plural form of authors/affiliations
    if (front_matter.authors.length === 1) {
      var grid = $('.authors-affiliations');
      grid.children('h3:contains("Authors")').text('Author');
      grid.children('h3:contains("Affiliations")').text('Affiliation');
    }

    // remove d-appendix and d-footnote-list local styles
    $('d-appendix > style:first-child').remove();
    $('d-footnote-list > style:first-child').remove();

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // hoverable references
    $('span.citation[data-cites]').each(function() {
      var refs = $(this).attr('data-cites').split(" ");
      var refHtml = refs.map(function(ref) {
        return "<p>" + $('#ref-' + ref).html() + "</p>";
      }).join("\n");
      window.tippy(this, {
        allowHTML: true,
        content: refHtml,
        maxWidth: 500,
        interactive: true,
        interactiveBorder: 10,
        theme: 'light-border',
        placement: 'bottom-start'
      });
    });

    // clear polling timer
    clearInterval(tid);

    // show body now that everything is ready
    on_load_complete();
  }

  var tid = setInterval(distill_post_process, 50);
  distill_post_process();

}

function init_downlevel() {

  init_common();

   // insert hr after d-title
  $('.d-title').after($('<hr class="section-separator"/>'));

  // check if we have authors
  var front_matter = JSON.parse($("#distill-front-matter").html());
  var have_authors = front_matter.authors && front_matter.authors.length > 0;

  // manage byline/border
  if (!have_authors)
    $('.d-byline').remove();
  $('.d-byline').after($('<hr class="section-separator"/>'));
  $('.d-byline a').remove();

  // remove toc
  $('.d-contents').remove();

  // move appendix elements
  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
  });


  // inject headers into references and footnotes
  var refs_header = $('<h3></h3>');
  refs_header.text('References');
  $('#refs').prepend(refs_header);

  var footnotes_header = $('<h3></h3');
  footnotes_header.text('Footnotes');
  $('.footnotes').children('hr').first().replaceWith(footnotes_header);

  // move appendix-bottom entries to the bottom
  $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
  $('.appendix-bottom').remove();

  // remove appendix if it's empty
  if ($('.d-appendix').children().length === 0)
    $('.d-appendix').remove();

  // prepend separator above appendix
  $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

  // trim code
  $('pre>code').each(function(i, val) {
    $(this).html($.trim($(this).html()));
  });

  // move posts-container right before article
  $('.posts-container').insertBefore($('.d-article'));

  $('body').addClass('downlevel');

  on_load_complete();
}


function init_common() {

  // jquery plugin to change element types
  (function($) {
    $.fn.changeElementType = function(newType) {
      var attrs = {};

      $.each(this[0].attributes, function(idx, attr) {
        attrs[attr.nodeName] = attr.nodeValue;
      });

      this.replaceWith(function() {
        return $("<" + newType + "/>", attrs).append($(this).contents());
      });
    };
  })(jQuery);

  // prevent underline for linked images
  $('a > img').parent().css({'border-bottom' : 'none'});

  // mark non-body figures created by knitr chunks as 100% width
  $('.layout-chunk').each(function(i, val) {
    var figures = $(this).find('img, .html-widget');
    if ($(this).attr('data-layout') !== "l-body") {
      figures.css('width', '100%');
    } else {
      figures.css('max-width', '100%');
      figures.filter("[width]").each(function(i, val) {
        var fig = $(this);
        fig.css('width', fig.attr('width') + 'px');
      });

    }
  });

  // auto-append index.html to post-preview links in file: protocol
  // and in rstudio ide preview
  $('.post-preview').each(function(i, val) {
    if (window.location.protocol === "file:")
      $(this).attr('href', $(this).attr('href') + "index.html");
  });

  // get rid of index.html references in header
  if (window.location.protocol !== "file:") {
    $('.distill-site-header a[href]').each(function(i,val) {
      $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
    });
  }

  // add class to pandoc style tables
  $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
  $('.kable-table').children('table').addClass('pandoc-table');

  // add figcaption style to table captions
  $('caption').parent('table').addClass("figcaption");

  // initialize posts list
  if (window.init_posts_list)
    window.init_posts_list();

  // implmement disqus comment link
  $('.disqus-comment-count').click(function() {
    window.headroom_prevent_pin = true;
    $('#disqus_thread').toggleClass('hidden');
    if (!$('#disqus_thread').hasClass('hidden')) {
      var offset = $(this).offset();
      $(window).resize();
      $('html, body').animate({
        scrollTop: offset.top - 35
      });
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  if (is_downlevel_browser())
    init_downlevel();
  else
    window.addEventListener('WebComponentsReady', init_distill);
});

</script>

<!--/radix_placeholder_distill-->
  <script src="../../site_libs/header-attrs-2.11/header-attrs.js"></script>
  <script src="../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="../../site_libs/popper-2.6.0/popper.min.js"></script>
  <link href="../../site_libs/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="../../site_libs/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="../../site_libs/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="../../site_libs/anchor-4.2.2/anchor.min.js"></script>
  <script src="../../site_libs/bowser-1.9.3/bowser.min.js"></script>
  <script src="../../site_libs/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="../../site_libs/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
<!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Sparse matrices part 6: To catch a derivative, first you've got to think like a derivative","description":"Open up the kennels, Kenneth. Mamma’s coming home tonight.","authors":[{"author":"Dan Simpson","authorURL":"https://dpsimpson.github.io","affiliation":"&nbsp;","affiliationURL":"#","orcidID":""}],"publishedDate":"2022-05-30T00:00:00.000+10:00","citationText":"Simpson, 2022"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<header class="header header--fixed" role="banner">
<nav class="distill-site-nav distill-site-header">
<div class="nav-left">
<a href="../../index.html" class="title">Un garçon pas comme les autres (Bayes)</a>
<input id="distill-search" class="nav-search hidden" type="text" placeholder="Search..."/>
</div>
<div class="nav-right">
<a href="../../index.html">Home</a>
<a href="../../about.html">About</a>
<a href="../../index.xml">
<i class="fa fa-rss" aria-hidden="true"></i>
</a>
<a href="javascript:void(0);" class="nav-toggle">&#9776;</a>
</div>
</nav>
</header>
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Sparse matrices part 6: To catch a derivative, first you’ve got to think like a derivative</h1>
<!--radix_placeholder_categories-->
<!--/radix_placeholder_categories-->
<p><p>Open up the kennels, Kenneth. Mamma’s coming home tonight.</p></p>
</div>

<div class="d-byline">
  Dan Simpson <a href="https://dpsimpson.github.io"
class="uri">https://dpsimpson.github.io</a> 
  
<br/>2022-05-30
</div>

<div class="d-article">
<p>Welcome to part six!!! of our ongoing series on making sparse linear
algebra differentiable in JAX with the eventual hope to be able to do
some <a
href="https://dansblog.netlify.app/posts/2022-03-22-a-linear-mixed-effects-model/">cool
statistical shit</a>. We are <em>nowhere near done</em>.</p>
<p><a
href="https://dansblog.netlify.app/posts/2022-05-14-sparse4-some-primatives/">Last
time</a>, we looked at making JAX primitives. We built four of them.
Today we are going to implement the corresponding differentiation rules!
For three<a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a> of them.</p>
<p>So strap yourselves in. This is gonna be detailed.</p>
<p>If you’re interested in the code<a href="#fn2" class="footnote-ref"
id="fnref2" role="doc-noteref"><sup>2</sup></a>, the git repo for this
post is linked at the bottom and in there you will find a folder with
the python code in a python file.</p>
<h2
id="she-is-beauty-and-she-is-grace.-she-is-queen-of-50-states.-she-is-elegance-and-taste.-she-is-miss-autodiff">She
is beauty and she is grace. She is queen of 50 states. She is elegance
and taste. She is miss autodiff</h2>
<p>Derivatives are computed in JAX through the glory and power of
automatic differentiation. If you came to this blog hoping for a great
description of how autodiff works, I am terribly sorry but I absolutely
do not have time for that. Might I suggest google? Or maybe flick
through <a href="https://arxiv.org/abs/1811.05031">this survey by
Charles Margossian.</a>.</p>
<p>The most important thing to remember about algorithmic
differentiation is that it is <em>not</em> symbolic differentiation.
That is, it does not create the functional form of the derivative of the
function and compute that. Instead, it is a system for cleverly
composing derivatives in each bit of the program to compute the
<em>value</em> of the derivative of the function.</p>
<p>But for that to work, we need to implement those clever little
mini-derivatives. In particular, every function <span
class="math inline">\(f(\cdot): \mathbb{R}^n \rightarrow
\mathbb{R}^m\)</span> needs to have a function to compute the
corresponding Jacobian-vector product <span class="math display">\[
(\theta, v) \rightarrow J(\theta) v,
\]</span> where the <span class="math inline">\(n \times m\)</span>
matrix <span class="math inline">\(J(\theta)\)</span> has entries <span
class="math display">\[
J(\theta)_{ij} = \frac{\partial f_j }{\partial \theta_j}.
\]</span></p>
<p>Ok. So let’s get onto this. We are going to derive and implement some
Jacobian-vector products. And all of the assorted accoutrement. And by
crikey. We are going to do it all in a JAX-traceable way.</p>
<h2 id="jvp-number-one-the-linear-solve.">JVP number one: The linear
solve.</h2>
<p>The first of the derivatives that we need to work out is the
derivative of a linear solve <span
class="math inline">\(A^{-1}b\)</span>. Now, intrepid readers, the
obvious thing to do is look the damn derivative up. You get exactly no
hero points for computing it yourself.</p>
<p>But I’m not you, I’m a dickhead.</p>
<p>So I’m going to derive it. I could pretend there are reasons<a
href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a>, but that would just be lying. I’m
doing it because I can.</p>
<p>Beyond the obvious fun of working out a matrix derivative from first
principles, this is fun because we have <em>two</em> arguments instead
of just one. Double the fun.</p>
<p>And we really should make sure the function is differentiated with
respect to every reasonable argument. Why? Because if you write code
other people might use, you don’t get to control how they use it (or
what they will email you about). So it’s always good practice to limit
surprises (like a function not being differentiable wrt some argument)
to cases<a href="#fn4" class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a> where it absolutely necessary. This
reduces the emails.</p>
<p>To that end, let’s take an arbitrary SPD matrix <span
class="math inline">\(A\)</span> with a <em>fixed</em> sparsity pattern.
Let’s take another symmetric matrix <span
class="math inline">\(\Delta\)</span> with <em>the same sparsity
pattern</em> and assume that <span class="math inline">\(\Delta\)</span>
is small enough<a href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"><sup>5</sup></a> that <span class="math inline">\(A +
\Delta\)</span> is still symmetric positive definite. We also need a
vector <span class="math inline">\(\delta\)</span> with a small <span
class="math inline">\(\|\delta\|\)</span>.</p>
<p>Now let’s get algebraing. <span class="math display">\[\begin{align*}
f(A + \Delta, b + \delta) &amp;= (A+\Delta)^{-1}(b + \delta) \\
&amp;= (I + A^{-1}\Delta)^{-1}A^{-1}(b + \delta) \\
&amp;= (I - A^{-1}\Delta + o(\|\Delta\|))A^{-1}(b + \delta) \\
&amp;= A^{-1}b + A^{-1}(\delta - \Delta A^{-1}b ) + o(\|\Delta\| +
\|\delta\|)
\end{align*}\]</span></p>
<p>Easy<a href="#fn6" class="footnote-ref" id="fnref6"
role="doc-noteref"><sup>6</sup></a> as.</p>
<p>We’ve actually calculated the derivative now, but it’s a little more
work to recognise it.</p>
<p>To do that, we need to remember the practical definition of the
Jacobian of a function <span class="math inline">\(f(x)\)</span> that
takes an <span class="math inline">\(n\)</span>-dimensional input and
produces an <span class="math inline">\(m\)</span>-dimensional output.
It is the <span class="math inline">\(n \times m\)</span> matrix <span
class="math inline">\(J_f(x)\)</span> such that <span
class="math display">\[
f(x + \delta)  = f(x) + J_f(x)\delta + o(\|\delta\|).
\]</span></p>
<p>The formulas further simplify if we write <span
class="math inline">\(c = A^{-1}b\)</span>. Then, if we want the
Jacobian-vector product for the first argument, it is <span
class="math display">\[
-A^{-1}\Delta c,
\]</span> while the Jacobian-vector product for the second argument is
<span class="math display">\[
A^{-1}\delta.
\]</span></p>
<p>The only wrinkle in doing this is we need to remember that we are
only storing the lower triangle of <span
class="math inline">\(A\)</span>. Because we need to represent <span
class="math inline">\(\Delta\)</span> the same way, it is represented as
a vector <code>Delta_x</code> that contains only the lower triangle of
<span class="math inline">\(\Delta\)</span>. So we need to make sure we
remember to form the <em>whole</em> matrix before we do the
matrix-vector product <span class="math inline">\(\Delta c\)</span>!</p>
<p>But otherwise, the implementation is going to be pretty
straightforward. The Jacobian-vector product costs one additional linear
solve (beyond the one needed to compute the value <span
class="math inline">\(c = A^{-1}b\)</span>).</p>
<p>In the language of JAX (and autodiff in general), we refer to <span
class="math inline">\(\Delta\)</span> and <span
class="math inline">\(\delta\)</span> as <em>tangent vectors</em>. In
search of a moderately coherent naming convention, we are going to refer
to the tangent associated with the variable <code>x</code> as
<code>xt</code>.</p>
<p>So let’s implement this. Remember: it needs<a href="#fn7"
class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> to
be JAX traceable.</p>
<h2 id="primitive-two-the-triangular-solve">Primitive two: The
triangular solve</h2>
<p>For some sense of continuity, we are going to keep the naming of the
primitives from the last blog post, but we are <em>not</em> going to
attack them in the same order. Why not? Because we work in order of
complexity.</p>
<p>So first off we are going to do the triangular solve. As I have yet
to package up the code (I promise, that will happen next<a href="#fn8"
class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>),
I’m just putting it here under the fold.</p>
<details>
<summary>
The primal implementation
</summary>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> sparse</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> numpy <span class="im">as</span> jnp</span></code></pre></div>
<pre><code>/Users/dsim0009/miniforge3/envs/myjaxenv/lib/python3.10/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.
  warnings.warn(&quot;JAX on Mac ARM machines is experimental and minimally tested. &quot;</code></pre>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> core</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax._src <span class="im">import</span> abstract_arrays</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> core</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>sparse_triangular_solve_p <span class="op">=</span> core.Primitive(<span class="st">&quot;sparse_triangular_solve&quot;</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sparse_triangular_solve(L_indices, L_indptr, L_x, b, <span class="op">*</span>, transpose: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>):</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  <span class="co">&quot;&quot;&quot;A JAX traceable sparse  triangular solve&quot;&quot;&quot;</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> sparse_triangular_solve_p.bind(L_indices, L_indptr, L_x, b, transpose <span class="op">=</span> transpose)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="at">@sparse_triangular_solve_p.def_impl</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sparse_triangular_solve_impl(L_indices, L_indptr, L_x, b, <span class="op">*</span>, transpose <span class="op">=</span> <span class="va">False</span>):</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>  <span class="co">&quot;&quot;&quot;The implementation of the sparse triangular solve. This is not JAX traceable.&quot;&quot;&quot;</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>  L <span class="op">=</span> sparse.csc_array((L_x, L_indices, L_indptr)) </span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>  <span class="cf">assert</span> L.shape[<span class="dv">0</span>] <span class="op">==</span> L.shape[<span class="dv">1</span>]</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>  <span class="cf">assert</span> L.shape[<span class="dv">0</span>] <span class="op">==</span> b.shape[<span class="dv">0</span>]</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> transpose:</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sparse.linalg.spsolve_triangular(L.T, b, lower <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sparse.linalg.spsolve_triangular(L.tocsr(), b, lower <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="at">@sparse_triangular_solve_p.def_abstract_eval</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sparse_triangular_solve_abstract_eval(L_indices, L_indptr, L_x, b, <span class="op">*</span>, transpose <span class="op">=</span> <span class="va">False</span>):</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>  <span class="cf">assert</span> L_indices.shape[<span class="dv">0</span>] <span class="op">==</span> L_x.shape[<span class="dv">0</span>]</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>  <span class="cf">assert</span> b.shape[<span class="dv">0</span>] <span class="op">==</span> L_indptr.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> abstract_arrays.ShapedArray(b.shape, b.dtype)</span></code></pre></div>
</div>
</details>
<h3 id="the-jacobian-vector-product">The Jacobian-vector product</h3>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax._src <span class="im">import</span> ad_util</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax.interpreters <span class="im">import</span> ad</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> lax</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax.experimental <span class="im">import</span> sparse <span class="im">as</span> jsparse</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent, <span class="op">*</span>, transpose):</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>  <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">  A jax-traceable jacobian-vector product. In order to make it traceable, </span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">  we use the experimental sparse CSC matrix in JAX.</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">  </span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">  Input:</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">    arg_values:   A tuple of (L_indices, L_indptr, L_x, b) that describe</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">                  the triangular matrix L and the rhs vector b</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co">    arg_tangent:  A tuple of tangent values (same lenght as arg_values).</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co">                  The first two values are nonsense - we don&#39;t differentiate</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co">                  wrt integers!</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co">    transpose:    (boolean) If true, solve L^Tx = b. Otherwise solve Lx = b.</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co">  Output:         A tuple containing the maybe_transpose(L)^{-1}b and the corresponding</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co">                  Jacobian-vector product.</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co">  &quot;&quot;&quot;</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>  L_indices, L_indptr, L_x, b <span class="op">=</span> arg_values</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>  _, _, L_xt, bt <span class="op">=</span> arg_tangent</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>  value <span class="op">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, b, transpose<span class="op">=</span>transpose)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">type</span>(bt) <span class="kw">is</span> ad.Zero <span class="kw">and</span> <span class="bu">type</span>(L_xt) <span class="kw">is</span> ad.Zero:</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># I legit do not think this ever happens. But I&#39;m honestly not sure.</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;I have arrived!&quot;</span>)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> value, lax.zeros_like_array(value) </span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">type</span>(L_xt) <span class="kw">is</span> <span class="kw">not</span> ad.Zero:</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># L is variable</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> transpose:</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>      Delta <span class="op">=</span> jsparse.CSC((L_xt, L_indices, L_indptr), shape <span class="op">=</span> (b.shape[<span class="dv">0</span>], b.shape[<span class="dv">0</span>])).transpose()</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>      Delta <span class="op">=</span> jsparse.CSC((L_xt, L_indices, L_indptr), shape <span class="op">=</span> (b.shape[<span class="dv">0</span>], b.shape[<span class="dv">0</span>]))</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    jvp_Lx <span class="op">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, Delta <span class="op">@</span> value, transpose <span class="op">=</span> transpose) </span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    jvp_Lx <span class="op">=</span> lax.zeros_like_array(value) </span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">type</span>(bt) <span class="kw">is</span> <span class="kw">not</span> ad.Zero:</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># b is variable</span></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>    jvp_b <span class="op">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, bt, transpose <span class="op">=</span> transpose)</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>    jvp_b <span class="op">=</span> lax.zeros_like_array(value)</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> value, jvp_b <span class="op">-</span> jvp_Lx</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>ad.primitive_jvps[sparse_triangular_solve_p] <span class="op">=</span> sparse_triangular_solve_value_and_jvp</span></code></pre></div>
</div>
<p>Before we see if this works, let’s first have talk about the
structure of the function I just wrote. Generally speaking, we want a
function that takes in the primals and tangents at tuples and then
returns the value and the<a href="#fn9" class="footnote-ref" id="fnref9"
role="doc-noteref"><sup>9</sup></a> Jacobian-vector product.</p>
<p>The main thing you will notice in the code is that there is <em>a
lot</em> of checking for <code>ad.Zero</code>. This is a special type
defined in JAX that is, essentially, telling the autodiff system that we
are not differentiating wrt that variable. This is different to a
tangent that just happens to be numerically equal to zero. Any code for
a Jacobian-vector product needs to handle this special value.</p>
<p>As we have two arguments, we have 3 interesting options:</p>
<ol type="1">
<li><p>Both <code>L_xt</code> and <code>bt</code> are
<code>ad.Zero</code>: This means the function is a constant and the
derivative is zero. I am fairly certain that we do not need to manually
handle this case, but because I don’t know and I do not like surprises,
it’s in there.</p></li>
<li><p><code>L_xt</code> is <em>not</em> <code>ad.Zero</code>: This
means that we need to differentiate wrt the matrix. In this case we need
to compute <span class="math inline">\(\Delta c\)</span> or <span
class="math inline">\(\Delta^T c\)</span>, depending on the
<code>transpose</code> argument. In order to do this, I used the
<code>jax.experimental.sparse.CSC</code> class, which has some very
limited sparse matrix support (basically matrix-vector products). This
is <em>extremely</em> convenient because it means I don’t need to write
the matrix-vector product myself!</p></li>
<li><p><code>bt</code> is <em>not</em> <code>ad.Zero</code>: This means
that we need to differentiate wrt the rhs vector. This part of the
formula is pretty straightforward: just an application of the
primal.</p></li>
</ol>
<p>In the case that either <code>L_xt</code> or <code>bt</code> are
<code>ad.Zero</code>, we simply set the corresponding contribution to
the jvp to zero.</p>
<p>It’s worth saying that you can bypass all of this
<code>ad.Zero</code> logic by writing separate functions for the JVP
contribution from each input and then chaining them together using<a
href="#fn10" class="footnote-ref" id="fnref10"
role="doc-noteref"><sup>10</sup></a> <code>ad.defjvp2()</code> to <a
href="https://github.com/google/jax/blob/41417d70c03b6089c93a42325111a0d8348c2fa3/jax/_src/lax/linalg.py#L791">chain
them together</a>. This is what the
<code>lax.linalg.triangular_solve()</code> implementation does.</p>
<p>So why didn’t I do this? I avoided this because in the other
primitives I have to implement, there are expensive computations (like
Cholesky factorisations) that I want to share between the primal and the
various tangent calculations. The <code>ad.defjvp</code> frameworks
don’t allow for that. So I decided not to demonstrate/learn two separate
patterns.</p>
<h3 id="transposition">Transposition</h3>
<p>Now I’ve never actively wanted a Jacobian-vector product in my whole
life. I’m sorry. I want a gradient. Gimme a gradient. I am the Veruca
Salt of gradients.</p>
<p>In may autodiff systems, if you want<a href="#fn11"
class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> a
gradient, you need to implement vector-Jacobian products<a href="#fn12"
class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>
explicitly.</p>
<p>One of the odder little innovations in JAX is that instead of forcing
you to implement this as well<a href="#fn13" class="footnote-ref"
id="fnref13" role="doc-noteref"><sup>13</sup></a>, you only need to
implement half of it.</p>
<p>You see, some clever analysis that, as far as I far as I can tell<a
href="#fn14" class="footnote-ref" id="fnref14"
role="doc-noteref"><sup>14</sup></a>, is detailed in <a
href="https://arxiv.org/abs/2204.10923">this paper</a> shows that you
only need to form explicit vector-Jacobian products for the structurally
linear arguments of the function.</p>
<p>In JAX (and maybe elsewhere), this is known as a <em>transposition
rule</em>. The combination of a transopition rule and a JAX-traceable
Jacobian-vector product is enough for JAX to compute all of the
directional derivatives and gradients we could ever hope for.</p>
<p>As far as I understand, it is all about functions that are
<em>structurally linear</em> in some arguments. For instance, if <span
class="math inline">\(A(x)\)</span> is a matrix-valued function and
<span class="math inline">\(x\)</span> and <span
class="math inline">\(y\)</span> are vectors, then the function <span
class="math display">\[
f(x, y) = A(x)y + g(x)
\]</span> is structurally linear in <span
class="math inline">\(y\)</span> in the sense that for every fixed value
of <span class="math inline">\(x\)</span>, the function <span
class="math display">\[
f_x(y) = A(x) y + g(x)
\]</span> is linear in <span class="math inline">\(y\)</span>. The
resulting transpositon rule is then</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f_transpose(x, y):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  Ax <span class="op">=</span> A(x)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  gx <span class="op">=</span> g(x)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> (<span class="va">None</span>, Ax.T <span class="op">@</span> y <span class="op">+</span> gx)</span></code></pre></div>
</div>
<p>The first element of the return is <code>None</code> because <span
class="math inline">\(f(x,y)\)</span> is not<a href="#fn15"
class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>
structurally linear in <span class="math inline">\(x\)</span> so there
is nothing to transpose. The second element simply takes the matrix in
the linear function and transposes it.</p>
<p>If you know anything about autodiff, you’ll think “this doesn’t
<em>feel</em> like enough” and it’s not. JAX deals with the non-linear
part of <span class="math inline">\(f(x,y)\)</span> by tracing the
evaluation tree for its Jacobian-vector product and … manipulating<a
href="#fn16" class="footnote-ref" id="fnref16"
role="doc-noteref"><sup>16</sup></a> it.</p>
<p>We already built the abstract evaluation function last time around,
so the tracing part can be done. All we need is the transposition
rule.</p>
<p>The linear solve <span class="math inline">\(f(A, b) =
A^{-1}b\)</span> is non-linear in the first argument but linear in the
second argument. So we only need to implement <span
class="math display">\[
J^T_b(A,b)w = A^{-T}w,
\]</span> where the subscript <span class="math inline">\(b\)</span>
indicates we’re only computing the Jacobian wrt <span
class="math inline">\(b\)</span>.</p>
<p>Initially, I struggled to work out what needed to be implemented
here. The thing that clarified the process for me was looking at JAX’s
<a
href="https://github.com/google/jax/blob/41417d70c03b6089c93a42325111a0d8348c2fa3/jax/_src/lax/linalg.py#L747">internal
implementation</a> of the Jacobian-vector product for a dense matrix.
From there, I understood what this had to look like for a vector-valued
function and this is the result.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sparse_triangular_solve_transpose_rule(cotangent, L_indices, L_indptr, L_x, b, <span class="op">*</span>, transpose):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co">  Transposition rule for the triangular solve. </span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co">  Translated from here https://github.com/google/jax/blob/41417d70c03b6089c93a42325111a0d8348c2fa3/jax/_src/lax/linalg.py#L747.</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co">  Inputs:</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co">    cotangent: Output cotangent (aka adjoint). (produced by JAX)</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co">    L_indices, L_indptr, L_x: Represenation of sparse matrix. L_x should be concrete</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co">    b: The right hand side. Must be an jax.interpreters.ad.UndefinedPrimal</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co">    transpose: (boolean) True: solve $L^Tx = b$. False: Solve $Lx = b$.</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co">  Output:</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co">    A 4-tuple with the adjoints (None, None, None, b_adjoint)</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co">  &quot;&quot;&quot;</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>  <span class="cf">assert</span> <span class="kw">not</span> ad.is_undefined_primal(L_x) <span class="kw">and</span> ad.is_undefined_primal(b)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">type</span>(cotangent) <span class="kw">is</span> ad_util.Zero:</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    cot_b <span class="op">=</span> ad_util.Zero(b.aval)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    cot_b <span class="op">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, cotangent, transpose <span class="op">=</span> <span class="kw">not</span> transpose)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="va">None</span>, <span class="va">None</span>, <span class="va">None</span>, cot_b</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>ad.primitive_transposes[sparse_triangular_solve_p] <span class="op">=</span> sparse_triangular_solve_transpose_rule</span></code></pre></div>
</div>
<p>If this doesn’t make a lot of sense to you, that’s because it’s
confusing.</p>
<p>One way to think of it is in terms of the more ordinary notation.
Mike Giles has <a
href="https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf">a classic
paper</a> that covers these results for basic linear algebra. The idea
is to imagine that, as part of your larger program, you need to compute
<span class="math inline">\(c = A^{-1}b\)</span>.</p>
<p>Forward-mode autodiff computes the <em>sensitivity</em> of <span
class="math inline">\(c\)</span>, usually denoted <span
class="math inline">\(\dot c\)</span> from the sensitivies <span
class="math inline">\(\dot A\)</span> and <span
class="math inline">\(\dot b\)</span>. These have already been computed.
The formula in Giles is <span class="math display">\[
\dot c = A^{-1}(\dot b - \dot A c).
\]</span> The canny reader will recognise this as exactly<a href="#fn17"
class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a>
the formula for the Jacobian-vector product.</p>
<p>So what does reverse-mode autodiff do? Well it moves through the
program in the other direction. So instead of starting with the
sensitivities <span class="math inline">\(\dot A\)</span> and <span
class="math inline">\(\dot b\)</span> already computed, we instead start
with the<a href="#fn18" class="footnote-ref" id="fnref18"
role="doc-noteref"><sup>18</sup></a> <em>adjoint sensitivity</em> <span
class="math inline">\(\bar c\)</span>. Our aim is to compute <span
class="math inline">\(\bar A\)</span> and <span
class="math inline">\(\bar b\)</span> from <span
class="math inline">\(\bar c\)</span>.</p>
<p>The details of how to do this are<a href="#fn19" class="footnote-ref"
id="fnref19" role="doc-noteref"><sup>19</sup></a> <em>beyond the
scope</em>, but without tooooooo much effort you can show that <span
class="math display">\[
\bar b = A^{-T} \bar c,
\]</span> which you should recognise as the equation that was just
implemented.</p>
<p>The thing that we <em>do not</em> have to implement in JAX is the
other adjoint that, for dense matrices<a href="#fn20"
class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a>,
is <span class="math display">\[
\bar{A} = -\bar{b}c^T.
\]</span> Through the healing power of … something?—Truly I do not
know.— JAX can work that bit out itself. woo.</p>
<h3
id="testing-the-numerical-implementation-of-the-jacobian-vector-product">Testing
the numerical implementation of the Jacobian-vector product</h3>
<p>So let’s see if this works. I’m not going to lie, I’m flying by the
seat of my pants here. I’m not super familiar with the JAX internals, so
I have written a lot of test cases. You may wish to skip this part. But
rest assured that almost every single one of these cases was useful to
me working out how this thing actually worked!</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_matrix(n):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    one_d <span class="op">=</span> sparse.diags([[<span class="op">-</span><span class="fl">1.</span>]<span class="op">*</span>(n<span class="op">-</span><span class="dv">1</span>), [<span class="fl">2.</span>]<span class="op">*</span>n, [<span class="op">-</span><span class="fl">1.</span>]<span class="op">*</span>(n<span class="op">-</span><span class="dv">1</span>)], [<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> (sparse.kronsum(one_d, one_d) <span class="op">+</span> sparse.eye(n<span class="op">*</span>n)).tocsc()</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    A_lower <span class="op">=</span> sparse.tril(A, <span class="bu">format</span> <span class="op">=</span> <span class="st">&quot;csc&quot;</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    A_index <span class="op">=</span> A_lower.indices</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    A_indptr <span class="op">=</span> A_lower.indptr</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    A_x <span class="op">=</span> A_lower.data</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (A_index, A_indptr, A_x, A)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>A_indices, A_indptr, A_x, A <span class="op">=</span> make_matrix(<span class="dv">10</span>)</span></code></pre></div>
</div>
<p>This is the same test case as the last blog. We will just use the
lower triangle of <span class="math inline">\(A\)</span> as the test
matrix.</p>
<p>First things first, let’s check out the numerical implementation of
the function. We will do that by comparing the implemented
Jacobian-vector product with the <em>definition</em> of the
Jacobian-vector product (aka the forward<a href="#fn21"
class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a>
difference approximation).</p>
<p>There are lots of things that we could do here to turn these into
<em>actual</em> tests. For instance, the test suite inside JAX has a lot
of nice convenience functions for checking implementations of
derivatives. But I went with homespun because that was how I was
feeling.</p>
<p>You’ll also notice that I’m using random numbers here, which is fine
for a blog. Not so fine for a test that you don’t want to be
potentially<a href="#fn22" class="footnote-ref" id="fnref22"
role="doc-noteref"><sup>22</sup></a> flaky.</p>
<p>The choice of <code>eps = 1e-4</code> is roughly<a href="#fn23"
class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a>
because it’s the square root of the single precision machine epsilon<a
href="#fn24" class="footnote-ref" id="fnref24"
role="doc-noteref"><sup>24</sup></a>. A very rough back of the envelope
calculation for the forward difference approximation to the derivative
shows that the square root of the machine epislon is about the size you
want your perturbation to be.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.random.standard_normal(<span class="dv">100</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>bt <span class="op">=</span> np.random.standard_normal(<span class="dv">100</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>bt <span class="op">/=</span> np.linalg.norm(bt)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>A_xt <span class="op">=</span> np.random.standard_normal(<span class="bu">len</span>(A_x))</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>A_xt <span class="op">/=</span> np.linalg.norm(A_xt)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>arg_values <span class="op">=</span> (A_indices, A_indptr, A_x, b )</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>arg_tangent_A <span class="op">=</span> (<span class="va">None</span>, <span class="va">None</span>, A_xt, ad.Zero(<span class="bu">type</span>(b)))</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>arg_tangent_b <span class="op">=</span> (<span class="va">None</span>, <span class="va">None</span>, ad.Zero(<span class="bu">type</span>(A_xt)), bt)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>arg_tangent_Ab <span class="op">=</span> (<span class="va">None</span>, <span class="va">None</span>, A_xt, bt)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>p, t_A <span class="op">=</span> sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_A, transpose <span class="op">=</span> <span class="va">False</span>)</span></code></pre></div>
<pre><code>WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)</code></pre>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>_, t_b <span class="op">=</span> sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_b, transpose <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>_, t_Ab <span class="op">=</span> sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_Ab, transpose <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>pT, t_AT <span class="op">=</span> sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_A, transpose <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>_, t_bT <span class="op">=</span> sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_b, transpose <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>eps <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>tt_A <span class="op">=</span> (sparse_triangular_solve(A_indices, A_indptr, A_x <span class="op">+</span> eps <span class="op">*</span> A_xt, b) <span class="op">-</span> p) <span class="op">/</span>eps</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>tt_b <span class="op">=</span> (sparse_triangular_solve(A_indices, A_indptr, A_x, b <span class="op">+</span> eps <span class="op">*</span> bt) <span class="op">-</span> p) <span class="op">/</span> eps</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>tt_Ab <span class="op">=</span> (sparse_triangular_solve(A_indices, A_indptr, A_x <span class="op">+</span> eps <span class="op">*</span> A_xt, b <span class="op">+</span> eps <span class="op">*</span> bt) <span class="op">-</span> p) <span class="op">/</span> eps</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>tt_AT <span class="op">=</span> (sparse_triangular_solve(A_indices, A_indptr, A_x <span class="op">+</span> eps <span class="op">*</span> A_xt, b, transpose <span class="op">=</span> <span class="va">True</span>) <span class="op">-</span> pT) <span class="op">/</span> eps</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>tt_bT <span class="op">=</span> (sparse_triangular_solve(A_indices, A_indptr, A_x, b <span class="op">+</span> eps <span class="op">*</span> bt, transpose <span class="op">=</span> <span class="va">True</span>) <span class="op">-</span> pT) <span class="op">/</span> eps</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;&quot;&quot;</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="ss">Transpose = False:</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="ss">  Error A varying: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(t_A <span class="op">-</span> tt_A)<span class="sc">: .2e}</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="ss">  Error b varying: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(t_b <span class="op">-</span> tt_b)<span class="sc">: .2e}</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="ss">  Error A and b varying: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(t_Ab <span class="op">-</span> tt_Ab)<span class="sc">: .2e}</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="ss">Transpose = True:</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="ss">  Error A varying: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(t_AT <span class="op">-</span> tt_AT)<span class="sc">: .2e}</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="ss">  Error b varying: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(t_bT <span class="op">-</span> tt_bT)<span class="sc">: .2e}</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="ss">&quot;&quot;&quot;</span>)</span></code></pre></div>
<pre><code>
Transpose = False:
  Error A varying:  1.18e-07
  Error b varying:  0.00e+00
  Error A and b varying:  4.82e-07

Transpose = True:
  Error A varying:  9.65e-08
  Error b varying:  0.00e+00</code></pre>
</div>
<p>Brilliant! Everythign correct withing single precision!</p>
<h3 id="checking-on-the-plumbing">Checking on the plumbing</h3>
<p>Making the numerical implementation work is only half the battle. We
also have to make it work <em>in the context of JAX</em>.</p>
<p>Now I would be lying if I pretended this process went smoothly. But
the first time is for experience. It’s mostly a matter of just reading
the documentation carefully and going through similar examples that have
already been implemented.</p>
<p>And testing. I learnt how this was supposed to work by testing
it.</p>
<p>(For full disclosure, I also wrote a big block f-string in the
<code>sparse_triangular_solve()</code> function at one point that told
me the types, shapes, and what <code>transpose</code> was, which was how
I worked out that my code was breaking because I forgot the first to
<code>None</code> outputs in the transposition rule. When it doubt,
print shit.)</p>
<p>As you will see from my testing code, I was not going for elegance. I
was running the damn permutations. If you’re looking for elegance, look
elsewhere.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> jvp, grad</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> scipy <span class="im">as</span> jsp</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(theta):</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> jnp.array(A_x)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> Ax_theta.at[A_indptr[<span class="dv">20</span>]].add(theta[<span class="dv">0</span>])</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> Ax_theta.at[A_indptr[<span class="dv">50</span>]].add(theta[<span class="dv">1</span>])</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> jnp.ones(<span class="dv">100</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> sparse_triangular_solve(A_indices, A_indptr, Ax_theta, b, transpose <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f_jax(theta):</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> jnp.array(sparse.tril(A).todense())</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> Ax_theta.at[<span class="dv">20</span>,<span class="dv">20</span>].add(theta[<span class="dv">0</span>])</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> Ax_theta.at[<span class="dv">50</span>,<span class="dv">50</span>].add(theta[<span class="dv">1</span>])</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> jnp.ones(<span class="dv">100</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> jsp.linalg.solve_triangular(Ax_theta, b, lower <span class="op">=</span> <span class="va">True</span>, trans <span class="op">=</span> <span class="st">&quot;T&quot;</span>)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> g(theta):</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> jnp.array(A_x)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> jnp.ones(<span class="dv">100</span>)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> b.at[<span class="dv">0</span>].<span class="bu">set</span>(theta[<span class="dv">0</span>])</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> b.at[<span class="dv">51</span>].<span class="bu">set</span>(theta[<span class="dv">1</span>])</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> sparse_triangular_solve(A_indices, A_indptr, Ax_theta, b, transpose <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> g_jax(theta):</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> jnp.array(sparse.tril(A).todense())</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> jnp.ones(<span class="dv">100</span>)</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> b.at[<span class="dv">0</span>].<span class="bu">set</span>(theta[<span class="dv">0</span>])</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> b.at[<span class="dv">51</span>].<span class="bu">set</span>(theta[<span class="dv">1</span>])</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> jsp.linalg.solve_triangular(Ax_theta, b, lower <span class="op">=</span> <span class="va">True</span>, trans <span class="op">=</span> <span class="st">&quot;T&quot;</span>)</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> h(theta):</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> jnp.array(A_x)</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> Ax_theta.at[A_indptr[<span class="dv">20</span>]].add(theta[<span class="dv">0</span>]) </span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> jnp.ones(<span class="dv">100</span>)</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> b.at[<span class="dv">51</span>].<span class="bu">set</span>(theta[<span class="dv">1</span>])</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> sparse_triangular_solve(A_indices, A_indptr, Ax_theta, b, transpose <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> h_jax(theta):</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> jnp.array(sparse.tril(A).todense())</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> Ax_theta.at[<span class="dv">20</span>,<span class="dv">20</span>].add(theta[<span class="dv">0</span>])</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> jnp.ones(<span class="dv">100</span>)</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> b.at[<span class="dv">51</span>].<span class="bu">set</span>(theta[<span class="dv">1</span>])</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> jsp.linalg.solve_triangular(Ax_theta, b, lower <span class="op">=</span> <span class="va">True</span>, trans <span class="op">=</span> <span class="st">&quot;N&quot;</span>)</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> no_diff(theta):</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> sparse_triangular_solve(A_indices, A_indptr, A_x, jnp.ones(<span class="dv">100</span>), transpose <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> no_diff_jax(theta):</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> jsp.linalg.solve_triangular(jnp.array(sparse.tril(A).todense()), jnp.ones(<span class="dv">100</span>), lower <span class="op">=</span> <span class="va">True</span>, trans <span class="op">=</span> <span class="st">&quot;N&quot;</span>)</span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>A_indices, A_indptr, A_x, A <span class="op">=</span> make_matrix(<span class="dv">10</span>)</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>primal1, jvp1 <span class="op">=</span> jvp(f, (jnp.array([<span class="op">-</span><span class="fl">142.</span>, <span class="fl">342.</span>]),), (jnp.array([<span class="fl">1.</span>, <span class="fl">2.</span>]),))</span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>primal2, jvp2 <span class="op">=</span> jvp(f_jax, (jnp.array([<span class="op">-</span><span class="fl">142.</span>, <span class="fl">342.</span>]),), (jnp.array([<span class="fl">1.</span>, <span class="fl">2.</span>]),))</span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a>grad1 <span class="op">=</span> grad(<span class="kw">lambda</span> x: jnp.mean(f(x)))(jnp.array([<span class="op">-</span><span class="fl">142.</span>, <span class="fl">342.</span>]))</span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>grad2 <span class="op">=</span> grad(<span class="kw">lambda</span> x: jnp.mean(f_jax(x)))(jnp.array([<span class="op">-</span><span class="fl">142.</span>, <span class="fl">342.</span>]))</span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>primal3, jvp3 <span class="op">=</span> jvp(g, (jnp.array([<span class="op">-</span><span class="fl">142.</span>, <span class="fl">342.</span>]),), (jnp.array([<span class="fl">1.</span>, <span class="fl">2.</span>]),))</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>primal4, jvp4 <span class="op">=</span> jvp(g_jax, (jnp.array([<span class="op">-</span><span class="fl">142.</span>, <span class="fl">342.</span>]),), (jnp.array([<span class="fl">1.</span>, <span class="fl">2.</span>]),))</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>grad3 <span class="op">=</span> grad(<span class="kw">lambda</span> x: jnp.mean(g(x)))(jnp.array([<span class="op">-</span><span class="fl">142.</span>, <span class="fl">342.</span>]))</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>grad4 <span class="op">=</span> grad(<span class="kw">lambda</span> x: jnp.mean(g_jax(x)))(jnp.array([<span class="op">-</span><span class="fl">142.</span>, <span class="fl">342.</span>]))  </span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a>primal5, jvp5 <span class="op">=</span> jvp(h, (jnp.array([<span class="op">-</span><span class="fl">142.</span>, <span class="fl">342.</span>]),), (jnp.array([<span class="fl">1.</span>, <span class="fl">2.</span>]),))</span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a>primal6, jvp6 <span class="op">=</span> jvp(h_jax, (jnp.array([<span class="op">-</span><span class="fl">142.</span>, <span class="fl">342.</span>]),), (jnp.array([<span class="fl">1.</span>, <span class="fl">2.</span>]),))</span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a>grad5 <span class="op">=</span> grad(<span class="kw">lambda</span> x: jnp.mean(h(x)))(jnp.array([<span class="op">-</span><span class="fl">142.</span>, <span class="fl">342.</span>]))</span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>grad6 <span class="op">=</span> grad(<span class="kw">lambda</span> x: jnp.mean(h_jax(x)))(jnp.array([<span class="op">-</span><span class="fl">142.</span>, <span class="fl">342.</span>]))</span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>primal7, jvp7 <span class="op">=</span> jvp(no_diff, (jnp.array([<span class="op">-</span><span class="fl">142.</span>, <span class="fl">342.</span>]),), (jnp.array([<span class="fl">1.</span>, <span class="fl">2.</span>]),))</span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a>primal8, jvp8 <span class="op">=</span> jvp(no_diff_jax, (jnp.array([<span class="op">-</span><span class="fl">142.</span>, <span class="fl">342.</span>]),), (jnp.array([<span class="fl">1.</span>, <span class="fl">2.</span>]),))</span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>grad7 <span class="op">=</span> grad(<span class="kw">lambda</span> x: jnp.mean(no_diff(x)))(jnp.array([<span class="op">-</span><span class="fl">142.</span>, <span class="fl">342.</span>]))</span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a>grad8 <span class="op">=</span> grad(<span class="kw">lambda</span> x: jnp.mean(no_diff_jax(x)))(jnp.array([<span class="op">-</span><span class="fl">142.</span>, <span class="fl">342.</span>]))</span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;&quot;&quot;</span></span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a><span class="ss">Variable L:</span></span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a><span class="ss">  Primal difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(primal1 <span class="op">-</span> primal2)<span class="sc">: .2e}</span></span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a><span class="ss">  JVP difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(jvp1 <span class="op">-</span> jvp2)<span class="sc">: .2e}</span></span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a><span class="ss">  Gradient difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(grad1 <span class="op">-</span> grad2)<span class="sc">: .2e}</span></span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a><span class="ss">Variable b:</span></span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a><span class="ss">  Primal difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(primal3 <span class="op">-</span> primal4)<span class="sc">: .2e}</span></span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a><span class="ss">  JVP difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(jvp3 <span class="op">-</span> jvp4)<span class="sc">: .2e}</span></span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a><span class="ss">  Gradient difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(grad3 <span class="op">-</span> grad4)<span class="sc">: .2e}</span><span class="ss"> </span></span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a><span class="ss">Variable L and b:</span></span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a><span class="ss">  Primal difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(primal5 <span class="op">-</span> primal6)<span class="sc">: .2e}</span></span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a><span class="ss">  JVP difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(jvp5 <span class="op">-</span> jvp6)<span class="sc">: .2e}</span></span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a><span class="ss">  Gradient difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(grad5 <span class="op">-</span> grad6)<span class="sc">: .2e}</span></span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a><span class="ss">No diff:</span></span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a><span class="ss">  Primal difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(primal7 <span class="op">-</span> primal8)<span class="sc">}</span></span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a><span class="ss">  JVP difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(jvp7 <span class="op">-</span> jvp8)<span class="sc">}</span></span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a><span class="ss">  Gradient difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(grad7 <span class="op">-</span> grad8)<span class="sc">}</span></span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a><span class="ss">&quot;&quot;&quot;</span>)</span></code></pre></div>
<pre><code>
Variable L:
  Primal difference:  1.98e-07
  JVP difference:  2.58e-12
  Gradient difference:  0.00e+00

Variable b:
  Primal difference:  7.94e-06
  JVP difference:  1.83e-08
  Gradient difference:  3.29e-10 

Variable L and b:
  Primal difference:  2.08e-06
  JVP difference:  1.08e-08
  Gradient difference:  2.33e-10

No diff:
  Primal difference: 2.2101993124579167e-07
  JVP difference: 0.0
  Gradient difference: 0.0</code></pre>
</div>
<p>Stunning!</p>
<h2 id="primitive-one-the-general-a-1b">Primitive one: The general <span
class="math inline">\(A^{-1}b\)</span></h2>
<p>Ok. So this is a very similar problem to the one that we just solved.
But, as fate would have it, the solution is going to look quite
different. Why? Because we need to compute a Cholesky factorisation.</p>
<p>First things first, though, we are going to need a JAX-traceable way
to compute a Cholesky factor. This means that we need<a href="#fn25"
class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a>
to tell our <code>sparse_solve</code> function the how many non-zeros
the sparse Cholesky will have. Why? Well. It has to do with how the
function is used.</p>
<p>When <code>sparse_cholesky()</code> is called with concrete inputs<a
href="#fn26" class="footnote-ref" id="fnref26"
role="doc-noteref"><sup>26</sup></a>, then it can quite happily work out
the sparsity structure of <span class="math inline">\(L\)</span>. But
when JAX is preparing to transform the code, eg when it’s building a
gradient, it calls <code>sparse_cholesky()</code> using abstract
arguments that only share the shape information from the inputs. This is
<em>not</em> enough to compute the sparsity structure. We <em>need</em>
the <code>indices</code> and <code>indptr</code> arrays.</p>
<p>This means that we need <code>sparse_cholesky()</code> to throw an
error if <code>L_nse</code> isn’t passed. This wasn’t implemented well
last time, so here it is done properly.</p>
<p>(If you’re wondering about that <code>None</code> argument, it is the
identity transform. So if <code>A_indices</code> is a concrete value,
<code>ind = A_indices</code>. Otherwise an error is called.)</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>sparse_cholesky_p <span class="op">=</span> core.Primitive(<span class="st">&quot;sparse_cholesky&quot;</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sparse_cholesky(A_indices, A_indptr, A_x, <span class="op">*</span>, L_nse: <span class="bu">int</span> <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">&quot;&quot;&quot;A JAX traceable sparse cholesky decomposition&quot;&quot;&quot;</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> L_nse <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    err_string <span class="op">=</span> <span class="st">&quot;You need to pass a value to L_nse when doing fancy sparse_cholesky.&quot;</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    ind <span class="op">=</span> core.concrete_or_error(<span class="va">None</span>, A_indices, err_string)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    ptr <span class="op">=</span> core.concrete_or_error(<span class="va">None</span>, A_indptr, err_string)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    L_ind, _ <span class="op">=</span> _symbolic_factor(ind, ptr)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    L_nse <span class="op">=</span> <span class="bu">len</span>(L_ind)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> sparse_cholesky_p.bind(A_indices, A_indptr, A_x, L_nse <span class="op">=</span> L_nse)</span></code></pre></div>
</div>
<details>
<summary>
The rest of the Choleksy code
</summary>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="at">@sparse_cholesky_p.def_impl</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sparse_cholesky_impl(A_indices, A_indptr, A_x, <span class="op">*</span>, L_nse):</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  <span class="co">&quot;&quot;&quot;The implementation of the sparse cholesky This is not JAX traceable.&quot;&quot;&quot;</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>  L_indices, L_indptr<span class="op">=</span> _symbolic_factor(A_indices, A_indptr)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> L_nse <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">len</span>(L_indices) <span class="op">==</span> L_nse</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>  L_x <span class="op">=</span> _structured_copy(A_indices, A_indptr, A_x, L_indices, L_indptr)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>  L_x <span class="op">=</span> _sparse_cholesky_impl(L_indices, L_indptr, L_x)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> L_indices, L_indptr, L_x</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _symbolic_factor(A_indices, A_indptr):</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Assumes A_indices and A_indptr index the lower triangle of $A$ ONLY.</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>  n <span class="op">=</span> <span class="bu">len</span>(A_indptr) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>  L_sym <span class="op">=</span> [np.array([], dtype<span class="op">=</span><span class="bu">int</span>) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n)]</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>  children <span class="op">=</span> [np.array([], dtype<span class="op">=</span><span class="bu">int</span>) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n)]</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    L_sym[j] <span class="op">=</span> A_indices[A_indptr[j]:A_indptr[j <span class="op">+</span> <span class="dv">1</span>]]</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> child <span class="kw">in</span> children[j]:</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>      tmp <span class="op">=</span> L_sym[child][L_sym[child] <span class="op">&gt;</span> j]</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>      L_sym[j] <span class="op">=</span> np.unique(np.append(L_sym[j], tmp))</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(L_sym[j]) <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>      p <span class="op">=</span> L_sym[j][<span class="dv">1</span>]</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>      children[p] <span class="op">=</span> np.append(children[p], j)</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>  L_indptr <span class="op">=</span> np.zeros(n<span class="op">+</span><span class="dv">1</span>, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>  L_indptr[<span class="dv">1</span>:] <span class="op">=</span> np.cumsum([<span class="bu">len</span>(x) <span class="cf">for</span> x <span class="kw">in</span> L_sym])</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>  L_indices <span class="op">=</span> np.concatenate(L_sym)</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> L_indices, L_indptr</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _structured_copy(A_indices, A_indptr, A_x, L_indices, L_indptr):</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>  n <span class="op">=</span> <span class="bu">len</span>(A_indptr) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>  L_x <span class="op">=</span> np.zeros(<span class="bu">len</span>(L_indices))</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n):</span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>    copy_idx <span class="op">=</span> np.nonzero(np.in1d(L_indices[L_indptr[j]:L_indptr[j <span class="op">+</span> <span class="dv">1</span>]],</span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>                                  A_indices[A_indptr[j]:A_indptr[j<span class="op">+</span><span class="dv">1</span>]]))[<span class="dv">0</span>]</span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>    L_x[L_indptr[j] <span class="op">+</span> copy_idx] <span class="op">=</span> A_x[A_indptr[j]:A_indptr[j<span class="op">+</span><span class="dv">1</span>]]</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> L_x</span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _sparse_cholesky_impl(L_indices, L_indptr, L_x):</span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>  n <span class="op">=</span> <span class="bu">len</span>(L_indptr) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>  descendant <span class="op">=</span> [[] <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n)]</span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n):</span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a>    tmp <span class="op">=</span> L_x[L_indptr[j]:L_indptr[j <span class="op">+</span> <span class="dv">1</span>]]</span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> bebe <span class="kw">in</span> descendant[j]:</span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a>      k <span class="op">=</span> bebe[<span class="dv">0</span>]</span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a>      Ljk<span class="op">=</span> L_x[bebe[<span class="dv">1</span>]]</span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a>      pad <span class="op">=</span> np.nonzero(                                                       <span class="op">\</span></span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a>          L_indices[L_indptr[k]:L_indptr[k<span class="op">+</span><span class="dv">1</span>]] <span class="op">==</span> L_indices[L_indptr[j]])[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a>      update_idx <span class="op">=</span> np.nonzero(np.in1d(                                        <span class="op">\</span></span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a>                    L_indices[L_indptr[j]:L_indptr[j<span class="op">+</span><span class="dv">1</span>]],                     <span class="op">\</span></span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a>                    L_indices[(L_indptr[k] <span class="op">+</span> pad):L_indptr[k<span class="op">+</span><span class="dv">1</span>]]))[<span class="dv">0</span>]</span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a>      tmp[update_idx] <span class="op">=</span> tmp[update_idx] <span class="op">-</span>                                     <span class="op">\</span></span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a>                        Ljk <span class="op">*</span> L_x[(L_indptr[k] <span class="op">+</span> pad):L_indptr[k <span class="op">+</span> <span class="dv">1</span>]]</span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a>    diag <span class="op">=</span> np.sqrt(tmp[<span class="dv">0</span>])</span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a>    L_x[L_indptr[j]] <span class="op">=</span> diag</span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a>    L_x[(L_indptr[j] <span class="op">+</span> <span class="dv">1</span>):L_indptr[j <span class="op">+</span> <span class="dv">1</span>]] <span class="op">=</span> tmp[<span class="dv">1</span>:] <span class="op">/</span> diag</span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(L_indptr[j] <span class="op">+</span> <span class="dv">1</span>, L_indptr[j <span class="op">+</span> <span class="dv">1</span>]):</span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a>      descendant[L_indices[idx]].append((j, idx))</span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> L_x</span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a><span class="at">@sparse_cholesky_p.def_abstract_eval</span></span>
<span id="cb15-70"><a href="#cb15-70" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sparse_cholesky_abstract_eval(A_indices, A_indptr, A_x, <span class="op">*</span>, L_nse):</span>
<span id="cb15-71"><a href="#cb15-71" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> core.ShapedArray((L_nse,), A_indices.dtype),                   <span class="op">\</span></span>
<span id="cb15-72"><a href="#cb15-72" aria-hidden="true" tabindex="-1"></a>         core.ShapedArray(A_indptr.shape, A_indptr.dtype),             <span class="op">\</span></span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true" tabindex="-1"></a>         core.ShapedArray((L_nse,), A_x.dtype)</span></code></pre></div>
</div>
</details>
<h3
id="why-do-we-need-a-new-pattern-for-this-very-very-similar-problem">Why
do we need a new pattern for this very very similar problem?</h3>
<p>Ok. So now on to the details. If we try to repeat our previous
pattern it would look like this.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>ef sparse_solve_value_and_jvp(arg_values, arg_tangents, <span class="op">*</span>, L_nse):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">&quot;&quot;&quot; </span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co">  Jax-traceable jacobian-vector product implmentation for sparse_solve.</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co">  &quot;&quot;&quot;</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>  A_indices, A_indptr, A_x, b <span class="op">=</span> arg_values</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>  _, _, A_xt, bt <span class="op">=</span> arg_tangents</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Needed for shared computation</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>  L_indices, L_indptr, L_x <span class="op">=</span> sparse_cholesky(A_indices, A_indptr, A_x)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Make the primal</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>  primal_out <span class="op">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, b, transpose <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>  primal_out <span class="op">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, primal_out, transpose <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">type</span>(A_xt) <span class="kw">is</span> <span class="kw">not</span> ad.Zero:</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>    Delta_lower <span class="op">=</span> jsparse.CSC((A_xt, A_indices, A_indptr), shape <span class="op">=</span> (b.shape[<span class="dv">0</span>], b.shape[<span class="dv">0</span>]))</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We need to do Delta @ primal_out, but we only have the lower triangle</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>    rhs <span class="op">=</span> Delta_lower <span class="op">@</span> primal_out <span class="op">+</span> Delta_lower.transpose() <span class="op">@</span> primal_out <span class="op">-</span> A_xt[A_indptr[:<span class="op">-</span><span class="dv">1</span>]] <span class="op">*</span> primal_out</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>    jvp_Ax <span class="op">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, rhs)</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>    jvp_Ax <span class="op">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, jvp_Ax, transpose <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    jvp_Ax <span class="op">=</span> lax.zeros_like_array(primal_out)</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">type</span>(bt) <span class="kw">is</span> <span class="kw">not</span> ad.Zero:</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>    jvp_b <span class="op">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, bt)</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>    jvp_b <span class="op">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, jvp_b, transpose <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>    jvp_b <span class="op">=</span> lax.zeros_like_array(primal_out)</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> primal_out, jvp_b <span class="op">-</span> jvp_Ax</span></code></pre></div>
</div>
<p>That’s all well and good. Nothing weird there.</p>
<p>The problem comes when you need to implement the transposition rule.
Remembering that <span class="math inline">\(\bar b = A^{-T}\bar c =
A^{-1}\bar c\)</span>, you might see the issue: we are going to need the
Cholesky factorisation. <em>But we have no way to pass</em> <span
class="math inline">\(L\)</span> <em>to the transpose function</em>.</p>
<p>This means that we would need to compute <em>two</em> Cholesky
factorisations per gradient instead of one. As the Cholesky
factorisation is our slowest operation, we do not want to do extra ones!
We want to compute the Cholesky triangle once and pass it around like a
party bottom<a href="#fn27" class="footnote-ref" id="fnref27"
role="doc-noteref"><sup>27</sup></a>. We do not want each of our
functions to have to make a deep and meaningful connection with the damn
matrix<a href="#fn28" class="footnote-ref" id="fnref28"
role="doc-noteref"><sup>28</sup></a>.</p>
<h3 id="a-different-solution">A different solution</h3>
<p>So how do we pass around our Cholesky triangle? Well, I do love a
good class so my first thought was “fuck it. I’ll make a class and I’ll
pass it that way”. But the developers of JAX had a <em>much</em> better
idea.</p>
<p>Their idea was to abstract the idea of a linear solve and its
gradients. They do this through <code>lax.custom_linear_solve</code>.
This is a function that takes all of the bits that you would need to
compute <span class="math inline">\(A^{-1}b\)</span> and all of its
derivatives. In particular it takes<a href="#fn29" class="footnote-ref"
id="fnref29" role="doc-noteref"><sup>29</sup></a>:</p>
<ul>
<li><code>matvec</code>: A function that <code>matvec(x)</code> that
computes <span class="math inline">\(Ax\)</span>. This might seem a bit
weird, but it’s the most common atrocity committed by mathematicians is
abstracting<a href="#fn30" class="footnote-ref" id="fnref30"
role="doc-noteref"><sup>30</sup></a> a matrix to a linear mapping. So we
might as well just suck it up.</li>
<li><code>b</code>: The right hand side vector<a href="#fn31"
class="footnote-ref" id="fnref31"
role="doc-noteref"><sup>31</sup></a></li>
<li><code>solve</code>: A function that takes takes the
<code>matvec</code> and a vector so that<a href="#fn32"
class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a>
<code>solve(matvec, matvec(x)) == x</code></li>
<li><code>symmetric</code>: A boolean indicating if <span
class="math inline">\(A\)</span> is symmetric.</li>
</ul>
<p>The idea (happily copped from the implementation of
<code>jax.scipy.linalg.solve</code>) is to wrap our Cholesky
decomposition in the solve function. Through the never ending miracle of
partial evaluation.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sparse_solve(A_indices, A_indptr, A_x, b, <span class="op">*</span>, L_nse <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co">  A JAX-traceable sparse solve. For this moment, only for vector b</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co">  &quot;&quot;&quot;</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">assert</span> b.shape[<span class="dv">0</span>] <span class="op">==</span> A_indptr.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">assert</span> b.ndim <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>  L_indices, L_indptr, L_x <span class="op">=</span> sparse_cholesky(</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    lax.stop_gradient(A_indices), </span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    lax.stop_gradient(A_indptr), </span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    lax.stop_gradient(A_x), L_nse <span class="op">=</span> L_nse)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> chol_solve(L_indices, L_indptr, L_x, b):</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, b, transpose <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sparse_triangular_solve(L_indices, L_indptr, L_x, out, transpose <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> matmult(A_indices, A_indptr, A_x, b):</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    A_lower <span class="op">=</span> jsparse.CSC((A_x, A_indices, A_indptr), shape <span class="op">=</span> (b.shape[<span class="dv">0</span>], b.shape[<span class="dv">0</span>]))</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> A_lower <span class="op">@</span> b <span class="op">+</span> A_lower.transpose() <span class="op">@</span> b <span class="op">-</span> A_x[A_indptr[:<span class="op">-</span><span class="dv">1</span>]] <span class="op">*</span> b</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>  solver <span class="op">=</span> partial(</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    lax.custom_linear_solve,</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">lambda</span> x: matmult(A_indices, A_indptr, A_x, x),</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    solve <span class="op">=</span> <span class="kw">lambda</span> _, x: chol_solve(L_indices, L_indptr, L_x, x),</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>    symmetric <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> solver(b)</span></code></pre></div>
</div>
<p>There are three things of note in that implementation.</p>
<ol type="1">
<li><p>The calls to <code>lax.stop_gradient()</code>: These tell JAX to
not bother computing the gradient of these terms. The relevant parts of
the derivatives are computed explicitly by
<code>lax.custom_linear_solve</code> in terms of <code>matmult</code>
and <code>solve</code>, neither of which need the explicit derivative of
the cholesky factorisation.!</p></li>
<li><p>That definition of <code>matmult()</code><a href="#fn33"
class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a>:
Look. I don’t know what to tell you. Neither addition nor indexing is
implemented for <code>jsparse.CSC</code> objects. So we did it the
semi-manual way. (I am thankful that matrix-vector multiplication is
available)</p></li>
<li><p>The definition of <code>solver()</code>: Partial evaluation is a
wonderful wonderful thing. <code>functools.partial()</code> transforms
<code>lax.custom_linear_solve()</code> from a function that takes 3
arguments (and some keywords), into a function <code>solver()</code>
that takes one<a href="#fn34" class="footnote-ref" id="fnref34"
role="doc-noteref"><sup>34</sup></a> argument<a href="#fn35"
class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a>
(<code>b</code>, the only positional argument of
<code>lax.custom_linear_solve()</code> that isn’t specified).</p></li>
</ol>
<h3 id="does-it-work">Does it work?</h3>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(theta):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> jnp.array(theta[<span class="dv">0</span>] <span class="op">*</span> A_x)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> Ax_theta.at[A_indptr[:<span class="op">-</span><span class="dv">1</span>]].add(theta[<span class="dv">1</span>])</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> jnp.ones(<span class="dv">100</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> sparse_solve(A_indices, A_indptr, Ax_theta, b)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f_jax(theta):</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> jnp.array(theta[<span class="dv">0</span>] <span class="op">*</span> A.todense())</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> Ax_theta.at[np.arange(<span class="dv">100</span>),np.arange(<span class="dv">100</span>)].add(theta[<span class="dv">1</span>])</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> jnp.ones(<span class="dv">100</span>)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> jsp.linalg.solve(Ax_theta, b)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> g(theta):</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> jnp.array(A_x)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> jnp.ones(<span class="dv">100</span>)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> b.at[<span class="dv">0</span>].<span class="bu">set</span>(theta[<span class="dv">0</span>])</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> b.at[<span class="dv">51</span>].<span class="bu">set</span>(theta[<span class="dv">1</span>])</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> sparse_solve(A_indices, A_indptr, Ax_theta, b)</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> g_jax(theta):</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> jnp.array(A.todense())</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> jnp.ones(<span class="dv">100</span>)</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> b.at[<span class="dv">0</span>].<span class="bu">set</span>(theta[<span class="dv">0</span>])</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> b.at[<span class="dv">51</span>].<span class="bu">set</span>(theta[<span class="dv">1</span>])</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> jsp.linalg.solve(Ax_theta, b)</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> h(theta):</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> jnp.array(A_x)</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> Ax_theta.at[A_indptr[:<span class="op">-</span><span class="dv">1</span>]].add(theta[<span class="dv">0</span>])</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> jnp.ones(<span class="dv">100</span>)</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> b.at[<span class="dv">51</span>].<span class="bu">set</span>(theta[<span class="dv">1</span>])</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> sparse_solve(A_indices, A_indptr, Ax_theta, b)</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> h_jax(theta):</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> jnp.array(A.todense())</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> Ax_theta.at[np.arange(<span class="dv">100</span>),np.arange(<span class="dv">100</span>)].add(theta[<span class="dv">0</span>])</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> jnp.ones(<span class="dv">100</span>)</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> b.at[<span class="dv">51</span>].<span class="bu">set</span>(theta[<span class="dv">1</span>])</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> jsp.linalg.solve(Ax_theta, b)</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>primal1, jvp1 <span class="op">=</span> jvp(f, (jnp.array([<span class="fl">2.</span>, <span class="fl">3.</span>]),), (jnp.array([<span class="fl">1.</span>, <span class="fl">2.</span>]),))</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>primal2, jvp2 <span class="op">=</span> jvp(f_jax, (jnp.array([<span class="fl">2.</span>, <span class="fl">3.</span>]),), (jnp.array([<span class="fl">1.</span>, <span class="fl">2.</span>]),))</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>grad1 <span class="op">=</span> grad(<span class="kw">lambda</span> x: jnp.mean(f(x)))(jnp.array([<span class="fl">2.</span>, <span class="fl">3.</span>]))</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>grad2 <span class="op">=</span> grad(<span class="kw">lambda</span> x: jnp.mean(f_jax(x)))(jnp.array([<span class="fl">2.</span>, <span class="fl">3.</span>]))</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>primal3, jvp3 <span class="op">=</span> jvp(g, (jnp.array([<span class="op">-</span><span class="fl">142.</span>, <span class="fl">342.</span>]),), (jnp.array([<span class="fl">1.</span>, <span class="fl">2.</span>]),))</span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>primal4, jvp4 <span class="op">=</span> jvp(g_jax, (jnp.array([<span class="op">-</span><span class="fl">142.</span>, <span class="fl">342.</span>]),), (jnp.array([<span class="fl">1.</span>, <span class="fl">2.</span>]),))</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>grad3 <span class="op">=</span> grad(<span class="kw">lambda</span> x: jnp.mean(g(x)))(jnp.array([<span class="op">-</span><span class="fl">142.</span>, <span class="fl">342.</span>]))</span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a>grad4 <span class="op">=</span> grad(<span class="kw">lambda</span> x: jnp.mean(g_jax(x)))(jnp.array([<span class="op">-</span><span class="fl">142.</span>, <span class="fl">342.</span>]))</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>primal5, jvp5 <span class="op">=</span> jvp(h, (jnp.array([<span class="fl">2.</span>, <span class="fl">342.</span>]),), (jnp.array([<span class="fl">1.</span>, <span class="fl">2.</span>]),))</span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>primal6, jvp6 <span class="op">=</span> jvp(h_jax, (jnp.array([<span class="fl">2.</span>, <span class="fl">342.</span>]),), (jnp.array([<span class="fl">1.</span>, <span class="fl">2.</span>]),))</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>grad5 <span class="op">=</span> grad(<span class="kw">lambda</span> x: jnp.mean(f(x)))(jnp.array([<span class="fl">2.</span>, <span class="fl">342.</span>]))</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a>grad6 <span class="op">=</span> grad(<span class="kw">lambda</span> x: jnp.mean(f_jax(x)))(jnp.array([<span class="fl">2.</span>, <span class="fl">342.</span>]))</span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;&quot;&quot;</span></span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a><span class="ss">Check the plumbing!</span></span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a><span class="ss">Variable A:</span></span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a><span class="ss">  Primal difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(primal1 <span class="op">-</span> primal2)<span class="sc">: .2e}</span></span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a><span class="ss">  JVP difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(jvp1 <span class="op">-</span> jvp2)<span class="sc">: .2e}</span></span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a><span class="ss">  Gradient difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(grad1 <span class="op">-</span> grad2)<span class="sc">: .2e}</span></span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a><span class="ss">  </span></span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a><span class="ss">Variable b:</span></span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a><span class="ss">  Primal difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(primal3 <span class="op">-</span> primal4)<span class="sc">: .2e}</span></span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a><span class="ss">  JVP difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(jvp3 <span class="op">-</span> jvp4)<span class="sc">: .2e}</span></span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a><span class="ss">  Gradient difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(grad3 <span class="op">-</span> grad4)<span class="sc">: .2e}</span><span class="ss"> </span></span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a><span class="ss">    </span></span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a><span class="ss">Variable A and b:</span></span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a><span class="ss">  Primal difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(primal5 <span class="op">-</span> primal6)<span class="sc">: .2e}</span></span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a><span class="ss">  JVP difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(jvp5 <span class="op">-</span> jvp6)<span class="sc">: .2e}</span></span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a><span class="ss">  Gradient difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(grad5 <span class="op">-</span> grad6)<span class="sc">: .2e}</span></span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true" tabindex="-1"></a><span class="ss">  &quot;&quot;&quot;</span>)</span></code></pre></div>
<pre><code>
Check the plumbing!
Variable A:
  Primal difference:  2.77e-07
  JVP difference:  1.99e-07
  Gradient difference:  2.27e-08
  
Variable b:
  Primal difference:  1.14e-05
  JVP difference:  1.26e-07
  Gradient difference:  9.31e-10 
    
Variable A and b:
  Primal difference:  4.59e-06
  JVP difference:  1.47e-06
  Gradient difference:  2.03e-12
  </code></pre>
</div>
<p>Yes.</p>
<h3
id="why-is-this-better-than-just-differentiating-through-the-cholesky-factorisation">Why
is this better than just differentiating through the Cholesky
factorisation?</h3>
<p>The other option for making this work would’ve been to implement the
Cholesky factorisation as a primitive (~which we are about to do!~ which
we will do another day) and then write the sparse solver directly as a
pure JAX function.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sparse_solve_direct(A_indices, A_indptr, A_x, b, <span class="op">*</span>, L_nse <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>  L_indices, L_indptr, L_x <span class="op">=</span> sparse_cholesky(A_indices, A_indptr, A_x)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>  out <span class="op">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, b)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> sparse_triangular_solve(L_indices, L_indptr, L_x, out, transpose <span class="op">=</span> <span class="va">True</span>)</span></code></pre></div>
</div>
<p>This function is JAX-traceable<a href="#fn36" class="footnote-ref"
id="fnref36" role="doc-noteref"><sup>36</sup></a> and, therefore, we
could compute the gradient of it directly. It turns out that this is
going to be a bad idea.</p>
<p>Why? Because the derivative of <code>sparse_cholesky</code>, which we
would have to chain together with the derivatives from the solver, is
pretty complicated. Basically, this means that we’d have to do a lot
more work<a href="#fn37" class="footnote-ref" id="fnref37"
role="doc-noteref"><sup>37</sup></a> than we do if we just implement the
symbolic formula for the derivatives.</p>
<h2 id="primitive-3-the-dreaded-log-determinant">Primitive 3: The
dreaded log determinant</h2>
<p>Ok, so now we get to the good one. The log-determinant of <span
class="math inline">\(A\)</span>. The first thing that we need to do is
wrench out a derivative. This is not as easy as it was for the linear
solve. So what follows is a modification for sparse matrices from
Appendix A of <a
href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Boyd’s
convex optimisation book</a>.</p>
<p>It’s pretty easy to convince yourself that <span
class="math display">\[\begin{align*}
\log(|A + \Delta|) &amp;= \log\left( \left|A^{1/2}(I + A^{-1/2}\Delta
A^{-1/2})A^{1/2}\right|\right) \\
&amp;= \log(|A|) + \log\left( \left|I + A^{-1/2}\Delta
A^{-1/2}\right|\right).
\end{align*}\]</span></p>
<p>It is harder to convince yourself how this could possibly be a useful
fact.</p>
<p>If we write <span class="math inline">\(\lambda_i\)</span>, <span
class="math inline">\(i = 1, \ldots, n\)</span> as the eigenvalues of
<span class="math inline">\(A^{-1/2}\Delta A^{-1/2}\)</span>, then we
have <span class="math display">\[
\log(|A + \Delta |) = \log(|A|) + \sum_{i=1}^n \log( 1 + \lambda_i).
\]</span> Remembering that <span class="math inline">\(\Delta\)</span>
is very small, it follows that <span
class="math inline">\(A^{-1/2}\Delta A^{-1/2}\)</span> will
<em>also</em> be small. That translates to the eigenvalues of <span
class="math inline">\(A^{-1/2}\Delta A^{-1/2}\)</span> all being small.
Therefore, we can use the approximation <span
class="math inline">\(\log(1 + \lambda_i) = \lambda_i +
\mathcal{O}(\lambda_i^2)\)</span>.</p>
<p>This means that<a href="#fn38" class="footnote-ref" id="fnref38"
role="doc-noteref"><sup>38</sup></a> <span
class="math display">\[\begin{align*}
\log(|A + \Delta |) &amp;= \log(|A|) + \sum_{i=1}^n  \lambda_i +
\mathcal{O}\left(\|\Delta\|^2\right) \\
&amp;=\log(|A|) + \operatorname{tr}\left(A^{-1/2} \Delta A^{-1} \right)
+ \mathcal{O}\left(\|\Delta\|^2\right) \\
&amp;= \log(|A|) + \operatorname{tr}\left(A^{-1} \Delta \right) +
\mathcal{O}\left(\|\Delta\|^2\right),
\end{align*}\]</span> which follows from the cyclic property of the
trace.</p>
<p>If we recall the formula from the last section defining the
Jacobian-vector product, in our context <span class="math inline">\(m =
1\)</span>, <span class="math inline">\(x\)</span> is the vector of
non-zero entries of the lower triangle of <span
class="math inline">\(A\)</span> stacked by column, and <span
class="math inline">\(\delta\)</span> is the vector of non-zero entries
of the lower triangle of <span class="math inline">\(\Delta\)</span>.
That means the Jacobian-vector product is <span class="math display">\[
J(x)\delta = \operatorname{tr}\left(A^{-1} \Delta \right) =
\sum_{i=1}^n\sum_{j=1}^n[A^{-1}]_{ij} \Delta_{ij}.
\]</span></p>
<p>Remembering that <span class="math inline">\(\Delta\)</span> is
sparse with the same sparsity pattern as <span
class="math inline">\(A\)</span>, we see that the Jacobian-vector
product requires us to know the values of <span
class="math inline">\(A^{-1}\)</span> that correspond to non-zero
elements of <span class="math inline">\(A\)</span>. That’s good news
because we will see that these entries are relatively cheap and easy to
compute. Whereas the full inverse is dense and very expensive to
compute.</p>
<p>But before we get to that, I need to point out a trap for young
players<a href="#fn39" class="footnote-ref" id="fnref39"
role="doc-noteref"><sup>39</sup></a>. Lest your implementations go down
faster than me when someone asks politely.</p>
<p>The problem comes from how we store our matrix. A mathematician would
suggest that it’s our representation. A physicist<a href="#fn40"
class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a>
would shit on about being coordinate free with such passion that he<a
href="#fn41" class="footnote-ref" id="fnref41"
role="doc-noteref"><sup>41</sup></a> will keep going even after you
quietly leave the room.</p>
<p>The problem is that we only store the non-zero entries of the
lower-triangular part of <span class="math inline">\(A\)</span>. This
means that <em>we need to be careful</em> that when we compute the
Jacobian-vector product that we properly compute the Matrix-vector
product.</p>
<p>Let <code>A_indices</code> and <code>A_indptr</code> define the
sparsity structure of <span class="math inline">\(A\)</span> (and <span
class="math inline">\(\Delta\)</span>). Then if <span
class="math inline">\(A_x\)</span> is our input and <span
class="math inline">\(v\)</span> is our vector, then we need to do the
follow steps to compute the Jacobian-vector product:</p>
<ol type="1">
<li>Compute <code>Ainv_x</code> (aka the non-zero elements of <span
class="math inline">\(A^{-1}\)</span> that correspond to the sparsity
pattern of <span class="math inline">\(A\)</span>)</li>
<li>Compute the matrix vector product as</li>
</ol>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>jvp <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> <span class="bu">sum</span>(Ainv_x <span class="op">*</span> v) <span class="op">-</span> <span class="bu">sum</span>(Ainv_x[A_indptr[:<span class="op">-</span><span class="dv">1</span>]] <span class="op">*</span> v[A_indptr[:<span class="op">-</span><span class="dv">1</span>]])</span></code></pre></div>
</div>
<p>Why does it look like that? Well we need to add the contribution from
the upper triangle as well as the lower triangle. And one way to do that
is to just double the sum and then subtract off the diagonal terms that
we’ve counted twice.</p>
<p>(I’m making a pretty big assumption here, which is fine in our
context, that <span class="math inline">\(A\)</span> has a non-zero
diagonal. If that doesn’t hold, it’s just a change of the indexing in
the second term to just pull out the diagonal terms.)</p>
<p>Using similar reasoning, we can compute the Jacobian as <span
class="math display">\[
[J_f(x)]_{i1} = \begin{cases}
\operatorname{partial-inverse}(x)_i, \qquad &amp; x_i  \text{ is a
diagonal element of }A \\
2\operatorname{partial-inverse}(x)_i, \qquad &amp; \text{otherwise},
\end{cases}
\]</span> where <span
class="math inline">\(\operatorname{partial-inverse}(x)\)</span> is the
vector that stacks the columns of the elements of <span
class="math inline">\(A^{-1}\)</span> that correspond to the non-zero
elements of <span class="math inline">\(A\)</span>. (Yikes!)</p>
<h3 id="computing-the-partial-inverse">Computing the partial
inverse</h3>
<p>So now we need to actually work out how to compute this <em>partial
inverse</em> of a symmetric positive definite matrix <span
class="math inline">\(A\)</span>. To do this, we are going to steal a
technique that goes back to Takahashi, Fagan, and Chen<a href="#fn42"
class="footnote-ref" id="fnref42" role="doc-noteref"><sup>42</sup></a>
in 1973. (For this presentation, I’m basically pillaging <a
href="https://www.sciencedirect.com/science/article/pii/S0378375807000845">Håvard
Rue and Sara Martino’s 2007 paper.</a>)</p>
<p>Their idea was that if we write <span class="math inline">\(A =
VDV^T\)</span>, where <span class="math inline">\(V\)</span> is a
lower-triangular matrix with ones on the diagonal and <span
class="math inline">\(D\)</span> is diagonal. This links up with our
usual Cholesky factorisation through the identity <span
class="math inline">\(L = VD^{1/2}\)</span>. It follows that if <span
class="math inline">\(S = A^{-1}\)</span>, then <span
class="math inline">\(VDV^TS = I\)</span>. Then, we make some magic
manipulations<a href="#fn43" class="footnote-ref" id="fnref43"
role="doc-noteref"><sup>43</sup></a>. <span
class="math display">\[\begin{align*}
V^TS &amp;= D^{-1}V^{-1} \\
S + V^TS &amp;= S + D^{-1}V^{-1} \\
S &amp;= D^{-1}V^{-1} + (I - V^T)S.
\end{align*}\]</span></p>
<p>Once again, this does not look super-useful. The trick is to notice 2
things.</p>
<ol type="1">
<li><p>Because <span class="math inline">\(V\)</span> is lower
triangular, <span class="math inline">\(V^{-1}\)</span> is also lower
triangular and the elements of <span
class="math inline">\(V^{-1}\)</span> are the inverse of the diagonal
elements of <span class="math inline">\(V\)</span> (aka they are all 1).
Therefore, <span class="math inline">\(D^{-1}V^{-1}\)</span> is a lower
triangular matrix with a diagonal given by the diagonal of <span
class="math inline">\(D^{-1}\)</span>.</p></li>
<li><p><span class="math inline">\(I - V^T\)</span> is an upper
triangular matrix and <span class="math inline">\([I - V^T]_{nn} =
0\)</span>.</p></li>
</ol>
<p>These two things together lead to the somewhat unexpected situation
where the upper triangle of <span class="math inline">\(S = D^{-1}V^{-1}
+ (I- V^T)S\)</span> defines a set of recursions for the upper triangle
of <span class="math inline">\(S\)</span>. (And, therefore, all of <span
class="math inline">\(S\)</span> because <span
class="math inline">\(S\)</span> is symmetric!) These are sometimes
referred to as the Takahashi recursions.</p>
<p>But we don’t want the whole upper triangle of <span
class="math inline">\(S\)</span>, we just want the ones that correspond
to the non-zero elements of <span class="math inline">\(A\)</span>.
Unfortunately, the set of recursions are not, in general, solveable
using only that subset of <span class="math inline">\(S\)</span>. But we
are in luck: they are solveable using the elements of <span
class="math inline">\(S\)</span> that correspond to the non-zeros of
<span class="math inline">\(L + L^T\)</span>, which, as we know from a
few posts ago, is a superset of the non-zero elements of <span
class="math inline">\(A\)</span>!</p>
<p>From this, we get the recursions running from <span
class="math inline">\(i = n, \ldots, 1\)</span>, <span
class="math inline">\(j = n, \ldots, i\)</span> (the order is
important!) such that <span class="math inline">\(L_{ji} \neq 0\)</span>
<span class="math display">\[
S_{ji} =   \begin{cases}
\frac{1}{L_{ii}^2} - \frac{1}{L_{ii}}\sum_{k=i+1}^{n} L_{ki} S_{kj}
\qquad&amp;  \text{if } i=j, \\         
- \frac{1}{L_{ii}}\sum_{k=i+1}^{n} L_{ki} S_{kj}  &amp;
\text{otherwise}.
\end{cases}
\]</span></p>
<p>If you recall our discussion way back when about the way the non-zero
structure of the <span class="math inline">\(j\)</span> the column of
<span class="math inline">\(L\)</span> relates to the non-zero structure
of the <span class="math inline">\(i\)</span> th column for <span
class="math inline">\(j \geq i\)</span>, it’s clear that we have
computed enough<a href="#fn44" class="footnote-ref" id="fnref44"
role="doc-noteref"><sup>44</sup></a> of <span
class="math inline">\(S\)</span> at every step to complete the
recursions.</p>
<p>Now we just need to Python it. (And thanks to Finn Lindgren who
helped me understand how to implement this, which he may or may not
remember because it happened about five years ago.)</p>
<p>Actually, we need this to be JAX-traceable, so we are going to
implement a very basic primitive. In particular, we don’t need to
implement a derivative or anything like that, just an abstract
evaluation and an implementation.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>sparse_partial_inverse_p <span class="op">=</span> core.Primitive(<span class="st">&quot;sparse_partial_inverse&quot;</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sparse_partial_inverse(L_indices, L_indptr, L_x, out_indices, out_indptr):</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="co">  Computes the elements (out_indices, out_indptr) of the inverse of a sparse matrix (A_indices, A_indptr, A_x)</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co">   with Choleksy factor (L_indices, L_indptr, L_x). (out_indices, out_indptr) is assumed to be either</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co">   the sparsity pattern of A or a subset of it in lower triangular form. </span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co">  &quot;&quot;&quot;</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> sparse_partial_inverse_p.bind(L_indices, L_indptr, L_x, out_indices, out_indptr)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="at">@sparse_partial_inverse_p.def_abstract_eval</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sparse_partial_inverse_abstract_eval(L_indices, L_indptr, L_x, out_indices, out_indptr):</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> abstract_arrays.ShapedArray(out_indices.shape, L_x.dtype)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a><span class="at">@sparse_partial_inverse_p.def_impl</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sparse_partial_inverse_impl(L_indices, L_indptr, L_x, out_indices, out_indptr):</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>  n <span class="op">=</span> <span class="bu">len</span>(L_indptr) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>  Linv <span class="op">=</span> sparse.dok_array((n,n), dtype <span class="op">=</span> L_x.dtype)</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>  counter <span class="op">=</span> <span class="bu">len</span>(L_x) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> col <span class="kw">in</span> <span class="bu">range</span>(n<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> row <span class="kw">in</span> L_indices[L_indptr[col]:L_indptr[col<span class="op">+</span><span class="dv">1</span>]][::<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> row <span class="op">!=</span> col:</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>        Linv[row, col] <span class="op">=</span> Linv[col, row] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>      <span class="cf">else</span>:</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>        Linv[row, col] <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> L_x[L_indptr[col]]<span class="op">**</span><span class="dv">2</span></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>      L_col  <span class="op">=</span> L_x[L_indptr[col]<span class="op">+</span><span class="dv">1</span>:L_indptr[col<span class="op">+</span><span class="dv">1</span>]] <span class="op">/</span> L_x[L_indptr[col]]</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> k, L_kcol <span class="kw">in</span> <span class="bu">zip</span>(L_indices[L_indptr[col]<span class="op">+</span><span class="dv">1</span>:L_indptr[col<span class="op">+</span><span class="dv">1</span>]], L_col):</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>         Linv[col,row] <span class="op">=</span> Linv[row,col] <span class="op">=</span>  Linv[row, col] <span class="op">-</span>  L_kcol <span class="op">*</span> Linv[k, row]</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>  Linv_x <span class="op">=</span> sparse.tril(Linv, <span class="bu">format</span> <span class="op">=</span> <span class="st">&quot;csc&quot;</span>).data</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">len</span>(out_indices) <span class="op">==</span> <span class="bu">len</span>(L_indices):</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Linv_x</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>  out_x <span class="op">=</span> np.zeros(<span class="bu">len</span>(out_indices))</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> col <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>    ind <span class="op">=</span> np.nonzero(np.in1d(L_indices[L_indptr[col]:L_indptr[col<span class="op">+</span><span class="dv">1</span>]],</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>      out_indices[out_indptr[col]:out_indptr[col<span class="op">+</span><span class="dv">1</span>]]))[<span class="dv">0</span>]</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a>    out_x[out_indptr[col]:out_indptr[col<span class="op">+</span><span class="dv">1</span>]] <span class="op">=</span> Linv_x[L_indptr[col] <span class="op">+</span> ind]</span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> out_x</span></code></pre></div>
</div>
<p>The implementation makes use of the<a href="#fn45"
class="footnote-ref" id="fnref45" role="doc-noteref"><sup>45</sup></a>
<em>dictionary of keys</em> representation of a sparse matrix from
<code>scipy.sparse</code>. This is an efficient storage scheme when you
need to modify the sparsity structure (as we are doing here) or do a lot
of indexing. It would definitely be possible to implement this directly
on the CSC data structure, but it gets a little bit tricky to access the
elements of <code>L_inv</code> that are above the diagonal. The
resulting code is honestly a mess and there’s lots of non-local memory
access anyway, so I implemented it this way.</p>
<p>But let’s be honest: this thing is crying out for a proper symmetric
matrix class with sensible reverse iterators. But hey. Python.</p>
<p>The second chunk of the code is just the opposite of our
<code>_structured_copy()</code> function. It takes a matrix with the
sparsity pattern of <span class="math inline">\(L\)</span> and returns
one with the sparsity pattern of <code>out</code> (which is assumed to
be a subset, and is usually the sparsity pattern of <span
class="math inline">\(A\)</span> or a diagonal matrix).</p>
<p>Let’s check that it works.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>A_indices, A_indptr, A_x, A <span class="op">=</span> make_matrix(<span class="dv">15</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(A_indptr) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>L_indices, L_indptr, L_x <span class="op">=</span> sparse_cholesky(A_indices, A_indptr, A_x)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>a_inv_L <span class="op">=</span> sparse_partial_inverse(L_indices, L_indptr, L_x, L_indices, L_indptr)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>col_counts_L <span class="op">=</span> [L_indptr[i<span class="op">+</span><span class="dv">1</span>] <span class="op">-</span> L_indptr[i] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n)]</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>cols_L <span class="op">=</span> np.repeat(<span class="bu">range</span>(n), col_counts_L)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>true_inv <span class="op">=</span> np.linalg.inv(A.todense())</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>truth_L <span class="op">=</span> true_inv[L_indices, cols_L]</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>a_inv_A <span class="op">=</span> sparse_partial_inverse(L_indices, L_indptr, L_x, A_indices, A_indptr)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>col_counts_A <span class="op">=</span> [A_indptr[i<span class="op">+</span><span class="dv">1</span>] <span class="op">-</span> A_indptr[i] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n)]</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>cols_A <span class="op">=</span> np.repeat(<span class="bu">range</span>(n), col_counts_A)</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>truth_A <span class="op">=</span> true_inv[A_indices, cols_A]</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;&quot;&quot;</span></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a><span class="ss">Error in partial inverse (all of L): </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(a_inv_L <span class="op">-</span> truth_L)<span class="sc">: .2e}</span></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a><span class="ss">Error in partial inverse (all of A): </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(a_inv_A <span class="op">-</span> truth_A)<span class="sc">: .2e}</span></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a><span class="ss">&quot;&quot;&quot;</span>)</span></code></pre></div>
<pre><code>
Error in partial inverse (all of L):  1.36e-15
Error in partial inverse (all of A):  1.32e-15</code></pre>
</div>
<h3 id="putting-the-log-determinant-together">Putting the
log-determinant together</h3>
<p>All of our bits are in place, so now all we need is to implement the
primitive for the log-determinant. One nice thing here is that we don’t
need to implement a transposition rule as the function is not
structurally linear in any of its arguments. At this point we take our
small wins where we can get them.</p>
<p>There isn’t anything particularly interesting in the implementation.
But do note that the trace has been implemented in a way that’s aware
that we’re only storing the bottom triangle of <span
class="math inline">\(A\)</span>.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>sparse_log_det_p <span class="op">=</span> core.Primitive(<span class="st">&quot;sparse_log_det&quot;</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sparse_log_det(A_indices, A_indptr, A_x):</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> sparse_log_det_p.bind(A_indices, A_indptr, A_x)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="at">@sparse_log_det_p.def_impl</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sparse_log_det_impl(A_indices, A_indptr, A_x):</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>  L_indices, L_indptr, L_x <span class="op">=</span> sparse_cholesky(A_indices, A_indptr, A_x)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="fl">2.0</span> <span class="op">*</span> jnp.<span class="bu">sum</span>(jnp.log(L_x[L_indptr[:<span class="op">-</span><span class="dv">1</span>]]))</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="at">@sparse_log_det_p.def_abstract_eval</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sparse_log_det_abstract_eval(A_indices, A_indptr, A_x):</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> abstract_arrays.ShapedArray((<span class="dv">1</span>,), A_x.dtype)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sparse_log_det_value_and_jvp(arg_values, arg_tangent):</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>  A_indices, A_indptr, A_x <span class="op">=</span> arg_values</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>  _, _, A_xt <span class="op">=</span> arg_tangent</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>  L_indices, L_indptr, L_x <span class="op">=</span> sparse_cholesky(A_indices, A_indptr, A_x)</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>  value <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> jnp.<span class="bu">sum</span>(jnp.log(L_x[L_indptr[:<span class="op">-</span><span class="dv">1</span>]]))</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>  Ainv_x <span class="op">=</span> sparse_partial_inverse(L_indices, L_indptr, L_x, A_indices, A_indptr)</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>  jvp <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> <span class="bu">sum</span>(Ainv_x <span class="op">*</span> A_xt) <span class="op">-</span> <span class="bu">sum</span>(Ainv_x[A_indptr[:<span class="op">-</span><span class="dv">1</span>]] <span class="op">*</span> A_xt[A_indptr[:<span class="op">-</span><span class="dv">1</span>]])</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> value, jvp</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>ad.primitive_jvps[sparse_log_det_p] <span class="op">=</span> sparse_log_det_value_and_jvp</span></code></pre></div>
</div>
<p>Finally, we can test it out.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>ld_true <span class="op">=</span> np.log(np.linalg.det(A.todense())) <span class="co">#np.sum(np.log(lu.U.diagonal()))</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Error in log-determinant = </span><span class="sc">{</span>ld_true <span class="op">-</span> sparse_log_det(A_indices, A_indptr, A_x)<span class="sc">: .2e}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>Error in log-determinant =  0.00e+00</code></pre>
<div class="sourceCode" id="cb28"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(theta):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> jnp.array(theta[<span class="dv">0</span>] <span class="op">*</span> A_x) <span class="op">/</span> n</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> Ax_theta.at[A_indptr[:<span class="op">-</span><span class="dv">1</span>]].add(theta[<span class="dv">1</span>])</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> sparse_log_det(A_indices, A_indptr, Ax_theta)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f_jax(theta):</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> jnp.array(theta[<span class="dv">0</span>] <span class="op">*</span> A.todense()) <span class="op">/</span> n </span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> Ax_theta.at[np.arange(n),np.arange(n)].add(theta[<span class="dv">1</span>])</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>  L <span class="op">=</span> jnp.linalg.cholesky(Ax_theta)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="fl">2.0</span><span class="op">*</span>jnp.<span class="bu">sum</span>(jnp.log(jnp.diag(L)))</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>primal1, jvp1 <span class="op">=</span> jvp(f, (jnp.array([<span class="fl">2.</span>, <span class="fl">3.</span>]),), (jnp.array([<span class="fl">1.</span>, <span class="fl">2.</span>]),))</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>primal2, jvp2 <span class="op">=</span> jvp(f_jax, (jnp.array([<span class="fl">2.</span>, <span class="fl">3.</span>]),), (jnp.array([<span class="fl">1.</span>, <span class="fl">2.</span>]),))</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>eps <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>jvp_fd <span class="op">=</span> (f(jnp.array([<span class="fl">2.</span>,<span class="fl">3.</span>]) <span class="op">+</span> eps <span class="op">*</span> jnp.array([<span class="fl">1.</span>, <span class="fl">2.</span>]) ) <span class="op">-</span> f(jnp.array([<span class="fl">2.</span>,<span class="fl">3.</span>]))) <span class="op">/</span> eps</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>grad1 <span class="op">=</span> grad(f)(jnp.array([<span class="fl">2.</span>, <span class="fl">3.</span>]))</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>grad2 <span class="op">=</span> grad(f_jax)(jnp.array([<span class="fl">2.</span>, <span class="fl">3.</span>]))</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;&quot;&quot;</span></span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a><span class="ss">Check the Derivatives!</span></span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a><span class="ss">Variable A:</span></span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a><span class="ss">  Primal difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(primal1 <span class="op">-</span> primal2)<span class="sc">}</span></span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a><span class="ss">  JVP difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(jvp1 <span class="op">-</span> jvp2)<span class="sc">}</span></span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a><span class="ss">  JVP difference (FD): </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(jvp1 <span class="op">-</span> jvp_fd)<span class="sc">}</span></span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a><span class="ss">  Gradient difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(grad1 <span class="op">-</span> grad2)<span class="sc">}</span></span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a><span class="ss">&quot;&quot;&quot;</span>)</span></code></pre></div>
<pre><code>
Check the Derivatives!
Variable A:
  Primal difference: 0.0
  JVP difference: 0.000885009765625
  JVP difference (FD): 0.527069091796875
  Gradient difference: 4.577776417136192e-05</code></pre>
</div>
<p>I’m not going to lie, I am <em>not happy</em> with that JVP
difference. I was somewhat concerned that there was a bug somewhere in
my code. I did a little bit of exploring and the error got larger as the
problem got larger. It also depended a little bit more than I was
comfortable on how I had implemented<a href="#fn46" class="footnote-ref"
id="fnref46" role="doc-noteref"><sup>46</sup></a> the baseline dense
version.</p>
<p>That second fact suggested to me that it might be a floating point
problem. By default, JAX uses single precision (32-bit) floating point.
Most modern systems that don’t try and run on GPUs use double precision
(64-bit) floating point. So I tried it with double precision and lo and
behold, the problem disappears.</p>
<p>Matrix factorisations are bloody hard in single precision.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb30"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax.config <span class="im">import</span> config</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>config.update(<span class="st">&quot;jax_enable_x64&quot;</span>, <span class="va">True</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>ld_true <span class="op">=</span> np.log(np.linalg.det(A.todense())) <span class="co">#np.sum(np.log(lu.U.diagonal()))</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Error in log-determinant = </span><span class="sc">{</span>ld_true <span class="op">-</span> sparse_log_det(A_indices, A_indptr, A_x)<span class="sc">: .2e}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>Error in log-determinant =  0.00e+00</code></pre>
<div class="sourceCode" id="cb32"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(theta):</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> jnp.array(theta[<span class="dv">0</span>] <span class="op">*</span> A_x, dtype <span class="op">=</span> jnp.float64) <span class="op">/</span> n</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> Ax_theta.at[A_indptr[:<span class="op">-</span><span class="dv">1</span>]].add(theta[<span class="dv">1</span>])</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> sparse_log_det(A_indices, A_indptr, Ax_theta)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f_jax(theta):</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> jnp.array(theta[<span class="dv">0</span>] <span class="op">*</span> A.todense(), dtype <span class="op">=</span> jnp.float64) <span class="op">/</span> n </span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>  Ax_theta <span class="op">=</span> Ax_theta.at[np.arange(n),np.arange(n)].add(theta[<span class="dv">1</span>])</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>  L <span class="op">=</span> jnp.linalg.cholesky(Ax_theta)</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="fl">2.0</span><span class="op">*</span>jnp.<span class="bu">sum</span>(jnp.log(jnp.diag(L)))</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>primal1, jvp1 <span class="op">=</span> jvp(f, (jnp.array([<span class="fl">2.</span>, <span class="fl">3.</span>], dtype <span class="op">=</span> jnp.float64),), (jnp.array([<span class="fl">1.</span>, <span class="fl">2.</span>], dtype <span class="op">=</span> jnp.float64),))</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>primal2, jvp2 <span class="op">=</span> jvp(f_jax, (jnp.array([<span class="fl">2.</span>, <span class="fl">3.</span>], dtype <span class="op">=</span> jnp.float64),), (jnp.array([<span class="fl">1.</span>, <span class="fl">2.</span>], dtype <span class="op">=</span> jnp.float64),))</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>eps <span class="op">=</span> <span class="fl">1e-7</span></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>jvp_fd <span class="op">=</span> (f(jnp.array([<span class="fl">2.</span>,<span class="fl">3.</span>], dtype <span class="op">=</span> jnp.float64) <span class="op">+</span> eps <span class="op">*</span> jnp.array([<span class="fl">1.</span>, <span class="fl">2.</span>], dtype <span class="op">=</span> jnp.float64) ) <span class="op">-</span> f(jnp.array([<span class="fl">2.</span>,<span class="fl">3.</span>], dtype <span class="op">=</span> jnp.float64))) <span class="op">/</span> eps</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>grad1 <span class="op">=</span> grad(f)(jnp.array([<span class="fl">2.</span>, <span class="fl">3.</span>], dtype <span class="op">=</span> jnp.float64))</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>grad2 <span class="op">=</span> grad(f_jax)(jnp.array([<span class="fl">2.</span>, <span class="fl">3.</span>], dtype <span class="op">=</span> jnp.float64))</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;&quot;&quot;</span></span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a><span class="ss">Check the Derivatives!</span></span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a><span class="ss">Variable A:</span></span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a><span class="ss">  Primal difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(primal1 <span class="op">-</span> primal2)<span class="sc">}</span></span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a><span class="ss">  JVP difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(jvp1 <span class="op">-</span> jvp2)<span class="sc">}</span></span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a><span class="ss">  JVP difference (FD): </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(jvp1 <span class="op">-</span> jvp_fd)<span class="sc">}</span></span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a><span class="ss">  Gradient difference: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(grad1 <span class="op">-</span> grad2)<span class="sc">}</span></span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a><span class="ss">&quot;&quot;&quot;</span>)</span></code></pre></div>
<pre><code>
Check the Derivatives!
Variable A:
  Primal difference: 0.0
  JVP difference: 8.810729923425242e-13
  JVP difference (FD): 4.455924994317684e-06
  Gradient difference: 2.220446049250313e-16</code></pre>
</div>
<p>Much better!</p>
<h2 id="wrapping-up">Wrapping up</h2>
<p>And that is where we will leave it for today. Next up, I’m probably
going to need to do the autodiff for the Cholesky factorisation. It’s
not <em>hard</em>, but it is tedious<a href="#fn47" class="footnote-ref"
id="fnref47" role="doc-noteref"><sup>47</sup></a> and this post is
already very long.</p>
<p>After that we need a few more things:</p>
<ol type="1">
<li><p>Compilation rules for all of these things. For the most part, we
can just wrap the relevant parts of <a
href="https://github.com/libigl/eigen">Eigen</a>. The only non-trivial
code would be the partial inverse. That will allow us to JIT
shit.</p></li>
<li><p>We need to beef up the sparse matrix class a little. In
particular, we are going to need addition and scalar multiplication at
the very minimum to make this useful.</p></li>
<li><p>Work out how <a
href="https://aesara.readthedocs.io/en/latest/">Aesara</a> works so we
can try to prototype a PyMC model.</p></li>
</ol>
<p>That will be <em>a lot</em> more blog posts. But I’m having fun. So
why the hell not.</p>
<div class="sourceCode" id="cb34"><pre
class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<section class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>I am sorry Cholesky factorisation,
this blog is already too long and there is simply too much code I need
to make nicer to even start on that journey. So it will happen in a
later blog.<a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Which I have spent <em>zero</em>
effort making pretty or taking to any level above scratch code<a
href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Like making it clear how this works
for a <em>sparse</em> matrix compared to a general one<a href="#fnref3"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>To the best of my knowledge, for
example, we don’t know how to differentiate with respect to the order
parameter <span class="math inline">\(\nu\)</span> in the modified
Bessel function of the second kind <span
class="math inline">\(K_\nu(x)\)</span>. This is important in spatial
statistics (and general GP stuff).<a href="#fnref4"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p><em>You</em> may need to convince
yourself that this is possible. But it is. The cone of SPD matrices is
very nice.<a href="#fnref5" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>Don’t despair if you don’t recognise
the third line, it’s the Neumann series, which gives an approximation to
<span class="math inline">\((I + B)^{-1}\)</span> whenever <span
class="math inline">\(\|B\| \ll 1\)</span>.<a href="#fnref6"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>I recognise that I’ve not explained
why everything needs to be JAX-traceable. Basically it’s because JAX
does clever transformations to the Jacobian-vector product code to
produce things like gradients. And the only way that can happen is if
the JVP code can take abstract JAX types. So we need to make it
traceable because we <em>really</em> want to have gradients!<a
href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p>Why not now, Daniel? Why not now?
Well mostly because I might need to do some tweaking down the line, so I
am not messing around until I am done.<a href="#fnref8"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>This is the primary difference
between implementing forward mode and reverse mode: there is only one
output here. When we move onto reverse mode, we will output a tuple
Jacobian-transpose-vector products, one for each input. You can see the
structure of that reflected in the transposition rule we are going to
write later.<a href="#fnref9" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p>Some things: Firstly your function
needs to have the correct signature for this to work. Secondly, you
could also use <code>ad.defjvp()</code> if you didn’t need to use the
primal value to define the tangent (recall one of our tangents is <span
class="math inline">\(A^{-1}\Delta c\)</span>, where <span
class="math inline">\(c = A^{-1}b\)</span> is the primal value).<a
href="#fnref10" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn11" role="doc-endnote"><p>This is because it is the efficient
way of computing a gradient. Forward-mode autodiff chains together
Jacobian-vector products in such a way that a single sweep of the entire
function computes a single directional derivative. Reverse-mode autodiff
chains together Jacobian-transpose-vector products (aka vector-Jacobian
products) in such a way that a single sweep produces an entire gradient.
(This happens at the cost of quite a bit of storage.) Depending on what
you are trying to do, you usually want one or the other (or sometimes a
clever combination of both).<a href="#fnref11" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn12" role="doc-endnote"><p>or gradients or some sort of
thing.<a href="#fnref12" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn13" role="doc-endnote"><p>to be honest, in Stan we sometimes
just don’t dick around with the forward-mode autodiff, because gradients
are our bread and butter.<a href="#fnref13" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn14" role="doc-endnote"><p>I mean, love you programming
language people. But fuck me this paper could’ve been written in
Babylonic cuneiform for all I understood it.<a href="#fnref14"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15" role="doc-endnote"><p>That is, if you fix a value of <span
class="math inline">\(y\)</span>, <span class="math inline">\(f_y(x) =
f(x, y)\)</span> is not an affine function.<a href="#fnref15"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16" role="doc-endnote"><p>Details bore me.<a href="#fnref16"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17" role="doc-endnote"><p>In general, there might need to be a
little bit of reshaping, but it’s equivalent.<a href="#fnref17"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18" role="doc-endnote"><p>Have you noticed this is like the
third name I’ve used for this equivalent concept. Or the fourth? The
code calls it a cotangent because that’s another damn synonym. I’m so
very sorry.<a href="#fnref18" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn19" role="doc-endnote"><p>not difficult, I’m just lazy and
Mike does it better that I can. Read his paper.<a href="#fnref19"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20" role="doc-endnote"><p>For sparse matrices it’s just the
non-zero mask of that.<a href="#fnref20" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn21" role="doc-endnote"><p>Yes. I know. Central differences. I
am what I am.<a href="#fnref21" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn22" role="doc-endnote"><p>Some of the stuff I’ve done like
normalising all of the inputs would help make these tests more stable.
You should also just pick up Nick Higham’s backwards error analysis book
to get some ideas of what your guarantees actually are in floating
point, but I truly cannot be bothered. This is scratch code.<a
href="#fnref22" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn23" role="doc-endnote"><p>It should be slightly bigger, it
isn’t.<a href="#fnref23" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn24" role="doc-endnote"><p>The largest number <span
class="math inline">\(\epsilon\)</span> such that
<code>float(1.0) == float(1.0 + machine_eps)</code> in single precision
floating point.<a href="#fnref24" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn25" role="doc-endnote"><p>Fun fact: I implemented this and the
error never spawned, so I guess JAX is keeping the index arrays
concrete, which is very nice of it!<a href="#fnref25"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26" role="doc-endnote"><p>actual damn numbers<a
href="#fnref26" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn27" role="doc-endnote"><p>We want that <a
href="https://youtu.be/wrnUJoj14ag?t=288">auld triangle to go jingle
bloody jangle</a><a href="#fnref27" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn28" role="doc-endnote"><p>We definitely do not want someone to
write an eight hour, two part play that really seems to have the point
of view that our Cholesky triangle deserved his downfall. Espoused while
periodically reading deadshit tumblr posts. I mean, it would win a Tony.
But we still do not want that.<a href="#fnref28" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn29" role="doc-endnote"><p>There are more arguments. Read the
help. This is what we need<a href="#fnref29" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn30" role="doc-endnote"><p>What if I told you that this would
work perfectly well if <span class="math inline">\(A\)</span> was a
linear partial differential operator or an integral operator? Probably
not much because why would you give a shit?<a href="#fnref30"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn31" role="doc-endnote"><p>It can be more general, but it
isn’t<a href="#fnref31" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn32" role="doc-endnote"><p>I think there is a typo in the
docs<a href="#fnref32" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn33" role="doc-endnote"><p>Full disclosure: I screwed this up
multiple times today and my tests caught it. What does that look like?
The derivatives for <span class="math inline">\(A\)</span> being off,
but everything else being good.<a href="#fnref33" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn34" role="doc-endnote"><p>And some optional keyword arguments,
but we don’t need to worry about those<a href="#fnref34"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn35" role="doc-endnote"><p>This is not quite the same but
similar to something that functional programming people call
<em>currying</em>, which was named after famous Australian Olympic
swimmer Lisa Curry.<a href="#fnref35" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn36" role="doc-endnote"><p>and a shitload simpler!<a
href="#fnref36" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn37" role="doc-endnote"><p>And we have to store a bunch more.
This is less of a big deal when <span class="math inline">\(L\)</span>
is sparse, but for an ordinary linear solve, we’d be hauling around an
extra <span class="math inline">\(\mathcal{O}(n^2)\)</span> floats
containing tangents for no good reason.<a href="#fnref37"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn38" role="doc-endnote"><p>If you are worrying about the
suppressed constant, remember that <span
class="math inline">\(A\)</span> (and therefore <span
class="math inline">\(n\)</span> and <span
class="math inline">\(\|A\|\)</span>) is fixed.<a href="#fnref38"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn39" role="doc-endnote"><p>I think I’ve made this mistake about
four times already while writing this blog. So I am going to write it
<em>out</em>.<a href="#fnref39" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn40" role="doc-endnote"><p>Not to “some of my best friends are
physicists”, but I do love them. I just wished a man would talk about me
the way they talk about being coordinate free. Rather than with the same
ambivalence physicist use when speaking about a specific atlas. I’ve
been listening to lesbian folk music all evening. I’m having feelings.<a
href="#fnref40" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn41" role="doc-endnote"><p>pronoun on purpose<a href="#fnref41"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn42" role="doc-endnote"><p>Takahashi, K., Fagan, J., Chen,
M.S., 1973. Formation of a sparse bus impedance matrix and its
application to short circuit study. In: Eighth PICA Conference
Proceedings.IEEE Power Engineering Society, pp. 63–69 (Papers Presented
at the 1973 Power Industry Computer Application Conference in
Minneapolis, MN).<a href="#fnref42" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn43" role="doc-endnote"><p>Thnks to Jerzy Baranowski for
finding a very very bad LaTeX erro that made these queations quite
wrong!<a href="#fnref43" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn44" role="doc-endnote"><p>Indeed, in the notation of post two
<span class="math inline">\(\mathcal{L}_i \cap \{i+1, \dots, n\}
\subseteq \mathcal{L}_j\)</span> for all <span class="math inline">\(i
\leq j\)</span>, where <span
class="math inline">\(\mathcal{L}_i\)</span> is the set of non-zeros in
the <span class="math inline">\(i\)</span>th column of <span
class="math inline">\(L\)</span>.<a href="#fnref44"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn45" role="doc-endnote"><p>The sparse matrix is stored as a
dictionary <code>{(i,j): value}</code>, which is a very natural way to
build a sparse matrix, even if its quite inefficient to do anything with
it in that form.<a href="#fnref45" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn46" role="doc-endnote"><p>You can’t just use
<code>jnp.linalg.det()</code> because there’s a tendency towards
<code>nan</code>s. (The true value is something like 6.1341871^{108}!)<a
href="#fnref46" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn47" role="doc-endnote"><p>Would it be less tedious if my
implementation of the Cholesky was less shit? Yes. But hey. It was the
first non-trivial piece of python code I’d written in more than a decade
(or maybe ever?) so it is what it is. Anyway. I’m gonna run into the
same problem I had in <a
href="https://dansblog.netlify.app/posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/">Part
3</a><a href="#fnref47" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
  <h3 id="updates-and-corrections">Corrections</h3>
  <p>If you see mistakes or want to suggest changes, please <a href="https://github.com/dpsimpson/blog/tree/master/_posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/issues/new">create an issue</a> on the source repository.</p>
  <h3 id="reuse">Reuse</h3>
  <p>Text and figures are licensed under Creative Commons Attribution <a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>. Source code is available at <a href="https://github.com/dpsimpson/blog/tree/master/_posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative">https://github.com/dpsimpson/blog/tree/master/_posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative</a>, unless otherwise noted. The figures that have been reused from other sources don't fall under this license and can be recognized by a note in their caption: "Figure from ...".</p>
  <h3 id="citation">Citation</h3>
  <p>For attribution, please cite this work as</p>
  <pre class="citation-appendix short">Simpson (2022, May 30). Un garçon pas comme les autres (Bayes): Sparse matrices part 6: To catch a derivative, first you've got to think like a derivative. Retrieved from https://dansblog.netlify.app/posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/</pre>
  <p>BibTeX citation</p>
  <pre class="citation-appendix long">@misc{simpson2022sparse,
  author = {Simpson, Dan},
  title = {Un garçon pas comme les autres (Bayes): Sparse matrices part 6: To catch a derivative, first you've got to think like a derivative},
  url = {https://dansblog.netlify.app/posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/},
  year = {2022}
}</pre>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
