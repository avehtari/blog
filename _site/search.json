[
  {
    "objectID": "posts/2021-10-11-n-sane-in-the-membrane/n-sane-in-the-membrane.html",
    "href": "posts/2021-10-11-n-sane-in-the-membrane/n-sane-in-the-membrane.html",
    "title": "\\((n-1)\\)-sane in the membrane",
    "section": "",
    "text": "And when you’re in a period of heavy teaching, every-fucking-thing is about teaching.\nSo this blogpost is about teaching.\nRight now, I’m coming to the end of a second year class called Statistical Thinking. It’s been fun to work out how to teach the material. It’s standard fare: sampling variation, tests, bootstraps1, regression, and just a hint of Bayes in the last 2 weeks that you incentivize by promising a bastard of an exam question. So you know, (arms up even though I’m Catholic) tradition!\n\nIf I were a rich man (Katrina Lenk with a violin)\nThe thing about teaching an intro stats class is that it brings screaming to mind that quote from Bennett’s The History Boys2: (paraphrasing) “How do I define [intro to Statistics]? It’s just one fucking thing after another”.\nConstructing twelve moderately sequential weeks from the whole mass of things that someone being introduced to statistics needs to know is not unlike being thrown in the middle of the lake with nothing but an ice-cream container and a desiccated whale penis: confusing, difficult, and rather damp.\nThe nice thing about building an intro stats course is you’re not alone. You’re adrift in a sea of shit ideas! (Also a lot of good ones3, but don’t ruin my flow!)\nThe trouble is that this sort of course is simultaneously teaching big concepts and complex details. And while it’s not toooooo hard to make the concepts build and reinforce as time inexorably marches on, the techniques and details needed to illuminate the big concepts are not quite as linear.\nThere are two routes through this conundrum: incantations inscribed onto books made of human skin using the blood of sacrificial virgins (aka gathered during engineering statistics service teaching) or computers.\nI went with computers because we are in lockdown and I couldn’t be bothered sourcing and bleeding virgins.\nThe downside is that you need the students to have a grip on R programming (and programmatic thinking). This only happens if the degree you are teaching in is built in such a way that these skills have already been taught. Otherwise you need to teach both (which is very possible, but you need to teach less statistical content).\nThis is not a postmortem on my teaching, but if it were, it would be about that last point.\n\n\nI saw Goody Proctor with the devil!\n\n\n\nA tweet from Sanjay Srivastava\n\n\nThis is a very long way to say I saw a tweet an had feelings.\nBecause I’m thinking about this stuff pretty hard right now, I am (as Hedwig would say) fully dilated.\nAnd my question is what is the use of teaching this distinction? Should anyone bother dividing by \\((n-1)\\) instead of \\(n\\) in their variance estimates?\nWell I guess the first question is is there a difference in this distinction? Let’s do the sort of R experiment I want my students to do!\n\n# Independent samples for a qq-plot!\n# Thanks to Rob Trangucci for catching this!\nlibrary(tidyverse)\nn_sim <- 100000\nn <- 10\nexperiments <- tibble(exp = rep(1:n_sim, each = n),\n                      sample = rnorm(n * n_sim),\n                      sample2 = rnorm(n * n_sim))\n\ncompare <- experiments %>%\n  group_by(exp) %>%\n  summarise(m = mean(sample),\n            m2 = mean(sample2),\n            var_bias = mean((sample - m)^2),\n            z_bias = m / sqrt(mean(var_bias)),\n            z = m2 / sd(sample2))\n\n\ncompare %>% \n  ggplot(aes(sort(z), sort(z_bias))) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0) +\n  theme_bw() + \n  coord_fixed(xlim = c(-2,2), y = c(-2,2))\n\n\n\n\nWell that is clear. There is not.\nOr, well, there is a small difference.\nBut to see it, you need a lot of samples! Why? Well the easy answer is maths.\nFor one thing, when \\(n=10\\), \\[\n\\frac{1}{n} - \\frac{1}{n-1} = \\frac{1}{90} = 0.01.\n\\] This does not compare well against the sampling variance, which (assuming \\(\\sigma^2\\approx 1\\), which is usual if you’ve scaled your problem correctly) is about \\(0.3\\).\nBut we could choose to do it properly. The bias in the MLE (aka the divide by \\(n\\)) variance estimate is \\[\n-\\frac{\\sigma^2}{n}.\n\\] This is a lot smaller than the sampling variability of the estimate (aka how much uncertainty you have because of the finite sample), which is \\[\n\\frac{\\sigma}{\\sqrt{n}}.\n\\]\nAnd that’s the whole story. Dividing by \\(n\\) instead of \\((n-1)\\) leaves you with a slightly biased estimate. But the bias if fucking tiny. It is possibly moving your second decimal place by about 1 number (assume our population variance is one). The sampling variably is moving the first decimal place by several digits.\nTruly. What is the point. The old guys4 who went wild about bias are now mostly dead. Or they’ve changed their minds (which is, you know, a reasonable thing to do as information about best practice is updated). The war against bias was lost before your undergraduates were born.\n\n\nEven in crisis, I maintain\nBut nevertheless, this whole DIVIDE BY N-1 OR THE BIAS MONSTER IS GONNA GET YA bullshit continues.\nAnd to some extent, maybe I shouldn’t care. I definitely shouldn’t care this many words about it.\nBut I do. And I do for a couple of reasons.\nReason One: What is the point teaching students about uncertainty and that you can’t just say “this number is different” because the estimate on a single sample is different. If I am to say that I need things to be at least5 \\(\\mathcal{O}(n^{-1/2})\\) apart before I’m willing to say they are maybe different, then why am I harping on about the much smaller difference?\nReason Two: It’s a shitty example. Bias and bias corrections have a role to play in statistics6. But if this is your first introduction to bias correction, you are going to teach either:\n\nBias is always bad, regardless of context / sampling variance / etc\nBias can be corrected, but it’s trivial and small.\n\nBoth of those things are bullshit. Just teach them how to bootstrap and teach the damn thing properly. You do not have to go very far to show bias actually making a difference!\nMaybe the only place the difference will be noticed is if you compare against the in-build var or sd functions. This is not the use case I would build my class around, but it is a thing you would need to be aware of.\n\n\nThe worlds is a question, this room is an answer. And the answer is no.\nIf you are going to teach statistics as more than just stale incantations and over-done fear-mongering, you need to construct the types of stakes that are simply not present in the \\(n\\) vs \\(n-1\\) bullshit.\nIt is present when you are teaching the normal vs t distribution. You are teaching that the design of your experiment changes the possible extreme behaviour and sometimes it can change a lot.\nThe \\(n\\) vs \\((n-1)\\) denominator for a variance estimator is a curiosity. It is the source of thrilling7 exercises or exam questions. But it is not interesting.\nIt could maybe set up the idea that MLEs are not unbiased. But even then, the useless correction term is not needed. Just let it be slightly biased and move on with your life.\nBecause if that is the biggest bias in your analysis, you are truly blessed.\nIn real life, bias is the price you pay for being good at statistics. And like any market, if you pay too much you’re maybe not that good. But if you pay nothing at all, you don’t get to play.\n\n\n\n\n\nFootnotes\n\n\nTo paraphrase Jimmy Somerville, tell me whyyyyyyyyy about 90% of the bootstrap material on the web is … misguided. And why tidymodels only has the shit bootstrap in it?↩︎\nOk. Straight up, “[Intro to statistics] is a commentary on the various and continuing incapabilities of men” would’ve also worked.↩︎\nThis course stands on the shoulders of giants: Di Cook and Catherine Forbes gave me a great base. And of course every single textbook (shout out to the OpenIntro crew!), blog post, weird subsection of some other book, paper from 1987 on some weird bootstrap, etc that I have used to make a course!↩︎\nYes. I used the word on purpose.↩︎\n\\(n\\) is the size of the sample.↩︎\nI spend most of my time doing Bayes shit, and we play this game somewhat differently. But the gist is the same.↩︎\nNot thrilling.↩︎\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{simpson2021,\n  author = {Dan Simpson},\n  editor = {},\n  title = {\\$(N-1)\\$-Sane in the Membrane},\n  date = {2021-10-14},\n  url = {https://dansblog.netlify.app/n-sane-in-the-membrane},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nDan Simpson. 2021. “$(N-1)$-Sane in the Membrane.” October\n14, 2021. https://dansblog.netlify.app/n-sane-in-the-membrane."
  },
  {
    "objectID": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html",
    "href": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html",
    "title": "Sparse matrices 6: To catch a derivative, first you’ve got to think like a derivative",
    "section": "",
    "text": "Welcome to part six!!! of our ongoing series on making sparse linear algebra differentiable in JAX with the eventual hope to be able to do some cool statistical shit. We are nowhere near done.\nLast time, we looked at making JAX primitives. We built four of them. Today we are going to implement the corresponding differentiation rules! For three1 of them.\nSo strap yourselves in. This is gonna be detailed.\nIf you’re interested in the code2, the git repo for this post is linked at the bottom and in there you will find a folder with the python code in a python file."
  },
  {
    "objectID": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#she-is-beauty-and-she-is-grace.-she-is-queen-of-50-states.-she-is-elegance-and-taste.-she-is-miss-autodiff",
    "href": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#she-is-beauty-and-she-is-grace.-she-is-queen-of-50-states.-she-is-elegance-and-taste.-she-is-miss-autodiff",
    "title": "Sparse matrices 6: To catch a derivative, first you’ve got to think like a derivative",
    "section": "She is beauty and she is grace. She is queen of 50 states. She is elegance and taste. She is miss autodiff",
    "text": "She is beauty and she is grace. She is queen of 50 states. She is elegance and taste. She is miss autodiff\nDerivatives are computed in JAX through the glory and power of automatic differentiation. If you came to this blog hoping for a great description of how autodiff works, I am terribly sorry but I absolutely do not have time for that. Might I suggest google? Or maybe flick through this survey by Charles Margossian..\nThe most important thing to remember about algorithmic differentiation is that it is not symbolic differentiation. That is, it does not create the functional form of the derivative of the function and compute that. Instead, it is a system for cleverly composing derivatives in each bit of the program to compute the value of the derivative of the function.\nBut for that to work, we need to implement those clever little mini-derivatives. In particular, every function \\(f(\\cdot): \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) needs to have a function to compute the corresponding Jacobian-vector product \\[\n(\\theta, v) \\rightarrow J(\\theta) v,\n\\] where the \\(n \\times m\\) matrix \\(J(\\theta)\\) has entries \\[\nJ(\\theta)_{ij} = \\frac{\\partial f_j }{\\partial \\theta_j}.\n\\]\nOk. So let’s get onto this. We are going to derive and implement some Jacobian-vector products. And all of the assorted accoutrement. And by crikey. We are going to do it all in a JAX-traceable way."
  },
  {
    "objectID": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#jvp-number-one-the-linear-solve.",
    "href": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#jvp-number-one-the-linear-solve.",
    "title": "Sparse matrices 6: To catch a derivative, first you’ve got to think like a derivative",
    "section": "JVP number one: The linear solve.",
    "text": "JVP number one: The linear solve.\nThe first of the derivatives that we need to work out is the derivative of a linear solve \\(A^{-1}b\\). Now, intrepid readers, the obvious thing to do is look the damn derivative up. You get exactly no hero points for computing it yourself.\nBut I’m not you, I’m a dickhead.\nSo I’m going to derive it. I could pretend there are reasons3, but that would just be lying. I’m doing it because I can.\nBeyond the obvious fun of working out a matrix derivative from first principles, this is fun because we have two arguments instead of just one. Double the fun.\nAnd we really should make sure the function is differentiated with respect to every reasonable argument. Why? Because if you write code other people might use, you don’t get to control how they use it (or what they will email you about). So it’s always good practice to limit surprises (like a function not being differentiable wrt some argument) to cases4 where it absolutely necessary. This reduces the emails.\nTo that end, let’s take an arbitrary SPD matrix \\(A\\) with a fixed sparsity pattern. Let’s take another symmetric matrix \\(\\Delta\\) with the same sparsity pattern and assume that \\(\\Delta\\) is small enough5 that \\(A + \\Delta\\) is still symmetric positive definite. We also need a vector \\(\\delta\\) with a small \\(\\|\\delta\\|\\).\nNow let’s get algebraing. \\[\\begin{align*}\nf(A + \\Delta, b + \\delta) &= (A+\\Delta)^{-1}(b + \\delta) \\\\\n&= (I + A^{-1}\\Delta)^{-1}A^{-1}(b + \\delta) \\\\\n&= (I - A^{-1}\\Delta + o(\\|\\Delta\\|))A^{-1}(b + \\delta) \\\\\n&= A^{-1}b + A^{-1}(\\delta - \\Delta A^{-1}b ) + o(\\|\\Delta\\| + \\|\\delta\\|)\n\\end{align*}\\]\nEasy6 as.\nWe’ve actually calculated the derivative now, but it’s a little more work to recognise it.\nTo do that, we need to remember the practical definition of the Jacobian of a function \\(f(x)\\) that takes an \\(n\\)-dimensional input and produces an \\(m\\)-dimensional output. It is the \\(n \\times m\\) matrix \\(J_f(x)\\) such that \\[\nf(x + \\delta)  = f(x) + J_f(x)\\delta + o(\\|\\delta\\|).\n\\]\nThe formulas further simplify if we write \\(c = A^{-1}b\\). Then, if we want the Jacobian-vector product for the first argument, it is \\[\n-A^{-1}\\Delta c,\n\\] while the Jacobian-vector product for the second argument is \\[\nA^{-1}\\delta.\n\\]\nThe only wrinkle in doing this is we need to remember that we are only storing the lower triangle of \\(A\\). Because we need to represent \\(\\Delta\\) the same way, it is represented as a vector Delta_x that contains only the lower triangle of \\(\\Delta\\). So we need to make sure we remember to form the whole matrix before we do the matrix-vector product \\(\\Delta c\\)!\nBut otherwise, the implementation is going to be pretty straightforward. The Jacobian-vector product costs one additional linear solve (beyond the one needed to compute the value \\(c = A^{-1}b\\)).\nIn the language of JAX (and autodiff in general), we refer to \\(\\Delta\\) and \\(\\delta\\) as tangent vectors. In search of a moderately coherent naming convention, we are going to refer to the tangent associated with the variable x as xt.\nSo let’s implement this. Remember: it needs7 to be JAX traceable."
  },
  {
    "objectID": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#primitive-two-the-triangular-solve",
    "href": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#primitive-two-the-triangular-solve",
    "title": "Sparse matrices 6: To catch a derivative, first you’ve got to think like a derivative",
    "section": "Primitive two: The triangular solve",
    "text": "Primitive two: The triangular solve\nFor some sense of continuity, we are going to keep the naming of the primitives from the last blog post, but we are not going to attack them in the same order. Why not? Because we work in order of complexity.\nSo first off we are going to do the triangular solve. As I have yet to package up the code (I promise, that will happen next8), I’m just putting it here under the fold.\n\n\nThe primal implementation\n\n\nfrom scipy import sparse\nimport numpy as np\nfrom jax import numpy as jnp\nfrom jax import core\nfrom jax._src import abstract_arrays\nfrom jax import core\n\nsparse_triangular_solve_p = core.Primitive(\"sparse_triangular_solve\")\n\ndef sparse_triangular_solve(L_indices, L_indptr, L_x, b, *, transpose: bool = False):\n  \"\"\"A JAX traceable sparse  triangular solve\"\"\"\n  return sparse_triangular_solve_p.bind(L_indices, L_indptr, L_x, b, transpose = transpose)\n\n@sparse_triangular_solve_p.def_impl\ndef sparse_triangular_solve_impl(L_indices, L_indptr, L_x, b, *, transpose = False):\n  \"\"\"The implementation of the sparse triangular solve. This is not JAX traceable.\"\"\"\n  L = sparse.csc_array((L_x, L_indices, L_indptr)) \n  \n  assert L.shape[0] == L.shape[1]\n  assert L.shape[0] == b.shape[0]\n  \n  if transpose:\n    return sparse.linalg.spsolve_triangular(L.T, b, lower = False)\n  else:\n    return sparse.linalg.spsolve_triangular(L.tocsr(), b, lower = True)\n\n@sparse_triangular_solve_p.def_abstract_eval\ndef sparse_triangular_solve_abstract_eval(L_indices, L_indptr, L_x, b, *, transpose = False):\n  assert L_indices.shape[0] == L_x.shape[0]\n  assert b.shape[0] == L_indptr.shape[0] - 1\n  return abstract_arrays.ShapedArray(b.shape, b.dtype)\n\n\n\nThe Jacobian-vector product\n\nfrom jax._src import ad_util\nfrom jax.interpreters import ad\nfrom jax import lax\nfrom jax.experimental import sparse as jsparse\n\ndef sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent, *, transpose):\n  \"\"\"\n  A jax-traceable jacobian-vector product. In order to make it traceable, \n  we use the experimental sparse CSC matrix in JAX.\n  \n  Input:\n    arg_values:   A tuple of (L_indices, L_indptr, L_x, b) that describe\n                  the triangular matrix L and the rhs vector b\n    arg_tangent:  A tuple of tangent values (same lenght as arg_values).\n                  The first two values are nonsense - we don't differentiate\n                  wrt integers!\n    transpose:    (boolean) If true, solve L^Tx = b. Otherwise solve Lx = b.\n  Output:         A tuple containing the maybe_transpose(L)^{-1}b and the corresponding\n                  Jacobian-vector product.\n  \"\"\"\n  L_indices, L_indptr, L_x, b = arg_values\n  _, _, L_xt, bt = arg_tangent\n  value = sparse_triangular_solve(L_indices, L_indptr, L_x, b, transpose=transpose)\n  if type(bt) is ad.Zero and type(L_xt) is ad.Zero:\n    # I legit do not think this ever happens. But I'm honestly not sure.\n    print(\"I have arrived!\")\n    return value, lax.zeros_like_array(value) \n  \n  if type(L_xt) is not ad.Zero:\n    # L is variable\n    if transpose:\n      Delta = jsparse.CSC((L_xt, L_indices, L_indptr), shape = (b.shape[0], b.shape[0])).transpose()\n    else:\n      Delta = jsparse.CSC((L_xt, L_indices, L_indptr), shape = (b.shape[0], b.shape[0]))\n\n    jvp_Lx = sparse_triangular_solve(L_indices, L_indptr, L_x, Delta @ value, transpose = transpose) \n  else:\n    jvp_Lx = lax.zeros_like_array(value) \n\n  if type(bt) is not ad.Zero:\n    # b is variable\n    jvp_b = sparse_triangular_solve(L_indices, L_indptr, L_x, bt, transpose = transpose)\n  else:\n    jvp_b = lax.zeros_like_array(value)\n\n  return value, jvp_b - jvp_Lx\n\nad.primitive_jvps[sparse_triangular_solve_p] = sparse_triangular_solve_value_and_jvp\n\nBefore we see if this works, let’s first have talk about the structure of the function I just wrote. Generally speaking, we want a function that takes in the primals and tangents at tuples and then returns the value and the9 Jacobian-vector product.\nThe main thing you will notice in the code is that there is a lot of checking for ad.Zero. This is a special type defined in JAX that is, essentially, telling the autodiff system that we are not differentiating wrt that variable. This is different to a tangent that just happens to be numerically equal to zero. Any code for a Jacobian-vector product needs to handle this special value.\nAs we have two arguments, we have 3 interesting options:\n\nBoth L_xt and bt are ad.Zero: This means the function is a constant and the derivative is zero. I am fairly certain that we do not need to manually handle this case, but because I don’t know and I do not like surprises, it’s in there.\nL_xt is not ad.Zero: This means that we need to differentiate wrt the matrix. In this case we need to compute \\(\\Delta c\\) or \\(\\Delta^T c\\), depending on the transpose argument. In order to do this, I used the jax.experimental.sparse.CSC class, which has some very limited sparse matrix support (basically matrix-vector products). This is extremely convenient because it means I don’t need to write the matrix-vector product myself!\nbt is not ad.Zero: This means that we need to differentiate wrt the rhs vector. This part of the formula is pretty straightforward: just an application of the primal.\n\nIn the case that either L_xt or bt are ad.Zero, we simply set the corresponding contribution to the jvp to zero.\nIt’s worth saying that you can bypass all of this ad.Zero logic by writing separate functions for the JVP contribution from each input and then chaining them together using10 ad.defjvp2() to chain them together. This is what the lax.linalg.triangular_solve() implementation does.\nSo why didn’t I do this? I avoided this because in the other primitives I have to implement, there are expensive computations (like Cholesky factorisations) that I want to share between the primal and the various tangent calculations. The ad.defjvp frameworks don’t allow for that. So I decided not to demonstrate/learn two separate patterns.\n\n\nTransposition\nNow I’ve never actively wanted a Jacobian-vector product in my whole life. I’m sorry. I want a gradient. Gimme a gradient. I am the Veruca Salt of gradients.\nIn may autodiff systems, if you want11 a gradient, you need to implement vector-Jacobian products12 explicitly.\nOne of the odder little innovations in JAX is that instead of forcing you to implement this as well13, you only need to implement half of it.\nYou see, some clever analysis that, as far as I far as I can tell14, is detailed in this paper shows that you only need to form explicit vector-Jacobian products for the structurally linear arguments of the function.\nIn JAX (and maybe elsewhere), this is known as a transposition rule. The combination of a transopition rule and a JAX-traceable Jacobian-vector product is enough for JAX to compute all of the directional derivatives and gradients we could ever hope for.\nAs far as I understand, it is all about functions that are structurally linear in some arguments. For instance, if \\(A(x)\\) is a matrix-valued function and \\(x\\) and \\(y\\) are vectors, then the function \\[\nf(x, y) = A(x)y + g(x)\n\\] is structurally linear in \\(y\\) in the sense that for every fixed value of \\(x\\), the function \\[\nf_x(y) = A(x) y + g(x)\n\\] is linear in \\(y\\). The resulting transpositon rule is then\n\ndef f_transpose(x, y):\n  Ax = A(x)\n  gx = g(x)\n  return (None, Ax.T @ y + gx)\n\nThe first element of the return is None because \\(f(x,y)\\) is not15 structurally linear in \\(x\\) so there is nothing to transpose. The second element simply takes the matrix in the linear function and transposes it.\nIf you know anything about autodiff, you’ll think “this doesn’t feel like enough” and it’s not. JAX deals with the non-linear part of \\(f(x,y)\\) by tracing the evaluation tree for its Jacobian-vector product and … manipulating16 it.\nWe already built the abstract evaluation function last time around, so the tracing part can be done. All we need is the transposition rule.\nThe linear solve \\(f(A, b) = A^{-1}b\\) is non-linear in the first argument but linear in the second argument. So we only need to implement \\[\nJ^T_b(A,b)w = A^{-T}w,\n\\] where the subscript \\(b\\) indicates we’re only computing the Jacobian wrt \\(b\\).\nInitially, I struggled to work out what needed to be implemented here. The thing that clarified the process for me was looking at JAX’s internal implementation of the Jacobian-vector product for a dense matrix. From there, I understood what this had to look like for a vector-valued function and this is the result.\n\ndef sparse_triangular_solve_transpose_rule(cotangent, L_indices, L_indptr, L_x, b, *, transpose):\n  \"\"\"\n  Transposition rule for the triangular solve. \n  Translated from here https://github.com/google/jax/blob/41417d70c03b6089c93a42325111a0d8348c2fa3/jax/_src/lax/linalg.py#L747.\n  Inputs:\n    cotangent: Output cotangent (aka adjoint). (produced by JAX)\n    L_indices, L_indptr, L_x: Represenation of sparse matrix. L_x should be concrete\n    b: The right hand side. Must be an jax.interpreters.ad.UndefinedPrimal\n    transpose: (boolean) True: solve $L^Tx = b$. False: Solve $Lx = b$.\n  Output:\n    A 4-tuple with the adjoints (None, None, None, b_adjoint)\n  \"\"\"\n  assert not ad.is_undefined_primal(L_x) and ad.is_undefined_primal(b)\n  if type(cotangent) is ad_util.Zero:\n    cot_b = ad_util.Zero(b.aval)\n  else:\n    cot_b = sparse_triangular_solve(L_indices, L_indptr, L_x, cotangent, transpose = not transpose)\n  return None, None, None, cot_b\n\nad.primitive_transposes[sparse_triangular_solve_p] = sparse_triangular_solve_transpose_rule\n\nIf this doesn’t make a lot of sense to you, that’s because it’s confusing.\nOne way to think of it is in terms of the more ordinary notation. Mike Giles has a classic paper that covers these results for basic linear algebra. The idea is to imagine that, as part of your larger program, you need to compute \\(c = A^{-1}b\\).\nForward-mode autodiff computes the sensitivity of \\(c\\), usually denoted \\(\\dot c\\) from the sensitivies \\(\\dot A\\) and \\(\\dot b\\). These have already been computed. The formula in Giles is \\[\n\\dot c = A^{-1}(\\dot b - \\dot A c).\n\\] The canny reader will recognise this as exactly17 the formula for the Jacobian-vector product.\nSo what does reverse-mode autodiff do? Well it moves through the program in the other direction. So instead of starting with the sensitivities \\(\\dot A\\) and \\(\\dot b\\) already computed, we instead start with the18 adjoint sensitivity \\(\\bar c\\). Our aim is to compute \\(\\bar A\\) and \\(\\bar b\\) from \\(\\bar c\\).\nThe details of how to do this are19 beyond the scope, but without tooooooo much effort you can show that \\[\n\\bar b = A^{-T} \\bar c,\n\\] which you should recognise as the equation that was just implemented.\nThe thing that we do not have to implement in JAX is the other adjoint that, for dense matrices20, is \\[\n\\bar{A} = -\\bar{b}c^T.\n\\] Through the healing power of … something?—Truly I do not know.— JAX can work that bit out itself. woo.\n\n\nTesting the numerical implementation of the Jacobian-vector product\nSo let’s see if this works. I’m not going to lie, I’m flying by the seat of my pants here. I’m not super familiar with the JAX internals, so I have written a lot of test cases. You may wish to skip this part. But rest assured that almost every single one of these cases was useful to me working out how this thing actually worked!\n\ndef make_matrix(n):\n    one_d = sparse.diags([[-1.]*(n-1), [2.]*n, [-1.]*(n-1)], [-1,0,1])\n    A = (sparse.kronsum(one_d, one_d) + sparse.eye(n*n)).tocsc()\n    A_lower = sparse.tril(A, format = \"csc\")\n    A_index = A_lower.indices\n    A_indptr = A_lower.indptr\n    A_x = A_lower.data\n    return (A_index, A_indptr, A_x, A)\n\nA_indices, A_indptr, A_x, A = make_matrix(10)\n\nThis is the same test case as the last blog. We will just use the lower triangle of \\(A\\) as the test matrix.\nFirst things first, let’s check out the numerical implementation of the function. We will do that by comparing the implemented Jacobian-vector product with the definition of the Jacobian-vector product (aka the forward21 difference approximation).\nThere are lots of things that we could do here to turn these into actual tests. For instance, the test suite inside JAX has a lot of nice convenience functions for checking implementations of derivatives. But I went with homespun because that was how I was feeling.\nYou’ll also notice that I’m using random numbers here, which is fine for a blog. Not so fine for a test that you don’t want to be potentially22 flaky.\nThe choice of eps = 1e-4 is roughly23 because it’s the square root of the single precision machine epsilon24. A very rough back of the envelope calculation for the forward difference approximation to the derivative shows that the square root of the machine epislon is about the size you want your perturbation to be.\n\nb = np.random.standard_normal(100)\n\nbt = np.random.standard_normal(100)\nbt /= np.linalg.norm(bt)\n\nA_xt = np.random.standard_normal(len(A_x))\nA_xt /= np.linalg.norm(A_xt)\n\narg_values = (A_indices, A_indptr, A_x, b )\n\narg_tangent_A = (None, None, A_xt, ad.Zero(type(b)))\narg_tangent_b = (None, None, ad.Zero(type(A_xt)), bt)\narg_tangent_Ab = (None, None, A_xt, bt)\n\np, t_A = sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_A, transpose = False)\n_, t_b = sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_b, transpose = False)\n_, t_Ab = sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_Ab, transpose = False)\npT, t_AT = sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_A, transpose = True)\n_, t_bT = sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_b, transpose = True)\n\neps = 1e-4\ntt_A = (sparse_triangular_solve(A_indices, A_indptr, A_x + eps * A_xt, b) - p) /eps\ntt_b = (sparse_triangular_solve(A_indices, A_indptr, A_x, b + eps * bt) - p) / eps\ntt_Ab = (sparse_triangular_solve(A_indices, A_indptr, A_x + eps * A_xt, b + eps * bt) - p) / eps\ntt_AT = (sparse_triangular_solve(A_indices, A_indptr, A_x + eps * A_xt, b, transpose = True) - pT) / eps\ntt_bT = (sparse_triangular_solve(A_indices, A_indptr, A_x, b + eps * bt, transpose = True) - pT) / eps\n\nprint(f\"\"\"\nTranspose = False:\n  Error A varying: {np.linalg.norm(t_A - tt_A): .2e}\n  Error b varying: {np.linalg.norm(t_b - tt_b): .2e}\n  Error A and b varying: {np.linalg.norm(t_Ab - tt_Ab): .2e}\n\nTranspose = True:\n  Error A varying: {np.linalg.norm(t_AT - tt_AT): .2e}\n  Error b varying: {np.linalg.norm(t_bT - tt_bT): .2e}\n\"\"\")\n\n\nTranspose = False:\n  Error A varying:  8.33e-08\n  Error b varying:  0.00e+00\n  Error A and b varying:  4.97e-07\n\nTranspose = True:\n  Error A varying:  9.90e-08\n  Error b varying:  0.00e+00\n\n\n\nBrilliant! Everythign correct withing single precision!\n\n\nChecking on the plumbing\nMaking the numerical implementation work is only half the battle. We also have to make it work in the context of JAX.\nNow I would be lying if I pretended this process went smoothly. But the first time is for experience. It’s mostly a matter of just reading the documentation carefully and going through similar examples that have already been implemented.\nAnd testing. I learnt how this was supposed to work by testing it.\n(For full disclosure, I also wrote a big block f-string in the sparse_triangular_solve() function at one point that told me the types, shapes, and what transpose was, which was how I worked out that my code was breaking because I forgot the first to None outputs in the transposition rule. When it doubt, print shit.)\nAs you will see from my testing code, I was not going for elegance. I was running the damn permutations. If you’re looking for elegance, look elsewhere.\n\nfrom jax import jvp, grad\nfrom jax import scipy as jsp\n\ndef f(theta):\n  Ax_theta = jnp.array(A_x)\n  Ax_theta = Ax_theta.at[A_indptr[20]].add(theta[0])\n  Ax_theta = Ax_theta.at[A_indptr[50]].add(theta[1])\n  b = jnp.ones(100)\n  return sparse_triangular_solve(A_indices, A_indptr, Ax_theta, b, transpose = True)\n\ndef f_jax(theta):\n  Ax_theta = jnp.array(sparse.tril(A).todense())\n  Ax_theta = Ax_theta.at[20,20].add(theta[0])\n  Ax_theta = Ax_theta.at[50,50].add(theta[1])\n  b = jnp.ones(100)\n  return jsp.linalg.solve_triangular(Ax_theta, b, lower = True, trans = \"T\")\n\ndef g(theta):\n  Ax_theta = jnp.array(A_x)\n  b = jnp.ones(100)\n  b = b.at[0].set(theta[0])\n  b = b.at[51].set(theta[1])\n  return sparse_triangular_solve(A_indices, A_indptr, Ax_theta, b, transpose = True)\n\ndef g_jax(theta):\n  Ax_theta = jnp.array(sparse.tril(A).todense())\n  b = jnp.ones(100)\n  b = b.at[0].set(theta[0])\n  b = b.at[51].set(theta[1])\n  return jsp.linalg.solve_triangular(Ax_theta, b, lower = True, trans = \"T\")\n\ndef h(theta):\n  Ax_theta = jnp.array(A_x)\n  Ax_theta = Ax_theta.at[A_indptr[20]].add(theta[0]) \n  b = jnp.ones(100)\n  b = b.at[51].set(theta[1])\n  return sparse_triangular_solve(A_indices, A_indptr, Ax_theta, b, transpose = False)\n\ndef h_jax(theta):\n  Ax_theta = jnp.array(sparse.tril(A).todense())\n  Ax_theta = Ax_theta.at[20,20].add(theta[0])\n  b = jnp.ones(100)\n  b = b.at[51].set(theta[1])\n  return jsp.linalg.solve_triangular(Ax_theta, b, lower = True, trans = \"N\")\n\ndef no_diff(theta):\n  return sparse_triangular_solve(A_indices, A_indptr, A_x, jnp.ones(100), transpose = False)\n\ndef no_diff_jax(theta):\n  return jsp.linalg.solve_triangular(jnp.array(sparse.tril(A).todense()), jnp.ones(100), lower = True, trans = \"N\")\n\nA_indices, A_indptr, A_x, A = make_matrix(10)\nprimal1, jvp1 = jvp(f, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\nprimal2, jvp2 = jvp(f_jax, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\ngrad1 = grad(lambda x: jnp.mean(f(x)))(jnp.array([-142., 342.]))\ngrad2 = grad(lambda x: jnp.mean(f_jax(x)))(jnp.array([-142., 342.]))\n\nprimal3, jvp3 = jvp(g, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\nprimal4, jvp4 = jvp(g_jax, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\ngrad3 = grad(lambda x: jnp.mean(g(x)))(jnp.array([-142., 342.]))\ngrad4 = grad(lambda x: jnp.mean(g_jax(x)))(jnp.array([-142., 342.]))  \n\nprimal5, jvp5 = jvp(h, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\nprimal6, jvp6 = jvp(h_jax, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\ngrad5 = grad(lambda x: jnp.mean(h(x)))(jnp.array([-142., 342.]))\ngrad6 = grad(lambda x: jnp.mean(h_jax(x)))(jnp.array([-142., 342.]))\n\nprimal7, jvp7 = jvp(no_diff, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\nprimal8, jvp8 = jvp(no_diff_jax, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\ngrad7 = grad(lambda x: jnp.mean(no_diff(x)))(jnp.array([-142., 342.]))\ngrad8 = grad(lambda x: jnp.mean(no_diff_jax(x)))(jnp.array([-142., 342.]))\n\nprint(f\"\"\"\nVariable L:\n  Primal difference: {np.linalg.norm(primal1 - primal2): .2e}\n  JVP difference: {np.linalg.norm(jvp1 - jvp2): .2e}\n  Gradient difference: {np.linalg.norm(grad1 - grad2): .2e}\n\nVariable b:\n  Primal difference: {np.linalg.norm(primal3 - primal4): .2e}\n  JVP difference: {np.linalg.norm(jvp3 - jvp4): .2e}\n  Gradient difference: {np.linalg.norm(grad3 - grad4): .2e} \n\nVariable L and b:\n  Primal difference: {np.linalg.norm(primal5 - primal6): .2e}\n  JVP difference: {np.linalg.norm(jvp5 - jvp6): .2e}\n  Gradient difference: {np.linalg.norm(grad5 - grad6): .2e}\n\nNo diff:\n  Primal difference: {np.linalg.norm(primal7 - primal8)}\n  JVP difference: {np.linalg.norm(jvp7 - jvp8)}\n  Gradient difference: {np.linalg.norm(grad7 - grad8)}\n\"\"\")\n\n\nVariable L:\n  Primal difference:  1.98e-07\n  JVP difference:  2.58e-12\n  Gradient difference:  0.00e+00\n\nVariable b:\n  Primal difference:  7.94e-06\n  JVP difference:  1.83e-08\n  Gradient difference:  3.29e-10 \n\nVariable L and b:\n  Primal difference:  2.08e-06\n  JVP difference:  1.08e-08\n  Gradient difference:  2.33e-10\n\nNo diff:\n  Primal difference: 2.2101993124579167e-07\n  JVP difference: 0.0\n  Gradient difference: 0.0\n\n\n\nStunning!"
  },
  {
    "objectID": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#primitive-one-the-general-a-1b",
    "href": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#primitive-one-the-general-a-1b",
    "title": "Sparse matrices 6: To catch a derivative, first you’ve got to think like a derivative",
    "section": "Primitive one: The general \\(A^{-1}b\\)",
    "text": "Primitive one: The general \\(A^{-1}b\\)\nOk. So this is a very similar problem to the one that we just solved. But, as fate would have it, the solution is going to look quite different. Why? Because we need to compute a Cholesky factorisation.\nFirst things first, though, we are going to need a JAX-traceable way to compute a Cholesky factor. This means that we need25 to tell our sparse_solve function the how many non-zeros the sparse Cholesky will have. Why? Well. It has to do with how the function is used.\nWhen sparse_cholesky() is called with concrete inputs26, then it can quite happily work out the sparsity structure of \\(L\\). But when JAX is preparing to transform the code, eg when it’s building a gradient, it calls sparse_cholesky() using abstract arguments that only share the shape information from the inputs. This is not enough to compute the sparsity structure. We need the indices and indptr arrays.\nThis means that we need sparse_cholesky() to throw an error if L_nse isn’t passed. This wasn’t implemented well last time, so here it is done properly.\n(If you’re wondering about that None argument, it is the identity transform. So if A_indices is a concrete value, ind = A_indices. Otherwise an error is called.)\n\nsparse_cholesky_p = core.Primitive(\"sparse_cholesky\")\n\ndef sparse_cholesky(A_indices, A_indptr, A_x, *, L_nse: int = None):\n  \"\"\"A JAX traceable sparse cholesky decomposition\"\"\"\n  if L_nse is None:\n    err_string = \"You need to pass a value to L_nse when doing fancy sparse_cholesky.\"\n    ind = core.concrete_or_error(None, A_indices, err_string)\n    ptr = core.concrete_or_error(None, A_indptr, err_string)\n    L_ind, _ = _symbolic_factor(ind, ptr)\n    L_nse = len(L_ind)\n  \n  return sparse_cholesky_p.bind(A_indices, A_indptr, A_x, L_nse = L_nse)\n\n\n\nThe rest of the Choleksy code\n\n\n@sparse_cholesky_p.def_impl\ndef sparse_cholesky_impl(A_indices, A_indptr, A_x, *, L_nse):\n  \"\"\"The implementation of the sparse cholesky This is not JAX traceable.\"\"\"\n  \n  L_indices, L_indptr= _symbolic_factor(A_indices, A_indptr)\n  if L_nse is not None:\n    assert len(L_indices) == L_nse\n    \n  L_x = _structured_copy(A_indices, A_indptr, A_x, L_indices, L_indptr)\n  L_x = _sparse_cholesky_impl(L_indices, L_indptr, L_x)\n  return L_indices, L_indptr, L_x\n\ndef _symbolic_factor(A_indices, A_indptr):\n  # Assumes A_indices and A_indptr index the lower triangle of $A$ ONLY.\n  n = len(A_indptr) - 1\n  L_sym = [np.array([], dtype=int) for j in range(n)]\n  children = [np.array([], dtype=int) for j in range(n)]\n  \n  for j in range(n):\n    L_sym[j] = A_indices[A_indptr[j]:A_indptr[j + 1]]\n    for child in children[j]:\n      tmp = L_sym[child][L_sym[child] > j]\n      L_sym[j] = np.unique(np.append(L_sym[j], tmp))\n    if len(L_sym[j]) > 1:\n      p = L_sym[j][1]\n      children[p] = np.append(children[p], j)\n        \n  L_indptr = np.zeros(n+1, dtype=int)\n  L_indptr[1:] = np.cumsum([len(x) for x in L_sym])\n  L_indices = np.concatenate(L_sym)\n  \n  return L_indices, L_indptr\n\n\n\ndef _structured_copy(A_indices, A_indptr, A_x, L_indices, L_indptr):\n  n = len(A_indptr) - 1\n  L_x = np.zeros(len(L_indices))\n  \n  for j in range(0, n):\n    copy_idx = np.nonzero(np.in1d(L_indices[L_indptr[j]:L_indptr[j + 1]],\n                                  A_indices[A_indptr[j]:A_indptr[j+1]]))[0]\n    L_x[L_indptr[j] + copy_idx] = A_x[A_indptr[j]:A_indptr[j+1]]\n  return L_x\n\ndef _sparse_cholesky_impl(L_indices, L_indptr, L_x):\n  n = len(L_indptr) - 1\n  descendant = [[] for j in range(0, n)]\n  for j in range(0, n):\n    tmp = L_x[L_indptr[j]:L_indptr[j + 1]]\n    for bebe in descendant[j]:\n      k = bebe[0]\n      Ljk= L_x[bebe[1]]\n      pad = np.nonzero(                                                       \\\n          L_indices[L_indptr[k]:L_indptr[k+1]] == L_indices[L_indptr[j]])[0][0]\n      update_idx = np.nonzero(np.in1d(                                        \\\n                    L_indices[L_indptr[j]:L_indptr[j+1]],                     \\\n                    L_indices[(L_indptr[k] + pad):L_indptr[k+1]]))[0]\n      tmp[update_idx] = tmp[update_idx] -                                     \\\n                        Ljk * L_x[(L_indptr[k] + pad):L_indptr[k + 1]]\n            \n    diag = np.sqrt(tmp[0])\n    L_x[L_indptr[j]] = diag\n    L_x[(L_indptr[j] + 1):L_indptr[j + 1]] = tmp[1:] / diag\n    for idx in range(L_indptr[j] + 1, L_indptr[j + 1]):\n      descendant[L_indices[idx]].append((j, idx))\n  return L_x\n\n@sparse_cholesky_p.def_abstract_eval\ndef sparse_cholesky_abstract_eval(A_indices, A_indptr, A_x, *, L_nse):\n  return core.ShapedArray((L_nse,), A_indices.dtype),                   \\\n         core.ShapedArray(A_indptr.shape, A_indptr.dtype),             \\\n         core.ShapedArray((L_nse,), A_x.dtype)\n\n\n\nWhy do we need a new pattern for this very very similar problem?\nOk. So now on to the details. If we try to repeat our previous pattern it would look like this.\n\ndef sparse_solve_value_and_jvp(arg_values, arg_tangents, *, L_nse):\n  \"\"\" \n  Jax-traceable jacobian-vector product implmentation for sparse_solve.\n  \"\"\"\n  \n  A_indices, A_indptr, A_x, b = arg_values\n  _, _, A_xt, bt = arg_tangents\n\n  # Needed for shared computation\n  L_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\n\n  # Make the primal\n  primal_out = sparse_triangular_solve(L_indices, L_indptr, L_x, b, transpose = False)\n  primal_out = sparse_triangular_solve(L_indices, L_indptr, L_x, primal_out, transpose = True)\n\n  if type(A_xt) is not ad.Zero:\n    Delta_lower = jsparse.CSC((A_xt, A_indices, A_indptr), shape = (b.shape[0], b.shape[0]))\n    # We need to do Delta @ primal_out, but we only have the lower triangle\n    rhs = Delta_lower @ primal_out + Delta_lower.transpose() @ primal_out - A_xt[A_indptr[:-1]] * primal_out\n    jvp_Ax = sparse_triangular_solve(L_indices, L_indptr, L_x, rhs)\n    jvp_Ax = sparse_triangular_solve(L_indices, L_indptr, L_x, jvp_Ax, transpose = True)\n  else:\n    jvp_Ax = lax.zeros_like_array(primal_out)\n\n  if type(bt) is not ad.Zero:\n    jvp_b = sparse_triangular_solve(L_indices, L_indptr, L_x, bt)\n    jvp_b = sparse_triangular_solve(L_indices, L_indptr, L_x, jvp_b, transpose = True)\n  else:\n    jvp_b = lax.zeros_like_array(primal_out)\n\n  return primal_out, jvp_b - jvp_Ax\n\nThat’s all well and good. Nothing weird there.\nThe problem comes when you need to implement the transposition rule. Remembering that \\(\\bar b = A^{-T}\\bar c = A^{-1}\\bar c\\), you might see the issue: we are going to need the Cholesky factorisation. But we have no way to pass \\(L\\) to the transpose function.\nThis means that we would need to compute two Cholesky factorisations per gradient instead of one. As the Cholesky factorisation is our slowest operation, we do not want to do extra ones! We want to compute the Cholesky triangle once and pass it around like a party bottom27. We do not want each of our functions to have to make a deep and meaningful connection with the damn matrix28.\n\n\nA different solution\nSo how do we pass around our Cholesky triangle? Well, I do love a good class so my first thought was “fuck it. I’ll make a class and I’ll pass it that way”. But the developers of JAX had a much better idea.\nTheir idea was to abstract the idea of a linear solve and its gradients. They do this through lax.custom_linear_solve. This is a function that takes all of the bits that you would need to compute \\(A^{-1}b\\) and all of its derivatives. In particular it takes29:\n\nmatvec: A function that matvec(x) that computes \\(Ax\\). This might seem a bit weird, but it’s the most common atrocity committed by mathematicians is abstracting30 a matrix to a linear mapping. So we might as well just suck it up.\nb: The right hand side vector31\nsolve: A function that takes takes the matvec and a vector so that32 solve(matvec, matvec(x)) == x\nsymmetric: A boolean indicating if \\(A\\) is symmetric.\n\nThe idea (happily copped from the implementation of jax.scipy.linalg.solve) is to wrap our Cholesky decomposition in the solve function. Through the never ending miracle of partial evaluation.\n\nfrom functools import partial\n\ndef sparse_solve(A_indices, A_indptr, A_x, b, *, L_nse = None):\n  \"\"\"\n  A JAX-traceable sparse solve. For this moment, only for vector b\n  \"\"\"\n  assert b.shape[0] == A_indptr.shape[0] - 1\n  assert b.ndim == 1\n  \n  L_indices, L_indptr, L_x = sparse_cholesky(\n    lax.stop_gradient(A_indices), \n    lax.stop_gradient(A_indptr), \n    lax.stop_gradient(A_x), L_nse = L_nse)\n  \n  def chol_solve(L_indices, L_indptr, L_x, b):\n    out = sparse_triangular_solve(L_indices, L_indptr, L_x, b, transpose = False)\n    return sparse_triangular_solve(L_indices, L_indptr, L_x, out, transpose = True)\n  \n  def matmult(A_indices, A_indptr, A_x, b):\n    A_lower = jsparse.CSC((A_x, A_indices, A_indptr), shape = (b.shape[0], b.shape[0]))\n    return A_lower @ b + A_lower.transpose() @ b - A_x[A_indptr[:-1]] * b\n\n  solver = partial(\n    lax.custom_linear_solve,\n    lambda x: matmult(A_indices, A_indptr, A_x, x),\n    solve = lambda _, x: chol_solve(L_indices, L_indptr, L_x, x),\n    symmetric = True)\n\n  return solver(b)\n\nThere are three things of note in that implementation.\n\nThe calls to lax.stop_gradient(): These tell JAX to not bother computing the gradient of these terms. The relevant parts of the derivatives are computed explicitly by lax.custom_linear_solve in terms of matmult and solve, neither of which need the explicit derivative of the cholesky factorisation.!\nThat definition of matmult()33: Look. I don’t know what to tell you. Neither addition nor indexing is implemented for jsparse.CSC objects. So we did it the semi-manual way. (I am thankful that matrix-vector multiplication is available)\nThe definition of solver(): Partial evaluation is a wonderful wonderful thing. functools.partial() transforms lax.custom_linear_solve() from a function that takes 3 arguments (and some keywords), into a function solver() that takes one34 argument35 (b, the only positional argument of lax.custom_linear_solve() that isn’t specified).\n\n\n\nDoes it work?\n\ndef f(theta):\n  Ax_theta = jnp.array(theta[0] * A_x)\n  Ax_theta = Ax_theta.at[A_indptr[:-1]].add(theta[1])\n  b = jnp.ones(100)\n  return sparse_solve(A_indices, A_indptr, Ax_theta, b)\n\ndef f_jax(theta):\n  Ax_theta = jnp.array(theta[0] * A.todense())\n  Ax_theta = Ax_theta.at[np.arange(100),np.arange(100)].add(theta[1])\n  b = jnp.ones(100)\n  return jsp.linalg.solve(Ax_theta, b)\n\ndef g(theta):\n  Ax_theta = jnp.array(A_x)\n  b = jnp.ones(100)\n  b = b.at[0].set(theta[0])\n  b = b.at[51].set(theta[1])\n  return sparse_solve(A_indices, A_indptr, Ax_theta, b)\n\ndef g_jax(theta):\n  Ax_theta = jnp.array(A.todense())\n  b = jnp.ones(100)\n  b = b.at[0].set(theta[0])\n  b = b.at[51].set(theta[1])\n  return jsp.linalg.solve(Ax_theta, b)\n\ndef h(theta):\n  Ax_theta = jnp.array(A_x)\n  Ax_theta = Ax_theta.at[A_indptr[:-1]].add(theta[0])\n  b = jnp.ones(100)\n  b = b.at[51].set(theta[1])\n  return sparse_solve(A_indices, A_indptr, Ax_theta, b)\n\ndef h_jax(theta):\n  Ax_theta = jnp.array(A.todense())\n  Ax_theta = Ax_theta.at[np.arange(100),np.arange(100)].add(theta[0])\n  b = jnp.ones(100)\n  b = b.at[51].set(theta[1])\n  return jsp.linalg.solve(Ax_theta, b)\n\nprimal1, jvp1 = jvp(f, (jnp.array([2., 3.]),), (jnp.array([1., 2.]),))\nprimal2, jvp2 = jvp(f_jax, (jnp.array([2., 3.]),), (jnp.array([1., 2.]),))\ngrad1 = grad(lambda x: jnp.mean(f(x)))(jnp.array([2., 3.]))\ngrad2 = grad(lambda x: jnp.mean(f_jax(x)))(jnp.array([2., 3.]))\n\n\nprimal3, jvp3 = jvp(g, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\nprimal4, jvp4 = jvp(g_jax, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\ngrad3 = grad(lambda x: jnp.mean(g(x)))(jnp.array([-142., 342.]))\ngrad4 = grad(lambda x: jnp.mean(g_jax(x)))(jnp.array([-142., 342.]))\n\nprimal5, jvp5 = jvp(h, (jnp.array([2., 342.]),), (jnp.array([1., 2.]),))\nprimal6, jvp6 = jvp(h_jax, (jnp.array([2., 342.]),), (jnp.array([1., 2.]),))\ngrad5 = grad(lambda x: jnp.mean(f(x)))(jnp.array([2., 342.]))\ngrad6 = grad(lambda x: jnp.mean(f_jax(x)))(jnp.array([2., 342.]))\n\nprint(f\"\"\"\nCheck the plumbing!\nVariable A:\n  Primal difference: {np.linalg.norm(primal1 - primal2): .2e}\n  JVP difference: {np.linalg.norm(jvp1 - jvp2): .2e}\n  Gradient difference: {np.linalg.norm(grad1 - grad2): .2e}\n  \nVariable b:\n  Primal difference: {np.linalg.norm(primal3 - primal4): .2e}\n  JVP difference: {np.linalg.norm(jvp3 - jvp4): .2e}\n  Gradient difference: {np.linalg.norm(grad3 - grad4): .2e} \n    \nVariable A and b:\n  Primal difference: {np.linalg.norm(primal5 - primal6): .2e}\n  JVP difference: {np.linalg.norm(jvp5 - jvp6): .2e}\n  Gradient difference: {np.linalg.norm(grad5 - grad6): .2e}\n  \"\"\")\n\n\nCheck the plumbing!\nVariable A:\n  Primal difference:  1.98e-07\n  JVP difference:  1.43e-07\n  Gradient difference:  0.00e+00\n  \nVariable b:\n  Primal difference:  4.56e-06\n  JVP difference:  6.52e-08\n  Gradient difference:  9.31e-10 \n    \nVariable A and b:\n  Primal difference:  8.10e-06\n  JVP difference:  1.83e-06\n  Gradient difference:  1.82e-12\n  \n\n\nYes.\n\n\nWhy is this better than just differentiating through the Cholesky factorisation?\nThe other option for making this work would’ve been to implement the Cholesky factorisation as a primitive (~which we are about to do!~ which we will do another day) and then write the sparse solver directly as a pure JAX function.\n\ndef sparse_solve_direct(A_indices, A_indptr, A_x, b, *, L_nse = None):\n  L_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\n  out = sparse_triangular_solve(L_indices, L_indptr, L_x, b)\n  return sparse_triangular_solve(L_indices, L_indptr, L_x, out, transpose = True)\n\nThis function is JAX-traceable36 and, therefore, we could compute the gradient of it directly. It turns out that this is going to be a bad idea.\nWhy? Because the derivative of sparse_cholesky, which we would have to chain together with the derivatives from the solver, is pretty complicated. Basically, this means that we’d have to do a lot more work37 than we do if we just implement the symbolic formula for the derivatives."
  },
  {
    "objectID": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#primitive-three-the-dreaded-log-determinant",
    "href": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#primitive-three-the-dreaded-log-determinant",
    "title": "Sparse matrices 6: To catch a derivative, first you’ve got to think like a derivative",
    "section": "Primitive three: The dreaded log determinant",
    "text": "Primitive three: The dreaded log determinant\nOk, so now we get to the good one. The log-determinant of \\(A\\). The first thing that we need to do is wrench out a derivative. This is not as easy as it was for the linear solve. So what follows is a modification for sparse matrices from Appendix A of Boyd’s convex optimisation book.\nIt’s pretty easy to convince yourself that \\[\\begin{align*}\n\\log(|A + \\Delta|) &= \\log\\left( \\left|A^{1/2}(I + A^{-1/2}\\Delta A^{-1/2})A^{1/2}\\right|\\right) \\\\\n&= \\log(|A|) + \\log\\left( \\left|I + A^{-1/2}\\Delta A^{-1/2}\\right|\\right).\n\\end{align*}\\]\nIt is harder to convince yourself how this could possibly be a useful fact.\nIf we write \\(\\lambda_i\\), \\(i = 1, \\ldots, n\\) as the eigenvalues of \\(A^{-1/2}\\Delta A^{-1/2}\\), then we have \\[\n\\log(|A + \\Delta |) = \\log(|A|) + \\sum_{i=1}^n \\log( 1 + \\lambda_i).\n\\] Remembering that \\(\\Delta\\) is very small, it follows that \\(A^{-1/2}\\Delta A^{-1/2}\\) will also be small. That translates to the eigenvalues of \\(A^{-1/2}\\Delta A^{-1/2}\\) all being small. Therefore, we can use the approximation \\(\\log(1 + \\lambda_i) = \\lambda_i + \\mathcal{O}(\\lambda_i^2)\\).\nThis means that38 \\[\\begin{align*}\n\\log(|A + \\Delta |) &= \\log(|A|) + \\sum_{i=1}^n  \\lambda_i + \\mathcal{O}\\left(\\|\\Delta\\|^2\\right) \\\\\n&=\\log(|A|) + \\operatorname{tr}\\left(A^{-1/2} \\Delta A^{-1} \\right) + \\mathcal{O}\\left(\\|\\Delta\\|^2\\right) \\\\\n&= \\log(|A|) + \\operatorname{tr}\\left(A^{-1} \\Delta \\right) + \\mathcal{O}\\left(\\|\\Delta\\|^2\\right),\n\\end{align*}\\] which follows from the cyclic property of the trace.\nIf we recall the formula from the last section defining the Jacobian-vector product, in our context \\(m = 1\\), \\(x\\) is the vector of non-zero entries of the lower triangle of \\(A\\) stacked by column, and \\(\\delta\\) is the vector of non-zero entries of the lower triangle of \\(\\Delta\\). That means the Jacobian-vector product is \\[\nJ(x)\\delta = \\operatorname{tr}\\left(A^{-1} \\Delta \\right) = \\sum_{i=1}^n\\sum_{j=1}^n[A^{-1}]_{ij} \\Delta_{ij}.\n\\]\nRemembering that \\(\\Delta\\) is sparse with the same sparsity pattern as \\(A\\), we see that the Jacobian-vector product requires us to know the values of \\(A^{-1}\\) that correspond to non-zero elements of \\(A\\). That’s good news because we will see that these entries are relatively cheap and easy to compute. Whereas the full inverse is dense and very expensive to compute.\nBut before we get to that, I need to point out a trap for young players39. Lest your implementations go down faster than me when someone asks politely.\nThe problem comes from how we store our matrix. A mathematician would suggest that it’s our representation. A physicist40 would shit on about being coordinate free with such passion that he41 will keep going even after you quietly leave the room.\nThe problem is that we only store the non-zero entries of the lower-triangular part of \\(A\\). This means that we need to be careful that when we compute the Jacobian-vector product that we properly compute the Matrix-vector product.\nLet A_indices and A_indptr define the sparsity structure of \\(A\\) (and \\(\\Delta\\)). Then if \\(A_x\\) is our input and \\(v\\) is our vector, then we need to do the follow steps to compute the Jacobian-vector product:\n\nCompute Ainv_x (aka the non-zero elements of \\(A^{-1}\\) that correspond to the sparsity pattern of \\(A\\))\nCompute the matrix vector product as\n\n\njvp = 2 * sum(Ainv_x * v) - sum(Ainv_x[A_indptr[:-1]] * v[A_indptr[:-1]])\n\nWhy does it look like that? Well we need to add the contribution from the upper triangle as well as the lower triangle. And one way to do that is to just double the sum and then subtract off the diagonal terms that we’ve counted twice.\n(I’m making a pretty big assumption here, which is fine in our context, that \\(A\\) has a non-zero diagonal. If that doesn’t hold, it’s just a change of the indexing in the second term to just pull out the diagonal terms.)\nUsing similar reasoning, we can compute the Jacobian as \\[\n[J_f(x)]_{i1} = \\begin{cases}\n\\operatorname{partial-inverse}(x)_i, \\qquad & x_i  \\text{ is a diagonal element of }A \\\\\n2\\operatorname{partial-inverse}(x)_i, \\qquad & \\text{otherwise},\n\\end{cases}\n\\] where \\(\\operatorname{partial-inverse}(x)\\) is the vector that stacks the columns of the elements of \\(A^{-1}\\) that correspond to the non-zero elements of \\(A\\). (Yikes!)\n\nComputing the partial inverse\nSo now we need to actually work out how to compute this partial inverse of a symmetric positive definite matrix \\(A\\). To do this, we are going to steal a technique that goes back to Takahashi, Fagan, and Chen42 in 1973. (For this presentation, I’m basically pillaging Håvard Rue and Sara Martino’s 2007 paper.)\nTheir idea was that if we write \\(A = VDV^T\\), where \\(V\\) is a lower-triangular matrix with ones on the diagonal and \\(D\\) is diagonal. This links up with our usual Cholesky factorisation through the identity \\(L = VD^{1/2}\\). It follows that if \\(S = A^{-1}\\), then \\(VDV^TS = I\\). Then, we make some magic manipulations43. \\[\\begin{align*}\nV^TS &= D^{-1}V^{-1} \\\\\nS + V^TS &= S + D^{-1}V^{-1} \\\\\nS &= D^{-1}V^{-1} + (I - V^T)S.\n\\end{align*}\\]\nOnce again, this does not look super-useful. The trick is to notice 2 things.\n\nBecause \\(V\\) is lower triangular, \\(V^{-1}\\) is also lower triangular and the elements of \\(V^{-1}\\) are the inverse of the diagonal elements of \\(V\\) (aka they are all 1). Therefore, \\(D^{-1}V^{-1}\\) is a lower triangular matrix with a diagonal given by the diagonal of \\(D^{-1}\\).\n\\(I - V^T\\) is an upper triangular matrix and \\([I - V^T]_{nn} = 0\\).\n\nThese two things together lead to the somewhat unexpected situation where the upper triangle of \\(S = D^{-1}V^{-1} + (I- V^T)S\\) defines a set of recursions for the upper triangle of \\(S\\). (And, therefore, all of \\(S\\) because \\(S\\) is symmetric!) These are sometimes referred to as the Takahashi recursions.\nBut we don’t want the whole upper triangle of \\(S\\), we just want the ones that correspond to the non-zero elements of \\(A\\). Unfortunately, the set of recursions are not, in general, solveable using only that subset of \\(S\\). But we are in luck: they are solveable using the elements of \\(S\\) that correspond to the non-zeros of \\(L + L^T\\), which, as we know from a few posts ago, is a superset of the non-zero elements of \\(A\\)!\nFrom this, we get the recursions running from \\(i = n, \\ldots, 1\\), \\(j = n, \\ldots, i\\) (the order is important!) such that \\(L_{ji} \\neq 0\\) \\[\nS_{ji} =   \\begin{cases}\n\\frac{1}{L_{ii}^2} - \\frac{1}{L_{ii}}\\sum_{k=i+1}^{n} L_{ki} S_{kj} \\qquad&  \\text{if } i=j, \\\\         \n- \\frac{1}{L_{ii}}\\sum_{k=i+1}^{n} L_{ki} S_{kj}  & \\text{otherwise}.\n\\end{cases}\n\\]\nIf you recall our discussion way back when about the way the non-zero structure of the \\(j\\) the column of \\(L\\) relates to the non-zero structure of the \\(i\\) th column for \\(j \\geq i\\), it’s clear that we have computed enough44 of \\(S\\) at every step to complete the recursions.\nNow we just need to Python it. (And thanks to Finn Lindgren who helped me understand how to implement this, which he may or may not remember because it happened about five years ago.)\nActually, we need this to be JAX-traceable, so we are going to implement a very basic primitive. In particular, we don’t need to implement a derivative or anything like that, just an abstract evaluation and an implementation.\n\nsparse_partial_inverse_p = core.Primitive(\"sparse_partial_inverse\")\n\ndef sparse_partial_inverse(L_indices, L_indptr, L_x, out_indices, out_indptr):\n  \"\"\"\n  Computes the elements (out_indices, out_indptr) of the inverse of a sparse matrix (A_indices, A_indptr, A_x)\n   with Choleksy factor (L_indices, L_indptr, L_x). (out_indices, out_indptr) is assumed to be either\n   the sparsity pattern of A or a subset of it in lower triangular form. \n  \"\"\"\n  return sparse_partial_inverse_p.bind(L_indices, L_indptr, L_x, out_indices, out_indptr)\n\n@sparse_partial_inverse_p.def_abstract_eval\ndef sparse_partial_inverse_abstract_eval(L_indices, L_indptr, L_x, out_indices, out_indptr):\n  return abstract_arrays.ShapedArray(out_indices.shape, L_x.dtype)\n\n@sparse_partial_inverse_p.def_impl\ndef sparse_partial_inverse_impl(L_indices, L_indptr, L_x, out_indices, out_indptr):\n  n = len(L_indptr) - 1\n  Linv = sparse.dok_array((n,n), dtype = L_x.dtype)\n  counter = len(L_x) - 1\n  for col in range(n-1, -1, -1):\n    for row in L_indices[L_indptr[col]:L_indptr[col+1]][::-1]:\n      if row != col:\n        Linv[row, col] = Linv[col, row] = 0.0\n      else:\n        Linv[row, col] = 1 / L_x[L_indptr[col]]**2\n      L_col  = L_x[L_indptr[col]+1:L_indptr[col+1]] / L_x[L_indptr[col]]\n \n      for k, L_kcol in zip(L_indices[L_indptr[col]+1:L_indptr[col+1]], L_col):\n         Linv[col,row] = Linv[row,col] =  Linv[row, col] -  L_kcol * Linv[k, row]\n        \n  Linv_x = sparse.tril(Linv, format = \"csc\").data\n  if len(out_indices) == len(L_indices):\n    return Linv_x\n\n  out_x = np.zeros(len(out_indices))\n  for col in range(n):\n    ind = np.nonzero(np.in1d(L_indices[L_indptr[col]:L_indptr[col+1]],\n      out_indices[out_indptr[col]:out_indptr[col+1]]))[0]\n    out_x[out_indptr[col]:out_indptr[col+1]] = Linv_x[L_indptr[col] + ind]\n  return out_x\n\nThe implementation makes use of the45 dictionary of keys representation of a sparse matrix from scipy.sparse. This is an efficient storage scheme when you need to modify the sparsity structure (as we are doing here) or do a lot of indexing. It would definitely be possible to implement this directly on the CSC data structure, but it gets a little bit tricky to access the elements of L_inv that are above the diagonal. The resulting code is honestly a mess and there’s lots of non-local memory access anyway, so I implemented it this way.\nBut let’s be honest: this thing is crying out for a proper symmetric matrix class with sensible reverse iterators. But hey. Python.\nThe second chunk of the code is just the opposite of our _structured_copy() function. It takes a matrix with the sparsity pattern of \\(L\\) and returns one with the sparsity pattern of out (which is assumed to be a subset, and is usually the sparsity pattern of \\(A\\) or a diagonal matrix).\nLet’s check that it works.\n\nA_indices, A_indptr, A_x, A = make_matrix(15)\nn = len(A_indptr) - 1\n\n\nL_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\n\na_inv_L = sparse_partial_inverse(L_indices, L_indptr, L_x, L_indices, L_indptr)\n\ncol_counts_L = [L_indptr[i+1] - L_indptr[i] for i in range(n)]\ncols_L = np.repeat(range(n), col_counts_L)\n\ntrue_inv = np.linalg.inv(A.todense())\ntruth_L = true_inv[L_indices, cols_L]\n\na_inv_A = sparse_partial_inverse(L_indices, L_indptr, L_x, A_indices, A_indptr)\ncol_counts_A = [A_indptr[i+1] - A_indptr[i] for i in range(n)]\ncols_A = np.repeat(range(n), col_counts_A)\ntruth_A = true_inv[A_indices, cols_A]\n\nprint(f\"\"\"\nError in partial inverse (all of L): {np.linalg.norm(a_inv_L - truth_L): .2e}\nError in partial inverse (all of A): {np.linalg.norm(a_inv_A - truth_A): .2e}\n\"\"\")\n\n\nError in partial inverse (all of L):  1.57e-15\nError in partial inverse (all of A):  1.53e-15\n\n\n\n\n\nPutting the log-determinant together\nAll of our bits are in place, so now all we need is to implement the primitive for the log-determinant. One nice thing here is that we don’t need to implement a transposition rule as the function is not structurally linear in any of its arguments. At this point we take our small wins where we can get them.\nThere isn’t anything particularly interesting in the implementation. But do note that the trace has been implemented in a way that’s aware that we’re only storing the bottom triangle of \\(A\\).\n\nsparse_log_det_p = core.Primitive(\"sparse_log_det\")\n\ndef sparse_log_det(A_indices, A_indptr, A_x):\n  return sparse_log_det_p.bind(A_indices, A_indptr, A_x)\n\n@sparse_log_det_p.def_impl\ndef sparse_log_det_impl(A_indices, A_indptr, A_x):\n  L_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\n  return 2.0 * jnp.sum(jnp.log(L_x[L_indptr[:-1]]))\n\n@sparse_log_det_p.def_abstract_eval\ndef sparse_log_det_abstract_eval(A_indices, A_indptr, A_x):\n  return abstract_arrays.ShapedArray((1,), A_x.dtype)\n\ndef sparse_log_det_value_and_jvp(arg_values, arg_tangent):\n  A_indices, A_indptr, A_x = arg_values\n  _, _, A_xt = arg_tangent\n  L_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\n  value = 2.0 * jnp.sum(jnp.log(L_x[L_indptr[:-1]]))\n  Ainv_x = sparse_partial_inverse(L_indices, L_indptr, L_x, A_indices, A_indptr)\n  jvp = 2.0 * sum(Ainv_x * A_xt) - sum(Ainv_x[A_indptr[:-1]] * A_xt[A_indptr[:-1]])\n  return value, jvp\n\nad.primitive_jvps[sparse_log_det_p] = sparse_log_det_value_and_jvp\n\nFinally, we can test it out.\n\nld_true = np.log(np.linalg.det(A.todense())) #np.sum(np.log(lu.U.diagonal()))\nprint(f\"Error in log-determinant = {ld_true - sparse_log_det(A_indices, A_indptr, A_x): .2e}\")\n\ndef f(theta):\n  Ax_theta = jnp.array(theta[0] * A_x) / n\n  Ax_theta = Ax_theta.at[A_indptr[:-1]].add(theta[1])\n  return sparse_log_det(A_indices, A_indptr, Ax_theta)\n\ndef f_jax(theta):\n  Ax_theta = jnp.array(theta[0] * A.todense()) / n \n  Ax_theta = Ax_theta.at[np.arange(n),np.arange(n)].add(theta[1])\n  L = jnp.linalg.cholesky(Ax_theta)\n  return 2.0*jnp.sum(jnp.log(jnp.diag(L)))\n\nprimal1, jvp1 = jvp(f, (jnp.array([2., 3.]),), (jnp.array([1., 2.]),))\nprimal2, jvp2 = jvp(f_jax, (jnp.array([2., 3.]),), (jnp.array([1., 2.]),))\n\neps = 1e-4\njvp_fd = (f(jnp.array([2.,3.]) + eps * jnp.array([1., 2.]) ) - f(jnp.array([2.,3.]))) / eps\n\ngrad1 = grad(f)(jnp.array([2., 3.]))\ngrad2 = grad(f_jax)(jnp.array([2., 3.]))\n\nprint(f\"\"\"\nCheck the Derivatives!\nVariable A:\n  Primal difference: {np.linalg.norm(primal1 - primal2)}\n  JVP difference: {np.linalg.norm(jvp1 - jvp2)}\n  JVP difference (FD): {np.linalg.norm(jvp1 - jvp_fd)}\n  Gradient difference: {np.linalg.norm(grad1 - grad2)}\n\"\"\")\n\nError in log-determinant =  0.00e+00\n\n\n\nCheck the Derivatives!\nVariable A:\n  Primal difference: 0.0\n  JVP difference: 0.000885009765625\n  JVP difference (FD): 0.221893310546875\n  Gradient difference: 1.526623782410752e-05\n\n\n\nI’m not going to lie, I am not happy with that JVP difference. I was somewhat concerned that there was a bug somewhere in my code. I did a little bit of exploring and the error got larger as the problem got larger. It also depended a little bit more than I was comfortable on how I had implemented46 the baseline dense version.\nThat second fact suggested to me that it might be a floating point problem. By default, JAX uses single precision (32-bit) floating point. Most modern systems that don’t try and run on GPUs use double precision (64-bit) floating point. So I tried it with double precision and lo and behold, the problem disappears.\nMatrix factorisations are bloody hard in single precision.\n\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\n\nld_true = np.log(np.linalg.det(A.todense())) #np.sum(np.log(lu.U.diagonal()))\nprint(f\"Error in log-determinant = {ld_true - sparse_log_det(A_indices, A_indptr, A_x): .2e}\")\n\ndef f(theta):\n  Ax_theta = jnp.array(theta[0] * A_x, dtype = jnp.float64) / n\n  Ax_theta = Ax_theta.at[A_indptr[:-1]].add(theta[1])\n  return sparse_log_det(A_indices, A_indptr, Ax_theta)\n\ndef f_jax(theta):\n  Ax_theta = jnp.array(theta[0] * A.todense(), dtype = jnp.float64) / n \n  Ax_theta = Ax_theta.at[np.arange(n),np.arange(n)].add(theta[1])\n  L = jnp.linalg.cholesky(Ax_theta)\n  return 2.0*jnp.sum(jnp.log(jnp.diag(L)))\n\nprimal1, jvp1 = jvp(f, (jnp.array([2., 3.], dtype = jnp.float64),), (jnp.array([1., 2.], dtype = jnp.float64),))\nprimal2, jvp2 = jvp(f_jax, (jnp.array([2., 3.], dtype = jnp.float64),), (jnp.array([1., 2.], dtype = jnp.float64),))\n\neps = 1e-7\njvp_fd = (f(jnp.array([2.,3.], dtype = jnp.float64) + eps * jnp.array([1., 2.], dtype = jnp.float64) ) - f(jnp.array([2.,3.], dtype = jnp.float64))) / eps\n\ngrad1 = grad(f)(jnp.array([2., 3.], dtype = jnp.float64))\ngrad2 = grad(f_jax)(jnp.array([2., 3.], dtype = jnp.float64))\n\nprint(f\"\"\"\nCheck the Derivatives!\nVariable A:\n  Primal difference: {np.linalg.norm(primal1 - primal2)}\n  JVP difference: {np.linalg.norm(jvp1 - jvp2)}\n  JVP difference (FD): {np.linalg.norm(jvp1 - jvp_fd)}\n  Gradient difference: {np.linalg.norm(grad1 - grad2)}\n\"\"\")\n\nError in log-determinant =  0.00e+00\n\n\n\nCheck the Derivatives!\nVariable A:\n  Primal difference: 0.0\n  JVP difference: 8.526512829121202e-13\n  JVP difference (FD): 4.171707900013644e-06\n  Gradient difference: 8.881784197001252e-16\n\n\n\nMuch better!"
  },
  {
    "objectID": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#wrapping-up",
    "href": "posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html#wrapping-up",
    "title": "Sparse matrices 6: To catch a derivative, first you’ve got to think like a derivative",
    "section": "Wrapping up",
    "text": "Wrapping up\nAnd that is where we will leave it for today. Next up, I’m probably going to need to do the autodiff for the Cholesky factorisation. It’s not hard, but it is tedious47 and this post is already very long.\nAfter that we need a few more things:\n\nCompilation rules for all of these things. For the most part, we can just wrap the relevant parts of Eigen. The only non-trivial code would be the partial inverse. That will allow us to JIT shit.\nWe need to beef up the sparse matrix class a little. In particular, we are going to need addition and scalar multiplication at the very minimum to make this useful.\nWork out how Aesara works so we can try to prototype a PyMC model.\n\nThat will be a lot more blog posts. But I’m having fun. So why the hell not."
  },
  {
    "objectID": "old_distill/index.html",
    "href": "old_distill/index.html",
    "title": "Un garçon pas comme les autres (Bayes)",
    "section": "",
    "text": "A footnote that got out of control.\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "old_distill/about.html",
    "href": "old_distill/about.html",
    "title": "About this blog",
    "section": "",
    "text": "Nobody asked for this. There is no reason for this to exist."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Un garçon pas comme les autres (Bayes)",
    "section": "",
    "text": "Sparse matrices 6: To catch a derivative, first you’ve got to think like a derivative\n\n\nOpen up the kennels, Kenneth. Mamma’s coming home tonight.\n\n\n\n\nJAX\n\n\nSparse matrices\n\n\nAutodiff\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2022\n\n\nDan Simpson\n\n\n\n\n\n\n  \n\n\n\n\n\\((n-1)\\)-sane in the membrane\n\n\nWindmills? Tilted. Topic? Boring. (n-1)? No.\n\n\n\n\nTeaching\n\n\nFundamentals\n\n\nOpinionated\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2021\n\n\nDan Simpson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this blog",
    "section": "",
    "text": "A footnote that got out of control."
  }
]