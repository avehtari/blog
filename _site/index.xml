<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Un garçon pas comme les autres (Bayes)</title>
<link>https://dansblog.netlify.app/index.html</link>
<atom:link href="https://dansblog.netlify.app/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.2.15</generator>
<lastBuildDate>Sun, 29 May 2022 14:00:00 GMT</lastBuildDate>
<item>
  <title>Sparse matrices 6: To catch a derivative, first you’ve got to think like a derivative</title>
  <dc:creator>Dan Simpson</dc:creator>
  <link>https://dansblog.netlify.app/posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html</link>
  <description><![CDATA[ 




<p>Welcome to part six!!! of our ongoing series on making sparse linear algebra differentiable in JAX with the eventual hope to be able to do some <a href="https://dansblog.netlify.app/posts/2022-03-22-a-linear-mixed-effects-model/">cool statistical shit</a>. We are <em>nowhere near done</em>.</p>
<p><a href="https://dansblog.netlify.app/posts/2022-05-14-sparse4-some-primatives/">Last time</a>, we looked at making JAX primitives. We built four of them. Today we are going to implement the corresponding differentiation rules! For three<sup>1</sup> of them.</p>
<p>So strap yourselves in. This is gonna be detailed.</p>
<p>If you’re interested in the code<sup>2</sup>, the git repo for this post is linked at the bottom and in there you will find a folder with the python code in a python file.</p>
<section id="she-is-beauty-and-she-is-grace.-she-is-queen-of-50-states.-she-is-elegance-and-taste.-she-is-miss-autodiff" class="level2">
<h2 class="anchored" data-anchor-id="she-is-beauty-and-she-is-grace.-she-is-queen-of-50-states.-she-is-elegance-and-taste.-she-is-miss-autodiff">She is beauty and she is grace. She is queen of 50 states. She is elegance and taste. She is miss autodiff</h2>
<p>Derivatives are computed in JAX through the glory and power of automatic differentiation. If you came to this blog hoping for a great description of how autodiff works, I am terribly sorry but I absolutely do not have time for that. Might I suggest google? Or maybe flick through <a href="https://arxiv.org/abs/1811.05031">this survey by Charles Margossian.</a>.</p>
<p>The most important thing to remember about algorithmic differentiation is that it is <em>not</em> symbolic differentiation. That is, it does not create the functional form of the derivative of the function and compute that. Instead, it is a system for cleverly composing derivatives in each bit of the program to compute the <em>value</em> of the derivative of the function.</p>
<p>But for that to work, we need to implement those clever little mini-derivatives. In particular, every function <img src="https://latex.codecogs.com/png.latex?f(%5Ccdot):%20%5Cmathbb%7BR%7D%5En%20%5Crightarrow%20%5Cmathbb%7BR%7D%5Em"> needs to have a function to compute the corresponding Jacobian-vector product <img src="https://latex.codecogs.com/png.latex?%0A(%5Ctheta,%20v)%20%5Crightarrow%20J(%5Ctheta)%20v,%0A"> where the <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20m"> matrix <img src="https://latex.codecogs.com/png.latex?J(%5Ctheta)"> has entries <img src="https://latex.codecogs.com/png.latex?%0AJ(%5Ctheta)_%7Bij%7D%20=%20%5Cfrac%7B%5Cpartial%20f_j%20%7D%7B%5Cpartial%20%5Ctheta_j%7D.%0A"></p>
<p>Ok. So let’s get onto this. We are going to derive and implement some Jacobian-vector products. And all of the assorted accoutrement. And by crikey. We are going to do it all in a JAX-traceable way.</p>
</section>
<section id="jvp-number-one-the-linear-solve." class="level2">
<h2 class="anchored" data-anchor-id="jvp-number-one-the-linear-solve.">JVP number one: The linear solve.</h2>
<p>The first of the derivatives that we need to work out is the derivative of a linear solve <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7Db">. Now, intrepid readers, the obvious thing to do is look the damn derivative up. You get exactly no hero points for computing it yourself.</p>
<p>But I’m not you, I’m a dickhead.</p>
<p>So I’m going to derive it. I could pretend there are reasons<sup>3</sup>, but that would just be lying. I’m doing it because I can.</p>
<p>Beyond the obvious fun of working out a matrix derivative from first principles, this is fun because we have <em>two</em> arguments instead of just one. Double the fun.</p>
<p>And we really should make sure the function is differentiated with respect to every reasonable argument. Why? Because if you write code other people might use, you don’t get to control how they use it (or what they will email you about). So it’s always good practice to limit surprises (like a function not being differentiable wrt some argument) to cases<sup>4</sup> where it absolutely necessary. This reduces the emails.</p>
<p>To that end, let’s take an arbitrary SPD matrix <img src="https://latex.codecogs.com/png.latex?A"> with a <em>fixed</em> sparsity pattern. Let’s take another symmetric matrix <img src="https://latex.codecogs.com/png.latex?%5CDelta"> with <em>the same sparsity pattern</em> and assume that <img src="https://latex.codecogs.com/png.latex?%5CDelta"> is small enough<sup>5</sup> that <img src="https://latex.codecogs.com/png.latex?A%20+%20%5CDelta"> is still symmetric positive definite. We also need a vector <img src="https://latex.codecogs.com/png.latex?%5Cdelta"> with a small <img src="https://latex.codecogs.com/png.latex?%5C%7C%5Cdelta%5C%7C">.</p>
<p>Now let’s get algebraing. <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0Af(A%20+%20%5CDelta,%20b%20+%20%5Cdelta)%20&amp;=%20(A+%5CDelta)%5E%7B-1%7D(b%20+%20%5Cdelta)%20%5C%5C%0A&amp;=%20(I%20+%20A%5E%7B-1%7D%5CDelta)%5E%7B-1%7DA%5E%7B-1%7D(b%20+%20%5Cdelta)%20%5C%5C%0A&amp;=%20(I%20-%20A%5E%7B-1%7D%5CDelta%20+%20o(%5C%7C%5CDelta%5C%7C))A%5E%7B-1%7D(b%20+%20%5Cdelta)%20%5C%5C%0A&amp;=%20A%5E%7B-1%7Db%20+%20A%5E%7B-1%7D(%5Cdelta%20-%20%5CDelta%20A%5E%7B-1%7Db%20)%20+%20o(%5C%7C%5CDelta%5C%7C%20+%20%5C%7C%5Cdelta%5C%7C)%0A%5Cend%7Balign*%7D"></p>
<p>Easy<sup>6</sup> as.</p>
<p>We’ve actually calculated the derivative now, but it’s a little more work to recognise it.</p>
<p>To do that, we need to remember the practical definition of the Jacobian of a function <img src="https://latex.codecogs.com/png.latex?f(x)"> that takes an <img src="https://latex.codecogs.com/png.latex?n">-dimensional input and produces an <img src="https://latex.codecogs.com/png.latex?m">-dimensional output. It is the <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20m"> matrix <img src="https://latex.codecogs.com/png.latex?J_f(x)"> such that <img src="https://latex.codecogs.com/png.latex?%0Af(x%20+%20%5Cdelta)%20%20=%20f(x)%20+%20J_f(x)%5Cdelta%20+%20o(%5C%7C%5Cdelta%5C%7C).%0A"></p>
<p>The formulas further simplify if we write <img src="https://latex.codecogs.com/png.latex?c%20=%20A%5E%7B-1%7Db">. Then, if we want the Jacobian-vector product for the first argument, it is <img src="https://latex.codecogs.com/png.latex?%0A-A%5E%7B-1%7D%5CDelta%20c,%0A"> while the Jacobian-vector product for the second argument is <img src="https://latex.codecogs.com/png.latex?%0AA%5E%7B-1%7D%5Cdelta.%0A"></p>
<p>The only wrinkle in doing this is we need to remember that we are only storing the lower triangle of <img src="https://latex.codecogs.com/png.latex?A">. Because we need to represent <img src="https://latex.codecogs.com/png.latex?%5CDelta"> the same way, it is represented as a vector <code>Delta_x</code> that contains only the lower triangle of <img src="https://latex.codecogs.com/png.latex?%5CDelta">. So we need to make sure we remember to form the <em>whole</em> matrix before we do the matrix-vector product <img src="https://latex.codecogs.com/png.latex?%5CDelta%20c">!</p>
<p>But otherwise, the implementation is going to be pretty straightforward. The Jacobian-vector product costs one additional linear solve (beyond the one needed to compute the value <img src="https://latex.codecogs.com/png.latex?c%20=%20A%5E%7B-1%7Db">).</p>
<p>In the language of JAX (and autodiff in general), we refer to <img src="https://latex.codecogs.com/png.latex?%5CDelta"> and <img src="https://latex.codecogs.com/png.latex?%5Cdelta"> as <em>tangent vectors</em>. In search of a moderately coherent naming convention, we are going to refer to the tangent associated with the variable <code>x</code> as <code>xt</code>.</p>
<p>So let’s implement this. Remember: it needs<sup>7</sup> to be JAX traceable.</p>
</section>
<section id="primitive-two-the-triangular-solve" class="level2">
<h2 class="anchored" data-anchor-id="primitive-two-the-triangular-solve">Primitive two: The triangular solve</h2>
<p>For some sense of continuity, we are going to keep the naming of the primitives from the last blog post, but we are <em>not</em> going to attack them in the same order. Why not? Because we work in order of complexity.</p>
<p>So first off we are going to do the triangular solve. As I have yet to package up the code (I promise, that will happen next<sup>8</sup>), I’m just putting it here under the fold.</p>
<details>
<summary>
The primal implementation
</summary>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">from</span> scipy <span class="im" style="color: #00769E;">import</span> sparse</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">from</span> jax <span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> jnp</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">from</span> jax <span class="im" style="color: #00769E;">import</span> core</span>
<span id="cb1-5"><span class="im" style="color: #00769E;">from</span> jax._src <span class="im" style="color: #00769E;">import</span> abstract_arrays</span>
<span id="cb1-6"><span class="im" style="color: #00769E;">from</span> jax <span class="im" style="color: #00769E;">import</span> core</span>
<span id="cb1-7"></span>
<span id="cb1-8">sparse_triangular_solve_p <span class="op" style="color: #5E5E5E;">=</span> core.Primitive(<span class="st" style="color: #20794D;">"sparse_triangular_solve"</span>)</span>
<span id="cb1-9"></span>
<span id="cb1-10"><span class="kw" style="color: #003B4F;">def</span> sparse_triangular_solve(L_indices, L_indptr, L_x, b, <span class="op" style="color: #5E5E5E;">*</span>, transpose: <span class="bu" style="color: null;">bool</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">False</span>):</span>
<span id="cb1-11">  <span class="co" style="color: #5E5E5E;">"""A JAX traceable sparse  triangular solve"""</span></span>
<span id="cb1-12">  <span class="cf" style="color: #003B4F;">return</span> sparse_triangular_solve_p.bind(L_indices, L_indptr, L_x, b, transpose <span class="op" style="color: #5E5E5E;">=</span> transpose)</span>
<span id="cb1-13"></span>
<span id="cb1-14"><span class="at" style="color: #657422;">@sparse_triangular_solve_p.def_impl</span></span>
<span id="cb1-15"><span class="kw" style="color: #003B4F;">def</span> sparse_triangular_solve_impl(L_indices, L_indptr, L_x, b, <span class="op" style="color: #5E5E5E;">*</span>, transpose <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">False</span>):</span>
<span id="cb1-16">  <span class="co" style="color: #5E5E5E;">"""The implementation of the sparse triangular solve. This is not JAX traceable."""</span></span>
<span id="cb1-17">  L <span class="op" style="color: #5E5E5E;">=</span> sparse.csc_array((L_x, L_indices, L_indptr)) </span>
<span id="cb1-18">  </span>
<span id="cb1-19">  <span class="cf" style="color: #003B4F;">assert</span> L.shape[<span class="dv" style="color: #AD0000;">0</span>] <span class="op" style="color: #5E5E5E;">==</span> L.shape[<span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb1-20">  <span class="cf" style="color: #003B4F;">assert</span> L.shape[<span class="dv" style="color: #AD0000;">0</span>] <span class="op" style="color: #5E5E5E;">==</span> b.shape[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb1-21">  </span>
<span id="cb1-22">  <span class="cf" style="color: #003B4F;">if</span> transpose:</span>
<span id="cb1-23">    <span class="cf" style="color: #003B4F;">return</span> sparse.linalg.spsolve_triangular(L.T, b, lower <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">False</span>)</span>
<span id="cb1-24">  <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb1-25">    <span class="cf" style="color: #003B4F;">return</span> sparse.linalg.spsolve_triangular(L.tocsr(), b, lower <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span>)</span>
<span id="cb1-26"></span>
<span id="cb1-27"><span class="at" style="color: #657422;">@sparse_triangular_solve_p.def_abstract_eval</span></span>
<span id="cb1-28"><span class="kw" style="color: #003B4F;">def</span> sparse_triangular_solve_abstract_eval(L_indices, L_indptr, L_x, b, <span class="op" style="color: #5E5E5E;">*</span>, transpose <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">False</span>):</span>
<span id="cb1-29">  <span class="cf" style="color: #003B4F;">assert</span> L_indices.shape[<span class="dv" style="color: #AD0000;">0</span>] <span class="op" style="color: #5E5E5E;">==</span> L_x.shape[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb1-30">  <span class="cf" style="color: #003B4F;">assert</span> b.shape[<span class="dv" style="color: #AD0000;">0</span>] <span class="op" style="color: #5E5E5E;">==</span> L_indptr.shape[<span class="dv" style="color: #AD0000;">0</span>] <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb1-31">  <span class="cf" style="color: #003B4F;">return</span> abstract_arrays.ShapedArray(b.shape, b.dtype)</span></code></pre></div>
</div>
</details>
<section id="the-jacobian-vector-product" class="level3">
<h3 class="anchored" data-anchor-id="the-jacobian-vector-product">The Jacobian-vector product</h3>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">from</span> jax._src <span class="im" style="color: #00769E;">import</span> ad_util</span>
<span id="cb2-2"><span class="im" style="color: #00769E;">from</span> jax.interpreters <span class="im" style="color: #00769E;">import</span> ad</span>
<span id="cb2-3"><span class="im" style="color: #00769E;">from</span> jax <span class="im" style="color: #00769E;">import</span> lax</span>
<span id="cb2-4"><span class="im" style="color: #00769E;">from</span> jax.experimental <span class="im" style="color: #00769E;">import</span> sparse <span class="im" style="color: #00769E;">as</span> jsparse</span>
<span id="cb2-5"></span>
<span id="cb2-6"><span class="kw" style="color: #003B4F;">def</span> sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent, <span class="op" style="color: #5E5E5E;">*</span>, transpose):</span>
<span id="cb2-7">  <span class="co" style="color: #5E5E5E;">"""</span></span>
<span id="cb2-8"><span class="co" style="color: #5E5E5E;">  A jax-traceable jacobian-vector product. In order to make it traceable, </span></span>
<span id="cb2-9"><span class="co" style="color: #5E5E5E;">  we use the experimental sparse CSC matrix in JAX.</span></span>
<span id="cb2-10"><span class="co" style="color: #5E5E5E;">  </span></span>
<span id="cb2-11"><span class="co" style="color: #5E5E5E;">  Input:</span></span>
<span id="cb2-12"><span class="co" style="color: #5E5E5E;">    arg_values:   A tuple of (L_indices, L_indptr, L_x, b) that describe</span></span>
<span id="cb2-13"><span class="co" style="color: #5E5E5E;">                  the triangular matrix L and the rhs vector b</span></span>
<span id="cb2-14"><span class="co" style="color: #5E5E5E;">    arg_tangent:  A tuple of tangent values (same lenght as arg_values).</span></span>
<span id="cb2-15"><span class="co" style="color: #5E5E5E;">                  The first two values are nonsense - we don't differentiate</span></span>
<span id="cb2-16"><span class="co" style="color: #5E5E5E;">                  wrt integers!</span></span>
<span id="cb2-17"><span class="co" style="color: #5E5E5E;">    transpose:    (boolean) If true, solve L^Tx = b. Otherwise solve Lx = b.</span></span>
<span id="cb2-18"><span class="co" style="color: #5E5E5E;">  Output:         A tuple containing the maybe_transpose(L)^{-1}b and the corresponding</span></span>
<span id="cb2-19"><span class="co" style="color: #5E5E5E;">                  Jacobian-vector product.</span></span>
<span id="cb2-20"><span class="co" style="color: #5E5E5E;">  """</span></span>
<span id="cb2-21">  L_indices, L_indptr, L_x, b <span class="op" style="color: #5E5E5E;">=</span> arg_values</span>
<span id="cb2-22">  _, _, L_xt, bt <span class="op" style="color: #5E5E5E;">=</span> arg_tangent</span>
<span id="cb2-23">  value <span class="op" style="color: #5E5E5E;">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, b, transpose<span class="op" style="color: #5E5E5E;">=</span>transpose)</span>
<span id="cb2-24">  <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">type</span>(bt) <span class="kw" style="color: #003B4F;">is</span> ad.Zero <span class="kw" style="color: #003B4F;">and</span> <span class="bu" style="color: null;">type</span>(L_xt) <span class="kw" style="color: #003B4F;">is</span> ad.Zero:</span>
<span id="cb2-25">    <span class="co" style="color: #5E5E5E;"># I legit do not think this ever happens. But I'm honestly not sure.</span></span>
<span id="cb2-26">    <span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"I have arrived!"</span>)</span>
<span id="cb2-27">    <span class="cf" style="color: #003B4F;">return</span> value, lax.zeros_like_array(value) </span>
<span id="cb2-28">  </span>
<span id="cb2-29">  <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">type</span>(L_xt) <span class="kw" style="color: #003B4F;">is</span> <span class="kw" style="color: #003B4F;">not</span> ad.Zero:</span>
<span id="cb2-30">    <span class="co" style="color: #5E5E5E;"># L is variable</span></span>
<span id="cb2-31">    <span class="cf" style="color: #003B4F;">if</span> transpose:</span>
<span id="cb2-32">      Delta <span class="op" style="color: #5E5E5E;">=</span> jsparse.CSC((L_xt, L_indices, L_indptr), shape <span class="op" style="color: #5E5E5E;">=</span> (b.shape[<span class="dv" style="color: #AD0000;">0</span>], b.shape[<span class="dv" style="color: #AD0000;">0</span>])).transpose()</span>
<span id="cb2-33">    <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb2-34">      Delta <span class="op" style="color: #5E5E5E;">=</span> jsparse.CSC((L_xt, L_indices, L_indptr), shape <span class="op" style="color: #5E5E5E;">=</span> (b.shape[<span class="dv" style="color: #AD0000;">0</span>], b.shape[<span class="dv" style="color: #AD0000;">0</span>]))</span>
<span id="cb2-35"></span>
<span id="cb2-36">    jvp_Lx <span class="op" style="color: #5E5E5E;">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, Delta <span class="op" style="color: #5E5E5E;">@</span> value, transpose <span class="op" style="color: #5E5E5E;">=</span> transpose) </span>
<span id="cb2-37">  <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb2-38">    jvp_Lx <span class="op" style="color: #5E5E5E;">=</span> lax.zeros_like_array(value) </span>
<span id="cb2-39"></span>
<span id="cb2-40">  <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">type</span>(bt) <span class="kw" style="color: #003B4F;">is</span> <span class="kw" style="color: #003B4F;">not</span> ad.Zero:</span>
<span id="cb2-41">    <span class="co" style="color: #5E5E5E;"># b is variable</span></span>
<span id="cb2-42">    jvp_b <span class="op" style="color: #5E5E5E;">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, bt, transpose <span class="op" style="color: #5E5E5E;">=</span> transpose)</span>
<span id="cb2-43">  <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb2-44">    jvp_b <span class="op" style="color: #5E5E5E;">=</span> lax.zeros_like_array(value)</span>
<span id="cb2-45"></span>
<span id="cb2-46">  <span class="cf" style="color: #003B4F;">return</span> value, jvp_b <span class="op" style="color: #5E5E5E;">-</span> jvp_Lx</span>
<span id="cb2-47"></span>
<span id="cb2-48">ad.primitive_jvps[sparse_triangular_solve_p] <span class="op" style="color: #5E5E5E;">=</span> sparse_triangular_solve_value_and_jvp</span></code></pre></div>
</div>
<p>Before we see if this works, let’s first have talk about the structure of the function I just wrote. Generally speaking, we want a function that takes in the primals and tangents at tuples and then returns the value and the<sup>9</sup> Jacobian-vector product.</p>
<p>The main thing you will notice in the code is that there is <em>a lot</em> of checking for <code>ad.Zero</code>. This is a special type defined in JAX that is, essentially, telling the autodiff system that we are not differentiating wrt that variable. This is different to a tangent that just happens to be numerically equal to zero. Any code for a Jacobian-vector product needs to handle this special value.</p>
<p>As we have two arguments, we have 3 interesting options:</p>
<ol type="1">
<li><p>Both <code>L_xt</code> and <code>bt</code> are <code>ad.Zero</code>: This means the function is a constant and the derivative is zero. I am fairly certain that we do not need to manually handle this case, but because I don’t know and I do not like surprises, it’s in there.</p></li>
<li><p><code>L_xt</code> is <em>not</em> <code>ad.Zero</code>: This means that we need to differentiate wrt the matrix. In this case we need to compute <img src="https://latex.codecogs.com/png.latex?%5CDelta%20c"> or <img src="https://latex.codecogs.com/png.latex?%5CDelta%5ET%20c">, depending on the <code>transpose</code> argument. In order to do this, I used the <code>jax.experimental.sparse.CSC</code> class, which has some very limited sparse matrix support (basically matrix-vector products). This is <em>extremely</em> convenient because it means I don’t need to write the matrix-vector product myself!</p></li>
<li><p><code>bt</code> is <em>not</em> <code>ad.Zero</code>: This means that we need to differentiate wrt the rhs vector. This part of the formula is pretty straightforward: just an application of the primal.</p></li>
</ol>
<p>In the case that either <code>L_xt</code> or <code>bt</code> are <code>ad.Zero</code>, we simply set the corresponding contribution to the jvp to zero.</p>
<p>It’s worth saying that you can bypass all of this <code>ad.Zero</code> logic by writing separate functions for the JVP contribution from each input and then chaining them together using<sup>10</sup> <code>ad.defjvp2()</code> to <a href="https://github.com/google/jax/blob/41417d70c03b6089c93a42325111a0d8348c2fa3/jax/_src/lax/linalg.py#L791">chain them together</a>. This is what the <code>lax.linalg.triangular_solve()</code> implementation does.</p>
<p>So why didn’t I do this? I avoided this because in the other primitives I have to implement, there are expensive computations (like Cholesky factorisations) that I want to share between the primal and the various tangent calculations. The <code>ad.defjvp</code> frameworks don’t allow for that. So I decided not to demonstrate/learn two separate patterns.</p>
</section>
<section id="transposition" class="level3">
<h3 class="anchored" data-anchor-id="transposition">Transposition</h3>
<p>Now I’ve never actively wanted a Jacobian-vector product in my whole life. I’m sorry. I want a gradient. Gimme a gradient. I am the Veruca Salt of gradients.</p>
<p>In may autodiff systems, if you want<sup>11</sup> a gradient, you need to implement vector-Jacobian products<sup>12</sup> explicitly.</p>
<p>One of the odder little innovations in JAX is that instead of forcing you to implement this as well<sup>13</sup>, you only need to implement half of it.</p>
<p>You see, some clever analysis that, as far as I far as I can tell<sup>14</sup>, is detailed in <a href="https://arxiv.org/abs/2204.10923">this paper</a> shows that you only need to form explicit vector-Jacobian products for the structurally linear arguments of the function.</p>
<p>In JAX (and maybe elsewhere), this is known as a <em>transposition rule</em>. The combination of a transopition rule and a JAX-traceable Jacobian-vector product is enough for JAX to compute all of the directional derivatives and gradients we could ever hope for.</p>
<p>As far as I understand, it is all about functions that are <em>structurally linear</em> in some arguments. For instance, if <img src="https://latex.codecogs.com/png.latex?A(x)"> is a matrix-valued function and <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y"> are vectors, then the function <img src="https://latex.codecogs.com/png.latex?%0Af(x,%20y)%20=%20A(x)y%20+%20g(x)%0A"> is structurally linear in <img src="https://latex.codecogs.com/png.latex?y"> in the sense that for every fixed value of <img src="https://latex.codecogs.com/png.latex?x">, the function <img src="https://latex.codecogs.com/png.latex?%0Af_x(y)%20=%20A(x)%20y%20+%20g(x)%0A"> is linear in <img src="https://latex.codecogs.com/png.latex?y">. The resulting transpositon rule is then</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;">def</span> f_transpose(x, y):</span>
<span id="cb3-2">  Ax <span class="op" style="color: #5E5E5E;">=</span> A(x)</span>
<span id="cb3-3">  gx <span class="op" style="color: #5E5E5E;">=</span> g(x)</span>
<span id="cb3-4">  <span class="cf" style="color: #003B4F;">return</span> (<span class="va" style="color: #111111;">None</span>, Ax.T <span class="op" style="color: #5E5E5E;">@</span> y <span class="op" style="color: #5E5E5E;">+</span> gx)</span></code></pre></div>
</div>
<p>The first element of the return is <code>None</code> because <img src="https://latex.codecogs.com/png.latex?f(x,y)"> is not<sup>15</sup> structurally linear in <img src="https://latex.codecogs.com/png.latex?x"> so there is nothing to transpose. The second element simply takes the matrix in the linear function and transposes it.</p>
<p>If you know anything about autodiff, you’ll think “this doesn’t <em>feel</em> like enough” and it’s not. JAX deals with the non-linear part of <img src="https://latex.codecogs.com/png.latex?f(x,y)"> by tracing the evaluation tree for its Jacobian-vector product and … manipulating<sup>16</sup> it.</p>
<p>We already built the abstract evaluation function last time around, so the tracing part can be done. All we need is the transposition rule.</p>
<p>The linear solve <img src="https://latex.codecogs.com/png.latex?f(A,%20b)%20=%20A%5E%7B-1%7Db"> is non-linear in the first argument but linear in the second argument. So we only need to implement <img src="https://latex.codecogs.com/png.latex?%0AJ%5ET_b(A,b)w%20=%20A%5E%7B-T%7Dw,%0A"> where the subscript <img src="https://latex.codecogs.com/png.latex?b"> indicates we’re only computing the Jacobian wrt <img src="https://latex.codecogs.com/png.latex?b">.</p>
<p>Initially, I struggled to work out what needed to be implemented here. The thing that clarified the process for me was looking at JAX’s <a href="https://github.com/google/jax/blob/41417d70c03b6089c93a42325111a0d8348c2fa3/jax/_src/lax/linalg.py#L747">internal implementation</a> of the Jacobian-vector product for a dense matrix. From there, I understood what this had to look like for a vector-valued function and this is the result.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;">def</span> sparse_triangular_solve_transpose_rule(cotangent, L_indices, L_indptr, L_x, b, <span class="op" style="color: #5E5E5E;">*</span>, transpose):</span>
<span id="cb4-2">  <span class="co" style="color: #5E5E5E;">"""</span></span>
<span id="cb4-3"><span class="co" style="color: #5E5E5E;">  Transposition rule for the triangular solve. </span></span>
<span id="cb4-4"><span class="co" style="color: #5E5E5E;">  Translated from here https://github.com/google/jax/blob/41417d70c03b6089c93a42325111a0d8348c2fa3/jax/_src/lax/linalg.py#L747.</span></span>
<span id="cb4-5"><span class="co" style="color: #5E5E5E;">  Inputs:</span></span>
<span id="cb4-6"><span class="co" style="color: #5E5E5E;">    cotangent: Output cotangent (aka adjoint). (produced by JAX)</span></span>
<span id="cb4-7"><span class="co" style="color: #5E5E5E;">    L_indices, L_indptr, L_x: Represenation of sparse matrix. L_x should be concrete</span></span>
<span id="cb4-8"><span class="co" style="color: #5E5E5E;">    b: The right hand side. Must be an jax.interpreters.ad.UndefinedPrimal</span></span>
<span id="cb4-9"><span class="co" style="color: #5E5E5E;">    transpose: (boolean) True: solve $L^Tx = b$. False: Solve $Lx = b$.</span></span>
<span id="cb4-10"><span class="co" style="color: #5E5E5E;">  Output:</span></span>
<span id="cb4-11"><span class="co" style="color: #5E5E5E;">    A 4-tuple with the adjoints (None, None, None, b_adjoint)</span></span>
<span id="cb4-12"><span class="co" style="color: #5E5E5E;">  """</span></span>
<span id="cb4-13">  <span class="cf" style="color: #003B4F;">assert</span> <span class="kw" style="color: #003B4F;">not</span> ad.is_undefined_primal(L_x) <span class="kw" style="color: #003B4F;">and</span> ad.is_undefined_primal(b)</span>
<span id="cb4-14">  <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">type</span>(cotangent) <span class="kw" style="color: #003B4F;">is</span> ad_util.Zero:</span>
<span id="cb4-15">    cot_b <span class="op" style="color: #5E5E5E;">=</span> ad_util.Zero(b.aval)</span>
<span id="cb4-16">  <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb4-17">    cot_b <span class="op" style="color: #5E5E5E;">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, cotangent, transpose <span class="op" style="color: #5E5E5E;">=</span> <span class="kw" style="color: #003B4F;">not</span> transpose)</span>
<span id="cb4-18">  <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">None</span>, <span class="va" style="color: #111111;">None</span>, <span class="va" style="color: #111111;">None</span>, cot_b</span>
<span id="cb4-19"></span>
<span id="cb4-20">ad.primitive_transposes[sparse_triangular_solve_p] <span class="op" style="color: #5E5E5E;">=</span> sparse_triangular_solve_transpose_rule</span></code></pre></div>
</div>
<p>If this doesn’t make a lot of sense to you, that’s because it’s confusing.</p>
<p>One way to think of it is in terms of the more ordinary notation. Mike Giles has <a href="https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf">a classic paper</a> that covers these results for basic linear algebra. The idea is to imagine that, as part of your larger program, you need to compute <img src="https://latex.codecogs.com/png.latex?c%20=%20A%5E%7B-1%7Db">.</p>
<p>Forward-mode autodiff computes the <em>sensitivity</em> of <img src="https://latex.codecogs.com/png.latex?c">, usually denoted <img src="https://latex.codecogs.com/png.latex?%5Cdot%20c"> from the sensitivies <img src="https://latex.codecogs.com/png.latex?%5Cdot%20A"> and <img src="https://latex.codecogs.com/png.latex?%5Cdot%20b">. These have already been computed. The formula in Giles is <img src="https://latex.codecogs.com/png.latex?%0A%5Cdot%20c%20=%20A%5E%7B-1%7D(%5Cdot%20b%20-%20%5Cdot%20A%20c).%0A"> The canny reader will recognise this as exactly<sup>17</sup> the formula for the Jacobian-vector product.</p>
<p>So what does reverse-mode autodiff do? Well it moves through the program in the other direction. So instead of starting with the sensitivities <img src="https://latex.codecogs.com/png.latex?%5Cdot%20A"> and <img src="https://latex.codecogs.com/png.latex?%5Cdot%20b"> already computed, we instead start with the<sup>18</sup> <em>adjoint sensitivity</em> <img src="https://latex.codecogs.com/png.latex?%5Cbar%20c">. Our aim is to compute <img src="https://latex.codecogs.com/png.latex?%5Cbar%20A"> and <img src="https://latex.codecogs.com/png.latex?%5Cbar%20b"> from <img src="https://latex.codecogs.com/png.latex?%5Cbar%20c">.</p>
<p>The details of how to do this are<sup>19</sup> <em>beyond the scope</em>, but without tooooooo much effort you can show that <img src="https://latex.codecogs.com/png.latex?%0A%5Cbar%20b%20=%20A%5E%7B-T%7D%20%5Cbar%20c,%0A"> which you should recognise as the equation that was just implemented.</p>
<p>The thing that we <em>do not</em> have to implement in JAX is the other adjoint that, for dense matrices<sup>20</sup>, is <img src="https://latex.codecogs.com/png.latex?%0A%5Cbar%7BA%7D%20=%20-%5Cbar%7Bb%7Dc%5ET.%0A"> Through the healing power of … something?—Truly I do not know.— JAX can work that bit out itself. woo.</p>
</section>
<section id="testing-the-numerical-implementation-of-the-jacobian-vector-product" class="level3">
<h3 class="anchored" data-anchor-id="testing-the-numerical-implementation-of-the-jacobian-vector-product">Testing the numerical implementation of the Jacobian-vector product</h3>
<p>So let’s see if this works. I’m not going to lie, I’m flying by the seat of my pants here. I’m not super familiar with the JAX internals, so I have written a lot of test cases. You may wish to skip this part. But rest assured that almost every single one of these cases was useful to me working out how this thing actually worked!</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;">def</span> make_matrix(n):</span>
<span id="cb5-2">    one_d <span class="op" style="color: #5E5E5E;">=</span> sparse.diags([[<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">1.</span>]<span class="op" style="color: #5E5E5E;">*</span>(n<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>), [<span class="fl" style="color: #AD0000;">2.</span>]<span class="op" style="color: #5E5E5E;">*</span>n, [<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">1.</span>]<span class="op" style="color: #5E5E5E;">*</span>(n<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)], [<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb5-3">    A <span class="op" style="color: #5E5E5E;">=</span> (sparse.kronsum(one_d, one_d) <span class="op" style="color: #5E5E5E;">+</span> sparse.eye(n<span class="op" style="color: #5E5E5E;">*</span>n)).tocsc()</span>
<span id="cb5-4">    A_lower <span class="op" style="color: #5E5E5E;">=</span> sparse.tril(A, <span class="bu" style="color: null;">format</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"csc"</span>)</span>
<span id="cb5-5">    A_index <span class="op" style="color: #5E5E5E;">=</span> A_lower.indices</span>
<span id="cb5-6">    A_indptr <span class="op" style="color: #5E5E5E;">=</span> A_lower.indptr</span>
<span id="cb5-7">    A_x <span class="op" style="color: #5E5E5E;">=</span> A_lower.data</span>
<span id="cb5-8">    <span class="cf" style="color: #003B4F;">return</span> (A_index, A_indptr, A_x, A)</span>
<span id="cb5-9"></span>
<span id="cb5-10">A_indices, A_indptr, A_x, A <span class="op" style="color: #5E5E5E;">=</span> make_matrix(<span class="dv" style="color: #AD0000;">10</span>)</span></code></pre></div>
</div>
<p>This is the same test case as the last blog. We will just use the lower triangle of <img src="https://latex.codecogs.com/png.latex?A"> as the test matrix.</p>
<p>First things first, let’s check out the numerical implementation of the function. We will do that by comparing the implemented Jacobian-vector product with the <em>definition</em> of the Jacobian-vector product (aka the forward<sup>21</sup> difference approximation).</p>
<p>There are lots of things that we could do here to turn these into <em>actual</em> tests. For instance, the test suite inside JAX has a lot of nice convenience functions for checking implementations of derivatives. But I went with homespun because that was how I was feeling.</p>
<p>You’ll also notice that I’m using random numbers here, which is fine for a blog. Not so fine for a test that you don’t want to be potentially<sup>22</sup> flaky.</p>
<p>The choice of <code>eps = 1e-4</code> is roughly<sup>23</sup> because it’s the square root of the single precision machine epsilon<sup>24</sup>. A very rough back of the envelope calculation for the forward difference approximation to the derivative shows that the square root of the machine epislon is about the size you want your perturbation to be.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1">b <span class="op" style="color: #5E5E5E;">=</span> np.random.standard_normal(<span class="dv" style="color: #AD0000;">100</span>)</span>
<span id="cb6-2"></span>
<span id="cb6-3">bt <span class="op" style="color: #5E5E5E;">=</span> np.random.standard_normal(<span class="dv" style="color: #AD0000;">100</span>)</span>
<span id="cb6-4">bt <span class="op" style="color: #5E5E5E;">/=</span> np.linalg.norm(bt)</span>
<span id="cb6-5"></span>
<span id="cb6-6">A_xt <span class="op" style="color: #5E5E5E;">=</span> np.random.standard_normal(<span class="bu" style="color: null;">len</span>(A_x))</span>
<span id="cb6-7">A_xt <span class="op" style="color: #5E5E5E;">/=</span> np.linalg.norm(A_xt)</span>
<span id="cb6-8"></span>
<span id="cb6-9">arg_values <span class="op" style="color: #5E5E5E;">=</span> (A_indices, A_indptr, A_x, b )</span>
<span id="cb6-10"></span>
<span id="cb6-11">arg_tangent_A <span class="op" style="color: #5E5E5E;">=</span> (<span class="va" style="color: #111111;">None</span>, <span class="va" style="color: #111111;">None</span>, A_xt, ad.Zero(<span class="bu" style="color: null;">type</span>(b)))</span>
<span id="cb6-12">arg_tangent_b <span class="op" style="color: #5E5E5E;">=</span> (<span class="va" style="color: #111111;">None</span>, <span class="va" style="color: #111111;">None</span>, ad.Zero(<span class="bu" style="color: null;">type</span>(A_xt)), bt)</span>
<span id="cb6-13">arg_tangent_Ab <span class="op" style="color: #5E5E5E;">=</span> (<span class="va" style="color: #111111;">None</span>, <span class="va" style="color: #111111;">None</span>, A_xt, bt)</span>
<span id="cb6-14"></span>
<span id="cb6-15">p, t_A <span class="op" style="color: #5E5E5E;">=</span> sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_A, transpose <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">False</span>)</span>
<span id="cb6-16">_, t_b <span class="op" style="color: #5E5E5E;">=</span> sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_b, transpose <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">False</span>)</span>
<span id="cb6-17">_, t_Ab <span class="op" style="color: #5E5E5E;">=</span> sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_Ab, transpose <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">False</span>)</span>
<span id="cb6-18">pT, t_AT <span class="op" style="color: #5E5E5E;">=</span> sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_A, transpose <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span>)</span>
<span id="cb6-19">_, t_bT <span class="op" style="color: #5E5E5E;">=</span> sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_b, transpose <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span>)</span>
<span id="cb6-20"></span>
<span id="cb6-21">eps <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">1e-4</span></span>
<span id="cb6-22">tt_A <span class="op" style="color: #5E5E5E;">=</span> (sparse_triangular_solve(A_indices, A_indptr, A_x <span class="op" style="color: #5E5E5E;">+</span> eps <span class="op" style="color: #5E5E5E;">*</span> A_xt, b) <span class="op" style="color: #5E5E5E;">-</span> p) <span class="op" style="color: #5E5E5E;">/</span>eps</span>
<span id="cb6-23">tt_b <span class="op" style="color: #5E5E5E;">=</span> (sparse_triangular_solve(A_indices, A_indptr, A_x, b <span class="op" style="color: #5E5E5E;">+</span> eps <span class="op" style="color: #5E5E5E;">*</span> bt) <span class="op" style="color: #5E5E5E;">-</span> p) <span class="op" style="color: #5E5E5E;">/</span> eps</span>
<span id="cb6-24">tt_Ab <span class="op" style="color: #5E5E5E;">=</span> (sparse_triangular_solve(A_indices, A_indptr, A_x <span class="op" style="color: #5E5E5E;">+</span> eps <span class="op" style="color: #5E5E5E;">*</span> A_xt, b <span class="op" style="color: #5E5E5E;">+</span> eps <span class="op" style="color: #5E5E5E;">*</span> bt) <span class="op" style="color: #5E5E5E;">-</span> p) <span class="op" style="color: #5E5E5E;">/</span> eps</span>
<span id="cb6-25">tt_AT <span class="op" style="color: #5E5E5E;">=</span> (sparse_triangular_solve(A_indices, A_indptr, A_x <span class="op" style="color: #5E5E5E;">+</span> eps <span class="op" style="color: #5E5E5E;">*</span> A_xt, b, transpose <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span>) <span class="op" style="color: #5E5E5E;">-</span> pT) <span class="op" style="color: #5E5E5E;">/</span> eps</span>
<span id="cb6-26">tt_bT <span class="op" style="color: #5E5E5E;">=</span> (sparse_triangular_solve(A_indices, A_indptr, A_x, b <span class="op" style="color: #5E5E5E;">+</span> eps <span class="op" style="color: #5E5E5E;">*</span> bt, transpose <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span>) <span class="op" style="color: #5E5E5E;">-</span> pT) <span class="op" style="color: #5E5E5E;">/</span> eps</span>
<span id="cb6-27"></span>
<span id="cb6-28"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"""</span></span>
<span id="cb6-29"><span class="ss" style="color: #20794D;">Transpose = False:</span></span>
<span id="cb6-30"><span class="ss" style="color: #20794D;">  Error A varying: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(t_A <span class="op" style="color: #5E5E5E;">-</span> tt_A)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb6-31"><span class="ss" style="color: #20794D;">  Error b varying: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(t_b <span class="op" style="color: #5E5E5E;">-</span> tt_b)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb6-32"><span class="ss" style="color: #20794D;">  Error A and b varying: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(t_Ab <span class="op" style="color: #5E5E5E;">-</span> tt_Ab)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb6-33"></span>
<span id="cb6-34"><span class="ss" style="color: #20794D;">Transpose = True:</span></span>
<span id="cb6-35"><span class="ss" style="color: #20794D;">  Error A varying: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(t_AT <span class="op" style="color: #5E5E5E;">-</span> tt_AT)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb6-36"><span class="ss" style="color: #20794D;">  Error b varying: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(t_bT <span class="op" style="color: #5E5E5E;">-</span> tt_bT)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb6-37"><span class="ss" style="color: #20794D;">"""</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Transpose = False:
  Error A varying:  8.47e-08
  Error b varying:  0.00e+00
  Error A and b varying:  4.20e-07

Transpose = True:
  Error A varying:  8.15e-08
  Error b varying:  0.00e+00
</code></pre>
</div>
</div>
<p>Brilliant! Everythign correct withing single precision!</p>
</section>
<section id="checking-on-the-plumbing" class="level3">
<h3 class="anchored" data-anchor-id="checking-on-the-plumbing">Checking on the plumbing</h3>
<p>Making the numerical implementation work is only half the battle. We also have to make it work <em>in the context of JAX</em>.</p>
<p>Now I would be lying if I pretended this process went smoothly. But the first time is for experience. It’s mostly a matter of just reading the documentation carefully and going through similar examples that have already been implemented.</p>
<p>And testing. I learnt how this was supposed to work by testing it.</p>
<p>(For full disclosure, I also wrote a big block f-string in the <code>sparse_triangular_solve()</code> function at one point that told me the types, shapes, and what <code>transpose</code> was, which was how I worked out that my code was breaking because I forgot the first to <code>None</code> outputs in the transposition rule. When it doubt, print shit.)</p>
<p>As you will see from my testing code, I was not going for elegance. I was running the damn permutations. If you’re looking for elegance, look elsewhere.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;">from</span> jax <span class="im" style="color: #00769E;">import</span> jvp, grad</span>
<span id="cb8-2"><span class="im" style="color: #00769E;">from</span> jax <span class="im" style="color: #00769E;">import</span> scipy <span class="im" style="color: #00769E;">as</span> jsp</span>
<span id="cb8-3"></span>
<span id="cb8-4"><span class="kw" style="color: #003B4F;">def</span> f(theta):</span>
<span id="cb8-5">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> jnp.array(A_x)</span>
<span id="cb8-6">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> Ax_theta.at[A_indptr[<span class="dv" style="color: #AD0000;">20</span>]].add(theta[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb8-7">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> Ax_theta.at[A_indptr[<span class="dv" style="color: #AD0000;">50</span>]].add(theta[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb8-8">  b <span class="op" style="color: #5E5E5E;">=</span> jnp.ones(<span class="dv" style="color: #AD0000;">100</span>)</span>
<span id="cb8-9">  <span class="cf" style="color: #003B4F;">return</span> sparse_triangular_solve(A_indices, A_indptr, Ax_theta, b, transpose <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span>)</span>
<span id="cb8-10"></span>
<span id="cb8-11"><span class="kw" style="color: #003B4F;">def</span> f_jax(theta):</span>
<span id="cb8-12">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> jnp.array(sparse.tril(A).todense())</span>
<span id="cb8-13">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> Ax_theta.at[<span class="dv" style="color: #AD0000;">20</span>,<span class="dv" style="color: #AD0000;">20</span>].add(theta[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb8-14">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> Ax_theta.at[<span class="dv" style="color: #AD0000;">50</span>,<span class="dv" style="color: #AD0000;">50</span>].add(theta[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb8-15">  b <span class="op" style="color: #5E5E5E;">=</span> jnp.ones(<span class="dv" style="color: #AD0000;">100</span>)</span>
<span id="cb8-16">  <span class="cf" style="color: #003B4F;">return</span> jsp.linalg.solve_triangular(Ax_theta, b, lower <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span>, trans <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"T"</span>)</span>
<span id="cb8-17"></span>
<span id="cb8-18"><span class="kw" style="color: #003B4F;">def</span> g(theta):</span>
<span id="cb8-19">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> jnp.array(A_x)</span>
<span id="cb8-20">  b <span class="op" style="color: #5E5E5E;">=</span> jnp.ones(<span class="dv" style="color: #AD0000;">100</span>)</span>
<span id="cb8-21">  b <span class="op" style="color: #5E5E5E;">=</span> b.at[<span class="dv" style="color: #AD0000;">0</span>].<span class="bu" style="color: null;">set</span>(theta[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb8-22">  b <span class="op" style="color: #5E5E5E;">=</span> b.at[<span class="dv" style="color: #AD0000;">51</span>].<span class="bu" style="color: null;">set</span>(theta[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb8-23">  <span class="cf" style="color: #003B4F;">return</span> sparse_triangular_solve(A_indices, A_indptr, Ax_theta, b, transpose <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span>)</span>
<span id="cb8-24"></span>
<span id="cb8-25"><span class="kw" style="color: #003B4F;">def</span> g_jax(theta):</span>
<span id="cb8-26">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> jnp.array(sparse.tril(A).todense())</span>
<span id="cb8-27">  b <span class="op" style="color: #5E5E5E;">=</span> jnp.ones(<span class="dv" style="color: #AD0000;">100</span>)</span>
<span id="cb8-28">  b <span class="op" style="color: #5E5E5E;">=</span> b.at[<span class="dv" style="color: #AD0000;">0</span>].<span class="bu" style="color: null;">set</span>(theta[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb8-29">  b <span class="op" style="color: #5E5E5E;">=</span> b.at[<span class="dv" style="color: #AD0000;">51</span>].<span class="bu" style="color: null;">set</span>(theta[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb8-30">  <span class="cf" style="color: #003B4F;">return</span> jsp.linalg.solve_triangular(Ax_theta, b, lower <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span>, trans <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"T"</span>)</span>
<span id="cb8-31"></span>
<span id="cb8-32"><span class="kw" style="color: #003B4F;">def</span> h(theta):</span>
<span id="cb8-33">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> jnp.array(A_x)</span>
<span id="cb8-34">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> Ax_theta.at[A_indptr[<span class="dv" style="color: #AD0000;">20</span>]].add(theta[<span class="dv" style="color: #AD0000;">0</span>]) </span>
<span id="cb8-35">  b <span class="op" style="color: #5E5E5E;">=</span> jnp.ones(<span class="dv" style="color: #AD0000;">100</span>)</span>
<span id="cb8-36">  b <span class="op" style="color: #5E5E5E;">=</span> b.at[<span class="dv" style="color: #AD0000;">51</span>].<span class="bu" style="color: null;">set</span>(theta[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb8-37">  <span class="cf" style="color: #003B4F;">return</span> sparse_triangular_solve(A_indices, A_indptr, Ax_theta, b, transpose <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">False</span>)</span>
<span id="cb8-38"></span>
<span id="cb8-39"><span class="kw" style="color: #003B4F;">def</span> h_jax(theta):</span>
<span id="cb8-40">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> jnp.array(sparse.tril(A).todense())</span>
<span id="cb8-41">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> Ax_theta.at[<span class="dv" style="color: #AD0000;">20</span>,<span class="dv" style="color: #AD0000;">20</span>].add(theta[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb8-42">  b <span class="op" style="color: #5E5E5E;">=</span> jnp.ones(<span class="dv" style="color: #AD0000;">100</span>)</span>
<span id="cb8-43">  b <span class="op" style="color: #5E5E5E;">=</span> b.at[<span class="dv" style="color: #AD0000;">51</span>].<span class="bu" style="color: null;">set</span>(theta[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb8-44">  <span class="cf" style="color: #003B4F;">return</span> jsp.linalg.solve_triangular(Ax_theta, b, lower <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span>, trans <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"N"</span>)</span>
<span id="cb8-45"></span>
<span id="cb8-46"><span class="kw" style="color: #003B4F;">def</span> no_diff(theta):</span>
<span id="cb8-47">  <span class="cf" style="color: #003B4F;">return</span> sparse_triangular_solve(A_indices, A_indptr, A_x, jnp.ones(<span class="dv" style="color: #AD0000;">100</span>), transpose <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">False</span>)</span>
<span id="cb8-48"></span>
<span id="cb8-49"><span class="kw" style="color: #003B4F;">def</span> no_diff_jax(theta):</span>
<span id="cb8-50">  <span class="cf" style="color: #003B4F;">return</span> jsp.linalg.solve_triangular(jnp.array(sparse.tril(A).todense()), jnp.ones(<span class="dv" style="color: #AD0000;">100</span>), lower <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span>, trans <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"N"</span>)</span>
<span id="cb8-51"></span>
<span id="cb8-52">A_indices, A_indptr, A_x, A <span class="op" style="color: #5E5E5E;">=</span> make_matrix(<span class="dv" style="color: #AD0000;">10</span>)</span>
<span id="cb8-53">primal1, jvp1 <span class="op" style="color: #5E5E5E;">=</span> jvp(f, (jnp.array([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">142.</span>, <span class="fl" style="color: #AD0000;">342.</span>]),), (jnp.array([<span class="fl" style="color: #AD0000;">1.</span>, <span class="fl" style="color: #AD0000;">2.</span>]),))</span>
<span id="cb8-54">primal2, jvp2 <span class="op" style="color: #5E5E5E;">=</span> jvp(f_jax, (jnp.array([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">142.</span>, <span class="fl" style="color: #AD0000;">342.</span>]),), (jnp.array([<span class="fl" style="color: #AD0000;">1.</span>, <span class="fl" style="color: #AD0000;">2.</span>]),))</span>
<span id="cb8-55">grad1 <span class="op" style="color: #5E5E5E;">=</span> grad(<span class="kw" style="color: #003B4F;">lambda</span> x: jnp.mean(f(x)))(jnp.array([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">142.</span>, <span class="fl" style="color: #AD0000;">342.</span>]))</span>
<span id="cb8-56">grad2 <span class="op" style="color: #5E5E5E;">=</span> grad(<span class="kw" style="color: #003B4F;">lambda</span> x: jnp.mean(f_jax(x)))(jnp.array([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">142.</span>, <span class="fl" style="color: #AD0000;">342.</span>]))</span>
<span id="cb8-57"></span>
<span id="cb8-58">primal3, jvp3 <span class="op" style="color: #5E5E5E;">=</span> jvp(g, (jnp.array([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">142.</span>, <span class="fl" style="color: #AD0000;">342.</span>]),), (jnp.array([<span class="fl" style="color: #AD0000;">1.</span>, <span class="fl" style="color: #AD0000;">2.</span>]),))</span>
<span id="cb8-59">primal4, jvp4 <span class="op" style="color: #5E5E5E;">=</span> jvp(g_jax, (jnp.array([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">142.</span>, <span class="fl" style="color: #AD0000;">342.</span>]),), (jnp.array([<span class="fl" style="color: #AD0000;">1.</span>, <span class="fl" style="color: #AD0000;">2.</span>]),))</span>
<span id="cb8-60">grad3 <span class="op" style="color: #5E5E5E;">=</span> grad(<span class="kw" style="color: #003B4F;">lambda</span> x: jnp.mean(g(x)))(jnp.array([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">142.</span>, <span class="fl" style="color: #AD0000;">342.</span>]))</span>
<span id="cb8-61">grad4 <span class="op" style="color: #5E5E5E;">=</span> grad(<span class="kw" style="color: #003B4F;">lambda</span> x: jnp.mean(g_jax(x)))(jnp.array([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">142.</span>, <span class="fl" style="color: #AD0000;">342.</span>]))  </span>
<span id="cb8-62"></span>
<span id="cb8-63">primal5, jvp5 <span class="op" style="color: #5E5E5E;">=</span> jvp(h, (jnp.array([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">142.</span>, <span class="fl" style="color: #AD0000;">342.</span>]),), (jnp.array([<span class="fl" style="color: #AD0000;">1.</span>, <span class="fl" style="color: #AD0000;">2.</span>]),))</span>
<span id="cb8-64">primal6, jvp6 <span class="op" style="color: #5E5E5E;">=</span> jvp(h_jax, (jnp.array([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">142.</span>, <span class="fl" style="color: #AD0000;">342.</span>]),), (jnp.array([<span class="fl" style="color: #AD0000;">1.</span>, <span class="fl" style="color: #AD0000;">2.</span>]),))</span>
<span id="cb8-65">grad5 <span class="op" style="color: #5E5E5E;">=</span> grad(<span class="kw" style="color: #003B4F;">lambda</span> x: jnp.mean(h(x)))(jnp.array([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">142.</span>, <span class="fl" style="color: #AD0000;">342.</span>]))</span>
<span id="cb8-66">grad6 <span class="op" style="color: #5E5E5E;">=</span> grad(<span class="kw" style="color: #003B4F;">lambda</span> x: jnp.mean(h_jax(x)))(jnp.array([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">142.</span>, <span class="fl" style="color: #AD0000;">342.</span>]))</span>
<span id="cb8-67"></span>
<span id="cb8-68">primal7, jvp7 <span class="op" style="color: #5E5E5E;">=</span> jvp(no_diff, (jnp.array([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">142.</span>, <span class="fl" style="color: #AD0000;">342.</span>]),), (jnp.array([<span class="fl" style="color: #AD0000;">1.</span>, <span class="fl" style="color: #AD0000;">2.</span>]),))</span>
<span id="cb8-69">primal8, jvp8 <span class="op" style="color: #5E5E5E;">=</span> jvp(no_diff_jax, (jnp.array([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">142.</span>, <span class="fl" style="color: #AD0000;">342.</span>]),), (jnp.array([<span class="fl" style="color: #AD0000;">1.</span>, <span class="fl" style="color: #AD0000;">2.</span>]),))</span>
<span id="cb8-70">grad7 <span class="op" style="color: #5E5E5E;">=</span> grad(<span class="kw" style="color: #003B4F;">lambda</span> x: jnp.mean(no_diff(x)))(jnp.array([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">142.</span>, <span class="fl" style="color: #AD0000;">342.</span>]))</span>
<span id="cb8-71">grad8 <span class="op" style="color: #5E5E5E;">=</span> grad(<span class="kw" style="color: #003B4F;">lambda</span> x: jnp.mean(no_diff_jax(x)))(jnp.array([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">142.</span>, <span class="fl" style="color: #AD0000;">342.</span>]))</span>
<span id="cb8-72"></span>
<span id="cb8-73"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"""</span></span>
<span id="cb8-74"><span class="ss" style="color: #20794D;">Variable L:</span></span>
<span id="cb8-75"><span class="ss" style="color: #20794D;">  Primal difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(primal1 <span class="op" style="color: #5E5E5E;">-</span> primal2)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb8-76"><span class="ss" style="color: #20794D;">  JVP difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(jvp1 <span class="op" style="color: #5E5E5E;">-</span> jvp2)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb8-77"><span class="ss" style="color: #20794D;">  Gradient difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(grad1 <span class="op" style="color: #5E5E5E;">-</span> grad2)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb8-78"></span>
<span id="cb8-79"><span class="ss" style="color: #20794D;">Variable b:</span></span>
<span id="cb8-80"><span class="ss" style="color: #20794D;">  Primal difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(primal3 <span class="op" style="color: #5E5E5E;">-</span> primal4)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb8-81"><span class="ss" style="color: #20794D;">  JVP difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(jvp3 <span class="op" style="color: #5E5E5E;">-</span> jvp4)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb8-82"><span class="ss" style="color: #20794D;">  Gradient difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(grad3 <span class="op" style="color: #5E5E5E;">-</span> grad4)<span class="sc" style="color: #5E5E5E;">: .2e}</span><span class="ss" style="color: #20794D;"> </span></span>
<span id="cb8-83"></span>
<span id="cb8-84"><span class="ss" style="color: #20794D;">Variable L and b:</span></span>
<span id="cb8-85"><span class="ss" style="color: #20794D;">  Primal difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(primal5 <span class="op" style="color: #5E5E5E;">-</span> primal6)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb8-86"><span class="ss" style="color: #20794D;">  JVP difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(jvp5 <span class="op" style="color: #5E5E5E;">-</span> jvp6)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb8-87"><span class="ss" style="color: #20794D;">  Gradient difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(grad5 <span class="op" style="color: #5E5E5E;">-</span> grad6)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb8-88"></span>
<span id="cb8-89"><span class="ss" style="color: #20794D;">No diff:</span></span>
<span id="cb8-90"><span class="ss" style="color: #20794D;">  Primal difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(primal7 <span class="op" style="color: #5E5E5E;">-</span> primal8)<span class="sc" style="color: #5E5E5E;">}</span></span>
<span id="cb8-91"><span class="ss" style="color: #20794D;">  JVP difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(jvp7 <span class="op" style="color: #5E5E5E;">-</span> jvp8)<span class="sc" style="color: #5E5E5E;">}</span></span>
<span id="cb8-92"><span class="ss" style="color: #20794D;">  Gradient difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(grad7 <span class="op" style="color: #5E5E5E;">-</span> grad8)<span class="sc" style="color: #5E5E5E;">}</span></span>
<span id="cb8-93"><span class="ss" style="color: #20794D;">"""</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Variable L:
  Primal difference:  1.98e-07
  JVP difference:  2.58e-12
  Gradient difference:  0.00e+00

Variable b:
  Primal difference:  7.94e-06
  JVP difference:  1.83e-08
  Gradient difference:  3.29e-10 

Variable L and b:
  Primal difference:  2.08e-06
  JVP difference:  1.08e-08
  Gradient difference:  2.33e-10

No diff:
  Primal difference: 2.2101993124579167e-07
  JVP difference: 0.0
  Gradient difference: 0.0
</code></pre>
</div>
</div>
<p>Stunning!</p>
</section>
</section>
<section id="primitive-one-the-general-a-1b" class="level2">
<h2 class="anchored" data-anchor-id="primitive-one-the-general-a-1b">Primitive one: The general <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7Db"></h2>
<p>Ok. So this is a very similar problem to the one that we just solved. But, as fate would have it, the solution is going to look quite different. Why? Because we need to compute a Cholesky factorisation.</p>
<p>First things first, though, we are going to need a JAX-traceable way to compute a Cholesky factor. This means that we need<sup>25</sup> to tell our <code>sparse_solve</code> function the how many non-zeros the sparse Cholesky will have. Why? Well. It has to do with how the function is used.</p>
<p>When <code>sparse_cholesky()</code> is called with concrete inputs<sup>26</sup>, then it can quite happily work out the sparsity structure of <img src="https://latex.codecogs.com/png.latex?L">. But when JAX is preparing to transform the code, eg when it’s building a gradient, it calls <code>sparse_cholesky()</code> using abstract arguments that only share the shape information from the inputs. This is <em>not</em> enough to compute the sparsity structure. We <em>need</em> the <code>indices</code> and <code>indptr</code> arrays.</p>
<p>This means that we need <code>sparse_cholesky()</code> to throw an error if <code>L_nse</code> isn’t passed. This wasn’t implemented well last time, so here it is done properly.</p>
<p>(If you’re wondering about that <code>None</code> argument, it is the identity transform. So if <code>A_indices</code> is a concrete value, <code>ind = A_indices</code>. Otherwise an error is called.)</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1">sparse_cholesky_p <span class="op" style="color: #5E5E5E;">=</span> core.Primitive(<span class="st" style="color: #20794D;">"sparse_cholesky"</span>)</span>
<span id="cb10-2"></span>
<span id="cb10-3"><span class="kw" style="color: #003B4F;">def</span> sparse_cholesky(A_indices, A_indptr, A_x, <span class="op" style="color: #5E5E5E;">*</span>, L_nse: <span class="bu" style="color: null;">int</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">None</span>):</span>
<span id="cb10-4">  <span class="co" style="color: #5E5E5E;">"""A JAX traceable sparse cholesky decomposition"""</span></span>
<span id="cb10-5">  <span class="cf" style="color: #003B4F;">if</span> L_nse <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>:</span>
<span id="cb10-6">    err_string <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"You need to pass a value to L_nse when doing fancy sparse_cholesky."</span></span>
<span id="cb10-7">    ind <span class="op" style="color: #5E5E5E;">=</span> core.concrete_or_error(<span class="va" style="color: #111111;">None</span>, A_indices, err_string)</span>
<span id="cb10-8">    ptr <span class="op" style="color: #5E5E5E;">=</span> core.concrete_or_error(<span class="va" style="color: #111111;">None</span>, A_indptr, err_string)</span>
<span id="cb10-9">    L_ind, _ <span class="op" style="color: #5E5E5E;">=</span> _symbolic_factor(ind, ptr)</span>
<span id="cb10-10">    L_nse <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(L_ind)</span>
<span id="cb10-11">  </span>
<span id="cb10-12">  <span class="cf" style="color: #003B4F;">return</span> sparse_cholesky_p.bind(A_indices, A_indptr, A_x, L_nse <span class="op" style="color: #5E5E5E;">=</span> L_nse)</span></code></pre></div>
</div>
<details>
<summary>
The rest of the Choleksy code
</summary>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="at" style="color: #657422;">@sparse_cholesky_p.def_impl</span></span>
<span id="cb11-2"><span class="kw" style="color: #003B4F;">def</span> sparse_cholesky_impl(A_indices, A_indptr, A_x, <span class="op" style="color: #5E5E5E;">*</span>, L_nse):</span>
<span id="cb11-3">  <span class="co" style="color: #5E5E5E;">"""The implementation of the sparse cholesky This is not JAX traceable."""</span></span>
<span id="cb11-4">  </span>
<span id="cb11-5">  L_indices, L_indptr<span class="op" style="color: #5E5E5E;">=</span> _symbolic_factor(A_indices, A_indptr)</span>
<span id="cb11-6">  <span class="cf" style="color: #003B4F;">if</span> L_nse <span class="kw" style="color: #003B4F;">is</span> <span class="kw" style="color: #003B4F;">not</span> <span class="va" style="color: #111111;">None</span>:</span>
<span id="cb11-7">    <span class="cf" style="color: #003B4F;">assert</span> <span class="bu" style="color: null;">len</span>(L_indices) <span class="op" style="color: #5E5E5E;">==</span> L_nse</span>
<span id="cb11-8">    </span>
<span id="cb11-9">  L_x <span class="op" style="color: #5E5E5E;">=</span> _structured_copy(A_indices, A_indptr, A_x, L_indices, L_indptr)</span>
<span id="cb11-10">  L_x <span class="op" style="color: #5E5E5E;">=</span> _sparse_cholesky_impl(L_indices, L_indptr, L_x)</span>
<span id="cb11-11">  <span class="cf" style="color: #003B4F;">return</span> L_indices, L_indptr, L_x</span>
<span id="cb11-12"></span>
<span id="cb11-13"><span class="kw" style="color: #003B4F;">def</span> _symbolic_factor(A_indices, A_indptr):</span>
<span id="cb11-14">  <span class="co" style="color: #5E5E5E;"># Assumes A_indices and A_indptr index the lower triangle of $A$ ONLY.</span></span>
<span id="cb11-15">  n <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(A_indptr) <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb11-16">  L_sym <span class="op" style="color: #5E5E5E;">=</span> [np.array([], dtype<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">int</span>) <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(n)]</span>
<span id="cb11-17">  children <span class="op" style="color: #5E5E5E;">=</span> [np.array([], dtype<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">int</span>) <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(n)]</span>
<span id="cb11-18">  </span>
<span id="cb11-19">  <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(n):</span>
<span id="cb11-20">    L_sym[j] <span class="op" style="color: #5E5E5E;">=</span> A_indices[A_indptr[j]:A_indptr[j <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span>]]</span>
<span id="cb11-21">    <span class="cf" style="color: #003B4F;">for</span> child <span class="kw" style="color: #003B4F;">in</span> children[j]:</span>
<span id="cb11-22">      tmp <span class="op" style="color: #5E5E5E;">=</span> L_sym[child][L_sym[child] <span class="op" style="color: #5E5E5E;">&gt;</span> j]</span>
<span id="cb11-23">      L_sym[j] <span class="op" style="color: #5E5E5E;">=</span> np.unique(np.append(L_sym[j], tmp))</span>
<span id="cb11-24">    <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">len</span>(L_sym[j]) <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">1</span>:</span>
<span id="cb11-25">      p <span class="op" style="color: #5E5E5E;">=</span> L_sym[j][<span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb11-26">      children[p] <span class="op" style="color: #5E5E5E;">=</span> np.append(children[p], j)</span>
<span id="cb11-27">        </span>
<span id="cb11-28">  L_indptr <span class="op" style="color: #5E5E5E;">=</span> np.zeros(n<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>, dtype<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">int</span>)</span>
<span id="cb11-29">  L_indptr[<span class="dv" style="color: #AD0000;">1</span>:] <span class="op" style="color: #5E5E5E;">=</span> np.cumsum([<span class="bu" style="color: null;">len</span>(x) <span class="cf" style="color: #003B4F;">for</span> x <span class="kw" style="color: #003B4F;">in</span> L_sym])</span>
<span id="cb11-30">  L_indices <span class="op" style="color: #5E5E5E;">=</span> np.concatenate(L_sym)</span>
<span id="cb11-31">  </span>
<span id="cb11-32">  <span class="cf" style="color: #003B4F;">return</span> L_indices, L_indptr</span>
<span id="cb11-33"></span>
<span id="cb11-34"></span>
<span id="cb11-35"></span>
<span id="cb11-36"><span class="kw" style="color: #003B4F;">def</span> _structured_copy(A_indices, A_indptr, A_x, L_indices, L_indptr):</span>
<span id="cb11-37">  n <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(A_indptr) <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb11-38">  L_x <span class="op" style="color: #5E5E5E;">=</span> np.zeros(<span class="bu" style="color: null;">len</span>(L_indices))</span>
<span id="cb11-39">  </span>
<span id="cb11-40">  <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, n):</span>
<span id="cb11-41">    copy_idx <span class="op" style="color: #5E5E5E;">=</span> np.nonzero(np.in1d(L_indices[L_indptr[j]:L_indptr[j <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span>]],</span>
<span id="cb11-42">                                  A_indices[A_indptr[j]:A_indptr[j<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>]]))[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb11-43">    L_x[L_indptr[j] <span class="op" style="color: #5E5E5E;">+</span> copy_idx] <span class="op" style="color: #5E5E5E;">=</span> A_x[A_indptr[j]:A_indptr[j<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>]]</span>
<span id="cb11-44">  <span class="cf" style="color: #003B4F;">return</span> L_x</span>
<span id="cb11-45"></span>
<span id="cb11-46"><span class="kw" style="color: #003B4F;">def</span> _sparse_cholesky_impl(L_indices, L_indptr, L_x):</span>
<span id="cb11-47">  n <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(L_indptr) <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb11-48">  descendant <span class="op" style="color: #5E5E5E;">=</span> [[] <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, n)]</span>
<span id="cb11-49">  <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, n):</span>
<span id="cb11-50">    tmp <span class="op" style="color: #5E5E5E;">=</span> L_x[L_indptr[j]:L_indptr[j <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span>]]</span>
<span id="cb11-51">    <span class="cf" style="color: #003B4F;">for</span> bebe <span class="kw" style="color: #003B4F;">in</span> descendant[j]:</span>
<span id="cb11-52">      k <span class="op" style="color: #5E5E5E;">=</span> bebe[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb11-53">      Ljk<span class="op" style="color: #5E5E5E;">=</span> L_x[bebe[<span class="dv" style="color: #AD0000;">1</span>]]</span>
<span id="cb11-54">      pad <span class="op" style="color: #5E5E5E;">=</span> np.nonzero(                                                       <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb11-55">          L_indices[L_indptr[k]:L_indptr[k<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>]] <span class="op" style="color: #5E5E5E;">==</span> L_indices[L_indptr[j]])[<span class="dv" style="color: #AD0000;">0</span>][<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb11-56">      update_idx <span class="op" style="color: #5E5E5E;">=</span> np.nonzero(np.in1d(                                        <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb11-57">                    L_indices[L_indptr[j]:L_indptr[j<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>]],                     <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb11-58">                    L_indices[(L_indptr[k] <span class="op" style="color: #5E5E5E;">+</span> pad):L_indptr[k<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>]]))[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb11-59">      tmp[update_idx] <span class="op" style="color: #5E5E5E;">=</span> tmp[update_idx] <span class="op" style="color: #5E5E5E;">-</span>                                     <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb11-60">                        Ljk <span class="op" style="color: #5E5E5E;">*</span> L_x[(L_indptr[k] <span class="op" style="color: #5E5E5E;">+</span> pad):L_indptr[k <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span>]]</span>
<span id="cb11-61">            </span>
<span id="cb11-62">    diag <span class="op" style="color: #5E5E5E;">=</span> np.sqrt(tmp[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb11-63">    L_x[L_indptr[j]] <span class="op" style="color: #5E5E5E;">=</span> diag</span>
<span id="cb11-64">    L_x[(L_indptr[j] <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span>):L_indptr[j <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span>]] <span class="op" style="color: #5E5E5E;">=</span> tmp[<span class="dv" style="color: #AD0000;">1</span>:] <span class="op" style="color: #5E5E5E;">/</span> diag</span>
<span id="cb11-65">    <span class="cf" style="color: #003B4F;">for</span> idx <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(L_indptr[j] <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span>, L_indptr[j <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span>]):</span>
<span id="cb11-66">      descendant[L_indices[idx]].append((j, idx))</span>
<span id="cb11-67">  <span class="cf" style="color: #003B4F;">return</span> L_x</span>
<span id="cb11-68"></span>
<span id="cb11-69"><span class="at" style="color: #657422;">@sparse_cholesky_p.def_abstract_eval</span></span>
<span id="cb11-70"><span class="kw" style="color: #003B4F;">def</span> sparse_cholesky_abstract_eval(A_indices, A_indptr, A_x, <span class="op" style="color: #5E5E5E;">*</span>, L_nse):</span>
<span id="cb11-71">  <span class="cf" style="color: #003B4F;">return</span> core.ShapedArray((L_nse,), A_indices.dtype),                   <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb11-72">         core.ShapedArray(A_indptr.shape, A_indptr.dtype),             <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb11-73">         core.ShapedArray((L_nse,), A_x.dtype)</span></code></pre></div>
</div>
</details>
<section id="why-do-we-need-a-new-pattern-for-this-very-very-similar-problem" class="level3">
<h3 class="anchored" data-anchor-id="why-do-we-need-a-new-pattern-for-this-very-very-similar-problem">Why do we need a new pattern for this very very similar problem?</h3>
<p>Ok. So now on to the details. If we try to repeat our previous pattern it would look like this.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="kw" style="color: #003B4F;">def</span> sparse_solve_value_and_jvp(arg_values, arg_tangents, <span class="op" style="color: #5E5E5E;">*</span>, L_nse):</span>
<span id="cb12-2">  <span class="co" style="color: #5E5E5E;">""" </span></span>
<span id="cb12-3"><span class="co" style="color: #5E5E5E;">  Jax-traceable jacobian-vector product implmentation for sparse_solve.</span></span>
<span id="cb12-4"><span class="co" style="color: #5E5E5E;">  """</span></span>
<span id="cb12-5">  </span>
<span id="cb12-6">  A_indices, A_indptr, A_x, b <span class="op" style="color: #5E5E5E;">=</span> arg_values</span>
<span id="cb12-7">  _, _, A_xt, bt <span class="op" style="color: #5E5E5E;">=</span> arg_tangents</span>
<span id="cb12-8"></span>
<span id="cb12-9">  <span class="co" style="color: #5E5E5E;"># Needed for shared computation</span></span>
<span id="cb12-10">  L_indices, L_indptr, L_x <span class="op" style="color: #5E5E5E;">=</span> sparse_cholesky(A_indices, A_indptr, A_x)</span>
<span id="cb12-11"></span>
<span id="cb12-12">  <span class="co" style="color: #5E5E5E;"># Make the primal</span></span>
<span id="cb12-13">  primal_out <span class="op" style="color: #5E5E5E;">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, b, transpose <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">False</span>)</span>
<span id="cb12-14">  primal_out <span class="op" style="color: #5E5E5E;">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, primal_out, transpose <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span>)</span>
<span id="cb12-15"></span>
<span id="cb12-16">  <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">type</span>(A_xt) <span class="kw" style="color: #003B4F;">is</span> <span class="kw" style="color: #003B4F;">not</span> ad.Zero:</span>
<span id="cb12-17">    Delta_lower <span class="op" style="color: #5E5E5E;">=</span> jsparse.CSC((A_xt, A_indices, A_indptr), shape <span class="op" style="color: #5E5E5E;">=</span> (b.shape[<span class="dv" style="color: #AD0000;">0</span>], b.shape[<span class="dv" style="color: #AD0000;">0</span>]))</span>
<span id="cb12-18">    <span class="co" style="color: #5E5E5E;"># We need to do Delta @ primal_out, but we only have the lower triangle</span></span>
<span id="cb12-19">    rhs <span class="op" style="color: #5E5E5E;">=</span> Delta_lower <span class="op" style="color: #5E5E5E;">@</span> primal_out <span class="op" style="color: #5E5E5E;">+</span> Delta_lower.transpose() <span class="op" style="color: #5E5E5E;">@</span> primal_out <span class="op" style="color: #5E5E5E;">-</span> A_xt[A_indptr[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]] <span class="op" style="color: #5E5E5E;">*</span> primal_out</span>
<span id="cb12-20">    jvp_Ax <span class="op" style="color: #5E5E5E;">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, rhs)</span>
<span id="cb12-21">    jvp_Ax <span class="op" style="color: #5E5E5E;">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, jvp_Ax, transpose <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span>)</span>
<span id="cb12-22">  <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb12-23">    jvp_Ax <span class="op" style="color: #5E5E5E;">=</span> lax.zeros_like_array(primal_out)</span>
<span id="cb12-24"></span>
<span id="cb12-25">  <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">type</span>(bt) <span class="kw" style="color: #003B4F;">is</span> <span class="kw" style="color: #003B4F;">not</span> ad.Zero:</span>
<span id="cb12-26">    jvp_b <span class="op" style="color: #5E5E5E;">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, bt)</span>
<span id="cb12-27">    jvp_b <span class="op" style="color: #5E5E5E;">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, jvp_b, transpose <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span>)</span>
<span id="cb12-28">  <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb12-29">    jvp_b <span class="op" style="color: #5E5E5E;">=</span> lax.zeros_like_array(primal_out)</span>
<span id="cb12-30"></span>
<span id="cb12-31">  <span class="cf" style="color: #003B4F;">return</span> primal_out, jvp_b <span class="op" style="color: #5E5E5E;">-</span> jvp_Ax</span></code></pre></div>
</div>
<p>That’s all well and good. Nothing weird there.</p>
<p>The problem comes when you need to implement the transposition rule. Remembering that <img src="https://latex.codecogs.com/png.latex?%5Cbar%20b%20=%20A%5E%7B-T%7D%5Cbar%20c%20=%20A%5E%7B-1%7D%5Cbar%20c">, you might see the issue: we are going to need the Cholesky factorisation. <em>But we have no way to pass</em> <img src="https://latex.codecogs.com/png.latex?L"> <em>to the transpose function</em>.</p>
<p>This means that we would need to compute <em>two</em> Cholesky factorisations per gradient instead of one. As the Cholesky factorisation is our slowest operation, we do not want to do extra ones! We want to compute the Cholesky triangle once and pass it around like a party bottom<sup>27</sup>. We do not want each of our functions to have to make a deep and meaningful connection with the damn matrix<sup>28</sup>.</p>
</section>
<section id="a-different-solution" class="level3">
<h3 class="anchored" data-anchor-id="a-different-solution">A different solution</h3>
<p>So how do we pass around our Cholesky triangle? Well, I do love a good class so my first thought was “fuck it. I’ll make a class and I’ll pass it that way”. But the developers of JAX had a <em>much</em> better idea.</p>
<p>Their idea was to abstract the idea of a linear solve and its gradients. They do this through <code>lax.custom_linear_solve</code>. This is a function that takes all of the bits that you would need to compute <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7Db"> and all of its derivatives. In particular it takes<sup>29</sup>:</p>
<ul>
<li><code>matvec</code>: A function that <code>matvec(x)</code> that computes <img src="https://latex.codecogs.com/png.latex?Ax">. This might seem a bit weird, but it’s the most common atrocity committed by mathematicians is abstracting<sup>30</sup> a matrix to a linear mapping. So we might as well just suck it up.</li>
<li><code>b</code>: The right hand side vector<sup>31</sup></li>
<li><code>solve</code>: A function that takes takes the <code>matvec</code> and a vector so that<sup>32</sup> <code>solve(matvec, matvec(x)) == x</code></li>
<li><code>symmetric</code>: A boolean indicating if <img src="https://latex.codecogs.com/png.latex?A"> is symmetric.</li>
</ul>
<p>The idea (happily copped from the implementation of <code>jax.scipy.linalg.solve</code>) is to wrap our Cholesky decomposition in the solve function. Through the never ending miracle of partial evaluation.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;">from</span> functools <span class="im" style="color: #00769E;">import</span> partial</span>
<span id="cb13-2"></span>
<span id="cb13-3"><span class="kw" style="color: #003B4F;">def</span> sparse_solve(A_indices, A_indptr, A_x, b, <span class="op" style="color: #5E5E5E;">*</span>, L_nse <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">None</span>):</span>
<span id="cb13-4">  <span class="co" style="color: #5E5E5E;">"""</span></span>
<span id="cb13-5"><span class="co" style="color: #5E5E5E;">  A JAX-traceable sparse solve. For this moment, only for vector b</span></span>
<span id="cb13-6"><span class="co" style="color: #5E5E5E;">  """</span></span>
<span id="cb13-7">  <span class="cf" style="color: #003B4F;">assert</span> b.shape[<span class="dv" style="color: #AD0000;">0</span>] <span class="op" style="color: #5E5E5E;">==</span> A_indptr.shape[<span class="dv" style="color: #AD0000;">0</span>] <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb13-8">  <span class="cf" style="color: #003B4F;">assert</span> b.ndim <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb13-9">  </span>
<span id="cb13-10">  L_indices, L_indptr, L_x <span class="op" style="color: #5E5E5E;">=</span> sparse_cholesky(</span>
<span id="cb13-11">    lax.stop_gradient(A_indices), </span>
<span id="cb13-12">    lax.stop_gradient(A_indptr), </span>
<span id="cb13-13">    lax.stop_gradient(A_x), L_nse <span class="op" style="color: #5E5E5E;">=</span> L_nse)</span>
<span id="cb13-14">  </span>
<span id="cb13-15">  <span class="kw" style="color: #003B4F;">def</span> chol_solve(L_indices, L_indptr, L_x, b):</span>
<span id="cb13-16">    out <span class="op" style="color: #5E5E5E;">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, b, transpose <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">False</span>)</span>
<span id="cb13-17">    <span class="cf" style="color: #003B4F;">return</span> sparse_triangular_solve(L_indices, L_indptr, L_x, out, transpose <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span>)</span>
<span id="cb13-18">  </span>
<span id="cb13-19">  <span class="kw" style="color: #003B4F;">def</span> matmult(A_indices, A_indptr, A_x, b):</span>
<span id="cb13-20">    A_lower <span class="op" style="color: #5E5E5E;">=</span> jsparse.CSC((A_x, A_indices, A_indptr), shape <span class="op" style="color: #5E5E5E;">=</span> (b.shape[<span class="dv" style="color: #AD0000;">0</span>], b.shape[<span class="dv" style="color: #AD0000;">0</span>]))</span>
<span id="cb13-21">    <span class="cf" style="color: #003B4F;">return</span> A_lower <span class="op" style="color: #5E5E5E;">@</span> b <span class="op" style="color: #5E5E5E;">+</span> A_lower.transpose() <span class="op" style="color: #5E5E5E;">@</span> b <span class="op" style="color: #5E5E5E;">-</span> A_x[A_indptr[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]] <span class="op" style="color: #5E5E5E;">*</span> b</span>
<span id="cb13-22"></span>
<span id="cb13-23">  solver <span class="op" style="color: #5E5E5E;">=</span> partial(</span>
<span id="cb13-24">    lax.custom_linear_solve,</span>
<span id="cb13-25">    <span class="kw" style="color: #003B4F;">lambda</span> x: matmult(A_indices, A_indptr, A_x, x),</span>
<span id="cb13-26">    solve <span class="op" style="color: #5E5E5E;">=</span> <span class="kw" style="color: #003B4F;">lambda</span> _, x: chol_solve(L_indices, L_indptr, L_x, x),</span>
<span id="cb13-27">    symmetric <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span>)</span>
<span id="cb13-28"></span>
<span id="cb13-29">  <span class="cf" style="color: #003B4F;">return</span> solver(b)</span></code></pre></div>
</div>
<p>There are three things of note in that implementation.</p>
<ol type="1">
<li><p>The calls to <code>lax.stop_gradient()</code>: These tell JAX to not bother computing the gradient of these terms. The relevant parts of the derivatives are computed explicitly by <code>lax.custom_linear_solve</code> in terms of <code>matmult</code> and <code>solve</code>, neither of which need the explicit derivative of the cholesky factorisation.!</p></li>
<li><p>That definition of <code>matmult()</code><sup>33</sup>: Look. I don’t know what to tell you. Neither addition nor indexing is implemented for <code>jsparse.CSC</code> objects. So we did it the semi-manual way. (I am thankful that matrix-vector multiplication is available)</p></li>
<li><p>The definition of <code>solver()</code>: Partial evaluation is a wonderful wonderful thing. <code>functools.partial()</code> transforms <code>lax.custom_linear_solve()</code> from a function that takes 3 arguments (and some keywords), into a function <code>solver()</code> that takes one<sup>34</sup> argument<sup>35</sup> (<code>b</code>, the only positional argument of <code>lax.custom_linear_solve()</code> that isn’t specified).</p></li>
</ol>
</section>
<section id="does-it-work" class="level3">
<h3 class="anchored" data-anchor-id="does-it-work">Does it work?</h3>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="kw" style="color: #003B4F;">def</span> f(theta):</span>
<span id="cb14-2">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> jnp.array(theta[<span class="dv" style="color: #AD0000;">0</span>] <span class="op" style="color: #5E5E5E;">*</span> A_x)</span>
<span id="cb14-3">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> Ax_theta.at[A_indptr[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]].add(theta[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb14-4">  b <span class="op" style="color: #5E5E5E;">=</span> jnp.ones(<span class="dv" style="color: #AD0000;">100</span>)</span>
<span id="cb14-5">  <span class="cf" style="color: #003B4F;">return</span> sparse_solve(A_indices, A_indptr, Ax_theta, b)</span>
<span id="cb14-6"></span>
<span id="cb14-7"><span class="kw" style="color: #003B4F;">def</span> f_jax(theta):</span>
<span id="cb14-8">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> jnp.array(theta[<span class="dv" style="color: #AD0000;">0</span>] <span class="op" style="color: #5E5E5E;">*</span> A.todense())</span>
<span id="cb14-9">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> Ax_theta.at[np.arange(<span class="dv" style="color: #AD0000;">100</span>),np.arange(<span class="dv" style="color: #AD0000;">100</span>)].add(theta[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb14-10">  b <span class="op" style="color: #5E5E5E;">=</span> jnp.ones(<span class="dv" style="color: #AD0000;">100</span>)</span>
<span id="cb14-11">  <span class="cf" style="color: #003B4F;">return</span> jsp.linalg.solve(Ax_theta, b)</span>
<span id="cb14-12"></span>
<span id="cb14-13"><span class="kw" style="color: #003B4F;">def</span> g(theta):</span>
<span id="cb14-14">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> jnp.array(A_x)</span>
<span id="cb14-15">  b <span class="op" style="color: #5E5E5E;">=</span> jnp.ones(<span class="dv" style="color: #AD0000;">100</span>)</span>
<span id="cb14-16">  b <span class="op" style="color: #5E5E5E;">=</span> b.at[<span class="dv" style="color: #AD0000;">0</span>].<span class="bu" style="color: null;">set</span>(theta[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb14-17">  b <span class="op" style="color: #5E5E5E;">=</span> b.at[<span class="dv" style="color: #AD0000;">51</span>].<span class="bu" style="color: null;">set</span>(theta[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb14-18">  <span class="cf" style="color: #003B4F;">return</span> sparse_solve(A_indices, A_indptr, Ax_theta, b)</span>
<span id="cb14-19"></span>
<span id="cb14-20"><span class="kw" style="color: #003B4F;">def</span> g_jax(theta):</span>
<span id="cb14-21">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> jnp.array(A.todense())</span>
<span id="cb14-22">  b <span class="op" style="color: #5E5E5E;">=</span> jnp.ones(<span class="dv" style="color: #AD0000;">100</span>)</span>
<span id="cb14-23">  b <span class="op" style="color: #5E5E5E;">=</span> b.at[<span class="dv" style="color: #AD0000;">0</span>].<span class="bu" style="color: null;">set</span>(theta[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb14-24">  b <span class="op" style="color: #5E5E5E;">=</span> b.at[<span class="dv" style="color: #AD0000;">51</span>].<span class="bu" style="color: null;">set</span>(theta[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb14-25">  <span class="cf" style="color: #003B4F;">return</span> jsp.linalg.solve(Ax_theta, b)</span>
<span id="cb14-26"></span>
<span id="cb14-27"><span class="kw" style="color: #003B4F;">def</span> h(theta):</span>
<span id="cb14-28">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> jnp.array(A_x)</span>
<span id="cb14-29">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> Ax_theta.at[A_indptr[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]].add(theta[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb14-30">  b <span class="op" style="color: #5E5E5E;">=</span> jnp.ones(<span class="dv" style="color: #AD0000;">100</span>)</span>
<span id="cb14-31">  b <span class="op" style="color: #5E5E5E;">=</span> b.at[<span class="dv" style="color: #AD0000;">51</span>].<span class="bu" style="color: null;">set</span>(theta[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb14-32">  <span class="cf" style="color: #003B4F;">return</span> sparse_solve(A_indices, A_indptr, Ax_theta, b)</span>
<span id="cb14-33"></span>
<span id="cb14-34"><span class="kw" style="color: #003B4F;">def</span> h_jax(theta):</span>
<span id="cb14-35">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> jnp.array(A.todense())</span>
<span id="cb14-36">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> Ax_theta.at[np.arange(<span class="dv" style="color: #AD0000;">100</span>),np.arange(<span class="dv" style="color: #AD0000;">100</span>)].add(theta[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb14-37">  b <span class="op" style="color: #5E5E5E;">=</span> jnp.ones(<span class="dv" style="color: #AD0000;">100</span>)</span>
<span id="cb14-38">  b <span class="op" style="color: #5E5E5E;">=</span> b.at[<span class="dv" style="color: #AD0000;">51</span>].<span class="bu" style="color: null;">set</span>(theta[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb14-39">  <span class="cf" style="color: #003B4F;">return</span> jsp.linalg.solve(Ax_theta, b)</span>
<span id="cb14-40"></span>
<span id="cb14-41">primal1, jvp1 <span class="op" style="color: #5E5E5E;">=</span> jvp(f, (jnp.array([<span class="fl" style="color: #AD0000;">2.</span>, <span class="fl" style="color: #AD0000;">3.</span>]),), (jnp.array([<span class="fl" style="color: #AD0000;">1.</span>, <span class="fl" style="color: #AD0000;">2.</span>]),))</span>
<span id="cb14-42">primal2, jvp2 <span class="op" style="color: #5E5E5E;">=</span> jvp(f_jax, (jnp.array([<span class="fl" style="color: #AD0000;">2.</span>, <span class="fl" style="color: #AD0000;">3.</span>]),), (jnp.array([<span class="fl" style="color: #AD0000;">1.</span>, <span class="fl" style="color: #AD0000;">2.</span>]),))</span>
<span id="cb14-43">grad1 <span class="op" style="color: #5E5E5E;">=</span> grad(<span class="kw" style="color: #003B4F;">lambda</span> x: jnp.mean(f(x)))(jnp.array([<span class="fl" style="color: #AD0000;">2.</span>, <span class="fl" style="color: #AD0000;">3.</span>]))</span>
<span id="cb14-44">grad2 <span class="op" style="color: #5E5E5E;">=</span> grad(<span class="kw" style="color: #003B4F;">lambda</span> x: jnp.mean(f_jax(x)))(jnp.array([<span class="fl" style="color: #AD0000;">2.</span>, <span class="fl" style="color: #AD0000;">3.</span>]))</span>
<span id="cb14-45"></span>
<span id="cb14-46"></span>
<span id="cb14-47">primal3, jvp3 <span class="op" style="color: #5E5E5E;">=</span> jvp(g, (jnp.array([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">142.</span>, <span class="fl" style="color: #AD0000;">342.</span>]),), (jnp.array([<span class="fl" style="color: #AD0000;">1.</span>, <span class="fl" style="color: #AD0000;">2.</span>]),))</span>
<span id="cb14-48">primal4, jvp4 <span class="op" style="color: #5E5E5E;">=</span> jvp(g_jax, (jnp.array([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">142.</span>, <span class="fl" style="color: #AD0000;">342.</span>]),), (jnp.array([<span class="fl" style="color: #AD0000;">1.</span>, <span class="fl" style="color: #AD0000;">2.</span>]),))</span>
<span id="cb14-49">grad3 <span class="op" style="color: #5E5E5E;">=</span> grad(<span class="kw" style="color: #003B4F;">lambda</span> x: jnp.mean(g(x)))(jnp.array([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">142.</span>, <span class="fl" style="color: #AD0000;">342.</span>]))</span>
<span id="cb14-50">grad4 <span class="op" style="color: #5E5E5E;">=</span> grad(<span class="kw" style="color: #003B4F;">lambda</span> x: jnp.mean(g_jax(x)))(jnp.array([<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">142.</span>, <span class="fl" style="color: #AD0000;">342.</span>]))</span>
<span id="cb14-51"></span>
<span id="cb14-52">primal5, jvp5 <span class="op" style="color: #5E5E5E;">=</span> jvp(h, (jnp.array([<span class="fl" style="color: #AD0000;">2.</span>, <span class="fl" style="color: #AD0000;">342.</span>]),), (jnp.array([<span class="fl" style="color: #AD0000;">1.</span>, <span class="fl" style="color: #AD0000;">2.</span>]),))</span>
<span id="cb14-53">primal6, jvp6 <span class="op" style="color: #5E5E5E;">=</span> jvp(h_jax, (jnp.array([<span class="fl" style="color: #AD0000;">2.</span>, <span class="fl" style="color: #AD0000;">342.</span>]),), (jnp.array([<span class="fl" style="color: #AD0000;">1.</span>, <span class="fl" style="color: #AD0000;">2.</span>]),))</span>
<span id="cb14-54">grad5 <span class="op" style="color: #5E5E5E;">=</span> grad(<span class="kw" style="color: #003B4F;">lambda</span> x: jnp.mean(f(x)))(jnp.array([<span class="fl" style="color: #AD0000;">2.</span>, <span class="fl" style="color: #AD0000;">342.</span>]))</span>
<span id="cb14-55">grad6 <span class="op" style="color: #5E5E5E;">=</span> grad(<span class="kw" style="color: #003B4F;">lambda</span> x: jnp.mean(f_jax(x)))(jnp.array([<span class="fl" style="color: #AD0000;">2.</span>, <span class="fl" style="color: #AD0000;">342.</span>]))</span>
<span id="cb14-56"></span>
<span id="cb14-57"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"""</span></span>
<span id="cb14-58"><span class="ss" style="color: #20794D;">Check the plumbing!</span></span>
<span id="cb14-59"><span class="ss" style="color: #20794D;">Variable A:</span></span>
<span id="cb14-60"><span class="ss" style="color: #20794D;">  Primal difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(primal1 <span class="op" style="color: #5E5E5E;">-</span> primal2)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb14-61"><span class="ss" style="color: #20794D;">  JVP difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(jvp1 <span class="op" style="color: #5E5E5E;">-</span> jvp2)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb14-62"><span class="ss" style="color: #20794D;">  Gradient difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(grad1 <span class="op" style="color: #5E5E5E;">-</span> grad2)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb14-63"><span class="ss" style="color: #20794D;">  </span></span>
<span id="cb14-64"><span class="ss" style="color: #20794D;">Variable b:</span></span>
<span id="cb14-65"><span class="ss" style="color: #20794D;">  Primal difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(primal3 <span class="op" style="color: #5E5E5E;">-</span> primal4)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb14-66"><span class="ss" style="color: #20794D;">  JVP difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(jvp3 <span class="op" style="color: #5E5E5E;">-</span> jvp4)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb14-67"><span class="ss" style="color: #20794D;">  Gradient difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(grad3 <span class="op" style="color: #5E5E5E;">-</span> grad4)<span class="sc" style="color: #5E5E5E;">: .2e}</span><span class="ss" style="color: #20794D;"> </span></span>
<span id="cb14-68"><span class="ss" style="color: #20794D;">    </span></span>
<span id="cb14-69"><span class="ss" style="color: #20794D;">Variable A and b:</span></span>
<span id="cb14-70"><span class="ss" style="color: #20794D;">  Primal difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(primal5 <span class="op" style="color: #5E5E5E;">-</span> primal6)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb14-71"><span class="ss" style="color: #20794D;">  JVP difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(jvp5 <span class="op" style="color: #5E5E5E;">-</span> jvp6)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb14-72"><span class="ss" style="color: #20794D;">  Gradient difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(grad5 <span class="op" style="color: #5E5E5E;">-</span> grad6)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb14-73"><span class="ss" style="color: #20794D;">  """</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Check the plumbing!
Variable A:
  Primal difference:  1.98e-07
  JVP difference:  1.43e-07
  Gradient difference:  0.00e+00
  
Variable b:
  Primal difference:  4.56e-06
  JVP difference:  6.52e-08
  Gradient difference:  9.31e-10 
    
Variable A and b:
  Primal difference:  8.10e-06
  JVP difference:  1.83e-06
  Gradient difference:  1.82e-12
  </code></pre>
</div>
</div>
<p>Yes.</p>
</section>
<section id="why-is-this-better-than-just-differentiating-through-the-cholesky-factorisation" class="level3">
<h3 class="anchored" data-anchor-id="why-is-this-better-than-just-differentiating-through-the-cholesky-factorisation">Why is this better than just differentiating through the Cholesky factorisation?</h3>
<p>The other option for making this work would’ve been to implement the Cholesky factorisation as a primitive (~which we are about to do!~ which we will do another day) and then write the sparse solver directly as a pure JAX function.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="kw" style="color: #003B4F;">def</span> sparse_solve_direct(A_indices, A_indptr, A_x, b, <span class="op" style="color: #5E5E5E;">*</span>, L_nse <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">None</span>):</span>
<span id="cb16-2">  L_indices, L_indptr, L_x <span class="op" style="color: #5E5E5E;">=</span> sparse_cholesky(A_indices, A_indptr, A_x)</span>
<span id="cb16-3">  out <span class="op" style="color: #5E5E5E;">=</span> sparse_triangular_solve(L_indices, L_indptr, L_x, b)</span>
<span id="cb16-4">  <span class="cf" style="color: #003B4F;">return</span> sparse_triangular_solve(L_indices, L_indptr, L_x, out, transpose <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</div>
<p>This function is JAX-traceable<sup>36</sup> and, therefore, we could compute the gradient of it directly. It turns out that this is going to be a bad idea.</p>
<p>Why? Because the derivative of <code>sparse_cholesky</code>, which we would have to chain together with the derivatives from the solver, is pretty complicated. Basically, this means that we’d have to do a lot more work<sup>37</sup> than we do if we just implement the symbolic formula for the derivatives.</p>
</section>
</section>
<section id="primitive-three-the-dreaded-log-determinant" class="level2">
<h2 class="anchored" data-anchor-id="primitive-three-the-dreaded-log-determinant">Primitive three: The dreaded log determinant</h2>
<p>Ok, so now we get to the good one. The log-determinant of <img src="https://latex.codecogs.com/png.latex?A">. The first thing that we need to do is wrench out a derivative. This is not as easy as it was for the linear solve. So what follows is a modification for sparse matrices from Appendix A of <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Boyd’s convex optimisation book</a>.</p>
<p>It’s pretty easy to convince yourself that <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Clog(%7CA%20+%20%5CDelta%7C)%20&amp;=%20%5Clog%5Cleft(%20%5Cleft%7CA%5E%7B1/2%7D(I%20+%20A%5E%7B-1/2%7D%5CDelta%20A%5E%7B-1/2%7D)A%5E%7B1/2%7D%5Cright%7C%5Cright)%20%5C%5C%0A&amp;=%20%5Clog(%7CA%7C)%20+%20%5Clog%5Cleft(%20%5Cleft%7CI%20+%20A%5E%7B-1/2%7D%5CDelta%20A%5E%7B-1/2%7D%5Cright%7C%5Cright).%0A%5Cend%7Balign*%7D"></p>
<p>It is harder to convince yourself how this could possibly be a useful fact.</p>
<p>If we write <img src="https://latex.codecogs.com/png.latex?%5Clambda_i">, <img src="https://latex.codecogs.com/png.latex?i%20=%201,%20%5Cldots,%20n"> as the eigenvalues of <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1/2%7D%5CDelta%20A%5E%7B-1/2%7D">, then we have <img src="https://latex.codecogs.com/png.latex?%0A%5Clog(%7CA%20+%20%5CDelta%20%7C)%20=%20%5Clog(%7CA%7C)%20+%20%5Csum_%7Bi=1%7D%5En%20%5Clog(%201%20+%20%5Clambda_i).%0A"> Remembering that <img src="https://latex.codecogs.com/png.latex?%5CDelta"> is very small, it follows that <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1/2%7D%5CDelta%20A%5E%7B-1/2%7D"> will <em>also</em> be small. That translates to the eigenvalues of <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1/2%7D%5CDelta%20A%5E%7B-1/2%7D"> all being small. Therefore, we can use the approximation <img src="https://latex.codecogs.com/png.latex?%5Clog(1%20+%20%5Clambda_i)%20=%20%5Clambda_i%20+%20%5Cmathcal%7BO%7D(%5Clambda_i%5E2)">.</p>
<p>This means that<sup>38</sup> <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Clog(%7CA%20+%20%5CDelta%20%7C)%20&amp;=%20%5Clog(%7CA%7C)%20+%20%5Csum_%7Bi=1%7D%5En%20%20%5Clambda_i%20+%20%5Cmathcal%7BO%7D%5Cleft(%5C%7C%5CDelta%5C%7C%5E2%5Cright)%20%5C%5C%0A&amp;=%5Clog(%7CA%7C)%20+%20%5Coperatorname%7Btr%7D%5Cleft(A%5E%7B-1/2%7D%20%5CDelta%20A%5E%7B-1%7D%20%5Cright)%20+%20%5Cmathcal%7BO%7D%5Cleft(%5C%7C%5CDelta%5C%7C%5E2%5Cright)%20%5C%5C%0A&amp;=%20%5Clog(%7CA%7C)%20+%20%5Coperatorname%7Btr%7D%5Cleft(A%5E%7B-1%7D%20%5CDelta%20%5Cright)%20+%20%5Cmathcal%7BO%7D%5Cleft(%5C%7C%5CDelta%5C%7C%5E2%5Cright),%0A%5Cend%7Balign*%7D"> which follows from the cyclic property of the trace.</p>
<p>If we recall the formula from the last section defining the Jacobian-vector product, in our context <img src="https://latex.codecogs.com/png.latex?m%20=%201">, <img src="https://latex.codecogs.com/png.latex?x"> is the vector of non-zero entries of the lower triangle of <img src="https://latex.codecogs.com/png.latex?A"> stacked by column, and <img src="https://latex.codecogs.com/png.latex?%5Cdelta"> is the vector of non-zero entries of the lower triangle of <img src="https://latex.codecogs.com/png.latex?%5CDelta">. That means the Jacobian-vector product is <img src="https://latex.codecogs.com/png.latex?%0AJ(x)%5Cdelta%20=%20%5Coperatorname%7Btr%7D%5Cleft(A%5E%7B-1%7D%20%5CDelta%20%5Cright)%20=%20%5Csum_%7Bi=1%7D%5En%5Csum_%7Bj=1%7D%5En%5BA%5E%7B-1%7D%5D_%7Bij%7D%20%5CDelta_%7Bij%7D.%0A"></p>
<p>Remembering that <img src="https://latex.codecogs.com/png.latex?%5CDelta"> is sparse with the same sparsity pattern as <img src="https://latex.codecogs.com/png.latex?A">, we see that the Jacobian-vector product requires us to know the values of <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D"> that correspond to non-zero elements of <img src="https://latex.codecogs.com/png.latex?A">. That’s good news because we will see that these entries are relatively cheap and easy to compute. Whereas the full inverse is dense and very expensive to compute.</p>
<p>But before we get to that, I need to point out a trap for young players<sup>39</sup>. Lest your implementations go down faster than me when someone asks politely.</p>
<p>The problem comes from how we store our matrix. A mathematician would suggest that it’s our representation. A physicist<sup>40</sup> would shit on about being coordinate free with such passion that he<sup>41</sup> will keep going even after you quietly leave the room.</p>
<p>The problem is that we only store the non-zero entries of the lower-triangular part of <img src="https://latex.codecogs.com/png.latex?A">. This means that <em>we need to be careful</em> that when we compute the Jacobian-vector product that we properly compute the Matrix-vector product.</p>
<p>Let <code>A_indices</code> and <code>A_indptr</code> define the sparsity structure of <img src="https://latex.codecogs.com/png.latex?A"> (and <img src="https://latex.codecogs.com/png.latex?%5CDelta">). Then if <img src="https://latex.codecogs.com/png.latex?A_x"> is our input and <img src="https://latex.codecogs.com/png.latex?v"> is our vector, then we need to do the follow steps to compute the Jacobian-vector product:</p>
<ol type="1">
<li>Compute <code>Ainv_x</code> (aka the non-zero elements of <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D"> that correspond to the sparsity pattern of <img src="https://latex.codecogs.com/png.latex?A">)</li>
<li>Compute the matrix vector product as</li>
</ol>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1">jvp <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">*</span> <span class="bu" style="color: null;">sum</span>(Ainv_x <span class="op" style="color: #5E5E5E;">*</span> v) <span class="op" style="color: #5E5E5E;">-</span> <span class="bu" style="color: null;">sum</span>(Ainv_x[A_indptr[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]] <span class="op" style="color: #5E5E5E;">*</span> v[A_indptr[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]])</span></code></pre></div>
</div>
<p>Why does it look like that? Well we need to add the contribution from the upper triangle as well as the lower triangle. And one way to do that is to just double the sum and then subtract off the diagonal terms that we’ve counted twice.</p>
<p>(I’m making a pretty big assumption here, which is fine in our context, that <img src="https://latex.codecogs.com/png.latex?A"> has a non-zero diagonal. If that doesn’t hold, it’s just a change of the indexing in the second term to just pull out the diagonal terms.)</p>
<p>Using similar reasoning, we can compute the Jacobian as <img src="https://latex.codecogs.com/png.latex?%0A%5BJ_f(x)%5D_%7Bi1%7D%20=%20%5Cbegin%7Bcases%7D%0A%5Coperatorname%7Bpartial-inverse%7D(x)_i,%20%5Cqquad%20&amp;%20x_i%20%20%5Ctext%7B%20is%20a%20diagonal%20element%20of%20%7DA%20%5C%5C%0A2%5Coperatorname%7Bpartial-inverse%7D(x)_i,%20%5Cqquad%20&amp;%20%5Ctext%7Botherwise%7D,%0A%5Cend%7Bcases%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7Bpartial-inverse%7D(x)"> is the vector that stacks the columns of the elements of <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D"> that correspond to the non-zero elements of <img src="https://latex.codecogs.com/png.latex?A">. (Yikes!)</p>
<section id="computing-the-partial-inverse" class="level3">
<h3 class="anchored" data-anchor-id="computing-the-partial-inverse">Computing the partial inverse</h3>
<p>So now we need to actually work out how to compute this <em>partial inverse</em> of a symmetric positive definite matrix <img src="https://latex.codecogs.com/png.latex?A">. To do this, we are going to steal a technique that goes back to Takahashi, Fagan, and Chen<sup>42</sup> in 1973. (For this presentation, I’m basically pillaging <a href="https://www.sciencedirect.com/science/article/pii/S0378375807000845">Håvard Rue and Sara Martino’s 2007 paper.</a>)</p>
<p>Their idea was that if we write <img src="https://latex.codecogs.com/png.latex?A%20=%20VDV%5ET">, where <img src="https://latex.codecogs.com/png.latex?V"> is a lower-triangular matrix with ones on the diagonal and <img src="https://latex.codecogs.com/png.latex?D"> is diagonal. This links up with our usual Cholesky factorisation through the identity <img src="https://latex.codecogs.com/png.latex?L%20=%20VD%5E%7B1/2%7D">. It follows that if <img src="https://latex.codecogs.com/png.latex?S%20=%20A%5E%7B-1%7D">, then <img src="https://latex.codecogs.com/png.latex?VDV%5ETS%20=%20I">. Then, we make some magic manipulations<sup>43</sup>. <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0AV%5ETS%20&amp;=%20D%5E%7B-1%7DV%5E%7B-1%7D%20%5C%5C%0AS%20+%20V%5ETS%20&amp;=%20S%20+%20D%5E%7B-1%7DV%5E%7B-1%7D%20%5C%5C%0AS%20&amp;=%20D%5E%7B-1%7DV%5E%7B-1%7D%20+%20(I%20-%20V%5ET)S.%0A%5Cend%7Balign*%7D"></p>
<p>Once again, this does not look super-useful. The trick is to notice 2 things.</p>
<ol type="1">
<li><p>Because <img src="https://latex.codecogs.com/png.latex?V"> is lower triangular, <img src="https://latex.codecogs.com/png.latex?V%5E%7B-1%7D"> is also lower triangular and the elements of <img src="https://latex.codecogs.com/png.latex?V%5E%7B-1%7D"> are the inverse of the diagonal elements of <img src="https://latex.codecogs.com/png.latex?V"> (aka they are all 1). Therefore, <img src="https://latex.codecogs.com/png.latex?D%5E%7B-1%7DV%5E%7B-1%7D"> is a lower triangular matrix with a diagonal given by the diagonal of <img src="https://latex.codecogs.com/png.latex?D%5E%7B-1%7D">.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?I%20-%20V%5ET"> is an upper triangular matrix and <img src="https://latex.codecogs.com/png.latex?%5BI%20-%20V%5ET%5D_%7Bnn%7D%20=%200">.</p></li>
</ol>
<p>These two things together lead to the somewhat unexpected situation where the upper triangle of <img src="https://latex.codecogs.com/png.latex?S%20=%20D%5E%7B-1%7DV%5E%7B-1%7D%20+%20(I-%20V%5ET)S"> defines a set of recursions for the upper triangle of <img src="https://latex.codecogs.com/png.latex?S">. (And, therefore, all of <img src="https://latex.codecogs.com/png.latex?S"> because <img src="https://latex.codecogs.com/png.latex?S"> is symmetric!) These are sometimes referred to as the Takahashi recursions.</p>
<p>But we don’t want the whole upper triangle of <img src="https://latex.codecogs.com/png.latex?S">, we just want the ones that correspond to the non-zero elements of <img src="https://latex.codecogs.com/png.latex?A">. Unfortunately, the set of recursions are not, in general, solveable using only that subset of <img src="https://latex.codecogs.com/png.latex?S">. But we are in luck: they are solveable using the elements of <img src="https://latex.codecogs.com/png.latex?S"> that correspond to the non-zeros of <img src="https://latex.codecogs.com/png.latex?L%20+%20L%5ET">, which, as we know from a few posts ago, is a superset of the non-zero elements of <img src="https://latex.codecogs.com/png.latex?A">!</p>
<p>From this, we get the recursions running from <img src="https://latex.codecogs.com/png.latex?i%20=%20n,%20%5Cldots,%201">, <img src="https://latex.codecogs.com/png.latex?j%20=%20n,%20%5Cldots,%20i"> (the order is important!) such that <img src="https://latex.codecogs.com/png.latex?L_%7Bji%7D%20%5Cneq%200"> <img src="https://latex.codecogs.com/png.latex?%0AS_%7Bji%7D%20=%20%20%20%5Cbegin%7Bcases%7D%0A%5Cfrac%7B1%7D%7BL_%7Bii%7D%5E2%7D%20-%20%5Cfrac%7B1%7D%7BL_%7Bii%7D%7D%5Csum_%7Bk=i+1%7D%5E%7Bn%7D%20L_%7Bki%7D%20S_%7Bkj%7D%20%5Cqquad&amp;%20%20%5Ctext%7Bif%20%7D%20i=j,%20%5C%5C%20%20%20%20%20%20%20%20%20%0A-%20%5Cfrac%7B1%7D%7BL_%7Bii%7D%7D%5Csum_%7Bk=i+1%7D%5E%7Bn%7D%20L_%7Bki%7D%20S_%7Bkj%7D%20%20&amp;%20%5Ctext%7Botherwise%7D.%0A%5Cend%7Bcases%7D%0A"></p>
<p>If you recall our discussion way back when about the way the non-zero structure of the <img src="https://latex.codecogs.com/png.latex?j"> the column of <img src="https://latex.codecogs.com/png.latex?L"> relates to the non-zero structure of the <img src="https://latex.codecogs.com/png.latex?i"> th column for <img src="https://latex.codecogs.com/png.latex?j%20%5Cgeq%20i">, it’s clear that we have computed enough<sup>44</sup> of <img src="https://latex.codecogs.com/png.latex?S"> at every step to complete the recursions.</p>
<p>Now we just need to Python it. (And thanks to Finn Lindgren who helped me understand how to implement this, which he may or may not remember because it happened about five years ago.)</p>
<p>Actually, we need this to be JAX-traceable, so we are going to implement a very basic primitive. In particular, we don’t need to implement a derivative or anything like that, just an abstract evaluation and an implementation.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1">sparse_partial_inverse_p <span class="op" style="color: #5E5E5E;">=</span> core.Primitive(<span class="st" style="color: #20794D;">"sparse_partial_inverse"</span>)</span>
<span id="cb18-2"></span>
<span id="cb18-3"><span class="kw" style="color: #003B4F;">def</span> sparse_partial_inverse(L_indices, L_indptr, L_x, out_indices, out_indptr):</span>
<span id="cb18-4">  <span class="co" style="color: #5E5E5E;">"""</span></span>
<span id="cb18-5"><span class="co" style="color: #5E5E5E;">  Computes the elements (out_indices, out_indptr) of the inverse of a sparse matrix (A_indices, A_indptr, A_x)</span></span>
<span id="cb18-6"><span class="co" style="color: #5E5E5E;">   with Choleksy factor (L_indices, L_indptr, L_x). (out_indices, out_indptr) is assumed to be either</span></span>
<span id="cb18-7"><span class="co" style="color: #5E5E5E;">   the sparsity pattern of A or a subset of it in lower triangular form. </span></span>
<span id="cb18-8"><span class="co" style="color: #5E5E5E;">  """</span></span>
<span id="cb18-9">  <span class="cf" style="color: #003B4F;">return</span> sparse_partial_inverse_p.bind(L_indices, L_indptr, L_x, out_indices, out_indptr)</span>
<span id="cb18-10"></span>
<span id="cb18-11"><span class="at" style="color: #657422;">@sparse_partial_inverse_p.def_abstract_eval</span></span>
<span id="cb18-12"><span class="kw" style="color: #003B4F;">def</span> sparse_partial_inverse_abstract_eval(L_indices, L_indptr, L_x, out_indices, out_indptr):</span>
<span id="cb18-13">  <span class="cf" style="color: #003B4F;">return</span> abstract_arrays.ShapedArray(out_indices.shape, L_x.dtype)</span>
<span id="cb18-14"></span>
<span id="cb18-15"><span class="at" style="color: #657422;">@sparse_partial_inverse_p.def_impl</span></span>
<span id="cb18-16"><span class="kw" style="color: #003B4F;">def</span> sparse_partial_inverse_impl(L_indices, L_indptr, L_x, out_indices, out_indptr):</span>
<span id="cb18-17">  n <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(L_indptr) <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb18-18">  Linv <span class="op" style="color: #5E5E5E;">=</span> sparse.dok_array((n,n), dtype <span class="op" style="color: #5E5E5E;">=</span> L_x.dtype)</span>
<span id="cb18-19">  counter <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(L_x) <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb18-20">  <span class="cf" style="color: #003B4F;">for</span> col <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(n<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>):</span>
<span id="cb18-21">    <span class="cf" style="color: #003B4F;">for</span> row <span class="kw" style="color: #003B4F;">in</span> L_indices[L_indptr[col]:L_indptr[col<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>]][::<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]:</span>
<span id="cb18-22">      <span class="cf" style="color: #003B4F;">if</span> row <span class="op" style="color: #5E5E5E;">!=</span> col:</span>
<span id="cb18-23">        Linv[row, col] <span class="op" style="color: #5E5E5E;">=</span> Linv[col, row] <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.0</span></span>
<span id="cb18-24">      <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb18-25">        Linv[row, col] <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">/</span> L_x[L_indptr[col]]<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb18-26">      L_col  <span class="op" style="color: #5E5E5E;">=</span> L_x[L_indptr[col]<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>:L_indptr[col<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>]] <span class="op" style="color: #5E5E5E;">/</span> L_x[L_indptr[col]]</span>
<span id="cb18-27"> </span>
<span id="cb18-28">      <span class="cf" style="color: #003B4F;">for</span> k, L_kcol <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">zip</span>(L_indices[L_indptr[col]<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>:L_indptr[col<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>]], L_col):</span>
<span id="cb18-29">         Linv[col,row] <span class="op" style="color: #5E5E5E;">=</span> Linv[row,col] <span class="op" style="color: #5E5E5E;">=</span>  Linv[row, col] <span class="op" style="color: #5E5E5E;">-</span>  L_kcol <span class="op" style="color: #5E5E5E;">*</span> Linv[k, row]</span>
<span id="cb18-30">        </span>
<span id="cb18-31">  Linv_x <span class="op" style="color: #5E5E5E;">=</span> sparse.tril(Linv, <span class="bu" style="color: null;">format</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"csc"</span>).data</span>
<span id="cb18-32">  <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">len</span>(out_indices) <span class="op" style="color: #5E5E5E;">==</span> <span class="bu" style="color: null;">len</span>(L_indices):</span>
<span id="cb18-33">    <span class="cf" style="color: #003B4F;">return</span> Linv_x</span>
<span id="cb18-34"></span>
<span id="cb18-35">  out_x <span class="op" style="color: #5E5E5E;">=</span> np.zeros(<span class="bu" style="color: null;">len</span>(out_indices))</span>
<span id="cb18-36">  <span class="cf" style="color: #003B4F;">for</span> col <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(n):</span>
<span id="cb18-37">    ind <span class="op" style="color: #5E5E5E;">=</span> np.nonzero(np.in1d(L_indices[L_indptr[col]:L_indptr[col<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>]],</span>
<span id="cb18-38">      out_indices[out_indptr[col]:out_indptr[col<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>]]))[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb18-39">    out_x[out_indptr[col]:out_indptr[col<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>]] <span class="op" style="color: #5E5E5E;">=</span> Linv_x[L_indptr[col] <span class="op" style="color: #5E5E5E;">+</span> ind]</span>
<span id="cb18-40">  <span class="cf" style="color: #003B4F;">return</span> out_x</span></code></pre></div>
</div>
<p>The implementation makes use of the<sup>45</sup> <em>dictionary of keys</em> representation of a sparse matrix from <code>scipy.sparse</code>. This is an efficient storage scheme when you need to modify the sparsity structure (as we are doing here) or do a lot of indexing. It would definitely be possible to implement this directly on the CSC data structure, but it gets a little bit tricky to access the elements of <code>L_inv</code> that are above the diagonal. The resulting code is honestly a mess and there’s lots of non-local memory access anyway, so I implemented it this way.</p>
<p>But let’s be honest: this thing is crying out for a proper symmetric matrix class with sensible reverse iterators. But hey. Python.</p>
<p>The second chunk of the code is just the opposite of our <code>_structured_copy()</code> function. It takes a matrix with the sparsity pattern of <img src="https://latex.codecogs.com/png.latex?L"> and returns one with the sparsity pattern of <code>out</code> (which is assumed to be a subset, and is usually the sparsity pattern of <img src="https://latex.codecogs.com/png.latex?A"> or a diagonal matrix).</p>
<p>Let’s check that it works.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1">A_indices, A_indptr, A_x, A <span class="op" style="color: #5E5E5E;">=</span> make_matrix(<span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb19-2">n <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(A_indptr) <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb19-3"></span>
<span id="cb19-4"></span>
<span id="cb19-5">L_indices, L_indptr, L_x <span class="op" style="color: #5E5E5E;">=</span> sparse_cholesky(A_indices, A_indptr, A_x)</span>
<span id="cb19-6"></span>
<span id="cb19-7">a_inv_L <span class="op" style="color: #5E5E5E;">=</span> sparse_partial_inverse(L_indices, L_indptr, L_x, L_indices, L_indptr)</span>
<span id="cb19-8"></span>
<span id="cb19-9">col_counts_L <span class="op" style="color: #5E5E5E;">=</span> [L_indptr[i<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>] <span class="op" style="color: #5E5E5E;">-</span> L_indptr[i] <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(n)]</span>
<span id="cb19-10">cols_L <span class="op" style="color: #5E5E5E;">=</span> np.repeat(<span class="bu" style="color: null;">range</span>(n), col_counts_L)</span>
<span id="cb19-11"></span>
<span id="cb19-12">true_inv <span class="op" style="color: #5E5E5E;">=</span> np.linalg.inv(A.todense())</span>
<span id="cb19-13">truth_L <span class="op" style="color: #5E5E5E;">=</span> true_inv[L_indices, cols_L]</span>
<span id="cb19-14"></span>
<span id="cb19-15">a_inv_A <span class="op" style="color: #5E5E5E;">=</span> sparse_partial_inverse(L_indices, L_indptr, L_x, A_indices, A_indptr)</span>
<span id="cb19-16">col_counts_A <span class="op" style="color: #5E5E5E;">=</span> [A_indptr[i<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>] <span class="op" style="color: #5E5E5E;">-</span> A_indptr[i] <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(n)]</span>
<span id="cb19-17">cols_A <span class="op" style="color: #5E5E5E;">=</span> np.repeat(<span class="bu" style="color: null;">range</span>(n), col_counts_A)</span>
<span id="cb19-18">truth_A <span class="op" style="color: #5E5E5E;">=</span> true_inv[A_indices, cols_A]</span>
<span id="cb19-19"></span>
<span id="cb19-20"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"""</span></span>
<span id="cb19-21"><span class="ss" style="color: #20794D;">Error in partial inverse (all of L): </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(a_inv_L <span class="op" style="color: #5E5E5E;">-</span> truth_L)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb19-22"><span class="ss" style="color: #20794D;">Error in partial inverse (all of A): </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(a_inv_A <span class="op" style="color: #5E5E5E;">-</span> truth_A)<span class="sc" style="color: #5E5E5E;">: .2e}</span></span>
<span id="cb19-23"><span class="ss" style="color: #20794D;">"""</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Error in partial inverse (all of L):  1.57e-15
Error in partial inverse (all of A):  1.53e-15
</code></pre>
</div>
</div>
</section>
<section id="putting-the-log-determinant-together" class="level3">
<h3 class="anchored" data-anchor-id="putting-the-log-determinant-together">Putting the log-determinant together</h3>
<p>All of our bits are in place, so now all we need is to implement the primitive for the log-determinant. One nice thing here is that we don’t need to implement a transposition rule as the function is not structurally linear in any of its arguments. At this point we take our small wins where we can get them.</p>
<p>There isn’t anything particularly interesting in the implementation. But do note that the trace has been implemented in a way that’s aware that we’re only storing the bottom triangle of <img src="https://latex.codecogs.com/png.latex?A">.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1">sparse_log_det_p <span class="op" style="color: #5E5E5E;">=</span> core.Primitive(<span class="st" style="color: #20794D;">"sparse_log_det"</span>)</span>
<span id="cb21-2"></span>
<span id="cb21-3"><span class="kw" style="color: #003B4F;">def</span> sparse_log_det(A_indices, A_indptr, A_x):</span>
<span id="cb21-4">  <span class="cf" style="color: #003B4F;">return</span> sparse_log_det_p.bind(A_indices, A_indptr, A_x)</span>
<span id="cb21-5"></span>
<span id="cb21-6"><span class="at" style="color: #657422;">@sparse_log_det_p.def_impl</span></span>
<span id="cb21-7"><span class="kw" style="color: #003B4F;">def</span> sparse_log_det_impl(A_indices, A_indptr, A_x):</span>
<span id="cb21-8">  L_indices, L_indptr, L_x <span class="op" style="color: #5E5E5E;">=</span> sparse_cholesky(A_indices, A_indptr, A_x)</span>
<span id="cb21-9">  <span class="cf" style="color: #003B4F;">return</span> <span class="fl" style="color: #AD0000;">2.0</span> <span class="op" style="color: #5E5E5E;">*</span> jnp.<span class="bu" style="color: null;">sum</span>(jnp.log(L_x[L_indptr[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]]))</span>
<span id="cb21-10"></span>
<span id="cb21-11"><span class="at" style="color: #657422;">@sparse_log_det_p.def_abstract_eval</span></span>
<span id="cb21-12"><span class="kw" style="color: #003B4F;">def</span> sparse_log_det_abstract_eval(A_indices, A_indptr, A_x):</span>
<span id="cb21-13">  <span class="cf" style="color: #003B4F;">return</span> abstract_arrays.ShapedArray((<span class="dv" style="color: #AD0000;">1</span>,), A_x.dtype)</span>
<span id="cb21-14"></span>
<span id="cb21-15"><span class="kw" style="color: #003B4F;">def</span> sparse_log_det_value_and_jvp(arg_values, arg_tangent):</span>
<span id="cb21-16">  A_indices, A_indptr, A_x <span class="op" style="color: #5E5E5E;">=</span> arg_values</span>
<span id="cb21-17">  _, _, A_xt <span class="op" style="color: #5E5E5E;">=</span> arg_tangent</span>
<span id="cb21-18">  L_indices, L_indptr, L_x <span class="op" style="color: #5E5E5E;">=</span> sparse_cholesky(A_indices, A_indptr, A_x)</span>
<span id="cb21-19">  value <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">2.0</span> <span class="op" style="color: #5E5E5E;">*</span> jnp.<span class="bu" style="color: null;">sum</span>(jnp.log(L_x[L_indptr[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]]))</span>
<span id="cb21-20">  Ainv_x <span class="op" style="color: #5E5E5E;">=</span> sparse_partial_inverse(L_indices, L_indptr, L_x, A_indices, A_indptr)</span>
<span id="cb21-21">  jvp <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">2.0</span> <span class="op" style="color: #5E5E5E;">*</span> <span class="bu" style="color: null;">sum</span>(Ainv_x <span class="op" style="color: #5E5E5E;">*</span> A_xt) <span class="op" style="color: #5E5E5E;">-</span> <span class="bu" style="color: null;">sum</span>(Ainv_x[A_indptr[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]] <span class="op" style="color: #5E5E5E;">*</span> A_xt[A_indptr[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]])</span>
<span id="cb21-22">  <span class="cf" style="color: #003B4F;">return</span> value, jvp</span>
<span id="cb21-23"></span>
<span id="cb21-24">ad.primitive_jvps[sparse_log_det_p] <span class="op" style="color: #5E5E5E;">=</span> sparse_log_det_value_and_jvp</span></code></pre></div>
</div>
<p>Finally, we can test it out.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1">ld_true <span class="op" style="color: #5E5E5E;">=</span> np.log(np.linalg.det(A.todense())) <span class="co" style="color: #5E5E5E;">#np.sum(np.log(lu.U.diagonal()))</span></span>
<span id="cb22-2"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Error in log-determinant = </span><span class="sc" style="color: #5E5E5E;">{</span>ld_true <span class="op" style="color: #5E5E5E;">-</span> sparse_log_det(A_indices, A_indptr, A_x)<span class="sc" style="color: #5E5E5E;">: .2e}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb22-3"></span>
<span id="cb22-4"><span class="kw" style="color: #003B4F;">def</span> f(theta):</span>
<span id="cb22-5">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> jnp.array(theta[<span class="dv" style="color: #AD0000;">0</span>] <span class="op" style="color: #5E5E5E;">*</span> A_x) <span class="op" style="color: #5E5E5E;">/</span> n</span>
<span id="cb22-6">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> Ax_theta.at[A_indptr[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]].add(theta[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb22-7">  <span class="cf" style="color: #003B4F;">return</span> sparse_log_det(A_indices, A_indptr, Ax_theta)</span>
<span id="cb22-8"></span>
<span id="cb22-9"><span class="kw" style="color: #003B4F;">def</span> f_jax(theta):</span>
<span id="cb22-10">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> jnp.array(theta[<span class="dv" style="color: #AD0000;">0</span>] <span class="op" style="color: #5E5E5E;">*</span> A.todense()) <span class="op" style="color: #5E5E5E;">/</span> n </span>
<span id="cb22-11">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> Ax_theta.at[np.arange(n),np.arange(n)].add(theta[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb22-12">  L <span class="op" style="color: #5E5E5E;">=</span> jnp.linalg.cholesky(Ax_theta)</span>
<span id="cb22-13">  <span class="cf" style="color: #003B4F;">return</span> <span class="fl" style="color: #AD0000;">2.0</span><span class="op" style="color: #5E5E5E;">*</span>jnp.<span class="bu" style="color: null;">sum</span>(jnp.log(jnp.diag(L)))</span>
<span id="cb22-14"></span>
<span id="cb22-15">primal1, jvp1 <span class="op" style="color: #5E5E5E;">=</span> jvp(f, (jnp.array([<span class="fl" style="color: #AD0000;">2.</span>, <span class="fl" style="color: #AD0000;">3.</span>]),), (jnp.array([<span class="fl" style="color: #AD0000;">1.</span>, <span class="fl" style="color: #AD0000;">2.</span>]),))</span>
<span id="cb22-16">primal2, jvp2 <span class="op" style="color: #5E5E5E;">=</span> jvp(f_jax, (jnp.array([<span class="fl" style="color: #AD0000;">2.</span>, <span class="fl" style="color: #AD0000;">3.</span>]),), (jnp.array([<span class="fl" style="color: #AD0000;">1.</span>, <span class="fl" style="color: #AD0000;">2.</span>]),))</span>
<span id="cb22-17"></span>
<span id="cb22-18">eps <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">1e-4</span></span>
<span id="cb22-19">jvp_fd <span class="op" style="color: #5E5E5E;">=</span> (f(jnp.array([<span class="fl" style="color: #AD0000;">2.</span>,<span class="fl" style="color: #AD0000;">3.</span>]) <span class="op" style="color: #5E5E5E;">+</span> eps <span class="op" style="color: #5E5E5E;">*</span> jnp.array([<span class="fl" style="color: #AD0000;">1.</span>, <span class="fl" style="color: #AD0000;">2.</span>]) ) <span class="op" style="color: #5E5E5E;">-</span> f(jnp.array([<span class="fl" style="color: #AD0000;">2.</span>,<span class="fl" style="color: #AD0000;">3.</span>]))) <span class="op" style="color: #5E5E5E;">/</span> eps</span>
<span id="cb22-20"></span>
<span id="cb22-21">grad1 <span class="op" style="color: #5E5E5E;">=</span> grad(f)(jnp.array([<span class="fl" style="color: #AD0000;">2.</span>, <span class="fl" style="color: #AD0000;">3.</span>]))</span>
<span id="cb22-22">grad2 <span class="op" style="color: #5E5E5E;">=</span> grad(f_jax)(jnp.array([<span class="fl" style="color: #AD0000;">2.</span>, <span class="fl" style="color: #AD0000;">3.</span>]))</span>
<span id="cb22-23"></span>
<span id="cb22-24"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"""</span></span>
<span id="cb22-25"><span class="ss" style="color: #20794D;">Check the Derivatives!</span></span>
<span id="cb22-26"><span class="ss" style="color: #20794D;">Variable A:</span></span>
<span id="cb22-27"><span class="ss" style="color: #20794D;">  Primal difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(primal1 <span class="op" style="color: #5E5E5E;">-</span> primal2)<span class="sc" style="color: #5E5E5E;">}</span></span>
<span id="cb22-28"><span class="ss" style="color: #20794D;">  JVP difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(jvp1 <span class="op" style="color: #5E5E5E;">-</span> jvp2)<span class="sc" style="color: #5E5E5E;">}</span></span>
<span id="cb22-29"><span class="ss" style="color: #20794D;">  JVP difference (FD): </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(jvp1 <span class="op" style="color: #5E5E5E;">-</span> jvp_fd)<span class="sc" style="color: #5E5E5E;">}</span></span>
<span id="cb22-30"><span class="ss" style="color: #20794D;">  Gradient difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(grad1 <span class="op" style="color: #5E5E5E;">-</span> grad2)<span class="sc" style="color: #5E5E5E;">}</span></span>
<span id="cb22-31"><span class="ss" style="color: #20794D;">"""</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Error in log-determinant =  0.00e+00</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Check the Derivatives!
Variable A:
  Primal difference: 0.0
  JVP difference: 0.000885009765625
  JVP difference (FD): 0.221893310546875
  Gradient difference: 1.526623782410752e-05
</code></pre>
</div>
</div>
<p>I’m not going to lie, I am <em>not happy</em> with that JVP difference. I was somewhat concerned that there was a bug somewhere in my code. I did a little bit of exploring and the error got larger as the problem got larger. It also depended a little bit more than I was comfortable on how I had implemented<sup>46</sup> the baseline dense version.</p>
<p>That second fact suggested to me that it might be a floating point problem. By default, JAX uses single precision (32-bit) floating point. Most modern systems that don’t try and run on GPUs use double precision (64-bit) floating point. So I tried it with double precision and lo and behold, the problem disappears.</p>
<p>Matrix factorisations are bloody hard in single precision.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="im" style="color: #00769E;">from</span> jax.config <span class="im" style="color: #00769E;">import</span> config</span>
<span id="cb25-2">config.update(<span class="st" style="color: #20794D;">"jax_enable_x64"</span>, <span class="va" style="color: #111111;">True</span>)</span>
<span id="cb25-3"></span>
<span id="cb25-4">ld_true <span class="op" style="color: #5E5E5E;">=</span> np.log(np.linalg.det(A.todense())) <span class="co" style="color: #5E5E5E;">#np.sum(np.log(lu.U.diagonal()))</span></span>
<span id="cb25-5"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Error in log-determinant = </span><span class="sc" style="color: #5E5E5E;">{</span>ld_true <span class="op" style="color: #5E5E5E;">-</span> sparse_log_det(A_indices, A_indptr, A_x)<span class="sc" style="color: #5E5E5E;">: .2e}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb25-6"></span>
<span id="cb25-7"><span class="kw" style="color: #003B4F;">def</span> f(theta):</span>
<span id="cb25-8">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> jnp.array(theta[<span class="dv" style="color: #AD0000;">0</span>] <span class="op" style="color: #5E5E5E;">*</span> A_x, dtype <span class="op" style="color: #5E5E5E;">=</span> jnp.float64) <span class="op" style="color: #5E5E5E;">/</span> n</span>
<span id="cb25-9">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> Ax_theta.at[A_indptr[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]].add(theta[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb25-10">  <span class="cf" style="color: #003B4F;">return</span> sparse_log_det(A_indices, A_indptr, Ax_theta)</span>
<span id="cb25-11"></span>
<span id="cb25-12"><span class="kw" style="color: #003B4F;">def</span> f_jax(theta):</span>
<span id="cb25-13">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> jnp.array(theta[<span class="dv" style="color: #AD0000;">0</span>] <span class="op" style="color: #5E5E5E;">*</span> A.todense(), dtype <span class="op" style="color: #5E5E5E;">=</span> jnp.float64) <span class="op" style="color: #5E5E5E;">/</span> n </span>
<span id="cb25-14">  Ax_theta <span class="op" style="color: #5E5E5E;">=</span> Ax_theta.at[np.arange(n),np.arange(n)].add(theta[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb25-15">  L <span class="op" style="color: #5E5E5E;">=</span> jnp.linalg.cholesky(Ax_theta)</span>
<span id="cb25-16">  <span class="cf" style="color: #003B4F;">return</span> <span class="fl" style="color: #AD0000;">2.0</span><span class="op" style="color: #5E5E5E;">*</span>jnp.<span class="bu" style="color: null;">sum</span>(jnp.log(jnp.diag(L)))</span>
<span id="cb25-17"></span>
<span id="cb25-18">primal1, jvp1 <span class="op" style="color: #5E5E5E;">=</span> jvp(f, (jnp.array([<span class="fl" style="color: #AD0000;">2.</span>, <span class="fl" style="color: #AD0000;">3.</span>], dtype <span class="op" style="color: #5E5E5E;">=</span> jnp.float64),), (jnp.array([<span class="fl" style="color: #AD0000;">1.</span>, <span class="fl" style="color: #AD0000;">2.</span>], dtype <span class="op" style="color: #5E5E5E;">=</span> jnp.float64),))</span>
<span id="cb25-19">primal2, jvp2 <span class="op" style="color: #5E5E5E;">=</span> jvp(f_jax, (jnp.array([<span class="fl" style="color: #AD0000;">2.</span>, <span class="fl" style="color: #AD0000;">3.</span>], dtype <span class="op" style="color: #5E5E5E;">=</span> jnp.float64),), (jnp.array([<span class="fl" style="color: #AD0000;">1.</span>, <span class="fl" style="color: #AD0000;">2.</span>], dtype <span class="op" style="color: #5E5E5E;">=</span> jnp.float64),))</span>
<span id="cb25-20"></span>
<span id="cb25-21">eps <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">1e-7</span></span>
<span id="cb25-22">jvp_fd <span class="op" style="color: #5E5E5E;">=</span> (f(jnp.array([<span class="fl" style="color: #AD0000;">2.</span>,<span class="fl" style="color: #AD0000;">3.</span>], dtype <span class="op" style="color: #5E5E5E;">=</span> jnp.float64) <span class="op" style="color: #5E5E5E;">+</span> eps <span class="op" style="color: #5E5E5E;">*</span> jnp.array([<span class="fl" style="color: #AD0000;">1.</span>, <span class="fl" style="color: #AD0000;">2.</span>], dtype <span class="op" style="color: #5E5E5E;">=</span> jnp.float64) ) <span class="op" style="color: #5E5E5E;">-</span> f(jnp.array([<span class="fl" style="color: #AD0000;">2.</span>,<span class="fl" style="color: #AD0000;">3.</span>], dtype <span class="op" style="color: #5E5E5E;">=</span> jnp.float64))) <span class="op" style="color: #5E5E5E;">/</span> eps</span>
<span id="cb25-23"></span>
<span id="cb25-24">grad1 <span class="op" style="color: #5E5E5E;">=</span> grad(f)(jnp.array([<span class="fl" style="color: #AD0000;">2.</span>, <span class="fl" style="color: #AD0000;">3.</span>], dtype <span class="op" style="color: #5E5E5E;">=</span> jnp.float64))</span>
<span id="cb25-25">grad2 <span class="op" style="color: #5E5E5E;">=</span> grad(f_jax)(jnp.array([<span class="fl" style="color: #AD0000;">2.</span>, <span class="fl" style="color: #AD0000;">3.</span>], dtype <span class="op" style="color: #5E5E5E;">=</span> jnp.float64))</span>
<span id="cb25-26"></span>
<span id="cb25-27"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"""</span></span>
<span id="cb25-28"><span class="ss" style="color: #20794D;">Check the Derivatives!</span></span>
<span id="cb25-29"><span class="ss" style="color: #20794D;">Variable A:</span></span>
<span id="cb25-30"><span class="ss" style="color: #20794D;">  Primal difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(primal1 <span class="op" style="color: #5E5E5E;">-</span> primal2)<span class="sc" style="color: #5E5E5E;">}</span></span>
<span id="cb25-31"><span class="ss" style="color: #20794D;">  JVP difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(jvp1 <span class="op" style="color: #5E5E5E;">-</span> jvp2)<span class="sc" style="color: #5E5E5E;">}</span></span>
<span id="cb25-32"><span class="ss" style="color: #20794D;">  JVP difference (FD): </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(jvp1 <span class="op" style="color: #5E5E5E;">-</span> jvp_fd)<span class="sc" style="color: #5E5E5E;">}</span></span>
<span id="cb25-33"><span class="ss" style="color: #20794D;">  Gradient difference: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span>linalg<span class="sc" style="color: #5E5E5E;">.</span>norm(grad1 <span class="op" style="color: #5E5E5E;">-</span> grad2)<span class="sc" style="color: #5E5E5E;">}</span></span>
<span id="cb25-34"><span class="ss" style="color: #20794D;">"""</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Error in log-determinant =  0.00e+00</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Check the Derivatives!
Variable A:
  Primal difference: 0.0
  JVP difference: 8.526512829121202e-13
  JVP difference (FD): 4.171707900013644e-06
  Gradient difference: 8.881784197001252e-16
</code></pre>
</div>
</div>
<p>Much better!</p>
</section>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping up</h2>
<p>And that is where we will leave it for today. Next up, I’m probably going to need to do the autodiff for the Cholesky factorisation. It’s not <em>hard</em>, but it is tedious<sup>47</sup> and this post is already very long.</p>
<p>After that we need a few more things:</p>
<ol type="1">
<li><p>Compilation rules for all of these things. For the most part, we can just wrap the relevant parts of <a href="https://github.com/libigl/eigen">Eigen</a>. The only non-trivial code would be the partial inverse. That will allow us to JIT shit.</p></li>
<li><p>We need to beef up the sparse matrix class a little. In particular, we are going to need addition and scalar multiplication at the very minimum to make this useful.</p></li>
<li><p>Work out how <a href="https://aesara.readthedocs.io/en/latest/">Aesara</a> works so we can try to prototype a PyMC model.</p></li>
</ol>
<p>That will be <em>a lot</em> more blog posts. But I’m having fun. So why the hell not.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>I am sorry Cholesky factorisation, this blog is already too long and there is simply too much code I need to make nicer to even start on that journey. So it will happen in a later blog.↩︎</p></li>
<li id="fn2"><p>Which I have spent <em>zero</em> effort making pretty or taking to any level above scratch code↩︎</p></li>
<li id="fn3"><p>Like making it clear how this works for a <em>sparse</em> matrix compared to a general one↩︎</p></li>
<li id="fn4"><p>To the best of my knowledge, for example, we don’t know how to differentiate with respect to the order parameter <img src="https://latex.codecogs.com/png.latex?%5Cnu"> in the modified Bessel function of the second kind <img src="https://latex.codecogs.com/png.latex?K_%5Cnu(x)">. This is important in spatial statistics (and general GP stuff).↩︎</p></li>
<li id="fn5"><p><em>You</em> may need to convince yourself that this is possible. But it is. The cone of SPD matrices is very nice.↩︎</p></li>
<li id="fn6"><p>Don’t despair if you don’t recognise the third line, it’s the Neumann series, which gives an approximation to <img src="https://latex.codecogs.com/png.latex?(I%20+%20B)%5E%7B-1%7D"> whenever <img src="https://latex.codecogs.com/png.latex?%5C%7CB%5C%7C%20%5Cll%201">.↩︎</p></li>
<li id="fn7"><p>I recognise that I’ve not explained why everything needs to be JAX-traceable. Basically it’s because JAX does clever transformations to the Jacobian-vector product code to produce things like gradients. And the only way that can happen is if the JVP code can take abstract JAX types. So we need to make it traceable because we <em>really</em> want to have gradients!↩︎</p></li>
<li id="fn8"><p>Why not now, Daniel? Why not now? Well mostly because I might need to do some tweaking down the line, so I am not messing around until I am done.↩︎</p></li>
<li id="fn9"><p>This is the primary difference between implementing forward mode and reverse mode: there is only one output here. When we move onto reverse mode, we will output a tuple Jacobian-transpose-vector products, one for each input. You can see the structure of that reflected in the transposition rule we are going to write later.↩︎</p></li>
<li id="fn10"><p>Some things: Firstly your function needs to have the correct signature for this to work. Secondly, you could also use <code>ad.defjvp()</code> if you didn’t need to use the primal value to define the tangent (recall one of our tangents is <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D%5CDelta%20c">, where <img src="https://latex.codecogs.com/png.latex?c%20=%20A%5E%7B-1%7Db"> is the primal value).↩︎</p></li>
<li id="fn11"><p>This is because it is the efficient way of computing a gradient. Forward-mode autodiff chains together Jacobian-vector products in such a way that a single sweep of the entire function computes a single directional derivative. Reverse-mode autodiff chains together Jacobian-transpose-vector products (aka vector-Jacobian products) in such a way that a single sweep produces an entire gradient. (This happens at the cost of quite a bit of storage.) Depending on what you are trying to do, you usually want one or the other (or sometimes a clever combination of both).↩︎</p></li>
<li id="fn12"><p>or gradients or some sort of thing.↩︎</p></li>
<li id="fn13"><p>to be honest, in Stan we sometimes just don’t dick around with the forward-mode autodiff, because gradients are our bread and butter.↩︎</p></li>
<li id="fn14"><p>I mean, love you programming language people. But fuck me this paper could’ve been written in Babylonic cuneiform for all I understood it.↩︎</p></li>
<li id="fn15"><p>That is, if you fix a value of <img src="https://latex.codecogs.com/png.latex?y">, <img src="https://latex.codecogs.com/png.latex?f_y(x)%20=%20f(x,%20y)"> is not an affine function.↩︎</p></li>
<li id="fn16"><p>Details bore me.↩︎</p></li>
<li id="fn17"><p>In general, there might need to be a little bit of reshaping, but it’s equivalent.↩︎</p></li>
<li id="fn18"><p>Have you noticed this is like the third name I’ve used for this equivalent concept. Or the fourth? The code calls it a cotangent because that’s another damn synonym. I’m so very sorry.↩︎</p></li>
<li id="fn19"><p>not difficult, I’m just lazy and Mike does it better that I can. Read his paper.↩︎</p></li>
<li id="fn20"><p>For sparse matrices it’s just the non-zero mask of that.↩︎</p></li>
<li id="fn21"><p>Yes. I know. Central differences. I am what I am.↩︎</p></li>
<li id="fn22"><p>Some of the stuff I’ve done like normalising all of the inputs would help make these tests more stable. You should also just pick up Nick Higham’s backwards error analysis book to get some ideas of what your guarantees actually are in floating point, but I truly cannot be bothered. This is scratch code.↩︎</p></li>
<li id="fn23"><p>It should be slightly bigger, it isn’t.↩︎</p></li>
<li id="fn24"><p>The largest number <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> such that <code>float(1.0) == float(1.0 + machine_eps)</code> in single precision floating point.↩︎</p></li>
<li id="fn25"><p>Fun fact: I implemented this and the error never spawned, so I guess JAX is keeping the index arrays concrete, which is very nice of it!↩︎</p></li>
<li id="fn26"><p>actual damn numbers↩︎</p></li>
<li id="fn27"><p>We want that <a href="https://youtu.be/wrnUJoj14ag?t=288">auld triangle to go jingle bloody jangle</a>↩︎</p></li>
<li id="fn28"><p>We definitely do not want someone to write an eight hour, two part play that really seems to have the point of view that our Cholesky triangle deserved his downfall. Espoused while periodically reading deadshit tumblr posts. I mean, it would win a Tony. But we still do not want that.↩︎</p></li>
<li id="fn29"><p>There are more arguments. Read the help. This is what we need↩︎</p></li>
<li id="fn30"><p>What if I told you that this would work perfectly well if <img src="https://latex.codecogs.com/png.latex?A"> was a linear partial differential operator or an integral operator? Probably not much because why would you give a shit?↩︎</p></li>
<li id="fn31"><p>It can be more general, but it isn’t↩︎</p></li>
<li id="fn32"><p>I think there is a typo in the docs↩︎</p></li>
<li id="fn33"><p>Full disclosure: I screwed this up multiple times today and my tests caught it. What does that look like? The derivatives for <img src="https://latex.codecogs.com/png.latex?A"> being off, but everything else being good.↩︎</p></li>
<li id="fn34"><p>And some optional keyword arguments, but we don’t need to worry about those↩︎</p></li>
<li id="fn35"><p>This is not quite the same but similar to something that functional programming people call <em>currying</em>, which was named after famous Australian Olympic swimmer Lisa Curry.↩︎</p></li>
<li id="fn36"><p>and a shitload simpler!↩︎</p></li>
<li id="fn37"><p>And we have to store a bunch more. This is less of a big deal when <img src="https://latex.codecogs.com/png.latex?L"> is sparse, but for an ordinary linear solve, we’d be hauling around an extra <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(n%5E2)"> floats containing tangents for no good reason.↩︎</p></li>
<li id="fn38"><p>If you are worrying about the suppressed constant, remember that <img src="https://latex.codecogs.com/png.latex?A"> (and therefore <img src="https://latex.codecogs.com/png.latex?n"> and <img src="https://latex.codecogs.com/png.latex?%5C%7CA%5C%7C">) is fixed.↩︎</p></li>
<li id="fn39"><p>I think I’ve made this mistake about four times already while writing this blog. So I am going to write it <em>out</em>.↩︎</p></li>
<li id="fn40"><p>Not to “some of my best friends are physicists”, but I do love them. I just wished a man would talk about me the way they talk about being coordinate free. Rather than with the same ambivalence physicist use when speaking about a specific atlas. I’ve been listening to lesbian folk music all evening. I’m having feelings.↩︎</p></li>
<li id="fn41"><p>pronoun on purpose↩︎</p></li>
<li id="fn42"><p>Takahashi, K., Fagan, J., Chen, M.S., 1973. Formation of a sparse bus impedance matrix and its application to short circuit study. In: Eighth PICA Conference Proceedings.IEEE Power Engineering Society, pp.&nbsp;63–69 (Papers Presented at the 1973 Power Industry Computer Application Conference in Minneapolis, MN).↩︎</p></li>
<li id="fn43"><p>Thanks to Jerzy Baranowski for finding a very very bad LaTeX error that made these questions quite wrong!↩︎</p></li>
<li id="fn44"><p>Indeed, in the notation of post two <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D_i%20%5Ccap%20%5C%7Bi+1,%20%5Cdots,%20n%5C%7D%20%5Csubseteq%20%5Cmathcal%7BL%7D_j"> for all <img src="https://latex.codecogs.com/png.latex?i%20%5Cleq%20j">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D_i"> is the set of non-zeros in the <img src="https://latex.codecogs.com/png.latex?i">th column of <img src="https://latex.codecogs.com/png.latex?L">.↩︎</p></li>
<li id="fn45"><p>The sparse matrix is stored as a dictionary <code>{(i,j): value}</code>, which is a very natural way to build a sparse matrix, even if its quite inefficient to do anything with it in that form.↩︎</p></li>
<li id="fn46"><p>You can’t just use <code>jnp.linalg.det()</code> because there’s a tendency towards <code>nan</code>s. (The true value is something like <code>r exp(250.49306761204593)</code>!)↩︎</p></li>
<li id="fn47"><p>Would it be less tedious if my implementation of the Cholesky was less shit? Yes. But hey. It was the first non-trivial piece of python code I’d written in more than a decade (or maybe ever?) so it is what it is. Anyway. I’m gonna run into the same problem I had in <a href="https://dansblog.netlify.app/posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/">Part 3</a>↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{simpson2022,
  author = {Dan Simpson},
  editor = {},
  title = {Sparse Matrices 6: {To} Catch a Derivative, First You’ve Got
    to Think Like a Derivative},
  date = {2022-05-30},
  url = {https://dansblog.netlify.app/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-simpson2022" class="csl-entry quarto-appendix-citeas">
Dan Simpson. 2022. <span>“Sparse Matrices 6: To Catch a Derivative,
First You’ve Got to Think Like a Derivative.”</span> May 30, 2022. <a href="https://dansblog.netlify.app/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative">https://dansblog.netlify.app/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative</a>.
</div></div></section></div> ]]></description>
  <category>JAX</category>
  <category>Sparse matrices</category>
  <category>Autodiff</category>
  <guid>https://dansblog.netlify.app/posts/2022-05-20-to-catch-a-derivative-first-youve-got-to-think-like-a-derivative/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative.html</guid>
  <pubDate>Sun, 29 May 2022 14:00:00 GMT</pubDate>
</item>
<item>
  <title>\((n-1)\)-sane in the membrane</title>
  <dc:creator>Dan Simpson</dc:creator>
  <link>https://dansblog.netlify.app/posts/2021-10-11-n-sane-in-the-membrane/n-sane-in-the-membrane.html</link>
  <description><![CDATA[ 




<p>I’ve been teaching a lot lately. That’s no huge surprise. It is my job. Maybe the one slight oddity this year is that I shifted jobs and, in switching hemispheres, I landed 3 consecutive teaching semesters. So. I’ve been teaching a lot lately.</p>
<p>And when you’re in a period of heavy teaching, every-fucking-thing is about teaching.</p>
<p>So this blogpost is about teaching.</p>
<p>Right now, I’m coming to the end of a second year class called <em>Statistical Thinking</em>. It’s been fun to work out how to teach the material. It’s standard fare: sampling variation, tests, bootstraps<sup>1</sup>, regression, and just a hint of Bayes in the last 2 weeks that you incentivize by promising a bastard of an exam question. So you know, (arms up even though I’m Catholic) <a href="https://www.youtube.com/watch?v=6nwj8nAYEM4"><strong>tradition</strong></a>!</p>
<section id="if-i-were-a-rich-man-katrina-lenk-with-a-violin" class="level3">
<h3 class="anchored" data-anchor-id="if-i-were-a-rich-man-katrina-lenk-with-a-violin">If I were a rich man (Katrina Lenk with a violin)</h3>
<p>The thing about teaching an intro stats class is that it brings screaming to mind that quote from Bennett’s The History Boys<sup>2</sup>: (paraphrasing) <em>“How do I define [intro to Statistics]? It’s just one fucking thing after another”</em>.</p>
<p>Constructing twelve moderately sequential weeks from the whole mass of things that someone being introduced to statistics needs to know is not unlike being thrown in the middle of the lake with nothing but an ice-cream container and a desiccated whale penis: confusing, difficult, and rather damp.</p>
<p>The nice thing about building an intro stats course is you’re not alone. You’re adrift in a sea of shit ideas! (Also a lot of good ones<sup>3</sup>, but don’t ruin my flow!)</p>
<p>The trouble is that this sort of course is simultaneously teaching <em>big concepts</em> and <em>complex details</em>. And while it’s not toooooo hard to make the concepts build and reinforce as time inexorably marches on, the techniques and details needed to illuminate the big concepts are not quite as linear.</p>
<p>There are two routes through this conundrum: incantations inscribed onto books made of human skin using the blood of sacrificial virgins (aka gathered during engineering statistics service teaching) or computers.</p>
<p>I went with computers because we are in lockdown and I couldn’t be bothered sourcing and bleeding virgins.</p>
<p>The downside is that you need the students to have a grip on R programming (and programmatic thinking). This only happens if the degree you are teaching in is built in such a way that these skills have already been taught. Otherwise you need to teach both (which is very possible, but you need to teach less statistical content).</p>
<p>This is not a postmortem on my teaching, but if it were, it would be about that last point.</p>
</section>
<section id="i-saw-goody-proctor-with-the-devil" class="level3">
<h3 class="anchored" data-anchor-id="i-saw-goody-proctor-with-the-devil">I saw Goody Proctor with the devil!</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://dansblog.netlify.app/posts/2021-10-11-n-sane-in-the-membrane/tweet_poll.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">A <a href="https://twitter.com/hardsci/status/1446498701615288320">tweet</a> from Sanjay Srivastava</figcaption><p></p>
</figure>
</div>
<p>This is a very long way to say <em>I saw a tweet an had feelings</em>.</p>
<p>Because I’m thinking about this stuff pretty hard right now, I am (as <a href="https://www.youtube.com/watch?v=x31XrfiikwQ">Hedwig</a> would say) fully dilated.</p>
<p>And my question is <em>what is the use of teaching this distinction?</em> Should anyone bother dividing by <img src="https://latex.codecogs.com/png.latex?(n-1)"> instead of <img src="https://latex.codecogs.com/png.latex?n"> in their variance estimates?</p>
<p>Well I guess the first question is <em>is there a difference in this distinction</em>? Let’s do the sort of R experiment I want my students to do!</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="co" style="color: #5E5E5E;"># Independent samples for a qq-plot!</span></span>
<span id="cb1-2"><span class="co" style="color: #5E5E5E;"># Thanks to Rob Trangucci for catching this!</span></span>
<span id="cb1-3"><span class="fu" style="color: #4758AB;">library</span>(tidyverse)</span>
<span id="cb1-4">n_sim <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">100000</span></span>
<span id="cb1-5">n <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">10</span></span>
<span id="cb1-6">experiments <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">tibble</span>(<span class="at" style="color: #657422;">exp =</span> <span class="fu" style="color: #4758AB;">rep</span>(<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span>n_sim, <span class="at" style="color: #657422;">each =</span> n),</span>
<span id="cb1-7">                      <span class="at" style="color: #657422;">sample =</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n <span class="sc" style="color: #5E5E5E;">*</span> n_sim),</span>
<span id="cb1-8">                      <span class="at" style="color: #657422;">sample2 =</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n <span class="sc" style="color: #5E5E5E;">*</span> n_sim))</span>
<span id="cb1-9"></span>
<span id="cb1-10">compare <span class="ot" style="color: #003B4F;">&lt;-</span> experiments <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb1-11">  <span class="fu" style="color: #4758AB;">group_by</span>(exp) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb1-12">  <span class="fu" style="color: #4758AB;">summarise</span>(<span class="at" style="color: #657422;">m =</span> <span class="fu" style="color: #4758AB;">mean</span>(sample),</span>
<span id="cb1-13">            <span class="at" style="color: #657422;">m2 =</span> <span class="fu" style="color: #4758AB;">mean</span>(sample2),</span>
<span id="cb1-14">            <span class="at" style="color: #657422;">var_bias =</span> <span class="fu" style="color: #4758AB;">mean</span>((sample <span class="sc" style="color: #5E5E5E;">-</span> m)<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span>),</span>
<span id="cb1-15">            <span class="at" style="color: #657422;">z_bias =</span> m <span class="sc" style="color: #5E5E5E;">/</span> <span class="fu" style="color: #4758AB;">sqrt</span>(<span class="fu" style="color: #4758AB;">mean</span>(var_bias)),</span>
<span id="cb1-16">            <span class="at" style="color: #657422;">z =</span> m2 <span class="sc" style="color: #5E5E5E;">/</span> <span class="fu" style="color: #4758AB;">sd</span>(sample2))</span>
<span id="cb1-17"></span>
<span id="cb1-18"></span>
<span id="cb1-19">compare <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-20">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="fu" style="color: #4758AB;">sort</span>(z), <span class="fu" style="color: #4758AB;">sort</span>(z_bias))) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-21">  <span class="fu" style="color: #4758AB;">geom_point</span>() <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-22">  <span class="fu" style="color: #4758AB;">geom_abline</span>(<span class="at" style="color: #657422;">slope =</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">intercept =</span> <span class="dv" style="color: #AD0000;">0</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-23">  <span class="fu" style="color: #4758AB;">theme_bw</span>() <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb1-24">  <span class="fu" style="color: #4758AB;">coord_fixed</span>(<span class="at" style="color: #657422;">xlim =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">2</span>), <span class="at" style="color: #657422;">y =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">2</span>))</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://dansblog.netlify.app/posts/2021-10-11-n-sane-in-the-membrane/n-sane-in-the-membrane_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Well that is clear. There is not.</p>
<p>Or, well, there is a <em>small</em> difference.</p>
<p>But to see it, you need a lot of samples! Why? Well the easy answer is maths.</p>
<p>For one thing, when <img src="https://latex.codecogs.com/png.latex?n=10">, <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7Bn%7D%20-%20%5Cfrac%7B1%7D%7Bn-1%7D%20=%20%5Cfrac%7B1%7D%7B90%7D%20=%200.01.%0A"> This does not compare well against the sampling variance, which (assuming <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2%5Capprox%201">, which is usual if you’ve scaled your problem correctly) is about <img src="https://latex.codecogs.com/png.latex?0.3">.</p>
<p>But we could choose to do it properly. The bias in the MLE (aka the divide by <img src="https://latex.codecogs.com/png.latex?n">) variance estimate is <img src="https://latex.codecogs.com/png.latex?%0A-%5Cfrac%7B%5Csigma%5E2%7D%7Bn%7D.%0A"> This is a lot smaller than the sampling variability of the estimate (aka how much uncertainty you have because of the finite sample), which is <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Csigma%7D%7B%5Csqrt%7Bn%7D%7D.%0A"></p>
<p>And that’s the whole story. Dividing by <img src="https://latex.codecogs.com/png.latex?n"> instead of <img src="https://latex.codecogs.com/png.latex?(n-1)"> leaves you with a <em>slightly</em> biased estimate. But the bias if <em>fucking tiny</em>. It is possibly moving your second decimal place by about 1 number (assume our population variance is one). The sampling variably is moving the <em>first</em> decimal place by several digits.</p>
<p>Truly. What is the point. The old guys<sup>4</sup> who went wild about bias are now mostly dead. Or they’ve changed their minds (which is, you know, a reasonable thing to do as information about best practice is updated). The war against bias was lost before your undergraduates were born.</p>
</section>
<section id="even-in-crisis-i-maintain" class="level3">
<h3 class="anchored" data-anchor-id="even-in-crisis-i-maintain">Even in crisis, I maintain</h3>
<p>But nevertheless, this whole DIVIDE BY N-1 OR THE BIAS MONSTER IS GONNA GET YA bullshit continues.</p>
<p>And to some extent, maybe I shouldn’t care. I definitely shouldn’t care this many words about it.</p>
<p>But I do. And I do for a couple of reasons.</p>
<p><strong>Reason One:</strong> What is the point teaching students about uncertainty and that you can’t just say “this number is different” because the estimate on a single sample is different. If I am to say that I need things to be <em>at least</em><sup>5</sup> <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(n%5E%7B-1/2%7D)"> apart before I’m willing to say they are maybe different, then why am I harping on about the much smaller difference?</p>
<p><strong>Reason Two:</strong> It’s a shitty example. Bias and bias corrections have a role to play in statistics<sup>6</sup>. But if this is your first introduction to bias correction, you are going to teach either:</p>
<ol type="a">
<li>Bias is always bad, regardless of context / sampling variance / etc</li>
<li>Bias can be corrected, but it’s trivial and small.</li>
</ol>
<p>Both of those things are bullshit. Just teach them how to bootstrap and teach the damn thing properly. You do not have to go very far to show bias actually making a difference!</p>
<p><em>Maybe</em> the only place the difference will be noticed is if you compare against the in-build <code>var</code> or <code>sd</code> functions. This is not the use case I would build my class around, but it is a thing you would need to be aware of.</p>
</section>
<section id="the-worlds-is-a-question-this-room-is-an-answer.-and-the-answer-is-no." class="level3">
<h3 class="anchored" data-anchor-id="the-worlds-is-a-question-this-room-is-an-answer.-and-the-answer-is-no.">The worlds is a question, this room is an answer. And the answer is no.</h3>
<p>If you are going to teach statistics as more than just stale incantations and over-done fear-mongering, you need to construct the types of stakes that are simply not present in the <img src="https://latex.codecogs.com/png.latex?n"> vs <img src="https://latex.codecogs.com/png.latex?n-1"> bullshit.</p>
<p>It is present when you are teaching the normal vs t distribution. You are teaching that the design of your experiment changes the possible extreme behaviour and sometimes it can change <em>a lot</em>.</p>
<p>The <img src="https://latex.codecogs.com/png.latex?n"> vs <img src="https://latex.codecogs.com/png.latex?(n-1)"> denominator for a variance estimator is a curiosity. It is the source of thrilling<sup>7</sup> exercises or exam questions. But it is not interesting.</p>
<p>It could maybe set up the idea that MLEs are not unbiased. But even then, the useless correction term is not needed. Just let it be slightly biased and move on with your life.</p>
<p>Because if that is the biggest bias in your analysis, you are truly blessed.</p>
<p>In real life, bias is the price you pay for being good at statistics. And like any market, if you pay too much you’re maybe not that good. But if you pay nothing at all, you don’t get to play.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>To paraphrase Jimmy Somerville, tell me <em>whyyyyyyyyy</em> about 90% of the bootstrap material on the web is … misguided. And why <code>tidymodels</code> only has the shit bootstrap in it?↩︎</p></li>
<li id="fn2"><p>Ok. Straight up, <em>“[Intro to statistics] is a commentary on the various and continuing incapabilities of men”</em> would’ve also worked.↩︎</p></li>
<li id="fn3"><p>This course stands on the shoulders of giants: Di Cook and Catherine Forbes gave me a great base. And of course every single textbook (shout out to the OpenIntro crew!), blog post, weird subsection of some other book, paper from 1987 on some weird bootstrap, etc that I have used to make a course!↩︎</p></li>
<li id="fn4"><p>Yes. I used the word on purpose.↩︎</p></li>
<li id="fn5"><p><img src="https://latex.codecogs.com/png.latex?n"> is the size of the sample.↩︎</p></li>
<li id="fn6"><p>I spend most of my time doing Bayes shit, and we play this game somewhat differently. But the gist is the same.↩︎</p></li>
<li id="fn7"><p>Not thrilling.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{simpson2021,
  author = {Dan Simpson},
  editor = {},
  title = {\$(N-1)\$-Sane in the Membrane},
  date = {2021-10-14},
  url = {https://dansblog.netlify.app/n-sane-in-the-membrane},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-simpson2021" class="csl-entry quarto-appendix-citeas">
Dan Simpson. 2021. <span>“$(N-1)$-Sane in the Membrane.”</span> October
14, 2021. <a href="https://dansblog.netlify.app/n-sane-in-the-membrane">https://dansblog.netlify.app/n-sane-in-the-membrane</a>.
</div></div></section></div> ]]></description>
  <category>Teaching</category>
  <category>Fundamentals</category>
  <category>Opinionated</category>
  <guid>https://dansblog.netlify.app/posts/2021-10-11-n-sane-in-the-membrane/n-sane-in-the-membrane.html</guid>
  <pubDate>Wed, 13 Oct 2021 13:00:00 GMT</pubDate>
  <media:content url="https://twitter.com/hardsci/status/1446498701615288320" medium="image"/>
</item>
</channel>
</rss>
