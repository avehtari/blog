---
title: "A linear mixed model"
description: |
  Hubris. Just hubris. 
author:
  - name: Dan Simpson
    url: https://dpsimpson.github.io
date: 03-22-2022
output:
  distill::distill_article:
    self_contained: false
draft: true
---

Sometimes, you've just got to set yourself a task. And I've decided,
for no particular reason at all, that I should be better at Python. This
is a modest goal. I am currently shit at Python. So I'm unlikely to get 
worse.

Part of the reason why I'm a shit Python programmer is I don't really have
any reason to use it in my day-to-day job. So I've been trying to come
up with one. And come up with one I did.

I want to test out some ideas I have for making linear mixed effects-type
models faster in Stan. "But Daniel", you sigh, "Stan is not written in Python." No it fucking isn't. It's written in C++ and, while I am actually
a better C++ programmer than I am a Python programmer, it is slow as balls
to prototype in and the overhead for something I'm not sure will work is
just too hard.

This has been flopping around in my haunted clown car of a brain for a 
while, but the other night I realised how I could actually go about 
doing it: I could use PyMC.

PyMC is written^[I haven't checked. Bits of it are probably at least a
little in C++. But hey. No one's perfect.] in Python, so extending it
would give me an excuse to buff up my Python skills. It will also give 
me a solid chance to actually let loose on JAX and see what it can do.

All in all, I expect this to be a shitshow, but maybe it'll be fun.

The (planned) order of service is as follows:

1. A quick write up on the model I want to fit.
2. A JAX-implementation of the log-likelihood. (This, I promise you, will hurt)
3. PyMC bindings!
4. Let's see if it works!
5. Can we make it better?

## A generalised linear mixed effects-ish model

If you were to open the correct textbook, or the [Bates, MÃ¤chler, Boler, and Walker 2015 masterpiece paper](https://www.jstatsoft.org/article/view/v067i01) that describes the workings of `lme4`, you will see the linear mixed model written as $$
y ~ X\beta + Zb + \epsilon,
$$
where 

- the columns of $X$ contain the covariates^[and the intercept if it's needed], 
- $\beta$ is a vector of unknown regression coefficients, 
- $Z$ is a known matrix that describes the random effects (basically which observation is linked to which random effect),
- $b \sim N(0, \Sigma_b)$ is the vector of random effects with some unknown covariance matrix $\Sigma_b$,
- and $\epsilon ~ N(0 ,\sigma^2 W)$ is the observation noise (here $W$ is a known diagonal matrix).

But unlike Doug Bates and his friends, my aim is to do Bayesian computation. In this situation, $\beta$ _also_ has a prior on it! In fact, I'm going to also put a prior $\beta \sim N(0, R)$, for some known^[Default options include the identity matrix or some multiple of the identity matrix.] matrix $R$.

This means that I can treat $\beta$ and $b$ the same^[REML heads don't dismay. You can do all kinds of weird shit by choosing some of these matrices in certain ways. I'm not gonna stop you. I love and support you. Good vibes only.] way! And I'm going to do just that. I'm going to put them together into a vector $u = (\beta^T, b^T)^T$. Because the prior on $u$ is Gaussian^[The priors on $\beta$ and $b$ are independent Gaussian so it has to be.], I'm sometimes going to call $u$ the _Gaussian component_ or even the _latent_^[Latent = not directly observed but you think it's there. Like latent homosexuality.] Gaussian component.

Now that I've smooshed my fixed and random effects together, I don't really need to keep $X$ and $Z$ separate. So I'm going push them together into a rectangular matrix $$
A = [X \vdots Z].
$$ 

This allows us to re-write the model as 
\begin{align*}
y \mid u, \sigma & \sim N(A u, \sigma^2 W)\\
u \mid \theta \sim N(0, Q(\theta)^{-1}).
\end{align*}

What the hell is $Q(\theta)$ and why are we suddenly parameterising a multivariate normal distribution by the inverse of its covariance matrix (which, if you're curious, is known as a _precision_ matrix)???

I will take your questions in reverse order. 

We are parameterising by the precision^[Inverse correlation matrix] matrix because it will simplify our formulas and lead to faster computations. This will be a major topic for us later!

As to what $Q(\theta)$ is, it is the matrix $$
Q(\theta) = \begin{pmatrix} \Sigma_b^{-1} & 0 \\ 0 & R^{-1}\end{pmatrix}
$$ and $\theta = (\sigma, \Sigma_b)$ is the collection of all^[excluding the fixed ones, like $W$ and $A$ and $R$. ] non-Gaussian parameters in the model.

This is a _very_ generic model. It happily contains things like 

- Linear regression!
- Linear regression with horseshoe priors!
- Linear mixed effects models!
- Linear regression with splines (smoothing or basis)!
- Spatial models like ICARs etc etc etc
- Gaussian processes (begrudgingly, those you should formulate in terms of the covariance matrix and not the precision)
- Any combination of these things!

So if I manage to get this implemented efficiently, all of these models will become efficient too. All it will cost is a truly shithouse^[I would suggest a lot of syntactic sugar if you were ever going to expose this stuff to users.] interface.

The only downside of this degree of flexibility compared to just implementing a straight linear mixed model with $X$ and $Z$ and $\beta$ and $b$ all living separately is that there are a couple of tricks^[See the Bates _et al._ paper. Their formulation is fabulous but doesn't extend nicely to the situations I care about! Basically they optimise for the situation where $\Sigma_b$ can be singular, which is an issue when you're doing optimisation. I care about the case where "$\Sigma_b^{-1}$" is defined as a singular matrix as this occurs in many important models like smoothing splines and ICAR models (which are extremely popular in spatial epidemiology).] to improve numerical stability that we can't use.

## Let's get the posterior!


