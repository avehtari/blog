{
  "hash": "8ef0f184ad22d740664dce3197f1b885",
  "result": {
    "markdown": "---\ntitle: \"Sparse matrices 6: To catch a derivative, first you've got to think like a derivative\"\ndescription: |\n  Open up the kennels, Kenneth. Mammaâ€™s coming home tonight.\ntwitter-card:\n  title: \"Sparse matrices 6: To catch a derivative, first you've got to think like a derivative\"\n  creator: \"@dan_p_simpson\"\ndate: 2022-05-30\nimage: sob.JPG\ncategories: [JAX, Sparse matrices, Autodiff]\ntwitter-card:\n  title: \"twitter title\"\n  description: \"twitter description\"\n  creator: \"@dan_p_simpson\"\ncitation: \n  url: https://dansblog.netlify.app/to-catch-a-derivative-first-youve-got-to-think-like-a-derivative\n---\n\nWelcome to part six!!! of our ongoing series on making sparse linear algebra\ndifferentiable in JAX with the eventual hope to be able to do some [cool statistical\nshit](https://dansblog.netlify.app/posts/2022-03-22-a-linear-mixed-effects-model/). We are _nowhere near done_.\n\n[Last time](https://dansblog.netlify.app/posts/2022-05-14-sparse4-some-primatives/), we looked at making JAX primitives. We built four of them. Today we\nare going to implement the corresponding differentiation rules! For three^[I am sorry Cholesky factorisation, this blog is already too long and there is simply too much code I need to make nicer to even start on that journey. So it will happen in a later blog.]\n of them.\n\nSo strap yourselves in. This is gonna be detailed.\n\nIf you're interested in the code^[Which I have spent _zero_ effort making pretty or taking to any level above scratch code], the git repo for this post is linked at the bottom and in there\nyou will find a folder with the python code in a python file.\n\n## She is beauty and she is grace. She is queen of 50 states. She is elegance and taste. She is miss autodiff\n\nDerivatives are computed in JAX through the glory and power of automatic differentiation.\nIf you came to this blog hoping for a great description of how autodiff works, I \nam terribly sorry but I absolutely do not have time for that. Might I suggest google?\nOr maybe flick through [this survey by Charles Margossian.](https://arxiv.org/abs/1811.05031).\n\nThe most important thing to remember about algorithmic differentiation is that it is _not_ symbolic\ndifferentiation. That is, it does not create the functional form of the derivative of the function\nand compute that. Instead, it is a system for cleverly composing derivatives in each\nbit of the program to compute the _value_ of the derivative of the function.\n\nBut for that to work, we need to implement those clever little mini-derivatives.\nIn particular, every function $f(\\cdot): \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ needs to have a function to compute the corresponding\nJacobian-vector product \n$$\n(\\theta, v) \\rightarrow J(\\theta) v,\n$$\nwhere the $n \\times m$ matrix $J(\\theta)$ has entries \n$$\nJ(\\theta)_{ij} = \\frac{\\partial f_j }{\\partial \\theta_j}.\n$$\n\nOk. So let's get onto this. We are going to derive and implement some Jacobian-vector products.\nAnd all of the assorted accoutrement. And by crikey. We are going to do it all\nin a JAX-traceable way. \n\n## JVP number one: The linear solve.\n\nThe first of the derivatives that we need to work out is the derivative of \na linear solve $A^{-1}b$. Now, intrepid \nreaders, the obvious thing to do is look the damn derivative up. You get exactly no\nhero points for computing it yourself.\n\nBut I'm not you, I'm a dickhead. \n\nSo I'm going to derive it. I could pretend there\nare reasons^[Like making it clear how this works for a _sparse_ matrix compared to a general one], but that\nwould just be lying. I'm doing it because I can. \n\nBeyond the obvious fun of working out a matrix derivative from first principles,\nthis is fun because we have _two_ arguments instead of just one. Double the fun.\n\nAnd we really should make sure the function is differentiated with respect\nto every reasonable argument. Why? Because if you write code other people might use, \nyou don't get to control how they use it (or what they will email you about). \nSo it's always good practice to limit surprises (like a function not being \ndifferentiable wrt some argument) to \ncases^[To the best of my knowledge, for example, we don't know how to differentiate with respect to the order parameter $\\nu$ in the modified Bessel function of the second kind $K_\\nu(x)$. This is important in spatial statistics (and general GP stuff).] where it absolutely necessary. This reduces the emails.\n\nTo that end, let's take an arbitrary SPD matrix $A$ with a _fixed_ sparsity pattern.\nLet's take another symmetric matrix $\\Delta$ with _the same sparsity pattern_ and\nassume that $\\Delta$ is small enough^[_You_ may need to convince yourself that this is possible. But it is. The cone of SPD matrices is very nice.] that $A + \\Delta$ is still symmetric positive definite. \nWe also need a vector $\\delta$ with a small $\\|\\delta\\|$.\n\nNow let's get algebraing.\n\\begin{align*}\nf(A + \\Delta, b + \\delta) &= (A+\\Delta)^{-1}(b + \\delta) \\\\\n&= (I + A^{-1}\\Delta)^{-1}A^{-1}(b + \\delta) \\\\\n&= (I - A^{-1}\\Delta + o(\\|\\Delta\\|))A^{-1}(b + \\delta) \\\\\n&= A^{-1}b + A^{-1}(\\delta - \\Delta A^{-1}b ) + o(\\|\\Delta\\| + \\|\\delta\\|)\n\\end{align*}\n\nEasy^[Don't despair if you don't recognise the third line, it's the Neumann series, which gives an approximation to $(I + B)^{-1}$ whenever $\\|B\\| \\ll 1$.] as.\n\nWe've actually calculated the derivative now, but it's a little more work to recognise it.\n\nTo do that, we need to remember the practical definition of the Jacobian of a function\n$f(x)$ that takes an $n$-dimensional input and produces an $m$-dimensional output.\nIt is the $n \\times m$ matrix $J_f(x)$ such that \n$$\nf(x + \\delta)  = f(x) + J_f(x)\\delta + o(\\|\\delta\\|).\n$$\n\n\nThe formulas further simplify if we write $c = A^{-1}b$. Then, if we want the Jacobian-vector product for the first argument, it is \n$$\n-A^{-1}\\Delta c,\n$$\nwhile the Jacobian-vector product for the second argument is \n$$\nA^{-1}\\delta.\n$$\n\nThe only wrinkle in doing this is we need to remember that we are only storing \nthe lower triangle of $A$. Because we need to represent $\\Delta$ the same way,\nit is represented as a vector `Delta_x` that contains only the lower triangle of \n$\\Delta$. So we need to make sure we remember to form the _whole_ matrix before \nwe do the matrix-vector product $\\Delta c$!\n\nBut otherwise, the implementation is going to be pretty straightforward. \nThe Jacobian-vector product costs one additional linear solve (beyond the one \nneeded to compute the value $c = A^{-1}b$). \n\nIn the language of JAX (and autodiff in general), we refer to $\\Delta$ and $\\delta$\nas _tangent vectors_. In search of a moderately coherent naming convention, we \nare going to refer to the tangent associated with the variable `x` as `xt`.\n\nSo let's implement this. Remember: it needs^[I recognise that I've not explained why everything needs to be JAX-traceable. Basically it's because JAX does clever transformations to the Jacobian-vector product code to produce things like gradients. And the only way that can happen is if the JVP code can take abstract JAX types. So we need to make it traceable because we _really_ want to have gradients!] to be JAX traceable.\n\n## Primitive two: The triangular solve\n\nFor some sense of continuity, we are going to keep the naming of the primitives\nfrom the last blog post, but we are _not_ going to attack them in the same order.\nWhy not? Because we work in order of complexity.\n\nSo first off we are going to do the triangular solve. As I have yet to package\nup the code (I promise, that will happen next^[Why not now, Daniel? Why not now? Well mostly because I might need to do some tweaking down the line, so I am not messing around until I am done.]), I'm just putting it here under the fold.\n\n<details><summary>The primal implementation</summary>\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom scipy import sparse\nimport numpy as np\nfrom jax import numpy as jnp\nfrom jax import core\nfrom jax._src import abstract_arrays\nfrom jax import core\n\nsparse_triangular_solve_p = core.Primitive(\"sparse_triangular_solve\")\n\ndef sparse_triangular_solve(L_indices, L_indptr, L_x, b, *, transpose: bool = False):\n  \"\"\"A JAX traceable sparse  triangular solve\"\"\"\n  return sparse_triangular_solve_p.bind(L_indices, L_indptr, L_x, b, transpose = transpose)\n\n@sparse_triangular_solve_p.def_impl\ndef sparse_triangular_solve_impl(L_indices, L_indptr, L_x, b, *, transpose = False):\n  \"\"\"The implementation of the sparse triangular solve. This is not JAX traceable.\"\"\"\n  L = sparse.csc_array((L_x, L_indices, L_indptr)) \n  \n  assert L.shape[0] == L.shape[1]\n  assert L.shape[0] == b.shape[0]\n  \n  if transpose:\n    return sparse.linalg.spsolve_triangular(L.T, b, lower = False)\n  else:\n    return sparse.linalg.spsolve_triangular(L.tocsr(), b, lower = True)\n\n@sparse_triangular_solve_p.def_abstract_eval\ndef sparse_triangular_solve_abstract_eval(L_indices, L_indptr, L_x, b, *, transpose = False):\n  assert L_indices.shape[0] == L_x.shape[0]\n  assert b.shape[0] == L_indptr.shape[0] - 1\n  return abstract_arrays.ShapedArray(b.shape, b.dtype)\n```\n:::\n\n\n</details>\n\n### The Jacobian-vector product\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom jax._src import ad_util\nfrom jax.interpreters import ad\nfrom jax import lax\nfrom jax.experimental import sparse as jsparse\n\ndef sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent, *, transpose):\n  \"\"\"\n  A jax-traceable jacobian-vector product. In order to make it traceable, \n  we use the experimental sparse CSC matrix in JAX.\n  \n  Input:\n    arg_values:   A tuple of (L_indices, L_indptr, L_x, b) that describe\n                  the triangular matrix L and the rhs vector b\n    arg_tangent:  A tuple of tangent values (same lenght as arg_values).\n                  The first two values are nonsense - we don't differentiate\n                  wrt integers!\n    transpose:    (boolean) If true, solve L^Tx = b. Otherwise solve Lx = b.\n  Output:         A tuple containing the maybe_transpose(L)^{-1}b and the corresponding\n                  Jacobian-vector product.\n  \"\"\"\n  L_indices, L_indptr, L_x, b = arg_values\n  _, _, L_xt, bt = arg_tangent\n  value = sparse_triangular_solve(L_indices, L_indptr, L_x, b, transpose=transpose)\n  if type(bt) is ad.Zero and type(L_xt) is ad.Zero:\n    # I legit do not think this ever happens. But I'm honestly not sure.\n    print(\"I have arrived!\")\n    return value, lax.zeros_like_array(value) \n  \n  if type(L_xt) is not ad.Zero:\n    # L is variable\n    if transpose:\n      Delta = jsparse.CSC((L_xt, L_indices, L_indptr), shape = (b.shape[0], b.shape[0])).transpose()\n    else:\n      Delta = jsparse.CSC((L_xt, L_indices, L_indptr), shape = (b.shape[0], b.shape[0]))\n\n    jvp_Lx = sparse_triangular_solve(L_indices, L_indptr, L_x, Delta @ value, transpose = transpose) \n  else:\n    jvp_Lx = lax.zeros_like_array(value) \n\n  if type(bt) is not ad.Zero:\n    # b is variable\n    jvp_b = sparse_triangular_solve(L_indices, L_indptr, L_x, bt, transpose = transpose)\n  else:\n    jvp_b = lax.zeros_like_array(value)\n\n  return value, jvp_b - jvp_Lx\n\nad.primitive_jvps[sparse_triangular_solve_p] = sparse_triangular_solve_value_and_jvp\n```\n:::\n\n\nBefore we see if this works, let's first have talk about the structure of the \nfunction I just wrote. Generally speaking, we want a function that takes in the \nprimals and tangents at tuples and then returns the value and the^[This is the primary difference between implementing forward mode and reverse mode: there is only one output here. When we move onto reverse mode, we will output a tuple Jacobian-transpose-vector products, one for each input. You can see the structure of that reflected in the transposition rule we are going to write later.] Jacobian-vector\nproduct. \n\nThe main thing you will notice in the code is that there is _a lot_ of checking for \n`ad.Zero`. This is a special type defined in JAX that is, essentially, telling\nthe autodiff system that we are not differentiating wrt that variable. This is \ndifferent to a tangent that just happens to be numerically equal to zero. \nAny code for a Jacobian-vector product needs to handle this special value. \n\nAs we have two arguments, we have 3 interesting options:\n\n1. Both `L_xt` and `bt` are `ad.Zero`: This means the function is a constant and\nthe derivative is zero. I am fairly certain that we do not need to manually handle\nthis case, but because I don't know and I do not like surprises, it's in there.\n\n2. `L_xt` is _not_ `ad.Zero`: This means that we need to differentiate\nwrt the matrix. In this case we need to compute $\\Delta c$ or $\\Delta^T c$, \ndepending on the `transpose` argument. In order to do this, I used the `jax.experimental.sparse.CSC` \nclass, which has some very limited sparse matrix support (basically matrix-vector products).\nThis is _extremely_ convenient because it means I don't need to write the matrix-vector product myself!\n\n3. `bt` is _not_ `ad.Zero`: This means that we need to differentiate wrt the rhs\nvector. This part of the formula is pretty straightforward: just an application of \nthe primal.\n\nIn the case that either `L_xt` or `bt` are `ad.Zero`, we simply set the corresponding\ncontribution to the jvp to zero.\n\nIt's worth saying that you can bypass all of this `ad.Zero` logic by writing\nseparate functions for the JVP contribution from each input and then chaining\nthem together using^[Some things: Firstly your function needs to have the correct signature for this to work. Secondly, you could also use `ad.defjvp()` if you didn't need to use the primal value to define the tangent (recall one of our tangents is $A^{-1}\\Delta c$, where $c = A^{-1}b$ is the primal value).] `ad.defjvp2()` to [chain them together](https://github.com/google/jax/blob/41417d70c03b6089c93a42325111a0d8348c2fa3/jax/_src/lax/linalg.py#L791). This is what the `lax.linalg.triangular_solve()` implementation does.\n\nSo why didn't I do this? I avoided this because in the other primitives I have to implement,\nthere are expensive computations (like Cholesky factorisations) that I want to share\nbetween the primal and the various tangent calculations. The `ad.defjvp` frameworks\ndon't allow for that. So I decided not to demonstrate/learn two separate patterns. \n\n### Transposition\n\nNow I've never actively wanted a Jacobian-vector product in my whole life. I'm \nsorry. I want a gradient. Gimme a gradient. I am the Veruca Salt of gradients.\n\nIn may autodiff systems, if you want^[This is because it is the efficient way of computing a gradient. Forward-mode autodiff chains together Jacobian-vector products in such a way that a single sweep of the entire function computes a single directional derivative. Reverse-mode autodiff chains together Jacobian-transpose-vector products (aka vector-Jacobian products) in such a way that a single sweep produces an entire gradient. (This happens at the cost of quite a bit of storage.) Depending on what you are trying to do, you usually want one or the other (or sometimes a clever combination of both).] a gradient, you need to implement vector-Jacobian products^[or gradients or some sort of thing.] explicitly.\n\nOne of the odder little innovations in JAX is that instead of forcing you to implement this\nas well^[to be honest, in Stan we sometimes just don't dick around with the forward-mode autodiff, because gradients are our bread and butter.], you only need to implement half of it.\n\n\nYou see, some clever analysis that, as far as I far as I can tell^[I mean, love you programming language people. But fuck me this paper could've been written in Babylonic cuneiform for all I understood it.],\nis detailed in [this paper](https://arxiv.org/abs/2204.10923) shows that you only \nneed to form explicit vector-Jacobian products for the structurally linear arguments of the function.\n\nIn JAX (and maybe elsewhere), this is known as a  _transposition rule_. The \ncombination of a transopition rule and a JAX-traceable Jacobian-vector product is enough for JAX to \ncompute all of the directional derivatives and gradients we could ever hope for.\n\nAs far as I understand, it is all about functions that are _structurally linear_ in \nsome arguments. For instance, if $A(x)$ is a matrix-valued function and $x$ and $y$ \nare vectors, then the function\n$$\nf(x, y) = A(x)y + g(x)\n$$\nis structurally linear in $y$ in the sense that for every fixed value of $x$, the \nfunction \n$$\nf_x(y) = A(x) y + g(x)\n$$\nis linear in $y$. The resulting transpositon rule is then \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef f_transpose(x, y):\n  Ax = A(x)\n  gx = g(x)\n  return (None, Ax.T @ y + gx)\n```\n:::\n\n\nThe first element of the return is `None` because $f(x,y)$ is not^[That is, if you fix a value of $y$, $f_y(x) = f(x, y)$ is not an affine function.] structurally \nlinear in $x$ so there is nothing to transpose. The second element simply\ntakes the matrix in the linear function and transposes it.\n\nIf you know anything about autodiff, you'll think \"this doesn't _feel_ like enough\"\nand it's not. JAX deals with the non-linear part of $f(x,y)$ by tracing the evaluation\ntree for its Jacobian-vector product and ... manipulating^[Details bore me.] it. \n\nWe already built the abstract evaluation function last time around, so the tracing\npart can be done. All we need is the transposition rule.\n\nThe linear solve $f(A, b) = A^{-1}b$ is non-linear in the first argument but linear\nin the second argument. So we only need to implement \n$$\nJ^T_b(A,b)w = A^{-T}w,\n$$ \nwhere the subscript $b$ indicates we're only computing the Jacobian wrt $b$.\n\nInitially, I struggled to work out what needed to be implemented here. The thing\nthat clarified the process for me was looking at JAX's [internal implementation]( https://github.com/google/jax/blob/41417d70c03b6089c93a42325111a0d8348c2fa3/jax/_src/lax/linalg.py#L747) of\nthe Jacobian-vector product for a dense matrix. From there, I understood what this\nhad to look like for a vector-valued function and this is the result.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndef sparse_triangular_solve_transpose_rule(cotangent, L_indices, L_indptr, L_x, b, *, transpose):\n  \"\"\"\n  Transposition rule for the triangular solve. \n  Translated from here https://github.com/google/jax/blob/41417d70c03b6089c93a42325111a0d8348c2fa3/jax/_src/lax/linalg.py#L747.\n  Inputs:\n    cotangent: Output cotangent (aka adjoint). (produced by JAX)\n    L_indices, L_indptr, L_x: Represenation of sparse matrix. L_x should be concrete\n    b: The right hand side. Must be an jax.interpreters.ad.UndefinedPrimal\n    transpose: (boolean) True: solve $L^Tx = b$. False: Solve $Lx = b$.\n  Output:\n    A 4-tuple with the adjoints (None, None, None, b_adjoint)\n  \"\"\"\n  assert not ad.is_undefined_primal(L_x) and ad.is_undefined_primal(b)\n  if type(cotangent) is ad_util.Zero:\n    cot_b = ad_util.Zero(b.aval)\n  else:\n    cot_b = sparse_triangular_solve(L_indices, L_indptr, L_x, cotangent, transpose = not transpose)\n  return None, None, None, cot_b\n\nad.primitive_transposes[sparse_triangular_solve_p] = sparse_triangular_solve_transpose_rule\n```\n:::\n\n\nIf this doesn't make a lot of sense to you, that's because it's confusing. \n\nOne way to think of it is in terms of the more ordinary notation. Mike Giles has\n[a classic paper](https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf) that \ncovers these results for basic linear algebra. The idea is to imagine that,\nas part of your larger program, you need to compute $c = A^{-1}b$.\n\nForward-mode autodiff computes the _sensitivity_ of $c$, usually denoted $\\dot c$ \nfrom the sensitivies $\\dot A$ and $\\dot b$. These have already been computed.\nThe formula in Giles is \n$$\n\\dot c = A^{-1}(\\dot b - \\dot A c).\n$$ \nThe canny reader will recognise this as exactly^[In general, there might need to be a little bit of reshaping, but it's equivalent.] the formula for the Jacobian-vector product.\n\nSo what does reverse-mode autodiff do? Well it moves through the program in the \nother direction. So instead of starting with the sensitivities $\\dot A$ and $\\dot b$\nalready computed, we instead start with the^[Have you noticed this is like the third name I've used for this equivalent concept. Or the fourth? The code calls it a cotangent because that's another damn synonym. I'm so very sorry.] _adjoint sensitivity_ $\\bar c$. Our\naim is to compute $\\bar A$ and $\\bar b$ from $\\bar c$.\n\nThe details of how to do this are^[not difficult, I'm just lazy and Mike does it better that I can. Read his paper.] _beyond the scope_, but without tooooooo much effort you can show that \n$$\n\\bar b = A^{-T} \\bar c,\n$$\nwhich you should recognise as the equation that was just implemented.\n\nThe thing that we _do not_ have to implement in JAX is the other adjoint that, for \ndense matrices^[For sparse matrices it's just the non-zero mask of that.], is\n$$\n\\bar{A} = -\\bar{b}c^T.\n$$\nThrough the healing power of ... something?---Truly I do not know.--- JAX can \nwork that bit out itself. woo.\n\n\n### Testing the numerical implementation of the Jacobian-vector product\n\nSo let's see if this works. I'm not going to lie, I'm flying by the seat of my\npants here. I'm not super familiar with the JAX internals, so I have written\na lot of test cases. You may wish to skip this part. But rest assured that almost\nevery single one of these cases was useful to me working out how this thing actually \nworked!\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndef make_matrix(n):\n    one_d = sparse.diags([[-1.]*(n-1), [2.]*n, [-1.]*(n-1)], [-1,0,1])\n    A = (sparse.kronsum(one_d, one_d) + sparse.eye(n*n)).tocsc()\n    A_lower = sparse.tril(A, format = \"csc\")\n    A_index = A_lower.indices\n    A_indptr = A_lower.indptr\n    A_x = A_lower.data\n    return (A_index, A_indptr, A_x, A)\n\nA_indices, A_indptr, A_x, A = make_matrix(10)\n```\n:::\n\n\nThis is the same test case as the last blog. We will just use the lower triangle of $A$ as the test matrix. \n\nFirst things first, let's check out the numerical implementation of the function.\nWe will do that by comparing the implemented Jacobian-vector product with \nthe _definition_ of the Jacobian-vector product (aka the forward^[Yes. I know. Central differences. I am what I am.] difference approximation).\n\nThere are lots of things that we could do here to turn these into _actual_ tests.\nFor instance, the test suite inside JAX has a lot of nice convenience functions\nfor checking implementations of derivatives. But I went with homespun because that\nwas how I was feeling.\n\nYou'll also notice that I'm using random numbers here, which is fine for a blog.\nNot so fine for a test that you don't want to be potentially^[Some of the stuff I've done like normalising all of the inputs would help make these tests more stable. You should also just pick up Nick Higham's backwards error analysis book to get some ideas of what your guarantees actually are in floating point, but I truly cannot be bothered. This is scratch code.] flaky. \n\nThe choice of `eps = 1e-4` is roughly^[It should be slightly bigger, it isn't.] \nbecause it's the square root of the single precision machine epsilon^[The largest number $\\epsilon$ such that `float(1.0) == float(1.0 + machine_eps)` in single precision floating point.].\nA very rough back of the envelope calculation for the forward difference approximation\nto the derivative shows that the square root of the machine epislon is about the \nsize you want your perturbation to be.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nb = np.random.standard_normal(100)\n\nbt = np.random.standard_normal(100)\nbt /= np.linalg.norm(bt)\n\nA_xt = np.random.standard_normal(len(A_x))\nA_xt /= np.linalg.norm(A_xt)\n\narg_values = (A_indices, A_indptr, A_x, b )\n\narg_tangent_A = (None, None, A_xt, ad.Zero(type(b)))\narg_tangent_b = (None, None, ad.Zero(type(A_xt)), bt)\narg_tangent_Ab = (None, None, A_xt, bt)\n\np, t_A = sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_A, transpose = False)\n_, t_b = sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_b, transpose = False)\n_, t_Ab = sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_Ab, transpose = False)\npT, t_AT = sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_A, transpose = True)\n_, t_bT = sparse_triangular_solve_value_and_jvp(arg_values, arg_tangent_b, transpose = True)\n\neps = 1e-4\ntt_A = (sparse_triangular_solve(A_indices, A_indptr, A_x + eps * A_xt, b) - p) /eps\ntt_b = (sparse_triangular_solve(A_indices, A_indptr, A_x, b + eps * bt) - p) / eps\ntt_Ab = (sparse_triangular_solve(A_indices, A_indptr, A_x + eps * A_xt, b + eps * bt) - p) / eps\ntt_AT = (sparse_triangular_solve(A_indices, A_indptr, A_x + eps * A_xt, b, transpose = True) - pT) / eps\ntt_bT = (sparse_triangular_solve(A_indices, A_indptr, A_x, b + eps * bt, transpose = True) - pT) / eps\n\nprint(f\"\"\"\nTranspose = False:\n  Error A varying: {np.linalg.norm(t_A - tt_A): .2e}\n  Error b varying: {np.linalg.norm(t_b - tt_b): .2e}\n  Error A and b varying: {np.linalg.norm(t_Ab - tt_Ab): .2e}\n\nTranspose = True:\n  Error A varying: {np.linalg.norm(t_AT - tt_AT): .2e}\n  Error b varying: {np.linalg.norm(t_bT - tt_bT): .2e}\n\"\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTranspose = False:\n  Error A varying:  1.08e-07\n  Error b varying:  0.00e+00\n  Error A and b varying:  4.19e-07\n\nTranspose = True:\n  Error A varying:  1.15e-07\n  Error b varying:  0.00e+00\n\n```\n:::\n:::\n\n\nBrilliant! Everythign correct withing single precision!\n\n### Checking on the plumbing\n\nMaking the numerical implementation work is only half the battle. We also have\nto make it work _in the context of JAX_. \n\nNow I would be lying if I pretended this process went smoothly. But the first time \nis for experience. It's mostly a matter of just reading the documentation carefully\nand going through similar examples that have already been implemented.\n\nAnd testing. I learnt how this was supposed to work by testing it.\n\n(For full disclosure, I also wrote a big block f-string in the `sparse_triangular_solve()`\nfunction at one point that told me the types, shapes, and what `transpose` was, which\nwas how I worked out that my code was breaking because I forgot the first to `None` outputs\nin the transposition rule. When it doubt, print shit.)\n\nAs you will see from my testing code, I was not going for elegance. I was running \nthe damn permutations. If you're looking for elegance, look elsewhere.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom jax import jvp, grad\nfrom jax import scipy as jsp\n\ndef f(theta):\n  Ax_theta = jnp.array(A_x)\n  Ax_theta = Ax_theta.at[A_indptr[20]].add(theta[0])\n  Ax_theta = Ax_theta.at[A_indptr[50]].add(theta[1])\n  b = jnp.ones(100)\n  return sparse_triangular_solve(A_indices, A_indptr, Ax_theta, b, transpose = True)\n\ndef f_jax(theta):\n  Ax_theta = jnp.array(sparse.tril(A).todense())\n  Ax_theta = Ax_theta.at[20,20].add(theta[0])\n  Ax_theta = Ax_theta.at[50,50].add(theta[1])\n  b = jnp.ones(100)\n  return jsp.linalg.solve_triangular(Ax_theta, b, lower = True, trans = \"T\")\n\ndef g(theta):\n  Ax_theta = jnp.array(A_x)\n  b = jnp.ones(100)\n  b = b.at[0].set(theta[0])\n  b = b.at[51].set(theta[1])\n  return sparse_triangular_solve(A_indices, A_indptr, Ax_theta, b, transpose = True)\n\ndef g_jax(theta):\n  Ax_theta = jnp.array(sparse.tril(A).todense())\n  b = jnp.ones(100)\n  b = b.at[0].set(theta[0])\n  b = b.at[51].set(theta[1])\n  return jsp.linalg.solve_triangular(Ax_theta, b, lower = True, trans = \"T\")\n\ndef h(theta):\n  Ax_theta = jnp.array(A_x)\n  Ax_theta = Ax_theta.at[A_indptr[20]].add(theta[0]) \n  b = jnp.ones(100)\n  b = b.at[51].set(theta[1])\n  return sparse_triangular_solve(A_indices, A_indptr, Ax_theta, b, transpose = False)\n\ndef h_jax(theta):\n  Ax_theta = jnp.array(sparse.tril(A).todense())\n  Ax_theta = Ax_theta.at[20,20].add(theta[0])\n  b = jnp.ones(100)\n  b = b.at[51].set(theta[1])\n  return jsp.linalg.solve_triangular(Ax_theta, b, lower = True, trans = \"N\")\n\ndef no_diff(theta):\n  return sparse_triangular_solve(A_indices, A_indptr, A_x, jnp.ones(100), transpose = False)\n\ndef no_diff_jax(theta):\n  return jsp.linalg.solve_triangular(jnp.array(sparse.tril(A).todense()), jnp.ones(100), lower = True, trans = \"N\")\n\nA_indices, A_indptr, A_x, A = make_matrix(10)\nprimal1, jvp1 = jvp(f, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\nprimal2, jvp2 = jvp(f_jax, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\ngrad1 = grad(lambda x: jnp.mean(f(x)))(jnp.array([-142., 342.]))\ngrad2 = grad(lambda x: jnp.mean(f_jax(x)))(jnp.array([-142., 342.]))\n\nprimal3, jvp3 = jvp(g, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\nprimal4, jvp4 = jvp(g_jax, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\ngrad3 = grad(lambda x: jnp.mean(g(x)))(jnp.array([-142., 342.]))\ngrad4 = grad(lambda x: jnp.mean(g_jax(x)))(jnp.array([-142., 342.]))  \n\nprimal5, jvp5 = jvp(h, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\nprimal6, jvp6 = jvp(h_jax, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\ngrad5 = grad(lambda x: jnp.mean(h(x)))(jnp.array([-142., 342.]))\ngrad6 = grad(lambda x: jnp.mean(h_jax(x)))(jnp.array([-142., 342.]))\n\nprimal7, jvp7 = jvp(no_diff, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\nprimal8, jvp8 = jvp(no_diff_jax, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\ngrad7 = grad(lambda x: jnp.mean(no_diff(x)))(jnp.array([-142., 342.]))\ngrad8 = grad(lambda x: jnp.mean(no_diff_jax(x)))(jnp.array([-142., 342.]))\n\nprint(f\"\"\"\nVariable L:\n  Primal difference: {np.linalg.norm(primal1 - primal2): .2e}\n  JVP difference: {np.linalg.norm(jvp1 - jvp2): .2e}\n  Gradient difference: {np.linalg.norm(grad1 - grad2): .2e}\n\nVariable b:\n  Primal difference: {np.linalg.norm(primal3 - primal4): .2e}\n  JVP difference: {np.linalg.norm(jvp3 - jvp4): .2e}\n  Gradient difference: {np.linalg.norm(grad3 - grad4): .2e} \n\nVariable L and b:\n  Primal difference: {np.linalg.norm(primal5 - primal6): .2e}\n  JVP difference: {np.linalg.norm(jvp5 - jvp6): .2e}\n  Gradient difference: {np.linalg.norm(grad5 - grad6): .2e}\n\nNo diff:\n  Primal difference: {np.linalg.norm(primal7 - primal8)}\n  JVP difference: {np.linalg.norm(jvp7 - jvp8)}\n  Gradient difference: {np.linalg.norm(grad7 - grad8)}\n\"\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nVariable L:\n  Primal difference:  1.98e-07\n  JVP difference:  2.58e-12\n  Gradient difference:  0.00e+00\n\nVariable b:\n  Primal difference:  7.94e-06\n  JVP difference:  1.83e-08\n  Gradient difference:  3.29e-10 \n\nVariable L and b:\n  Primal difference:  2.08e-06\n  JVP difference:  1.08e-08\n  Gradient difference:  2.33e-10\n\nNo diff:\n  Primal difference: 2.2101993124579167e-07\n  JVP difference: 0.0\n  Gradient difference: 0.0\n\n```\n:::\n:::\n\n\nStunning!\n\n## Primitive one: The general $A^{-1}b$\n\nOk. So this is a very similar problem to the one that we just solved. But, as\nfate would have it, the solution is going to look quite different. Why? Because \nwe need to compute a Cholesky factorisation.\n\nFirst things first, though, we are going to need a JAX-traceable way to compute\na Cholesky factor. This means that we need^[Fun fact: I implemented this and the error never spawned, so I guess JAX is keeping the index arrays concrete, which is very nice of it!] to tell our `sparse_solve` function\nthe how many non-zeros the sparse Cholesky will have. Why? Well. It has to do\nwith how the function is used. \n\nWhen `sparse_cholesky()` is called with concrete inputs^[actual damn numbers],\nthen it can quite happily work out the sparsity structure of $L$. But when JAX\nis preparing to transform the code, eg when it's building a gradient, it calls\n`sparse_cholesky()` using abstract arguments that only share the shape information\nfrom the inputs. This is _not_ enough to compute the sparsity structure. We _need_\nthe `indices` and `indptr` arrays.\n\nThis means that we need `sparse_cholesky()` to throw an error if `L_nse` isn't passed.\nThis wasn't implemented well last time, so here it is done properly.\n\n(If you're wondering about that `None` argument, it is the identity transform. So\nif `A_indices` is a concrete value, `ind = A_indices`. Otherwise an error is called.)\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nsparse_cholesky_p = core.Primitive(\"sparse_cholesky\")\n\ndef sparse_cholesky(A_indices, A_indptr, A_x, *, L_nse: int = None):\n  \"\"\"A JAX traceable sparse cholesky decomposition\"\"\"\n  if L_nse is None:\n    err_string = \"You need to pass a value to L_nse when doing fancy sparse_cholesky.\"\n    ind = core.concrete_or_error(None, A_indices, err_string)\n    ptr = core.concrete_or_error(None, A_indptr, err_string)\n    L_ind, _ = _symbolic_factor(ind, ptr)\n    L_nse = len(L_ind)\n  \n  return sparse_cholesky_p.bind(A_indices, A_indptr, A_x, L_nse = L_nse)\n```\n:::\n\n\n<details><summary>The rest of the Choleksy code</summary>\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n@sparse_cholesky_p.def_impl\ndef sparse_cholesky_impl(A_indices, A_indptr, A_x, *, L_nse):\n  \"\"\"The implementation of the sparse cholesky This is not JAX traceable.\"\"\"\n  \n  L_indices, L_indptr= _symbolic_factor(A_indices, A_indptr)\n  if L_nse is not None:\n    assert len(L_indices) == L_nse\n    \n  L_x = _structured_copy(A_indices, A_indptr, A_x, L_indices, L_indptr)\n  L_x = _sparse_cholesky_impl(L_indices, L_indptr, L_x)\n  return L_indices, L_indptr, L_x\n\ndef _symbolic_factor(A_indices, A_indptr):\n  # Assumes A_indices and A_indptr index the lower triangle of $A$ ONLY.\n  n = len(A_indptr) - 1\n  L_sym = [np.array([], dtype=int) for j in range(n)]\n  children = [np.array([], dtype=int) for j in range(n)]\n  \n  for j in range(n):\n    L_sym[j] = A_indices[A_indptr[j]:A_indptr[j + 1]]\n    for child in children[j]:\n      tmp = L_sym[child][L_sym[child] > j]\n      L_sym[j] = np.unique(np.append(L_sym[j], tmp))\n    if len(L_sym[j]) > 1:\n      p = L_sym[j][1]\n      children[p] = np.append(children[p], j)\n        \n  L_indptr = np.zeros(n+1, dtype=int)\n  L_indptr[1:] = np.cumsum([len(x) for x in L_sym])\n  L_indices = np.concatenate(L_sym)\n  \n  return L_indices, L_indptr\n\n\n\ndef _structured_copy(A_indices, A_indptr, A_x, L_indices, L_indptr):\n  n = len(A_indptr) - 1\n  L_x = np.zeros(len(L_indices))\n  \n  for j in range(0, n):\n    copy_idx = np.nonzero(np.in1d(L_indices[L_indptr[j]:L_indptr[j + 1]],\n                                  A_indices[A_indptr[j]:A_indptr[j+1]]))[0]\n    L_x[L_indptr[j] + copy_idx] = A_x[A_indptr[j]:A_indptr[j+1]]\n  return L_x\n\ndef _sparse_cholesky_impl(L_indices, L_indptr, L_x):\n  n = len(L_indptr) - 1\n  descendant = [[] for j in range(0, n)]\n  for j in range(0, n):\n    tmp = L_x[L_indptr[j]:L_indptr[j + 1]]\n    for bebe in descendant[j]:\n      k = bebe[0]\n      Ljk= L_x[bebe[1]]\n      pad = np.nonzero(                                                       \\\n          L_indices[L_indptr[k]:L_indptr[k+1]] == L_indices[L_indptr[j]])[0][0]\n      update_idx = np.nonzero(np.in1d(                                        \\\n                    L_indices[L_indptr[j]:L_indptr[j+1]],                     \\\n                    L_indices[(L_indptr[k] + pad):L_indptr[k+1]]))[0]\n      tmp[update_idx] = tmp[update_idx] -                                     \\\n                        Ljk * L_x[(L_indptr[k] + pad):L_indptr[k + 1]]\n            \n    diag = np.sqrt(tmp[0])\n    L_x[L_indptr[j]] = diag\n    L_x[(L_indptr[j] + 1):L_indptr[j + 1]] = tmp[1:] / diag\n    for idx in range(L_indptr[j] + 1, L_indptr[j + 1]):\n      descendant[L_indices[idx]].append((j, idx))\n  return L_x\n\n@sparse_cholesky_p.def_abstract_eval\ndef sparse_cholesky_abstract_eval(A_indices, A_indptr, A_x, *, L_nse):\n  return core.ShapedArray((L_nse,), A_indices.dtype),                   \\\n         core.ShapedArray(A_indptr.shape, A_indptr.dtype),             \\\n         core.ShapedArray((L_nse,), A_x.dtype)\n```\n:::\n\n\n</details>\n\n### Why do we need a new pattern for this very very similar problem?\n\nOk. So now on to the details. If we try to repeat our previous pattern it would\nlook like this.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ndef sparse_solve_value_and_jvp(arg_values, arg_tangents, *, L_nse):\n  \"\"\" \n  Jax-traceable jacobian-vector product implmentation for sparse_solve.\n  \"\"\"\n  \n  A_indices, A_indptr, A_x, b = arg_values\n  _, _, A_xt, bt = arg_tangents\n\n  # Needed for shared computation\n  L_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\n\n  # Make the primal\n  primal_out = sparse_triangular_solve(L_indices, L_indptr, L_x, b, transpose = False)\n  primal_out = sparse_triangular_solve(L_indices, L_indptr, L_x, primal_out, transpose = True)\n\n  if type(A_xt) is not ad.Zero:\n    Delta_lower = jsparse.CSC((A_xt, A_indices, A_indptr), shape = (b.shape[0], b.shape[0]))\n    # We need to do Delta @ primal_out, but we only have the lower triangle\n    rhs = Delta_lower @ primal_out + Delta_lower.transpose() @ primal_out - A_xt[A_indptr[:-1]] * primal_out\n    jvp_Ax = sparse_triangular_solve(L_indices, L_indptr, L_x, rhs)\n    jvp_Ax = sparse_triangular_solve(L_indices, L_indptr, L_x, jvp_Ax, transpose = True)\n  else:\n    jvp_Ax = lax.zeros_like_array(primal_out)\n\n  if type(bt) is not ad.Zero:\n    jvp_b = sparse_triangular_solve(L_indices, L_indptr, L_x, bt)\n    jvp_b = sparse_triangular_solve(L_indices, L_indptr, L_x, jvp_b, transpose = True)\n  else:\n    jvp_b = lax.zeros_like_array(primal_out)\n\n  return primal_out, jvp_b - jvp_Ax\n```\n:::\n\n\nThat's all well and good. Nothing weird there.\n\nThe problem comes when you need to implement the transposition rule. Remembering \nthat $\\bar b = A^{-T}\\bar c = A^{-1}\\bar c$, you might see the issue: we are going\nto need the Cholesky factorisation. _But we have no way to pass_ $L$ _to the transpose function_.\n\nThis means that we would need to compute _two_ Cholesky factorisations per gradient \ninstead of one. As the Cholesky factorisation is our slowest operation, we do not \nwant to do extra ones! We want to compute the Cholesky triangle once and pass\nit around like a party bottom^[We want that [auld triangle to go jingle bloody jangle](https://youtu.be/wrnUJoj14ag?t=288)]. We do not want each of our functions to have to \nmake a deep and meaningful connection with the damn matrix^[We definitely do not want someone to write an eight hour, two part play that really seems to have the point of view that our Cholesky triangle deserved his downfall. Espoused while periodically reading deadshit tumblr posts. I mean, it would win a Tony. But we still do not want that.]. \n\n### A different solution\n\nSo how do we pass around our Cholesky triangle? Well, I do love a good class so\nmy first thought was \"fuck it. I'll make a class and I'll pass it that way\". \nBut the developers of JAX had a _much_ better idea.\n\nTheir idea was to abstract the idea of a linear solve and its gradients. They \ndo this through `lax.custom_linear_solve`. This is a function that takes all\nof the bits that you would need to compute $A^{-1}b$ and all of its derivatives.\nIn particular it takes^[There are more arguments. Read the help. This is what we need]:\n\n- `matvec`: A function that `matvec(x)` that computes $Ax$. This might seem a bit\nweird, but it's the most common atrocity committed by mathematicians is abstracting^[What if I told you that this would work perfectly well if $A$ was a linear partial differential operator or an integral operator? Probably not much because why would you give a shit?]\na matrix to a linear mapping. So we might as well just suck it up.\n- `b`: The right hand side vector^[It can be more general, but it isn't]\n- `solve`: A function that takes takes the `matvec` and a vector so that^[I think there is a typo in the docs] `solve(matvec, matvec(x)) == x`\n- `symmetric`: A boolean indicating if $A$ is symmetric.\n\nThe idea (happily copped from the implementation of `jax.scipy.linalg.solve`)\nis to wrap our Cholesky decomposition in the solve function. Through the never\nending miracle of partial evaluation.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nfrom functools import partial\n\ndef sparse_solve(A_indices, A_indptr, A_x, b, *, L_nse = None):\n  \"\"\"\n  A JAX-traceable sparse solve. For this moment, only for vector b\n  \"\"\"\n  assert b.shape[0] == A_indptr.shape[0] - 1\n  assert b.ndim == 1\n  \n  L_indices, L_indptr, L_x = sparse_cholesky(\n    lax.stop_gradient(A_indices), \n    lax.stop_gradient(A_indptr), \n    lax.stop_gradient(A_x), L_nse = L_nse)\n  \n  def chol_solve(L_indices, L_indptr, L_x, b):\n    out = sparse_triangular_solve(L_indices, L_indptr, L_x, b, transpose = False)\n    return sparse_triangular_solve(L_indices, L_indptr, L_x, out, transpose = True)\n  \n  def matmult(A_indices, A_indptr, A_x, b):\n    A_lower = jsparse.CSC((A_x, A_indices, A_indptr), shape = (b.shape[0], b.shape[0]))\n    return A_lower @ b + A_lower.transpose() @ b - A_x[A_indptr[:-1]] * b\n\n  solver = partial(\n    lax.custom_linear_solve,\n    lambda x: matmult(A_indices, A_indptr, A_x, x),\n    solve = lambda _, x: chol_solve(L_indices, L_indptr, L_x, x),\n    symmetric = True)\n\n  return solver(b)\n```\n:::\n\n\nThere are three things of note in that implementation.\n\n1. The calls to `lax.stop_gradient()`: These tell JAX to not bother computing \nthe gradient of these terms. The relevant parts of the derivatives are computed\nexplicitly by `lax.custom_linear_solve` in terms of `matmult` and `solve`, neither\nof which need the explicit derivative of the cholesky factorisation.!\n\n2. That definition of `matmult()`^[Full disclosure: I screwed this up multiple times today and my tests caught it. What does that look like? The derivatives for $A$ being off, but everything else being good.]: Look. I don't know what to tell you. Neither\naddition nor indexing is implemented for `jsparse.CSC` objects. So we did it the\nsemi-manual way. (I am thankful that matrix-vector multiplication is available)\n\n3. The definition of `solver()`: Partial evaluation is a wonderful wonderful thing.\n`functools.partial()` transforms `lax.custom_linear_solve()` from a function that \ntakes 3 arguments (and some keywords), into a function \n`solver()` that takes one^[And some optional keyword arguments, but we don't need to worry about those] argument^[This is not quite the same but similar to something that functional programming people call _currying_, which was named after famous Australian Olympic swimmer Lisa Curry.] (`b`, the only positional argument of `lax.custom_linear_solve()` that isn't \nspecified).\n\n### Does it work?\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ndef f(theta):\n  Ax_theta = jnp.array(theta[0] * A_x)\n  Ax_theta = Ax_theta.at[A_indptr[:-1]].add(theta[1])\n  b = jnp.ones(100)\n  return sparse_solve(A_indices, A_indptr, Ax_theta, b)\n\ndef f_jax(theta):\n  Ax_theta = jnp.array(theta[0] * A.todense())\n  Ax_theta = Ax_theta.at[np.arange(100),np.arange(100)].add(theta[1])\n  b = jnp.ones(100)\n  return jsp.linalg.solve(Ax_theta, b)\n\ndef g(theta):\n  Ax_theta = jnp.array(A_x)\n  b = jnp.ones(100)\n  b = b.at[0].set(theta[0])\n  b = b.at[51].set(theta[1])\n  return sparse_solve(A_indices, A_indptr, Ax_theta, b)\n\ndef g_jax(theta):\n  Ax_theta = jnp.array(A.todense())\n  b = jnp.ones(100)\n  b = b.at[0].set(theta[0])\n  b = b.at[51].set(theta[1])\n  return jsp.linalg.solve(Ax_theta, b)\n\ndef h(theta):\n  Ax_theta = jnp.array(A_x)\n  Ax_theta = Ax_theta.at[A_indptr[:-1]].add(theta[0])\n  b = jnp.ones(100)\n  b = b.at[51].set(theta[1])\n  return sparse_solve(A_indices, A_indptr, Ax_theta, b)\n\ndef h_jax(theta):\n  Ax_theta = jnp.array(A.todense())\n  Ax_theta = Ax_theta.at[np.arange(100),np.arange(100)].add(theta[0])\n  b = jnp.ones(100)\n  b = b.at[51].set(theta[1])\n  return jsp.linalg.solve(Ax_theta, b)\n\nprimal1, jvp1 = jvp(f, (jnp.array([2., 3.]),), (jnp.array([1., 2.]),))\nprimal2, jvp2 = jvp(f_jax, (jnp.array([2., 3.]),), (jnp.array([1., 2.]),))\ngrad1 = grad(lambda x: jnp.mean(f(x)))(jnp.array([2., 3.]))\ngrad2 = grad(lambda x: jnp.mean(f_jax(x)))(jnp.array([2., 3.]))\n\n\nprimal3, jvp3 = jvp(g, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\nprimal4, jvp4 = jvp(g_jax, (jnp.array([-142., 342.]),), (jnp.array([1., 2.]),))\ngrad3 = grad(lambda x: jnp.mean(g(x)))(jnp.array([-142., 342.]))\ngrad4 = grad(lambda x: jnp.mean(g_jax(x)))(jnp.array([-142., 342.]))\n\nprimal5, jvp5 = jvp(h, (jnp.array([2., 342.]),), (jnp.array([1., 2.]),))\nprimal6, jvp6 = jvp(h_jax, (jnp.array([2., 342.]),), (jnp.array([1., 2.]),))\ngrad5 = grad(lambda x: jnp.mean(f(x)))(jnp.array([2., 342.]))\ngrad6 = grad(lambda x: jnp.mean(f_jax(x)))(jnp.array([2., 342.]))\n\nprint(f\"\"\"\nCheck the plumbing!\nVariable A:\n  Primal difference: {np.linalg.norm(primal1 - primal2): .2e}\n  JVP difference: {np.linalg.norm(jvp1 - jvp2): .2e}\n  Gradient difference: {np.linalg.norm(grad1 - grad2): .2e}\n  \nVariable b:\n  Primal difference: {np.linalg.norm(primal3 - primal4): .2e}\n  JVP difference: {np.linalg.norm(jvp3 - jvp4): .2e}\n  Gradient difference: {np.linalg.norm(grad3 - grad4): .2e} \n    \nVariable A and b:\n  Primal difference: {np.linalg.norm(primal5 - primal6): .2e}\n  JVP difference: {np.linalg.norm(jvp5 - jvp6): .2e}\n  Gradient difference: {np.linalg.norm(grad5 - grad6): .2e}\n  \"\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCheck the plumbing!\nVariable A:\n  Primal difference:  1.98e-07\n  JVP difference:  1.43e-07\n  Gradient difference:  0.00e+00\n  \nVariable b:\n  Primal difference:  4.56e-06\n  JVP difference:  6.52e-08\n  Gradient difference:  9.31e-10 \n    \nVariable A and b:\n  Primal difference:  8.10e-06\n  JVP difference:  1.83e-06\n  Gradient difference:  1.82e-12\n  \n```\n:::\n:::\n\n\nYes.\n\n### Why is this better than just differentiating through the Cholesky factorisation?\n\nThe other option for making this work would've been to implement the Cholesky \nfactorisation as a primitive (~which we are about to do!~ which we will do another day) and then write the \nsparse solver directly as a pure JAX function.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ndef sparse_solve_direct(A_indices, A_indptr, A_x, b, *, L_nse = None):\n  L_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\n  out = sparse_triangular_solve(L_indices, L_indptr, L_x, b)\n  return sparse_triangular_solve(L_indices, L_indptr, L_x, out, transpose = True)\n```\n:::\n\n\nThis function is JAX-traceable^[and a shitload simpler!] and, therefore, we could compute the gradient of \nit directly. It turns out that this is going to be a bad idea. \n\nWhy? Because the derivative of `sparse_cholesky`, which we would have to \nchain together with the derivatives from the solver, is pretty complicated.\nBasically, this means that we'd have to do a lot more work^[And we have to store a bunch more. This is less of a big deal when $L$ is sparse, but for an ordinary linear solve, we'd be hauling around an extra $\\mathcal{O}(n^2)$ floats containing tangents for no good reason.] than we do if we \njust implement the symbolic formula for the derivatives.\n\n\n\n## Primitive three: The dreaded log determinant\n\nOk, so now we get to the good one. The log-determinant of $A$. The first thing that we \nneed to do is wrench out a derivative. This is not as easy as it was for the linear solve.\nSo what follows is a modification for sparse matrices from Appendix A of \n[Boyd's convex optimisation book](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf).\n\nIt's pretty easy to convince yourself that \n\\begin{align*}\n\\log(|A + \\Delta|) &= \\log\\left( \\left|A^{1/2}(I + A^{-1/2}\\Delta A^{-1/2})A^{1/2}\\right|\\right) \\\\\n&= \\log(|A|) + \\log\\left( \\left|I + A^{-1/2}\\Delta A^{-1/2}\\right|\\right).\n\\end{align*}\n\nIt is harder to convince yourself how this could possibly be a useful fact.\n\nIf we write $\\lambda_i$, $i = 1, \\ldots, n$ as the eigenvalues of $A^{-1/2}\\Delta A^{-1/2}$, \nthen we have \n$$\n\\log(|A + \\Delta |) = \\log(|A|) + \\sum_{i=1}^n \\log( 1 + \\lambda_i).\n$$\nRemembering that $\\Delta$ is very small, it follows that $A^{-1/2}\\Delta A^{-1/2}$ will _also_ \nbe small. That translates to the eigenvalues of $A^{-1/2}\\Delta A^{-1/2}$ all being small.\nTherefore, we can use the approximation $\\log(1 + \\lambda_i)  = \\lambda_i  + \\mathcal{O}(\\lambda_i^2)$.\n\nThis means that^[If you are worrying about the suppressed constant, remember that $A$ (and therefore $n$ and $\\|A\\|$) is fixed.] \n\\begin{align*}\n\\log(|A + \\Delta |) &= \\log(|A|) + \\sum_{i=1}^n  \\lambda_i + \\mathcal{O}\\left(\\|\\Delta\\|^2\\right) \\\\\n&=\\log(|A|) + \\operatorname{tr}\\left(A^{-1/2} \\Delta A^{-1} \\right) + \\mathcal{O}\\left(\\|\\Delta\\|^2\\right) \\\\\n&= \\log(|A|) + \\operatorname{tr}\\left(A^{-1} \\Delta \\right) + \\mathcal{O}\\left(\\|\\Delta\\|^2\\right),\n\\end{align*}\nwhich follows from the cyclic property of the trace.\n\nIf we recall the formula from the last section defining the Jacobian-vector product, in our context $m = 1$, $x$ is the vector of non-zero entries of the lower triangle of $A$\nstacked by column, and $\\delta$ is the vector of non-zero entries of the lower triangle of $\\Delta$.\nThat means the Jacobian-vector product is \n$$\nJ(x)\\delta = \\operatorname{tr}\\left(A^{-1} \\Delta \\right) = \\sum_{i=1}^n\\sum_{j=1}^n[A^{-1}]_{ij} \\Delta_{ij}.\n$$\n\nRemembering that $\\Delta$ is sparse with the same sparsity pattern as $A$, we \nsee that the Jacobian-vector product requires us to know the values of $A^{-1}$ \nthat correspond to non-zero elements of $A$. That's good news because we will see that these entries are\nrelatively cheap and easy to compute. Whereas the full inverse is dense and very \nexpensive to compute.\n\nBut before we get to that, I need to point out a trap for young players^[I think I've made this mistake about four times already while writing this blog. So I am going to write it _out_.]. Lest your \nimplementations go down faster than me when someone asks politely. \n\nThe problem comes from how we store our matrix. A mathematician would suggest that it's our representation.\nA physicist^[Not to \"some of my best friends are physicists\", but I do love them. I just wished a man would talk about me the way they talk about being coordinate free. Rather than with the same ambivalence physicist use when speaking about a specific atlas. I've been listening to lesbian folk music all evening. I'm having feelings.] would shit on about being coordinate free with such passion that he^[pronoun on purpose]\nwill keep going even after you quietly leave the room. \n\nThe problem is that we only store the non-zero entries of the lower-triangular part of $A$.\nThis means that _we need to be careful_ that when we compute the Jacobian-vector product that we\nproperly compute the Matrix-vector product. \n\nLet `A_indices` and `A_indptr` define the sparsity structure of $A$ (and $\\Delta$).\nThen if $A_x$ is our input and $v$ is our vector, then we need to do the follow steps\nto compute the Jacobian-vector product:\n\n1. Compute `Ainv_x` (aka the non-zero elements of $A^{-1}$ that correspond to the sparsity pattern of $A$)\n2. Compute the matrix vector product as\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\njvp = 2 * sum(Ainv_x * v) - sum(Ainv_x[A_indptr[:-1]] * v[A_indptr[:-1]])\n```\n:::\n\n\nWhy does it look like that? Well we need to add the contribution from the upper triangle\nas well as the lower triangle. And one way to do that is to just double the sum\nand then subtract off the diagonal terms that we've counted twice.\n\n(I'm making a pretty big assumption here, which is fine in our context, that $A$ has a non-zero diagonal.\nIf that doesn't hold, it's just a change of the indexing in the second term to just\npull out the diagonal terms.)\n\nUsing similar reasoning, we can compute the Jacobian as \n$$\n[J_f(x)]_{i1} = \\begin{cases}\n\\operatorname{partial-inverse}(x)_i, \\qquad & x_i  \\text{ is a diagonal element of }A \\\\\n2\\operatorname{partial-inverse}(x)_i, \\qquad & \\text{otherwise},\n\\end{cases}\n$$\nwhere $\\operatorname{partial-inverse}(x)$ is the vector that stacks the columns\nof the elements of $A^{-1}$ that correspond to the non-zero elements of $A$. (Yikes!)\n\n### Computing the partial inverse \n\nSo now we need to actually work out how to compute this _partial inverse_ of a \nsymmetric positive definite matrix $A$. To do this, we are going to steal a technique\nthat goes back to Takahashi, Fagan, and Chen^[Takahashi, K., Fagan, J., Chen, M.S., 1973. Formation of a sparse bus impedance matrix and its application to short circuit study. In: Eighth\nPICA Conference Proceedings.IEEE Power Engineering Society, pp. 63â€“69 (Papers Presented at the 1973 Power Industry Computer Application\nConference in Minneapolis, MN).] in 1973. \n(For this presentation, I'm basically pillaging [HÃ¥vard Rue and Sara Martino's 2007 paper.](https://www.sciencedirect.com/science/article/pii/S0378375807000845))\n\nTheir idea was that if we write $A = VDV^T$, where $V$ is a lower-triangular matrix\nwith ones on the diagonal and $D$ is diagonal. This links up with our usual Cholesky\nfactorisation through the identity $L = VD^{1/2}$. It follows that if $S = A^{-1}$, \nthen $VDV^TS = I$.  Then, we make some magic manipulations^[Thanks to Jerzy Baranowski for finding a very very bad LaTeX error that made these questions quite wrong!].\n\\begin{align*}\nV^TS &= D^{-1}V^{-1} \\\\\nS + V^TS &= S + D^{-1}V^{-1} \\\\\nS &= D^{-1}V^{-1} + (I - V^T)S.\n\\end{align*}\n\nOnce again, this does not look super-useful. The trick is to notice 2 things. \n\n1. Because $V$ is lower triangular, $V^{-1}$ is also lower triangular and the elements\nof $V^{-1}$ are the inverse of the diagonal elements of $V$ (aka they are all 1). Therefore,\n$D^{-1}V^{-1}$ is a lower triangular matrix with a diagonal given by the diagonal of $D^{-1}$.\n\n2. $I - V^T$ is an upper triangular matrix and $[I - V^T]_{nn} = 0$.\n\nThese two things together lead to the somewhat unexpected situation where the upper \ntriangle of $S = D^{-1}V^{-1} + (I-  V^T)S$ defines a set of recursions\nfor the upper triangle of $S$. (And, therefore, all of $S$ because $S$ is symmetric!)\nThese are sometimes referred to as the Takahashi recursions. \n\nBut we don't want the whole upper triangle of $S$, we just want the ones that \ncorrespond to the non-zero elements of $A$. Unfortunately, the set of recursions are not,\nin general, solveable using only that subset of $S$. But we are in luck: they are solveable \nusing the elements of $S$ that correspond to the non-zeros of $L + L^T$, which, as\nwe know from a few posts ago, is a superset of the non-zero elements of $A$!\n\nFrom this, we get the recursions running from $i = n, \\ldots, 1$, $j = n, \\ldots, i$ \n(the order is important!) such that $L_{ji} \\neq 0$\n$$\nS_{ji} =   \\begin{cases}\n\\frac{1}{L_{ii}^2} - \\frac{1}{L_{ii}}\\sum_{k=i+1}^{n} L_{ki} S_{kj} \\qquad&  \\text{if } i=j, \\\\         \n- \\frac{1}{L_{ii}}\\sum_{k=i+1}^{n} L_{ki} S_{kj}  & \\text{otherwise}.\n\\end{cases}\n$$\n\nIf you recall our discussion way back when about the way the non-zero structure of the $j$\nthe column of $L$ relates to the non-zero structure of the $i$ th column for $j \\geq i$, \nit's clear that we have computed enough^[Indeed, in the notation of post two $\\mathcal{L}_i \\cap \\{i+1, \\dots, n\\} \\subseteq \\mathcal{L}_j$ for all $i \\leq j$, where $\\mathcal{L}_i$ is the set of non-zeros in the $i$th column of $L$.] of $S$ at every step to complete the recursions.\n\nNow we just need to Python it. (And thanks to Finn Lindgren who helped me understand how to implement this, which he may or may not remember because it happened about five years ago.)\n\nActually, we need this to be JAX-traceable, so we are going to implement a very \nbasic primitive. In particular, we don't need to implement a derivative or anything\nlike that, just an abstract evaluation and an implementation.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nsparse_partial_inverse_p = core.Primitive(\"sparse_partial_inverse\")\n\ndef sparse_partial_inverse(L_indices, L_indptr, L_x, out_indices, out_indptr):\n  \"\"\"\n  Computes the elements (out_indices, out_indptr) of the inverse of a sparse matrix (A_indices, A_indptr, A_x)\n   with Choleksy factor (L_indices, L_indptr, L_x). (out_indices, out_indptr) is assumed to be either\n   the sparsity pattern of A or a subset of it in lower triangular form. \n  \"\"\"\n  return sparse_partial_inverse_p.bind(L_indices, L_indptr, L_x, out_indices, out_indptr)\n\n@sparse_partial_inverse_p.def_abstract_eval\ndef sparse_partial_inverse_abstract_eval(L_indices, L_indptr, L_x, out_indices, out_indptr):\n  return abstract_arrays.ShapedArray(out_indices.shape, L_x.dtype)\n\n@sparse_partial_inverse_p.def_impl\ndef sparse_partial_inverse_impl(L_indices, L_indptr, L_x, out_indices, out_indptr):\n  n = len(L_indptr) - 1\n  Linv = sparse.dok_array((n,n), dtype = L_x.dtype)\n  counter = len(L_x) - 1\n  for col in range(n-1, -1, -1):\n    for row in L_indices[L_indptr[col]:L_indptr[col+1]][::-1]:\n      if row != col:\n        Linv[row, col] = Linv[col, row] = 0.0\n      else:\n        Linv[row, col] = 1 / L_x[L_indptr[col]]**2\n      L_col  = L_x[L_indptr[col]+1:L_indptr[col+1]] / L_x[L_indptr[col]]\n \n      for k, L_kcol in zip(L_indices[L_indptr[col]+1:L_indptr[col+1]], L_col):\n         Linv[col,row] = Linv[row,col] =  Linv[row, col] -  L_kcol * Linv[k, row]\n        \n  Linv_x = sparse.tril(Linv, format = \"csc\").data\n  if len(out_indices) == len(L_indices):\n    return Linv_x\n\n  out_x = np.zeros(len(out_indices))\n  for col in range(n):\n    ind = np.nonzero(np.in1d(L_indices[L_indptr[col]:L_indptr[col+1]],\n      out_indices[out_indptr[col]:out_indptr[col+1]]))[0]\n    out_x[out_indptr[col]:out_indptr[col+1]] = Linv_x[L_indptr[col] + ind]\n  return out_x\n```\n:::\n\n\nThe implementation makes use of the^[The sparse matrix is stored as a \ndictionary `{(i,j): value}`, which is a very natural way to build a sparse matrix,\neven if its quite inefficient to do anything with it in that form.] \n_dictionary of keys_ representation of\na sparse matrix from `scipy.sparse`. This is an efficient storage scheme\nwhen you need to modify the sparsity structure (as we are doing here) or\ndo a lot of indexing. It would definitely be possible to implement this directly\non the CSC data structure, but it gets a little bit tricky to access the elements\nof `L_inv` that are above the diagonal. The resulting code is honestly a mess \nand there's lots of non-local memory access anyway, so I implemented it this way.\n\nBut let's be honest: this thing is crying out for a proper symmetric matrix class\nwith sensible reverse iterators. But hey. Python.\n\nThe second chunk of the code is just the opposite of our `_structured_copy()` \nfunction. It takes a matrix with the sparsity pattern of $L$ and returns one\nwith the sparsity pattern of `out` (which is assumed to be a subset, and is\nusually the sparsity pattern of $A$ or a diagonal matrix).\n\nLet's check that it works.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nA_indices, A_indptr, A_x, A = make_matrix(15)\nn = len(A_indptr) - 1\n\n\nL_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\n\na_inv_L = sparse_partial_inverse(L_indices, L_indptr, L_x, L_indices, L_indptr)\n\ncol_counts_L = [L_indptr[i+1] - L_indptr[i] for i in range(n)]\ncols_L = np.repeat(range(n), col_counts_L)\n\ntrue_inv = np.linalg.inv(A.todense())\ntruth_L = true_inv[L_indices, cols_L]\n\na_inv_A = sparse_partial_inverse(L_indices, L_indptr, L_x, A_indices, A_indptr)\ncol_counts_A = [A_indptr[i+1] - A_indptr[i] for i in range(n)]\ncols_A = np.repeat(range(n), col_counts_A)\ntruth_A = true_inv[A_indices, cols_A]\n\nprint(f\"\"\"\nError in partial inverse (all of L): {np.linalg.norm(a_inv_L - truth_L): .2e}\nError in partial inverse (all of A): {np.linalg.norm(a_inv_A - truth_A): .2e}\n\"\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nError in partial inverse (all of L):  1.57e-15\nError in partial inverse (all of A):  1.53e-15\n\n```\n:::\n:::\n\n\n### Putting the log-determinant together\n\nAll of our bits are in place, so now all we need is to implement the primitive for\nthe log-determinant. One nice thing here is that we don't need to implement \na transposition rule as the function is not structurally linear in any of its\narguments. At this point we take our small wins where we can get them.\n\nThere isn't anything particularly interesting in the implementation. But do note\nthat the trace has been implemented in a way that's aware that we're only storing\nthe bottom triangle of $A$.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nsparse_log_det_p = core.Primitive(\"sparse_log_det\")\n\ndef sparse_log_det(A_indices, A_indptr, A_x):\n  return sparse_log_det_p.bind(A_indices, A_indptr, A_x)\n\n@sparse_log_det_p.def_impl\ndef sparse_log_det_impl(A_indices, A_indptr, A_x):\n  L_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\n  return 2.0 * jnp.sum(jnp.log(L_x[L_indptr[:-1]]))\n\n@sparse_log_det_p.def_abstract_eval\ndef sparse_log_det_abstract_eval(A_indices, A_indptr, A_x):\n  return abstract_arrays.ShapedArray((1,), A_x.dtype)\n\ndef sparse_log_det_value_and_jvp(arg_values, arg_tangent):\n  A_indices, A_indptr, A_x = arg_values\n  _, _, A_xt = arg_tangent\n  L_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\n  value = 2.0 * jnp.sum(jnp.log(L_x[L_indptr[:-1]]))\n  Ainv_x = sparse_partial_inverse(L_indices, L_indptr, L_x, A_indices, A_indptr)\n  jvp = 2.0 * sum(Ainv_x * A_xt) - sum(Ainv_x[A_indptr[:-1]] * A_xt[A_indptr[:-1]])\n  return value, jvp\n\nad.primitive_jvps[sparse_log_det_p] = sparse_log_det_value_and_jvp\n```\n:::\n\n\nFinally, we can test it out.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nld_true = np.log(np.linalg.det(A.todense())) #np.sum(np.log(lu.U.diagonal()))\nprint(f\"Error in log-determinant = {ld_true - sparse_log_det(A_indices, A_indptr, A_x): .2e}\")\n\ndef f(theta):\n  Ax_theta = jnp.array(theta[0] * A_x) / n\n  Ax_theta = Ax_theta.at[A_indptr[:-1]].add(theta[1])\n  return sparse_log_det(A_indices, A_indptr, Ax_theta)\n\ndef f_jax(theta):\n  Ax_theta = jnp.array(theta[0] * A.todense()) / n \n  Ax_theta = Ax_theta.at[np.arange(n),np.arange(n)].add(theta[1])\n  L = jnp.linalg.cholesky(Ax_theta)\n  return 2.0*jnp.sum(jnp.log(jnp.diag(L)))\n\nprimal1, jvp1 = jvp(f, (jnp.array([2., 3.]),), (jnp.array([1., 2.]),))\nprimal2, jvp2 = jvp(f_jax, (jnp.array([2., 3.]),), (jnp.array([1., 2.]),))\n\neps = 1e-4\njvp_fd = (f(jnp.array([2.,3.]) + eps * jnp.array([1., 2.]) ) - f(jnp.array([2.,3.]))) / eps\n\ngrad1 = grad(f)(jnp.array([2., 3.]))\ngrad2 = grad(f_jax)(jnp.array([2., 3.]))\n\nprint(f\"\"\"\nCheck the Derivatives!\nVariable A:\n  Primal difference: {np.linalg.norm(primal1 - primal2)}\n  JVP difference: {np.linalg.norm(jvp1 - jvp2)}\n  JVP difference (FD): {np.linalg.norm(jvp1 - jvp_fd)}\n  Gradient difference: {np.linalg.norm(grad1 - grad2)}\n\"\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in log-determinant =  0.00e+00\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCheck the Derivatives!\nVariable A:\n  Primal difference: 0.0\n  JVP difference: 0.000885009765625\n  JVP difference (FD): 0.221893310546875\n  Gradient difference: 1.526623782410752e-05\n\n```\n:::\n:::\n\n\nI'm not going to lie, I am _not happy_ with that JVP difference. I was \nsomewhat concerned that there was a bug somewhere in my code. I did a little\nbit of exploring and the error got larger as the problem got larger. It also \ndepended a little bit more than I was comfortable on how I had implemented^[You can't just use `jnp.linalg.det()` because there's a tendency towards `nan`s. (The true value is something like `r exp(250.49306761204593)`!)] the \nbaseline dense version.\n\nThat second fact suggested to me that it might be a floating point problem. \nBy default, JAX uses single precision (32-bit) floating point. Most modern systems\nthat don't try and run on GPUs use double precision (64-bit) floating point.\nSo I tried it with double precision and lo and behold, the problem disappears.\n\nMatrix factorisations are bloody hard in single precision.\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\n\nld_true = np.log(np.linalg.det(A.todense())) #np.sum(np.log(lu.U.diagonal()))\nprint(f\"Error in log-determinant = {ld_true - sparse_log_det(A_indices, A_indptr, A_x): .2e}\")\n\ndef f(theta):\n  Ax_theta = jnp.array(theta[0] * A_x, dtype = jnp.float64) / n\n  Ax_theta = Ax_theta.at[A_indptr[:-1]].add(theta[1])\n  return sparse_log_det(A_indices, A_indptr, Ax_theta)\n\ndef f_jax(theta):\n  Ax_theta = jnp.array(theta[0] * A.todense(), dtype = jnp.float64) / n \n  Ax_theta = Ax_theta.at[np.arange(n),np.arange(n)].add(theta[1])\n  L = jnp.linalg.cholesky(Ax_theta)\n  return 2.0*jnp.sum(jnp.log(jnp.diag(L)))\n\nprimal1, jvp1 = jvp(f, (jnp.array([2., 3.], dtype = jnp.float64),), (jnp.array([1., 2.], dtype = jnp.float64),))\nprimal2, jvp2 = jvp(f_jax, (jnp.array([2., 3.], dtype = jnp.float64),), (jnp.array([1., 2.], dtype = jnp.float64),))\n\neps = 1e-7\njvp_fd = (f(jnp.array([2.,3.], dtype = jnp.float64) + eps * jnp.array([1., 2.], dtype = jnp.float64) ) - f(jnp.array([2.,3.], dtype = jnp.float64))) / eps\n\ngrad1 = grad(f)(jnp.array([2., 3.], dtype = jnp.float64))\ngrad2 = grad(f_jax)(jnp.array([2., 3.], dtype = jnp.float64))\n\nprint(f\"\"\"\nCheck the Derivatives!\nVariable A:\n  Primal difference: {np.linalg.norm(primal1 - primal2)}\n  JVP difference: {np.linalg.norm(jvp1 - jvp2)}\n  JVP difference (FD): {np.linalg.norm(jvp1 - jvp_fd)}\n  Gradient difference: {np.linalg.norm(grad1 - grad2)}\n\"\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in log-determinant =  0.00e+00\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCheck the Derivatives!\nVariable A:\n  Primal difference: 0.0\n  JVP difference: 8.526512829121202e-13\n  JVP difference (FD): 4.171707900013644e-06\n  Gradient difference: 8.881784197001252e-16\n\n```\n:::\n:::\n\n\nMuch better!\n\n## Wrapping up\n\nAnd that is where we will leave it for today. Next up, I'm probably going to \nneed to do the autodiff for the Cholesky factorisation. It's not _hard_, but it\nis tedious^[Would it be less tedious if my implementation of the Cholesky was less shit? Yes. But hey. It was the first non-trivial piece of python code I'd written in more than a decade (or maybe ever?) so it is what it is. Anyway. I'm gonna run into the same problem I had in [Part 3](https://dansblog.netlify.app/posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/)] and this post is already very long.\n\nAfter that we need a few more things:\n\n1. Compilation rules for all of these things. For the most part, we can just wrap\nthe relevant parts of [Eigen](https://github.com/libigl/eigen). The only non-trivial\ncode would be the partial inverse. That will allow us to JIT shit.\n\n2. We need to beef up the sparse matrix class a little. In particular, we are going\nto need addition and scalar multiplication at the very minimum to make this useful.\n\n3. Work out how [Aesara](https://aesara.readthedocs.io/en/latest/) works so we \ncan try to prototype a PyMC model.\n\nThat will be _a lot_ more blog posts. But I'm having fun. So why the hell not.\n\n",
    "supporting": [
      "to-catch-a-derivative-first-youve-got-to-think-like-a-derivative_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}