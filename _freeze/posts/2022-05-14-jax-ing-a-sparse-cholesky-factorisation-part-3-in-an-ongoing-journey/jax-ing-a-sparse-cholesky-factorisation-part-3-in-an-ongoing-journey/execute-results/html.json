{
  "hash": "df4f46cc15b9bf2ff847a6c87a2dfed3",
  "result": {
    "markdown": "---\ntitle: \"Sparse Matrices 3: Failing at JAX\"\ndescription: |\n  _Takes a long drag on cigarette._ JAX? Where was he when I had my cancer?\ndate: 2022-05-14\nimage: alien.JPG\nrepository_url: https://github.com/dpsimpson/blog/tree/master/_posts/\ncategories: [Sparse matrices, Sparse Cholesky factorisation, Python, JAX]\ntwitter-card:\n  title: \"Sparse Matrices 3: Failing at JAX\"\n  creator: \"@dan_p_simpson\"\ncitation: \n  url: https://dansblog.netlify.app/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey\n\n---\n\nThis is part three of an ongoing exercise in hubris. [Part one is here.](https://dansblog.netlify.app/posts/2022-03-22-a-linear-mixed-effects-model/) \n[Part two is here.](https://dansblog.netlify.app/posts/2022-03-23-getting-jax-to-love-sparse-matrices/)\nThe overall aim of this series of posts is to look at how sparse Cholesky factorisations \nwork, how JAX works, and how to marry the two with the ultimate aim of putting a \nbit of sparse matrix support into PyMC, which should allow for faster inference \nin linear mixed models, Gaussian spatial models. And hopefully, if anyone ever \ngets around to putting the Laplace approximation in, all sorts of GLMMs and non-Gaussian\nmodels with splines and spatial effects.\n\nIt's been a couple of weeks since the last blog, but I'm going to just assume that you are \nfully on top of all of those details. To that end, let's jump in.\n\n## What is JAX?\n\n[JAX](https://jax.readthedocs.io/en/latest/index.html) is a minor miracle. It will take python+numpy code and make it cool. It will let\nyou JIT^[If you've never come across this term before, you can Google it for actual details, but the squishy version is that it will _compile_ your code so it runs fast (like C code) instead of slow (like python code). JIT stands for _just in time_, which means that the code is compiled when it's needed rather than before everything else is run. It's a good thing. It makes the machine go _bing_ faster.] compile it! It will let you differentiate it! It will let you batch^[I give less of a shit about the third transformation in this context. I'm not completely sure what you would batch when you're dealing with a linear mixed-ish model. But hey. Why not.].  JAX refers to these three operations as _transformations_.\n\nBut, as The Mountain Goats tell us [_God is present in the sweeping gesture, but the devil is in the details_](https://www.youtube.com/watch?v=-E4XeV33TvE). And oh boy are those details\ngoing to be really fucking important to us.\n\nThere are going to be two key things that will make our lives more difficult:\n\n1. Not every operation can be transformed by every operation. For example, you \ncan't always JIT or take gradients of a `for` loop. This means that some things \nhave to be re-written carefully to make sure it's possible to get the advantages we need.\n\n2. JAX arrays are _immutable_. That means that once a variable is defined it _cannot be changed_. This means that things like `a = a + 1` is not allowed! If you've come from an R/Python/C/Fortran world, this is the weirdest thing to deal with.\n\nThere are really excellent reasons for both of these restrictions. And looking into the \nreasons is fascinating. But not a topic for this blog^[If you've ever spoken to a Scala advocate (or any other pure functional language), you can probably see the edges of why the arrays need to be immutable.$$\n\\phantom{a}\n$$ Restrictions to JIT-able control flow has to do with how it's translated onto the XLA compiler, which involves _tracing_ through the code with an abstract data type with the same shape as the one that it's being called with. Because this abstract data type does not have any values, structural parts of the code that _require_ knowledge of specific values of the arguments will be lost. You can get around this partially by declaring those important values to be _static_, which would make the JIT compiler re-compile the function each time that value changes. We are not going to do that. $$\n\\phantom{a}\n$$ Restrictions to gradients have to do (I assume) with reverse-mode autodiff needing to construct the autodiff tree at compile time, which means you need to be able to compute the number of operations from the types and shapes of the input variables and not from their values.]\n\nJAX has some pretty decent^[Coverage is pretty good on the _using_ bit, but, as is usual, the bits on extending the system are occasionally a bit ... sparse. (What in the hairy Christ is a [transposition](https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html#transposition) rule actually supposed to do????)] documentation, a core piece of which outlines some of the [sharp edges](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) you will run into. \nAs you read through the documentation, the design choices become clearer.\n\nSo let's go and find some sharp edges together!\n\n## To JAX or not to JAX\n\nBut first, we need to ask ourselves _which functions do we need to JAX_?\n\nIn the context of our problem we, so far, have three functions:\n\n1. `_symbolic_factor_csc(A_indices, A_indptr)`, which finds the non-zero indices of the sparse Cholesky factor and return them in CSC format,\n2. `_deep_copy_csc(A_indices, A_indptr, A_x, L_indices, L_indptr)`, which takes the _entries_ of the matrix $A$ and re-creates them so they can be indexed within the larger pattern of non-zero elements of $L$,\n3. `_sparse_cholesky_csc_impl(L_indices, L_indptr, L_x)`, which actually does the sparse Cholesky factorisation.\n\nLet's take them piece by piece, which is also a good opportunity to remind everyone what the code looked like.\n\n## Symbolic factorisation\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\ndef _symbolic_factor_csc(A_indices, A_indptr):\n  # Assumes A_indices and A_indptr index the lower triangle of $A$ ONLY.\n  n = len(A_indptr) - 1\n  L_sym = [np.array([], dtype=int) for j in range(n)]\n  children = [np.array([], dtype=int) for j in range(n)]\n  \n  for j in range(n):\n    L_sym[j] = A_indices[A_indptr[j]:A_indptr[j + 1]]\n    for child in children[j]:\n      tmp = L_sym[child][L_sym[child] > j]\n      L_sym[j] = np.unique(np.append(L_sym[j], tmp))\n    if len(L_sym[j]) > 1:\n      p = L_sym[j][1]\n      children[p] = np.append(children[p], j)\n        \n  L_indptr = np.zeros(n+1, dtype=int)\n  L_indptr[1:] = np.cumsum([len(x) for x in L_sym])\n  L_indices = np.concatenate(L_sym)\n  \n  return L_indices, L_indptr\n```\n:::\n\n\nThis function only needs to be computed once per non-zero pattern. In the applications \nI outlined in the first post, this non-zero pattern is _fixed_. This means that you \nonly need to run this function _once_ per analysis (unlike the others, that you will have to run\nonce per iteration!). \n\nAs a general rule, if you only do something once, it isn't all that necessary to \ndevote _too much_ time into optimising it.  There are, however, some obvious things \nwe could do.\n\nIt is, for instance, pretty easy to see how you would implement this with an explicit\ntree^[Forest] structure instead of constantly `np.append`ing the `children` array.\nThis is _far_ better from a memory standpoint.\n\nIt's also easy to imagine this as a two-pass algorithm, where you build the tree\nand count the number of non-zero elements in the first pass and then build and \npopulate `L_indices` in the second pass.\n\nThe thing is, neither of these things fixes the core problem for using JAX to JIT this:\nthe dimensions of the internal arrays depend on the _values_ of the inputs. This \nis not possible.\n\nIt seems like this would be a huge limitation, but in reality it isn't. Most \nfunctions aren't like this one! And, if we remember that JAX is a domain language\nfocussing mainly on ML applications, this is _very rarely_ the case. It is always good to remember context!\n\nSo what are our options? We have two.\n\n1. Leave it in Python and just eat the speed.\n2. Build a [new JAX primitive](https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html) and write the XLA compilation rule^[aka implement the damn thing in C++ and then do some proper work on it.].\n\nToday are opting for the first option! \n\n## The structure-changing copy\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndef _deep_copy_csc(A_indices, A_indptr, A_x, L_indices, L_indptr):\n  n = len(A_indptr) - 1\n  L_x = np.zeros(len(L_indices))\n  \n  for j in range(0, n):\n    copy_idx = np.nonzero(np.in1d(L_indices[L_indptr[j]:L_indptr[j + 1]],\n                                  A_indices[A_indptr[j]:A_indptr[j+1]]))[0]\n    L_x[L_indptr[j] + copy_idx] = A_x[A_indptr[j]:A_indptr[j+1]]\n  return L_x\n```\n:::\n\n\nThis is, fundamentally, a piece of bookkeeping. An annoyance of sparse matrices.\nOr, if you will, explicit _cast_ between different sparse matrix types^[It is useful to think of a sparse matrix type as the triple `(value_type, indices, indptr)`. This means that if we are going to do something like add sparse matrices, we need to first cast them both to have the same type. After the cast, addition of two different sparse matrices becomes the addition of their `x` attributes. The same holds for scalar multiplication. Sparse matrix-matrix multiplication is a bit different because you once again need to symbolically work out the sparsity structure (aka the type) of the product. ]. This is a thing that we do actually need to be able to differentiate, so it needs to live in JAX.\n\nSo where are the potential problems? Let's go line by line.\n\n1. `n = len(A_indptr) - 1`: This is lovely. `n` is used in a for loop later, but\nbecause it is a function of the _shape_ of `A_indptr`, it is considered static and \nwe will be able to JIT over it!\n\n2. `L_x = np.zeros(len(L_indices))`: Again, this is fine. Sizes are derived from shapes,\nlife is peachy.\n\n3. `for j in range(0, n):`: This could be a problem if `n` was an argument or \nderived from _values_ of the arguments, but it's derived from a shape so it is static. \nPraise be! Well, actually it's a bit more involved than that. \n\nThe problem with the `for` loop is what will happen when it is JIT'd. Essentially,\nthe loop will be statically unrolled^[I think. That's certainly what's implied [by the docs](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#python-control-flow-jit), but I don't want to give the impression that I'm sure. Because this is [complicated.](https://www.youtube.com/watch?v=5NPBIwQyPWE)]. That is fine for small loops, but it's a bit of a pain in the arse when `n` is large.\n\nIn this case, we might want to use the structured control flow in \n`jax.lax`^[What is `jax.lax`? Oh honey you don't want to know.]\nIn this case we would need `jax.lax.fori_loop(start, end, body_fun, init_value)`.\nThis makes the code look less _pythonic_, but probably should make it faster. \nIt is also, and I cannot stress this enough, an absolute dick to use.\n\n(In actuality, we will see that we do not need this particular corner of the language here!)\n\n4. `copy_idx = np.nonzero(...)`: This looks like it's going to be complicated, \nbut actually it is a perfectly reasonable composition of `numpy` functions. Hence,\nwe can use the same `jax.numpy` functions with minimal changes. The one change that \nwe are going to need to make in order to end up with a JIT-able and differentiable \nfunction is that we need to tell JAX how many non-zero elements there are. Thankfully, \nwe know this! Because the non-zero pattern of $A$ is a subset of the non-zero pattern of $L$,\nwe know that \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nnp.in1d(L_indices[L_indptr[j]:L_indptr[j + 1]], A_indices[A_indptr[j]:A_indptr[j+1]])\n```\n:::\n\n\nwill have exactly `len(A_indices[A_indptr[j]:A_indptr[j+1]])` `True` values, and so \n`np.nonzero(...)` will have that many. We can pass this information to `jnp.nonzero()` using\nthe optional `size` argument.\n\n**Oh no! We have a problem!** This return size is _a function of the values_ of `A_indptr` \nrather than a function of the shape. This means we're a bit fucked.\n\nThere are two routes out:\n\n1. Declare `A_indptr` to be a static parameter, or\n2. Change the representation from CSC to something more convenient.\n\nIn this case we could do either of these things, but I'm going to opt for the \nsecond option, as it's going to be more useful going forward.\n\nBut before we do that, let's look at the final line in the code.\n\n5. `L_x[L_indptr[j] + copy_idx] = A_x[A_indptr[j]:A_indptr[j+1]]`: \nThe final non-trivial line of the code is also a problem. The issue is that these arrays are _immutable_ and we are asking to change the values!\nThat is not allowed!\n\nThe solution here is to use a clunkier syntax. In JAX, we need to replace \n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nx[ind] = a\n```\n:::\n\n\nwith  the less pleasant \n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nx = x.at[ind].set(a)\n```\n:::\n\n\nWhat is going on under the hood to make the second option ok while the first is an error \nis well beyond the scope of this little post. But the important thing is that they _compile down_ \nto an in-place^[aka there's no weird copying] update, which is all we really care about.\n\n\n\n## Re-doing the data structure.\n\nOk. So we need a new data structure. That's annoying. The rule, I guess, is always\nthat if you need to innovate, you should innovate very little if you can get away\nwith it, or a lot if you have to.\n\nWe are going to innovate only the tiniest of bits.\n\nThe idea is to keep the core structure of the CSC data structure, but to replace \nthe `indptr` array with explicitly storing the row indices and row values as a _list_\nof `np.arrays`.  So `A_index` will now be a _list_ of `n` arrays that contain the row indices\nof the non-zero elements of $A$, while `A_x`will now be a _list_ of `n` arrays that \ncontain the values of the non-zero elements of $A$.\n\nThis means that the matrix \n$$\nB = \\begin{pmatrix}\n1 &&5 \\\\\n2&3& \\\\\n&4&6\n\\end{pmatrix}\n$$ \nwould be stored as \n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nB_index = [np.array([0,1]), np.array([1,2]), np.array([0,2])]\nB_x = [np.array([1,2]), np.array([3,4]), np.array([5,6])]\n```\n:::\n\n\nThis is a considerably more _pythonic_^[[Whatever that means anyway](https://www.youtube.com/watch?v=1hRvQqyeI2g)] version of CSC. So I guess that's an advantage.\n\nWe can easily go from CSC storage to this modified storage.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndef to_pythonic_csc(indices, indptr, x):\n  index = np.split(indices, indptr[1:-1])\n  x = np.split(x, indptr[1,-1])\n  return index, x\n```\n:::\n\n\n## A JAX-tracable structure-changing copy\n\nSo now it's time to come back to that damn `for` loop. As flagged earlier, `for`\nloops can be a bit picky in JAX. If we use them _as is_, then the code that is \ngenerated and then compiled is _unrolled_. You can think of this as if the JIT compiler\nautomatically writes a C++ program and then compiles it. If you were to examine that \ncode, the for loop would be replaced by `n` almost identical blocks of code with only\nthe index `j` changing between them. This leads to a potentially very large program\nto compile^[slowwwwww to compile] and it limits the compiler's ability to do clever\nthings to make the compiled code run faster^[The XLA compiler does very clever things. Incidentally, loop unrolling is actually one of the optimisations that compilers have in their pocket. Just not one that's usually used for loops as large as this.].\n\nThe `lax.fori_loop()` function, on the other hand, compiles down to the equivalent of a single operation^[Read about XLA High Level Operations (HLOs) [here](https://www.tensorflow.org/xla/architecture). The XLA documentation is not extensive, but there's still a lot to read.]. This lets the compiler be super clever.\n\nBut we don't actually need this here. Because if you take a look at the original \nfor loop we are just applying the same two lines of code to each triple of lists \nin `A_index`, `A_x`, and `L_index` (in our new^[This is why we have a new data structure.] \ndata structure).\n\nThis just _screams_ out for a map applying a single function independently to each column.\n\nThe challenge is to find the right map function. An obvious hope would be `jax.vmap`. \nSadly, `jax.vmap` does not do that. (At least not without more padding^[My kingdom for a ragged array.] \nthan a drag queen.) The problem here is a misunderstanding of what different parts of JAX\nare for. Functions like `jax.vmap` are made for applying the same function to arrays _of the same size_.\nThis makes sense in their context. (JAX is, after all, made for machine learning and \nthese shape assumptions fit really well in that paradigm. They just don't fit here.)\n\nAnd I won't lie. After this point I went _wild_. `lax.map` did not help. And I honest to \ngod tried `lax.scan`, which is will solve the problem but [at what cost?](https://www.youtube.com/watch?v=AOGzY9xShEI).\n\nBut at some point, you read enough of the docs to find the answer.\n\nThe correct answer here is to use the JAX concept of a `pytree`. \nPytrees are essentially^[Yes. They are more complicated than this. But for our purposes they are lists of arrays.]\nlists of arrays. They're very flexible and they have a `jax.tree_map` function that lets\nyou map over them! We are saved!\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np\nfrom jax import numpy as jnp\nfrom jax import tree_map\n\ndef _structured_copy_csc(A_index, A_x, L_index):\n    def body_fun(A_rows, A_vals, L_rows):\n      out = jnp.zeros(len(L_rows))\n      copy_idx =  jnp.nonzero(jnp.in1d(L_rows, A_rows), size = len(A_rows))[0] \n      out = out.at[copy_idx].set(A_vals)\n      return out\n    L_x = tree_map(body_fun, A_index, A_x, L_index)\n    return L_x\n```\n:::\n\n\n### Testing it out\n\nOk so now lets see if it works. To do that I'm going to define a very simple\nfunction \n$$\nf(A, \\alpha, \\beta) = \\|\\alpha I + \\beta \\operatorname{tril}(A)\\|_F^2,\n$$\nthat is the sum of the squares\nof all of the elements of $\\alpha I + \\beta \\operatorname{tril}(A)$. There's obviously an easy\nway to do this, but I'm going to do it in a\nway that uses the function we just built.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndef test_func(A_index, A_x, params):\n  I_index = [jnp.array([j]) for j in range(len(A_index))]\n  I_x = [jnp.array([params[0]]) for j in range(len(A_index))]\n  I_x2 = _structured_copy_csc(I_index, I_x, A_index)\n  return jnp.sum((jnp.concatenate(I_x2) + params[1] * jnp.concatenate(A_x))**2)\n```\n:::\n\n\nNext, we need a test case. Once again, we will use the 2D Laplacian on a regular\n$n \\times n$ grid (up to a scaling). This is a nice little function because it's \neasy to make test problems of different sizes.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfrom scipy import sparse\n\ndef make_matrix(n):\n    one_d = sparse.diags([[-1.]*(n-1), [2.]*n, [-1.]*(n-1)], [-1,0,1])\n    A_lower = sparse.tril(sparse.kronsum(one_d, one_d) + sparse.eye(n*n), format = \"csc\")\n    A_index = jnp.split(jnp.array(A_lower.indices), A_lower.indptr[1:-1])\n    A_x = jnp.split(jnp.array(A_lower.data), A_lower.indptr[1:-1])\n    return (A_index, A_x)\n```\n:::\n\n\nWith our test case in hand, we can check to see if JAX will differentiate for us!\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nfrom jax import grad, jit\nfrom jax.test_util import check_grads\n\ngrad_func = grad(test_func, argnums = 2)\n\nA_index, A_x = make_matrix(50)\nprint(f\"The value at (2.0, 2.0) is {test_func(A_index, A_x, (2.0, 2.0))}.\")\nprint(f\"The gradient is {np.array(grad_func(A_index, A_x, (2.0, 2.0)))}.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe value at (2.0, 2.0) is 379600.0.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nThe gradient is [ 60000. 319600.].\n```\n:::\n:::\n\n\nFabulous! That works! \n\n## But what about JIT?\nJIT took fucking _ages_. I'm talking \"it threw a message\" amounts of time. I'm\nnot even going to pretend that I understand why. But I can hazard a guess.\n\nMy running assumption, taken from the docs, is that as long as the function only \nrelies of quantities that are derived from the _shapes_ of the inputs (and not the\nvalues), then JAX will be able to trace through and JIT through the functions with ease.\n\nThis might not be true for `tree_map`s. The docs are, as far as I can tell, silent\non this matter. And a cursory look through the github repo did not give me any\nhints as to how `tree_map()` is translated.\n\nLet's take a look to see if this is true.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nimport timeit\nfrom functools import partial\njit_test_func = jit(test_func)\n\nA_index, A_x = make_matrix(5)\ntimes = timeit.repeat(partial(jit_test_func, A_index, A_x, (2.0, 2.0)), number = 1)\nprint(f\"n = 5: {[round(t, 4) for t in times]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nn = 5: [1.6695, 0.0001, 0.0, 0.0, 0.0]\n```\n:::\n:::\n\n\nWe can see that the first run includes compilation time, but after that it \nruns a bunch faster. This is how a JIT system is supposed to work! But the question\nis: will it recompile when we run it for a different matrix?\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n_ = jit_test_func(A_index, A_x, (2.0, 2.0)) \nA_index, A_x = make_matrix(20)\ntimes = timeit.repeat(partial(jit_test_func, A_index, A_x, (2.0, 2.0)), number = 1)\nprint(f\"n = 20: {[round(t, 4) for t in times]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nn = 20: [38.5779, 0.0006, 0.0003, 0.0003, 0.0003]\n```\n:::\n:::\n\n\nDamn. It recompiles. But, as we will see, it does not recompile if we only change\n`A_x`.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n# What if we change A_x only\n_ = jit_test_func(A_index, A_x, (2.0, 2.0)) \nA_x2 = tree_map(lambda x: x + 1.0, A_x)\ntimes = timeit.repeat(partial(jit_test_func, A_index, A_x2, (2.0, 2.0)), number = 1)\nprint(f\"n = 20, new A_x: {[round(t, 4) for t in times]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nn = 20, new A_x: [0.0006, 0.0007, 0.0005, 0.0003, 0.0003]\n```\n:::\n:::\n\n\nThis gives us some hope! This is because the _structure_ of A (aka `A_index`)\nis fixed in our application, but the values `A_x` changes. So as long as the initial\nJIT compilation is reasonable, we should be ok.\n\nUnfortunately, there is something bad happening with the compilation. For $n=10$, \nit takes (on my machine) about 2 seconds for the initial compilation. For $n=20$,\nthat increases to 16 seconds. Once $n = 30$, this balloons up to 51 seconds. Once\nwe reach the lofty peaks^[$n=50$ takes so long it prints a message telling us what to do if we need to do if we want to file a bug! Compilation eventually clocks in at 361 seconds.] of $n=40$, we are up at 149 seconds to compile. \n\nThis is not good. The function we are JIT-ing is _very_ simple: just one `tree_map`.\nI do not know enough^[aka I know sweet bugger all] about the internals of JAX, \nso I don't want to speculate too wildly. But it seems like it might be unrolling\nthe `tree_map` before compilation, which is ... bad.\n\n## Let's admit failure\n\nOk. So that didn't bloody work. \nI'm not going to make such broad statements as _you can't use the JAX library in python\nto write a transformable sparse Cholesky factorisation_, but I am more than prepared\nto say that _I_ cannot do such a thing.\n\nBut, if I'm totally honest, I'm not _enormously_ surprised. Even in looking at the \nvery simple operation we focussed on today, it's pretty clear that the operations\nrequired to work on a sparse matrix don't look an awful lot like the types of operations\nyou need to do the types of machine learning work that is JAX's _raison d'être_.\n\nAnd it is _never_ surprising to find that a library designed to do a fundamentally different\nthing does not easily adapt to whatever random task I decide to throw at it.\n\nBut there is a light: JAX is an extensible language. We can build a new JAX primitive\n(or, new JAX primitives) and manually write all of the transformations (batching, JIT, and autodiffing).\n\nAnd that is what we shall do next! It's gonna be a blast!\n\n",
    "supporting": [
      "jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}