{
  "hash": "baea21df5e813939829b98de3d494a34",
  "result": {
    "markdown": "---\ntitle: \"Tail stabilization of importance sampling etimators: A bit of theory\"\ndescription: |\n  Look. I had to do it so I wrote it out in detail. This is some of the convergence theory for truncated and winzorised importance sampling estimators\ndate: 2022-06-15\nrepository_url: https://github.com/dpsimpson/blog/tree/master/_posts/2022-06-03-that-psis-proof\nimage: judy.JPG\ncategories: [Importance sampling, Computation, Truncated importance sampling, Windsorized importance sampling, Pareto smoothed importance sampling, PSIS]\ntwitter-card:\n  title: \"Sparse Matrices 4: Design is my passion\"\n  creator: \"@dan_p_simpson\"\ncitation: \n  url: https://dansblog.netlify.app/2022-06-03-that-psis-proof\n\n---\n\n\nImagine you have a target probability distribution $p(\\theta)$ and you want\nto estimate the expectation $I_h = \\int h(\\theta) p(\\theta)\\,d(\\theta)$. That's lovely\nand everything, but if it was easy none of us would have jobs. High-dimensional\nquadrature is a pain in the arse.\n\nA very simple way to get an decent estimate of $I_h$ is to use _importance sampling_,\nthat is taking draws $\\theta_s$, $s = 1,\\ldots, S$ from some proposal distribution\n$\\theta_s \\sim g(\\theta)$. Then, noting that \n$$\nI_h = \\int h(\\theta) p (\\theta)\\,d\\theta = \\int h(\\theta) \\underbrace{\\frac{p(\\theta)}{g(\\theta)}}_{r(\\theta)}g(\\theta)\\,d\\theta,\n$$\nwe can use Monte Carlo to estimate the second integral. This leads to the importance\nsampling estimator \n$$\nI_h^S = \\sum_{s=1}^S h(\\theta_s) r(\\theta_s).\n$$\n\nThis all seems marvellous, but there is a problem. Even though $h$ is probably \na very pleasant function and $g$ is a nice friendly distribution, $r(\\theta)$\ncan be an absolute beast. Why? Well it's^[proportional to] the ratio of two \ndensities and there is no guarantee that the ratio of two nice functions is\nitself a nice function. In particular, if the bulk of the distributions $p$ and $g$ are in\ndifferent places, you'll end up with the situation where for most draws $r(\\theta_s)$\nis very small^[because $p(\\theta_s)$ is very small] and a few will be HUGE^[because $p(\\theta_s)$ is a reasonable size, but $g(\\theta_s)$ is tiny.].\n\nThis will lead to an extremely unstable estimator.\n\nIt is pretty well known that the raw importance sampler $I_h^S$ will behave nicely\n(that is will be unbiased with finite variance) precisely when the distribution of \n$r_s = r(\\theta_s)$ has finite variance.\n\nElementary treatments stop there, but they miss two very big problems. The most\nobvious one is that it's basically impossible to check if the variance of $r_s$ \nis finite. A second, much larger but much more subtle problem, is that the variance\ncan be finite but _massive_. This is probably the most common case in high dimensions.\nMcKay has an excellent example where the importance ratios are _bounded_, but that \nbound is so large that it is infinite for all intents and purposes.\n\nAll of which is to say that importance sampling doesn't work unless you work on it.\n\n## Truncated importance sampling\n\nIf the problem is the fucking ratios then by gum we will fix the fucking ratios.\nOr so the saying goes.\n\nThe trick turns out to be modifying the largest ratios enough that we stabilise the\nvariance, but not so much as to overly bias the estimate.\n\nThe first version of this was [truncated importance sampling](https://www.jstor.org/stable/27594308?seq=1) (TIS), which selects a \nthreshold $T$ and estimates the expectation as \n$$\nI_\\text{TIS}^S = \\frac{1}{S}\\sum_{s= 1}^S h(\\theta_s) \\max\\{r(\\theta_s), T\\}.\n$$\nIt's pretty obvious that $I^S_\\text{TIS}$ has finite variance for any fixed $T$, but\nwe should be pretty worried about the bias. Unsurprisingly, there is going to \nbe a trade-off between the variance and the bias. So let's explore that.\n\n### The bias of TIS\n\nTo get an expression for the bias, first let us write $r_s = r(\\theta_s)$ and \n$h_s = h(\\theta_s)$ for $\\theta_s \\sim g$. Occasionally we will talk about the \njoint distribution or $(r_s,h_s) \\sim (R,H)$. Sometimes we will also need to\nuse the indicator variables $z_i = 1_{r_i < T}$.\n\nThen, we can write^[I have surreptitiously dropped the $h$ subscript because I am gay and sneaky.] \n$$\nI = \\mathbb{E}(HR \\mid R \\leq T) \\Pr(R \\leq T) + \\mathbb{E}(HR \\mid R > T) \\Pr(R > T).\n$$\n\nHow does this related to TIS? Well. Let $M = \\sum_{s=1}^S z_i$ be the random variable denoting the number\nof times $r_i > T$. Then,\n\\begin{align*}\n\\mathbb{E}(I_\\text{TIC}^S) &= \\mathbb{E}\\left( \\frac{1}{S}\\sum_{s=1}^Sz_ih_ir_i\\right)  + \\mathbb{E}\\left( \\frac{T}{S}\\sum_{s=1}^S(1-z_i)h_i\\right) \\\\\n&=\\mathbb{E}_M\\left[\\frac{S-M}{S}\\mathbb{E}(HR \\mid R < T) + \\frac{MT}{S}\\mathbb{E}(H \\mid R > T)\\right] \\\\\n&=\\mathbb{E}(HR \\mid R \\leq T) \\Pr(R \\leq T) + T\\mathbb{E}(H \\mid R > T) \\Pr(R > T).\n\\end{align*}\n\nHence the bias in TIS is \n$$\nI - \\mathbb{E}(I_\\text{TIS}^S) = \\mathbb{E}(H(R-T) \\mid R > T) \\Pr(R > T).\n$$\n\nTo be honest, this doesn't look phenomenally interesting for fixed $T$, however\nif we let $T = T_S$ depend on the sample size then as long as $T_S \\rightarrow \\infty$\nwe get vanishing bias.\n\nWe can get more specific if we make the assumption about the tail of the importance \nratios. In particular, we will assume that^[That it's parameterised by $1/k$ is an artefact of history.] $1-R(r) = \\Pr(R > r) = cr^{-1/k}(1+o(1))$ for some^[We need $\\mathbb{E}(R)$ to be finite, so we need $k<1$.] $k<1$.\n\nWhile it seems like this will only be useful for estimating $\\Pr(R>T)$, it turns out that under\nsome mild^[very fucking complex] technical conditions, the conditional excess \ndistribution function^[I have used that old trick of using the same letter for the CDF as the \nrandom variable when I have a lot of random variables. ] \n$$\nR_T(y) = \\Pr(R - T \\leq y \\mid R > T) = \\frac{R(T + y) - R(T)}{1-R(T)},\n$$\nis well approximated by a Generalised Pareto Distribution as $T\\rightarrow \\infty$.\nOr, in maths, as $T\\rightarrow \\infty$, \n$$\nR_T(y) \\rightarrow \\begin{cases} 1- \\left(1 + \\frac{ky}{\\sigma}\\right)^{-1/k}, \\quad & k \\neq 0 \\\\\n1- \\mathrm{e}^{-y/\\sigma}, \\quad &k = 0,\n\\end{cases}\n$$\nfor some $\\sigma > 0$ and $k \\in \\mathbb{R}$. The shape^[aka the tail index] parameter $k$ is very\nimportant for us, as it tells us how many moments the distribution has. In particular, \nif a distribution $X$ has shape parameter $k$, then \n$$\n\\mathbb{E}|X|^\\alpha < \\infty, \\quad \\forall \\alpha < \\frac{1}{k}.\n$$\nWe will focus exclusively on the case where $k < 1$. When $k < 1/2$, the distribution\nhas finite variance.\n\nIf $1- R(r) = cr^{-1/k}(1+  o(1))$, then the conditional exceedence function is \n\\begin{align*}\nR_T(y) &=  \\frac{cT^{-1/k}(1+  o(1)) - c(T+y)^{-1/k}(1+  o(1))}{cT^{-1/k}(1+  o(1)))} \\\\\n&= \\left[1 - \\left(1 + \\frac{y}{T}\\right)^{-1/k}\\right](1 + o(1)),\n\\end{align*}\nwhich suggests that as $T\\rightarrow \\infty$, $R_T$ converges to a generalised \nPareto distribution with shape parameter $k$ and scale parameter $\\mathcal{O}(T)$.\n\nAll of this work lets us approximate the distribution of $(R-T \\mid R>T )$ and use\nthe formula for the mean of a generalised Pareto distribution. This gives us the estimate \n$$\n\\mathbb{E}(R- T \\mid R>T) \\approx \\frac{T}{1-k},\n$$\nwhich estimates the bias when $h(\\theta)$ is constant^[This is a relevant case. But if you think a little bit about it, our problem happens when $r(\\theta)$ grows _much_ faster than $h(\\theta)$. For example if $P = \\operatorname{Exp}(1)$ and $G = \\operatorname{Exp}(1/\\lambda)$ for $\\lambda>1$, then $k = 1-1/\\lambda$, $r(\\theta) = \\exp((\\lambda-1)\\theta)$ and if $|h(\\theta)| < |\\theta|^\\alpha$, then $|h(\\theta)| \\leq C \\log(r)^\\alpha$, which is a slowly growing function.] as \n$$\nI - \\mathbb{E}(I_\\text{TIS}^S) \\approx \\mathcal{O}\\left(T^{1-1/k}\\right).\n$$\n\nFor what it's worth, Ionides got the same result more directly in the TIS paper, but he wasn't\ntrying to do what I'm trying to do. \n\n### The variance in TIS\n\nThe variance is a little bit more annoying. We want it to go to zero.\n\nAs before, we condition on $z_s$ (or, equivalently, $M$) and then use the law of \ntotal variance. We know from the bias calculation that \n$$\n\\mathbb{E}(I_\\text{TIS}^S \\mid M) =\\frac{S-M}{S}\\mathbb{E}(HR \\mid R>T) + \\frac{TM}{S}\\mathbb{E}(H \\mid R>T).\n$$\n\nA similarly quick calculation tells us that \n$$\n\\mathbb{V}(I_\\text{TIS}^S \\mid M) = \\frac{S-M}{S^2}\\mathbb{V}(HR \\mid R \\leq T) +\\frac{MT^2}{S^2}\\mathbb{V}(H \\mid R>T).\n$$\nTo close it out, we recall that $M$ is the sum of Bernoulli random variables so \n$$\nM \\sim \\text{Binomial}(S, \\Pr(R > T)).\n$$\n\nWith this, we can get an expression for the unconditional variance. To simplify\nthe expression, let's write $p_T = \\Pr(R > T)$. Then,\n\\begin{align*}\n\\mathbb{V}(I_\\text{TIS}^S) &=\\mathbb{E}_M\\mathbb{V}(I_\\text{TIS}^S \\mid M) + \\mathbb{V}_M\\mathbb{E}(I_\\text{TIS}^S \\mid M) \\\\\n&= S^{-1}(1-p_T)\\mathbb{V}(HR \\mid R \\leq T) +S^{-1}T^2p_T\\mathbb{V}(H \\mid R>T)\\\\\n&\\quad + S^{-1}p_T(1-p_T)\\mathbb{E}(HR \\mid R>T)^2 + S^{-1}Tp_T(1-p_T)\\mathbb{E}(H \\mid R>T)^2.\n\\end{align*}\n\nThere are four terms in the variance. The first and third terms are clearly harmless: they \ngo to zero no matter how we choose $T_S$. Our problem terms are the second and fourth.\nWe can tame the fourth term if we choose $T_S = o(S)$. But that doesn't seem to help with\nthe second term. But it turns out it is enough. To see this, we note that \n\\begin{align*}\nTp_T\\mathbb{V}(H\\mid R>T) &=\\leq Tp_T\\mathbb{E}(H^2 \\mid R>T)\\\\\n&\\leq p_T\\mathbb{E}(H^2 R\\mid R>T) \\\\\n&\\leq \\mathbb{E}(H^2 R)\\\\\n&= \\int h(\\theta)^2 p(\\theta)\\,d\\theta < \\infty.\n\\end{align*}\nwhere the second inequality uses the fact that $R>T$ and the third comes from the law of total probability.\n\nSo the TIS estimator has vanishing bias and variance as long as the truncation $T_S \\rightarrow \\infty$\nand $T_S = o(S)$. Once again, this is in the TIS paper, where it is proved in a much \nmore compact way.\n\n### Asymptotic properties\n\nIt can also be useful to have an understanding of how wild the fluctuations $I - I_\\text{TIS}^S$ are.\nFor traditional importance sampling, we know that if $\\mathbb{E}(R^2)$ is finite,\nthen then the fluctuations are, asymptotically, normally distributed with mean zero.\nNon-asymptotic results were given by [Chatterjee and Diaconis](https://arxiv.org/abs/1511.01437)\nthat also hold even when the estimator has infinite variance.\n\nFor TIS, it's pretty obvious that for fixed $T$ and $h \\geq 0$, $I_\\text{TIS}^S$ will be asymptotically\nnormal (it is, after all, the sum of bounded random variables). For growing sequences\n$T_S$ it's a tiny bit more involved: it is now a triangular array^[Because the truncation depends on $S$, moving from the $S$th partial sum to the $S+1$th partial sum changes the distribution of $z_ih_ir_i$. This is exactly why the dead Russians gifted us with triangular arrays.] rather than a sequence\nof random variables. But in the end very classical results tell us that for bounded^[Also practical unbounded $h$, but it's just easier for bounded $h$] $h$, the fluctuations of the TIS estimator\nare asymptotically normal.\n\nIt's worth saying that when $h(\\theta)$ is unbounded, it _might_ be necessary to \ntruncate the product $h_ir_i$ rather than just $r_i$. This is especially relevant if\n$\\mathbb{E}(H \\mid R=r)$ grows rapidly with $r$. Personally, I can't think of a case \nwhere this happens: $r(\\theta)$ usually grows (super-)exponentially in $\\theta$ while\n$h(\\theta)$ usually grows polynomially, which implies $\\mathbb{E}(H \\mid R=r)$ grows (poly-)logarithmically.\n\nThe other important edge case is that when $h(\\theta)$ can be both positive and negative,\nit might be necessary to truncate $h_ir_i$ both above _and_ below.\n\n## Winsorised importance sampling\n\nTIS has lovely theoretical properties, but it's a bit challenging to use in \npractice. The problem is, there's really no practical guidance on how to \nchoose the truncation sequence.\n\nSo let's do this differently. What if instead of specifying a threshold directly,\nwe instead decided that the largest $M$ values are potentially problematic and should\nbe modified? Recall that for TIS, the number of samples that exceeded the threshold, $M$,\nwas random while the threshold was fixed. This is the opposite situation: the number\nof exceedences is fixed but the threshold is random. \n\nThe threshold is now the $M$th largest value of $r_s$. We denote this using order\nstatistics notation: we re-order the sample so that \n$$\nr_{1:S} \\leq r_{2:S}\\leq \\ldots r_{S:S}.\n$$ \nWith this notation, the threshold is $T = r_{S-M+1:S}$ and the Winsorized importance sampler (WIS)\nis \n$$\nI^S_\\text{WIS} = \\frac{1}{S}\\sum_{s = 1}^{S-M} h_{s:S}r_{s:S} + \\frac{r_{S-M+1:S}}{S}\\sum_{s=S-M+1}^S h_{s:S},\n$$\nwhere $(r_{s:S}, h_{s:S})$ are the $(r_s, h_s)$ pairs _ordered_ so that $r_{1:S} \\leq r_{2:S}\\leq \\cdots \\leq r_{S:S}$. Note that $h_{s:S}$ are not necessarily in increasing order: they are\nknown as _concomitants_ of $r_{s:S}$, which is just a fancy way to say that they're along for the ride.\nIt's _very_ important that we reorder the $h_s$ when we reorder the $r_s$, otherwise\nwe won't preserve the joint distribution and we'll end up with absolute rubbish.\n\nWe can already see that this is both much nicer and much wilder than the TIS distribution.\nIt is _convenient_ that $M$ is no longer random! But what the hell are we going to do\nabout those order statistics? Well, the answer is very much the same thing as before: condition on them and hope for the best.\n\nConditioned on the event^[Shut up. I know. Don't care.] $\\{r_{S-M+1:S} = T\\}$, we get\n$$\n\\mathbb{E}\\left(I_\\text{WIS}^S \\mid r_{S-M+1:S} = T\\right) = \\left(1 - \\frac{M}{S}\\right)\\mathbb{E}(RH \\mid R < T) + \\frac{MT}{S} \\mathbb{E}(H \\mid R \\geq T).\n$$\nFrom this, we get that the bias, conditional on $r_{S-M+1:S} = T$ is \n\\begin{multline*}\n\\left|I - \\mathbb{E}\\left(I_\\text{WIS}^S \\mid r_{S-M+1:S} = T\\right)\\right| =\\left|\\left[\\Pr(R < T) - \\left(1 - \\frac{M}{S}\\right)\\right]\\mathbb{E}(RH \\mid R < T) \\right.\\\\ \n\\left.+ \\left[\\Pr(R \\geq T) - \\frac{M}{S}\\right] \\mathbb{E}(H(R - T) \\mid R \\geq T)\\right|.\n\\end{multline*}\n\nYou should immediately notice that we are in quite a different situation from TIS,\nwhere only the tail contributed to the bias. By fixing $M$ and randomising the threshold,\nwe have bias contributions from both the bulk (due, essentially, to a weighting error) \nand from the tail (due to both the weighting error and the truncation). This\nis going to require us to be a bit creative.\n\n \nWe could probably do something more subtle and clever here, but that is not my way.\nInstead, let's use the triangle inequality to say\n$$\n\\left|\\mathbb{E}(RH \\mid R > T)\\right| \\leq \\frac{\\mathbb{E}(R |H| 1(R<T))}{\\Pr(R <T)} \\leq \\frac{\\|h\\|_{L^1(p)}}{\\Pr(R  <T)}\n$$\nand so the first term in the bias can be bounded if we can bound the relative error \n$$\n\\mathbb{E}\\left|1 - \\frac{1- M/S}{\\Pr(R < r_{S-M+1:S})}\\right|.\n$$\n\nNow the more sensible among you will say _[Daniel, No!](https://www.youtube.com/watch?v=R-HryG35A2E) That's a ratio! That's going to be hard to bound_.\nAnd, of course, you are right. But here's the thing: if $M$ is small relative to $S$,\nit is _tremendously_ unlikely that $r_{S-M+1:S}$ is anywhere near zero. This is \nintuitively true, but also mathematically true. \n\nTo attack this expectation, we are going to look at a slightly different quantity\nthat has the good grace of being non-negative.\n\n::: {#lem-lem1} \nLet  $X_s$, $s= 1, \\ldots S$ be an iid sample from $F_X$, let $0\\leq k\\leq S$\nbe an integer. Then \n$$\n\\frac{p}{F_X(x_{k:S})} -p \\stackrel{d}{=} \\frac{p(S-k+1)}{k} \\mathcal{F},\n$$\nand \n$$\n\\frac{1-p}{1- F_x/(x_{k:S})} - (1-p) \\stackrel{d}{=} \\frac{k(1-p)}{S-k+1}\\mathcal{F}^{-1}\n$$\nwhere $\\mathcal{F}$ is an F-distributed random variable with parameters $(2(S-k+1), 2k)$.\n:::\n\n::: {.proof}\n\nFor any $t\\geq 0$, \n\\begin{align*}\n\\Pr\\left(\\frac{p}{F_X(x_{k:S})} - p \\leq t\\right) &=\\Pr\\left(p - pF_X(x_{k:S}) \\leq tF_X(x_{k:S})\\right) \\\\\n&= \\Pr\\left(p  \\leq (t+p)F_X(x_{k:S})\\right) \\\\\n&=\\Pr\\left(F_X(x_{k:S}) \\geq \\frac{p}{p+t}\\right)\\\\\n&= \\Pr\\left(x_{k:S} \\geq F_X^{-1}\\left(\\frac{p}{p+t}\\right)\\right)\\\\\n&= 1- I_{\\frac{p}{p+t}}(k, S-k+1) \\\\\n&= I_{\\frac{t}{p+t}}(S-k+1, k),\n\\end{align*}\nwhere $I_p(a,b)$ is the incomplete Beta function.\n\nYou could, quite reasonably, ask where the hell that incomplete Beta function \ncame from. And if I had thought to look this up, I would say that it came from\nEquation 2.1.5 in David and Nagaraja's book on order statistics. Unfortunately, \nI did not look this up. I derived it, which is honestly not very difficult.\nThe trick is to basically note that the event $\\{x_{k:S} \\leq \\tau\\}$ is the same\nas the event that at least $k$ of the samples $x_s$ are less than or equal to $\\tau$.\nBecause the $x_s$ are independent, this is the probability of observing at least\n$k$ heads from a coin with the probability of a head $\\Pr(x \\leq \\tau) = F_X(\\tau)$.\nIf you look this up on Wikipedia^[or, hell, even in a book] you see^[Straight up, though, I spent 2 days dicking around with tail bounds on sums of Bernoulli random variables for some bloody reason before I just looked at the damn formula.] that it is  $I_{1-F_X(\\tau)}(k,S-k+1)$. The rest just come from noting that $\\tau = F_X^{-1}(t/(p+t))$\nand using the symmetry $1-I_p(a,b) = I_{1-p}(b,a)$.\n\nTo finish this off, we note that \n$$\n\\Pr(\\mathcal{F} \\leq x) = I_{\\frac{S-k+1}{(S-k+1)x+ k}}(S-k+1,k).\n$$\nFrom which, we see that \n\\begin{align*}\n\\Pr\\left(\\frac{p}{F_X(x_{k:S})} - p \\leq t\\right) &=\\Pr\\left(\\mathcal{F} \\leq \\frac{k}{p(S-k+1)}t\\right) \\\\\n&= \\Pr\\left(\\frac{p(S-k+1)}{k}\\mathcal{F} \\leq t\\right).\n\\end{align*}\n\nThe second result follows the same way and by noting that $\\mathcal{F}^{-1}$ is \nalso F-distributed with parameters $(k, S-k+1)$.\n\n*The proof has ended*\n:::\n\nNow, obviously, in this house we do not trust mathematics. Which is to say that\nI made a stupid mistake the first time I did this and forgot that when $Z$ is binomial,\n$\\Pr(Z \\geq k) = 1 - \\Pr(Z \\leq k-1)$ and had a persistent off-by-one error in\nmy derivation. But we test out our results so we don't end up doing the dumb thing.\n\nSo let's do that. For this example, we will use generalised Pareto-distributed\n$X$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nxi <- 0.7\ns <- 2\nu <- 4\n\nsamp <- function(S, k, p, \n                 Q = \\(x) u + s*((1-x)^(-xi)-1)/xi, \n                 F = \\(x) 1 - (1 + xi*(x - u)/s)^(-1/xi)) {\n  # Use theory to draw x_{k:S}\n  xk <- Q(rbeta(1, k, S - k + 1))\n  c(1 - p / F(xk), 1-(1-p)/(1-F(xk)))\n}\n\nS <- 1000\nM <- 50\nk <- S - M + 1\np <- 1-M/S\nN <- 100000\n\nfs <- rf(N, 2 * (S - k + 1), 2 * k )\ntibble(theoretical = 1-p - p * fs * (S - k + 1)/k,\n       xks = map_dbl(1:N, \\(x) samp(S, k, p)[1])) %>%\n  ggplot() + stat_ecdf(aes(x = xks), colour = \"black\") + \n  stat_ecdf(aes(x = theoretical), colour = \"red\", linetype = \"dashed\") +\n  ggtitle(expression(1 - frac(1-M/S , R(r[S-M+1:S]))))\n```\n\n::: {.cell-output-display}\n![](that-psis-proof_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntibble(theoretical = p - (1-p) * k/(fs * (S - k + 1)),\n       xks = map_dbl(1:N, \\(x) samp(S, k, p)[2])) %>%\n  ggplot() + stat_ecdf(aes(x = xks), colour = \"black\") + \n  stat_ecdf(aes(x = theoretical), colour = \"red\", linetype = \"dashed\") +\n  ggtitle(expression(1 - frac(M/S , 1-R(r[S-M+1:S]))))\n```\n\n::: {.cell-output-display}\n![](that-psis-proof_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n:::\n\n\nFabulous. It follow then that \n$$\n\\left|1 - \\frac{1-M/S}{R(r_{S-M+1})} \\right| \\stackrel{d}= \\left|\\frac{M}{S} -  \\frac{M(S-M)}{S(S-M-1)}\\mathcal{F}\\right| \\leq \\frac{M}{S} +  \\frac{M(S-M)}{S(S-M-1)} \\mathcal{F},\n$$\nwhere $\\mathcal{F}$ has an F-distribution with $(M, S-M+1)$ degrees of freedom.\nAs $\\mathbb{E}(\\mathcal{F}) = 1 + 1/(S-M-1)$, it follows that this term goes to zero\nas long as $M = o(S)$. This shows that the first term in the bias goes to zero.\n\nIt's worth noting here that we've also calculated that the bias is _at most_ $\\mathcal{O}(M/S)$, \nhowever, this rate is extremely sloppy. That upper bound we just computed is _unlikely_ to be tight. A better person than me would probably check, but honestly I just don't give a shit^[Ok. I checked. And yeah. Same technique as below using Jensen in its $\\mathbb{E}(|X-\\mathbb{E}(X)|)^2 \\leq \\mathbb{V}(X)$. If you put that together you get something that goes to zero like $M^{1/2}S^{-1}$, which is $\\mathcal{O}(S^{-3/4})$ for our usual choice of $M$. Which confirms the suspicion that the first term in the bias goes to zero _much_ faster than the second (remembering, of course, that Jensen's inequality is notoriously loose!).]\n\nThe second term in the bias is \n$$\n\\left[\\Pr(R \\geq T) - \\frac{M}{S}\\right] \\mathbb{E}(H(R - T) \\mid R \\geq T).\n$$\nAs before, we can write this as \n$$\n\\left(1 - \\frac{M/S}{1-R(T)}\\right)|\\mathbb{E}(H(R - T) 1_{R \\geq T})| \\leq \\left|1 - \\frac{M/S}{1-R(T)}\\right|\\|h\\|_{L^1(p)}.\n$$\nBy our lemma, we know that the distribution of the term in the absolute value \nwhen $T = r_{S-M+1}$ is the same as \n$$\n1-\\frac{M}{S} -\\left(1 - \\frac{M}{S} + \\frac{1}{S}\\right)\\mathcal{F} = (\\mu_F-\\mathcal{F})  +\\frac{M}{S}(\\mathcal{F}-\\mu_F) - \\frac{1}{S}\\mathcal{F} +  \\frac{1}{M-1}\\left(\\frac{M}{S} - 1\\right),\n$$\nwhere $\\mathcal{F} \\sim \\text{F}_{2(S-M+1), 2M}$, which has mean $\\mu_F = 1+(M-1)^{-1}$ and variance \n$$\n\\sigma^2_F = \\frac{M^2S}{(S-M+1)(M-1)^2(M-2)} = \\frac{1}{M}(1 + \\mathcal{O}(M^{-1} + MS^{-1}).\n$$\nFrom Jensen's inequality, we get \n$$\n\\mathbb{E}(|\\mathcal{F} - \\mu_F|) \\leq \\sigma_F = M^{-1/2}(1 + o(1)).\n$$\nIf follows that \n$$\n\\mathbb{E}\\left|1 - \\frac{M/S}{1-R(r_{S-M+1:S})}\\right| \\leq M^{-1/2}(1+o(1))M^{1/2}S^{-1}(1 + o(1)) + S^{-1}(1+ o(1)) + (M-1)^{-1}(1+o(1)),\n$$\nand so we get vanishing bias as long as $M\\rightarrow \\infty$ and $M/S \\rightarrow 0$.\n\nOnce again, I make no claims of tightness^[It's Pride month]. Just because it's \na bit sloppy at this point doesn't mean the job isn't done.\n\n\n::: {#thm-thm1}\nLet $\\theta_s$, $s = 1,\\ldots, S$ be an iid sample from $G$ and let $r_s = r(\\theta_s) \\sim R$. Assume that\n\n1. $R$ is absolutely continuous\n\n2. $M  \\rightarrow \\infty$ and $S^{-1}M \\rightarrow 0$\n\n3. $h \\in L^1(p)$\n\nThen  Winsorized importance sampling converges in $L^1$ and is asymptotically unbiased.\n\n:::\n\nOk so that's nice. But you'll notice that I did not mention our piss-poor rate.\nThat's because there is absolutely no way in hell that the bias is\n$\\mathcal{O}(M^{-1/2})$! That rate is an artefact of a _very_ sloppy bound on\n$\\mathbb{E}|1-\\mathcal{F}|$. \n\nUnfortunately, Mathematica couldn't help me out. Its asymptotic abilities shit the\nbed at the sight of ${}_2F_1(a,b;c;z))$, which is everywhere in the exact expression (which \nI've put below in the fold.\n\n<details><summary>Mathematica expression for $\\mathbb{E}|1-\\mathcal{F}|$.</summary>\n```\n-(((M/(1 + S))^(-(1/2) - S/2)*Gamma[(1 + S)/2]*\n     (6*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) - \n        5*M*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) + \n        M^2*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) + \n        8*S*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) - \n        6*M*S*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) + \n        M^2*S*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) + \n        2*S^2*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) - \n        M*S^2*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) - \n         6*Sqrt[-(M/(-1 + M - S))]*Sqrt[(-1 - S)/(-1 + M - S)]*\n        (M/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[1, (1/2)*(-1 + M - S), \n                                                      M/2, M/(-1 + M - S)] + 8*M*Sqrt[-(M/(-1 + M - S))]*\n        Sqrt[(-1 - S)/(-1 + M - S)]*(M/(1 - M + S))^(M/2 + S/2)*\n        Hypergeometric2F1[1, (1/2)*(-1 + M - S), M/2, M/(-1 + M - S)] - \n        2*M^2*Sqrt[-(M/(-1 + M - S))]*Sqrt[(-1 - S)/(-1 + M - S)]*\n        (M/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[1, (1/2)*(-1 + M - S), \n                                                      M/2, M/(-1 + M - S)] - 8*Sqrt[-(M/(-1 + M - S))]*\n        Sqrt[(-1 - S)/(-1 + M - S)]*S*(M/(1 - M + S))^(M/2 + S/2)*\n        Hypergeometric2F1[1, (1/2)*(-1 + M - S), M/2, M/(-1 + M - S)] + \n        4*M*Sqrt[-(M/(-1 + M - S))]*Sqrt[(-1 - S)/(-1 + M - S)]*S*\n        (M/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[1, (1/2)*(-1 + M - S), \n                                                      M/2, M/(-1 + M - S)] - 2*Sqrt[-(M/(-1 + M - S))]*\n        Sqrt[(-1 - S)/(-1 + M - S)]*S^2*(M/(1 - M + S))^(M/2 + S/2)*\n        Hypergeometric2F1[1, (1/2)*(-1 + M - S), M/2, M/(-1 + M - S)] + \n        6*M*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^(M/2 + S/2)*\n        Hypergeometric2F1[(1 + S)/2, (1/2)*(1 - M + S), (1/2)*(3 - M + S), \n                          (-1 + M - S)/M] - 5*M^2*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^\n        (M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, (1/2)*(1 - M + S), \n                                      (1/2)*(3 - M + S), (-1 + M - S)/M] + M^3*(M/(1 + S))^(M/2)*\n        ((1 + S)/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, \n                                                            (1/2)*(1 - M + S), (1/2)*(3 - M + S), (-1 + M - S)/M] + \n        2*M*S*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^(M/2 + S/2)*\n        Hypergeometric2F1[(1 + S)/2, (1/2)*(1 - M + S), (1/2)*(3 - M + S), \n                          (-1 + M - S)/M] - M^2*S*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^\n        (M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, (1/2)*(1 - M + S), \n                                      (1/2)*(3 - M + S), (-1 + M - S)/M] - 2*M*(M/(1 + S))^(M/2)*\n        ((1 + S)/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, \n                                                            (1/2)*(3 - M + S), (1/2)*(5 - M + S), (-1 + M - S)/M] + \n        3*M^2*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^(M/2 + S/2)*\n        Hypergeometric2F1[(1 + S)/2, (1/2)*(3 - M + S), (1/2)*(5 - M + S), \n                          (-1 + M - S)/M] - M^3*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^\n        (M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, (1/2)*(3 - M + S), \n                                      (1/2)*(5 - M + S), (-1 + M - S)/M] - 2*M*S*(M/(1 + S))^(M/2)*\n        ((1 + S)/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, \n                                                            (1/2)*(3 - M + S), (1/2)*(5 - M + S), (-1 + M - S)/M] + \n        M^2*S*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^(M/2 + S/2)*\n        Hypergeometric2F1[(1 + S)/2, (1/2)*(3 - M + S), (1/2)*(5 - M + S), \n                          (-1 + M - S)/M]))/(((1 + S)/(1 - M + S))^S*\n                                               (2*(-2 + M)*M*Sqrt[(-1 - S)/(-1 + M - S)]*Gamma[M/2]*\n                                                  Gamma[(1/2)*(5 - M + S)])))\n```\n</details>\n\nBut do not fear: we can recover. At the cost of an assumption about the tails \nof $R$. (We're also going to assume that $h$ is bounded because it makes things\never so slightly easier, although unbounded $h$ is ok^[The result holds exactly if $\\mathbb{E}(H \\mid R=r) = \\mathcal{O}(\\log^k(r))$ and with a $k$ turning up somewhere if it's $o(r^{1/k - 1})$.] as long as it doesn't grow too quickly relative to $r$.)\n\nWe are going to make the assumption that $R - T \\mid R\\geq T$ is in \nthe domain of attraction of a generalized Pareto distribution with shape parameter $k$.\nA sufficient condition, due to von Mises, is that \n$$\n\\lim_{r\\rightarrow \\infty} \\frac{r R'(r)}{1-R(r)} = \\frac{1}{k}.\n$$\n\nThis seems like a weird condition, but it's basically just a regularity condition at infinity.\nFor example if $1-R(r)$ is regularly varying at infinity^[$1-R(r) \\sim c r^{(-1/k)}\\mathcal{L(r)}$ for a slowly varying function (eg a power of a logarithm) $\\mathcal{L}(r)$.] and $R'(r)$ is, eventually, monotone^[A property that implies this is that $1-R(r)$ is differentiable and  _convex at infinity_, which is to say that there is some finite $r_0$ such that $R'(r)$ exists for all $r \\geq r_0$ and $1-R(r)$ is a monotone function on $[r_0, \\infty)$.] decreasing, then this condition holds.\n\nThe von Mises condition is very natural for us as [Falk and Marohn (1993)](https://projecteuclid.org/journals/annals-of-probability/volume-21/issue-3/Von-Mises-Conditions-Revisited/10.1214/aop/1176989120.full) show that the relative error we get\nwhen approximating the tail of $R$ by a generalised Pareto density is the \nsame as the relative error in the von Mises condition. That is if \n$$\n\\frac{rR'(r)}{1-R(r)} = \\frac{1}{k}(1 + \\mathcal{O}(r^{-\\alpha}))\n$$\nthen \n$$\nR'(r) = c w(cr - d)(1 + \\mathcal{O}(r^{-\\alpha})),\n$$\nwhere $c,d$ are constants and $w$ is the density of a generalised Pareto distribution.\n\n\nAnyway, under those two assumptions, we can swap out the density of $(R-T)\\mid R>T$ with\nits asymptotic approximation and get that, conditional on $T=  r_{S-M+1:S}$, \n$$\n\\mathbb{E}(H(R-T) \\mid R>T) = (k-1)^{-1}T.\n$$\n\nHence, the second term in the bias goes to zero if \n$$\n\\mathbb{E}\\left(r_{S-M+1:S}\\left(1 - R(r_{s-M+1:S}) - \\frac{M}{S}\\right)\\right)\n$$\ngoes to zero.\n\nNow this is not particularly pleasant, but it helps to recognise that even if \na distribution doesn't have finite moments, away from the extremes, its order \nstatistics always do. This means that we can use Cauchy-Schwartz to get \n$$\n\\left|\\mathbb{E}\\left(r_{S-M+1:S}\\left(1 - R(r_{s-M+1:S}) - \\frac{M}{S}\\right)\\right)\\right| \\leq\\mathbb{E}\\left(r_{S-M+1:S}^2\\right)^{1/2}\\mathbb{E}\\left[\\left(1 - R(r_{s-M+1:S}) - \\frac{M}{S}\\right)^2\\right]^{1/2}.\n$$\n\nArguably, the most alarming term is the first one, but that can^[There's a condition here that $S$ has to be large enough, but it's enough if $(S-M+1) > 2$.] be tamed. To do this,\nwe lean into a result from [Bickel (1967)](https://projecteuclid.org/proceedings/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Fifth-Berkeley-Symposium-on-Mathematical-Statistics-and/Chapter/Some-contributions-to-the-theory-of-order-statistics/bsmsp/1200513012) who, if you examine the proof and translate some obscurely-stated conditions and fix a typo^[The first $k$ in the equation below is missing in the paper. If you miss this, you suddenly get the expected value converging to zero, which would be _very_ surprising. Always sense-check the proofs, people. Even if a famous person did it in the 60s.], you get that \n$$\n\\mathbb{E}(r_{k:M}^2) \\leq C k\\begin{pmatrix} S \\\\ k\\end{pmatrix} \\int_0^1 t^{k-2-1}(1-t)^{S-k-2}\\,dt.\n$$\nYou might worry that this is going to grow too quickly. But it doesn't. Noting \nthat $B(n,m) = \\Gamma(n)\\Gamma(m)/\\Gamma(n+m)$, we can rewrite the upper bound in\nterms of the Beta function to  get \n$$\n\\mathbb{E}(r_{k:M}^2) \\leq C \\frac{\\Gamma(S+1)}{\\Gamma(S-3)} \\frac{\\Gamma(k-2)}{\\Gamma(k+1)}\\frac{\\Gamma(S-k-1)}{\\Gamma(S-k+1)}.\n$$\n\nTo show that this doesn't grow too quickly, we use the identity \n$$\n\\frac{\\Gamma(x + a)}{\\Gamma(x + b)} \\propto x^{a-b}(1 + \\mathcal{O}(x^{-1})).\n$$\nFrom this, it follows that \n$$\n\\mathbb{E}(r_{k:M}^2) \\leq C S^4k^{-3}(S-k)^{-2}(1+ \\mathcal{O}(S^{-1}))(1+ \\mathcal{O}(k^{-1}))(1+ \\mathcal{O}((S+k)^{-1})).\n$$\nIn this case, we are interested in $k = S-M+1$, so \n$$\n\\mathbb{E}(r_{k:M}^2) \\leq C S^4S^{-3}M^{-2}(1 - M/S + 1/S)^{-3}(1 - 1/M)^{-2}(1+ \\mathcal{O}(S^{-1}))(1+ \\mathcal{O}(S^{-1}))(1+ \\mathcal{O}(M^{-1})).\n$$\n\nHence the we get that $\\mathbb{E}(r_{k:M}^2) = \\mathcal{O}(SM^{-2})$. This is increasing^[We need to take $M = \\mathcal{O}(S^{1/2})$ to be able to estimate the tail index $k$\nfrom a sample, which gives an upper bound by a constant.] in \n$S$, but we will see that it is not going up too fast.\n\nFor the second half of this shindig, we are going to attack \n$$\n\\mathbb{E}\\left[\\left(1 - R(r_{s-M+1:S}) - \\frac{M}{S}\\right)^2\\right] = \\mathbb{E}\\left[\\left(1 - R(r_{s-M+1:S})\\right)^2 - 2\\left(1 - R(r_{s-M+1:S})\\right)\\frac{M}{S} +\\left(\\frac{M}{S}\\right)^2\\right].\n$$\nA standard result^[Note  that if $U \\sim \\text{Unif}(0,1)$, then $R^{-1}(U) \\sim R$. Because this is monotone, it doesn't change ordering of the sample] from extreme value theory is that $R(r_{k:S})$ has the same \ndistribution as the $k$th order statistics from a sample of $S$ iid $\\text{Uniform}([0,1])$ \nrandom variables. Hence^[This is, incidentally, how Bickel got the upper bound on the moments. He combined this with an upper bound on the quantile function.],\n$$\nR(r_{S-M+1:S}) \\sim \\text{Beta}(S-M+1, M).\n$$\nIf follows^[Save the cheerleader, save the world. Except it's one minus a beta is still beta but with the parameters reversed.] that \n$$\n\\mathbb{E}(1- R(r_{S-M+1:S})) = \\frac{M}{S+1} = \\frac{M}{S}\\frac{1}{1+S^{-1}}\n$$\nand \n$$\n\\mathbb{E}((1- R(r_{S-M+1:S}))^2) = \\frac{M(M+1)}{(S+1)(S+2)} = \\frac{M^2}{S^2}\\left(\\frac{1 + M^{-1}}{1 + 3S^{-1} + 2S^{-2}}\\right).\n$$\nAdding these together and doing some asymptotic expansions, we get \n$$\n\\mathbb{E}\\left[\\left(1 - R(r_{s-M+1:S}) - \\frac{M}{S}\\right)^2\\right] = \\frac{M^2}{S^2} + \\mathcal{O}\\left(\\frac{M}{S^2}\\right),\n$$\nwhich goes to  zero^[As long as $M = o(S)$] like $\\mathcal{O}(S^{-1})$ if $M = \\mathcal{O}(S^{1/2})$.\n\nWe can multiply this rate together and get that the second term in the bias \nis bounded above by \n$$\n\\left[\\left(\\frac{S}{M^2} (1 + \\mathcal{O}(M^{-1} + MS^{-1}))\\right)\\left(\\frac{M^2}{S^2} (1 + \\mathcal{O}(M^{-1} + MS^{-1})\\right)\\right]^{1/2} = S^{-1/2}(1 + o(1)).\n$$\n\nPutting all of this together we have proved the following Corollary.\n\n::: {#cor-cor1}\nLet $\\theta_s$, $s = 1,\\ldots, S$ be an iid sample from $G$ and let $r_s = r(\\theta_s) \\sim R$. Assume that\n\n1. $R$ is absolutely continuous and satisfies the von Mises condition^[The rate here is probably not optimal, but it will guarantee that the error in the Pareto approximation doesn't swamp the other terms.] \n$$\n\\frac{rR'(r)}{1-R(r)} = \\frac{1}{k}(1 +\\mathcal{O}(r^{-1})).\n$$\n\n2. $M  = o(S)$\n\n3. $h$ is bounded^[Or $\\mathbb{E}(h(\\theta) \\mid r(\\theta) = r)$ doesn't grow to quickly, with some modification of the rates in the unlikely case that it grows polynomially.]\n\nWinsorized importance sampling converges in $L^1$ with rate of, at most, $\\mathcal{O}(MS^{-1} + S^{-1/2})$,\nwhich is balanced when $M = \\mathcal{O}(S^{1/2})$. Hence, WIS is^[almost, there's an epsilon gap but I don't give a shit] $\\sqrt{n}$-consistent.\n:::\n\n### Variance of Winsorized Importance Sampling\n\nRight, that was a bit of a journey, but let's keep going to the variance. \n\nIt turns out that following the route I thought I was going to follow does not\nend well. That lovely set of tricks breaking up the variance into two conditional\nterms turns out to be very very unnecessary. Which is good, because I thoroughly failed\nto make the argument work.\n\nIf you're curious, the problem is that the random variable \n$$\n\\frac{Mr_{S-M+1:S}}{S} \\mathbb{E}(H \\mid R \\geq r_{S-M+1:S}) = \\frac{Mr_{S-M+1:S}}{S(1-R(r_{S-M+1:S}))} \\mathbb{E}(H 1_{R \\geq r_{S-M+1:S}})\n$$\nis an absolute _bastard_ to bound. The problem is that $1- R({r_{S-M+1:S}}) \\approx M/S$\nand so the usual trick of bounding that truncated expectation by $\\|h\\|$ or some such thing \nwill prove that the variance is _finite_ but not that it goes to zero. There is a \nsolid chance that the Cauchy-Schwartz inequality \n$$\n\\frac{Mr_{S-M+1:S}^{1/2}}{S(1-R(r_{S-M+1:S}))} \\mathbb{E}(r_{S-M+1:S}^{1/2}H 1_{R \\geq r_{S-M+1:S}}) \\leq\\frac{Mr_{S-M+1:S}^{1/2}}{S(1-R(r_{S-M+1:S}))}R(r_{S-M+1:S})\\|h\\|_{L^2(p)}\n$$\nwould work. But truly that is just bloody messy^[And girl do not get me started on messy. I ended up going down a route where I used the [inequality]((https://www.sciencedirect.com/science/article/pii/0167715288900077) $$\n\\mathbb{V}(g(U)) \\leq \\mathbb{E}(U)\\int_0^1\\left[F_U(u) - \\frac{\\mathbb{E}(U1_{U\\leq u})}{\\mathbb{E}(U)}\\right][g'(u)]^2\\,du\n$$ which holds for any $U$ supported on $[0,1]$ with differentiable density. And let me tell you. If you dick around with enough beta distributions you can get something. Is it what you want? Fucking no. It is _a lot_ of work, including having to differentiate the conditional expectation, and it gives you sweet bugger all.].\n\nSo let's do it the easy way, shall we. Fundamentally, we will use\n$$\n\\mathbb{V}\\left(I_\\text{WIS}^S\\right) \\leq \\mathbb{E}\\left([I_\\text{WIS}^S]^2\\right).\n$$\nNoting that we can write $I_\\text{WIS}^S$ compactly as \n$$\nI_\\text{WIS}^S = \\frac{1}{S}\\sum_{s=1}^S h(\\theta_s)\\min\\{r(\\theta_s), r_{S-M+1:S}\\}.\n$$\nHence,\n\\begin{align*}\n\\mathbb{E}\\left([I_\\text{WIS}^S]^2\\right) &= \\mathbb{E}_{T\\sim r_{S-M+1:S}}\\left[\\mathbb{E}\\left([I_\\text{WIS}^S]^2 \\mid r_{S-M+1:S} = T\\right)\\right]\\\\\n&=\\frac{1}{S^2}\\mathbb{E}_{T\\sim r_{S-M+1:S}}\\left[\\mathbb{E}\\left(H^2 \\min\\{R^2,T^2\\} \\mid r_{S-M+1:S} = T\\right)\\right]\\\\\n&\\leq\\frac{1}{S^2}\\mathbb{E}_{T\\sim r_{S-M+1:S}}\\left[\\mathbb{E}\\left(RTH^2 \\mid r_{S-M+1:S} = T\\right)\\right] \\\\ \n&\\leq\\frac{1}{S^2}\\mathbb{E}_{T\\sim r_{S-M+1:S}}\\left[T\\|h\\|_{L^2(p)}^2\\right] \n\\end{align*}\n\nThis goes to zero as long as $\\mathbb{E}(r_{S-M+1:S}) = o(S^2)$.\n\n[Bickel (1967)](https://projecteuclid.org/proceedings/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Fifth-Berkeley-Symposium-on-Mathematical-Statistics-and/Chapter/Some-contributions-to-the-theory-of-order-statistics/bsmsp/1200513012) shows that,\nnoting that $\\mathbb{E}(R) < \\infty$, \n$$\n\\mathbb{E}(r_{S-M+1:S}) \\leq C (S-M+1)\\frac{\\Gamma(S+1)\\Gamma(S-M+1-1)\\Gamma(M)}{\\Gamma(S-M+1+1)\\Gamma(M+1)\\Gamma(S-1)} = \\frac{S}{M}(1 + o(1)),\n$$\nand so the variance is bounded.\n\nThe previous argument shows that the variance is $\\mathcal{O}(M^{-1}S^{-1})$. We\ncan refine that if we assume the von Mises \ncondition hold.  In that case we know that $R(r) = 1- cr^{-1/k} + o(1)$ as $r\\rightarrow \\infty$  and therefore \n\\begin{align*}\nR\\left(R^{-1}\\left(1-\\frac{M}{S}\\right)\\right) &= 1-\\frac{M}{S+1}\\\\\n1 - cR^{-1}\\left(1-\\frac{M}{S+1}\\right)^{-1/k}(1+o(1)) &= 1- \\frac{M}{S+1} \\\\\nR^{-1}\\left(1-\\frac{M}{S+1}\\right) &= c^{-k}\\left(\\frac{M}{S+1}\\right)^{-k}(1 + o(1)).\n\\end{align*}\nBickel (1967) shows that $\\mathbb{E}(r_{k:S}) = R^{-1}(1-M/(S+1)) + o(1)$ so combining\nthis with the previous result gives a variance of $\\mathcal{O}((M/S)^{k-2})$. If \nwe take $M =\\mathcal{O}(S^{1/2})$, this gives $\\mathcal{S}^{k/2-1}$, which is \nsmaller than the previous bound for $k<1$. It's worth noting that \nHence the variance goes to zero.\n\nThe argument that we used here is a modification of the argument in the TIS paper.\nThis lead to a great deal of panic: did I just make my life extremely difficult? \nCould I have modified the TIS proof to show the bias goes to zero? To be honest,\nsomeone might be able to, but I can't. \n\nSo anyway, we've proved the following theorem.\n\n::: {#thm-thm2}\nLet $\\theta_s$, $s = 1,\\ldots, S$ be an iid sample from $G$ and let $r_s = r(\\theta_s) \\sim R$. Assume that\n\n1. $R$ is absolutely continuous\n\n2. $M \\rightarrow \\infty$ and $M^{-1}S \\rightarrow 0$\n\n3. $h \\in L^2(p)$.\n\nThe variance in Winsorized importance sampling is at most $\\mathcal{O}(M^{-1}S)$.\n:::\n\n## Pareto-smoothed importance sampling\n\nPareto-smoothed importance sampling (or PSIS) takes the observation that the\ntails are approximately Pareto distributed to add some bias correction to the mix.\nEssentially, it works by noting that approximating \n$$\n(1-R(r_{S-M+1:S}))\\mathbb{E}(HR \\mid R>r_{S-M+1:S}) \\approx \\frac{1}{S}\\sum_{m=1}^M w_m h_{S-M+m:S},\n$$\nwhere $w_m$ is the median^[Or, the expected within $o(S^{-1/2})$] $m$th order statistic in an iid sample of $M$ Generalised Pareto random variables with tail parameters fitted to the distribution.\n\nThis is a ... funky ... quadrature rule. To see that, we can write \n$$\n\\mathbb{E}(HR \\mid R>T) = \\mathbb{E}(R \\mathbb{E}(H \\mid R)).\n$$\nIf we approximate the distribution of $R > T$ by \n$$\n\\tilde{R}_\\text{PSIS}(r) = \\frac{1}{M}\\sum_{m=1}^M 1( w_m<r)\n$$\nand approximate the conditional probability by \n$$\n\\Pr(H < h\\mid R = w_m) \\approx 1(h_{S-M+m:S}< h).\n$$\n\nEmpirically, this is a very good choice (with the mild caveat that you need to \ntruncate the largest expected order statistic by the observed maximum in order\nto avoid some variability issues). I would love to have a good analysis of why\nthat is so, but honest I do not.\n\nBut, to the issue of this blog post the convergence and vanishing variance\nstill holds. To see this, we note that \n$$\nw_m = r_{S-M+1}  + k^{-1}\\sigma\\left[\\left(1-\\frac{j-1/2}{M}\\right)^{-k} -1\\right].\n$$\nSo we are just re-weighting our tail $H$ samples by \n$$\n1 + \\frac{\\sigma}{kr_{S-M+1:S}}\\left[\\left(1-\\frac{j-1/2}{M}\\right)^{-k} -1\\right].\n$$\n\nRecalling that when $R(r) = 1- cr^{-1/k}(1+ o(1))$, we had $\\sigma = \\mathcal{O}(r_{S-M+1:S})$,\nthis term is at most $\\mathcal{O}(1 + M^{-k})$. This will not trouble either of our convergence proofs.\n\nThis leads to the following modification of our previous results.\n\n::: {#thm-thm3}\nLet $\\theta_s$, $s = 1,\\ldots, S$ be an iid sample from $G$ and let $r_s = r(\\theta_s) \\sim R$. Assume that\n\n1. $R$ is absolutely continuous.\n\n2. $M  = \\mathcal{O}(S^{1/2})$\n\n3. $h \\in L^2(p)$\n\n4. $k$ and $\\sigma$ are known with $\\sigma = \\mathcal{O}(r_{S-M+1:S})$.\n\nPareto smoothed importance sampling converges in $L^1$ and its variance goes to zero and \nit is consistent and asymptotically unbiased.\n:::\n\n::: {#cor-cor2}\nAssume further that\n\n1. R satisfies the von Mises condition^[The rate here is probably not optimal, but it will guarantee that the error in the Pareto approximation doesn't swamp the other terms.] \n$$\n\\frac{rR'(r)}{1-R(r)} = \\frac{1}{k}(1 +\\mathcal{O}(r^{-1})).\n$$\n\n2. $h$ is bounded^[Or $\\mathbb{E}(h(\\theta) \\mid r(\\theta) = r)$ doesn't grow to quickly, with some modification of the rates in the unlikely case that it grows polynomially.].\n\nThen the L^1 convergence occurs at a rate of of, at most, $\\mathcal{O}(S^{-1/2})$. \nFurthermore, the variance of the PSIS estimator goes to zero at least as fast as  $\\mathcal{O}(S^{k/2-1})$. \n:::\n\nHence, under these additional conditions PSIS is^[almost, there's an epsilon gap but I don't give a shit] $\\sqrt{n}$-consistent.\n\n## Final thoughts\n\nSo that's what truncation and winsorization does to importance sampling estimates.\nI haven't touched on the fairly important topic of asymptotic normality. Essentially,\n[Griffin (1988)](https://www.sciencedirect.com/science/article/pii/0304414988900312), in   a fairly complex^[I mean, the tools are elementary. It's just a lot of detailed estimates and Berry-Esseen as far as the eye can see.] paper that suggests that if you winsorize the product $(h(\\theta_s)r(\\theta_s))$ _and_ winsorize it at both ends, the von Mises condition^[and more general things] imply that the WIS estimator is asymptotically normal.\n\nWhy is this important, well the same proof shows that doubly winsorized importance sampling (dWIS) applied to the vector valued function $\\tilde h(\\theta) = (h(\\theta),1)$ will also\nbe asymptotically normal, which implies, via the delta method, that the _self normalized_ dWIS \nestimator \n$$\nI^S_\\text{SN-IS} = \\frac{\\sum_{s=1}^S\\max\\{\\min\\{h(\\theta_i) r(\\theta_i),T_{S-M+1:S}\\}, T_{M:S}\\}}{\\sum_{s=1}^S\\max\\{\\min\\{r(\\theta_i),T_{S-M+1:S}\\},T_{M:S}\\}}\n$$\nis consistent, where $T_{m:S}$ is the $m$th order statistic of $\\max\\{h(\\theta_s)r(\\theta_s), r(\\theta_s)\\}$.\n\nIt is very very likely that this can be shown (perhaps under some assumptions)\nfor something closer to the version of PSIS we use in practice. But that is an open question.\n\n",
    "supporting": [
      "that-psis-proof_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}