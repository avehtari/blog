{
  "hash": "51c8e232df6441abfc2ff459deeced38",
  "result": {
    "markdown": "---\ntitle: \"Sparse Matrices 5: I bind you Nancy\"\ndescription: |\n  A new JAX primitive? In this economy?\ndate: 2022-05-20\ncategories: [Sparse matrices, Sparse Cholesky factorisation, Python, JAX]\nimage: nancy.jpg\ntwitter-card:\n  title: \"Sparse Matrices 5: I bind you Nancy\"\n  creator: \"@dan_p_simpson\"\ncitation: \n  url: https://dansblog.netlify.app/2022-05-18-sparse4-some-primatives\n\n---\n\nThis is part _five_ of our [ongoing](https://dansblog.netlify.app/posts/2022-03-22-a-linear-mixed-effects-model/) [series](https://dansblog.netlify.app/posts/2022-03-23-getting-jax-to-love-sparse-matrices/) [on](https://dansblog.netlify.app/posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/) [implementing](https://dansblog.netlify.app/posts/2022-05-16-design-is-my-passion-sparse-matrices-part-four/) differentiable sparse\nlinear algebra in JAX. In some sense this is the last boring post before we get\nto the derivatives. Was this post going to include the derivatives? It sure was\nbut then I realised that a different choice was to go to bed so I can get up \nnice and early in the morning and vote in our election.\n\nIt goes without saying that before I split the posts, it was more than twice as long\nand I was nowhere near finished. So probably the split was a good choice.\n\n## But how do you add a primative to JAX?\n\nWell, the first step is you [read the docs.](https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html)\n\nThey tell you that you need to implement a few things:\n\n- An implementation of the call with \"abstract types\"\n- An implementation of the call with concrete types (aka evaluation the damn function)\n\n\nThen, \n\n- if you want your primitive to be JIT-able, you need to implement a compilation rule.\n\n- if you want your primitive to be batch-able, you need to implement a batching rule.\n\n- if you want your primitive to be differentiable, you need to implement the derivatives in a way that allows them to be propagated appropriately.\n\nIn this post, we are going to do the first task: we are going to register JAX-traceable\nversions of the four main primitives we are going to need for our task. For the\nmost part, the implementations here will be replaced with C++ bindings (because\nonly a fool writes their own linear algebra code). But this is the \nbeginning^[The second half of this post is half written but, to be honest, I want to go to bed more than I want to implement more derivatives, so I'm splitting the post.] of\nour serious journey into JAX.\n\n## First things first, some primitives\n\nIn JAX-speak, a primitive is a function that is JAX-traceable^[aka JAX can map out how the pieces of the function go together and it can then use that map to make its weird transformations]. It is not necessary\nfor every possible transformation to be implemented. In fact, today I'm not going to \nimplement _any_ transformations. That is a problem for future Dan.\n\nWe have enough today problems.\n\nBecause today we need to write four new primitives. \n\nBut first of all, let's build up a test matrix so we can at least check that\nthis code runs. This is the same example from [blog 3](https://dansblog.netlify.app/posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/). You can tell my PhD was\nin numerical analysis because I fucking love a 2D Laplacian.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom scipy import sparse\nimport numpy as np\n\ndef make_matrix(n):\n    one_d = sparse.diags([[-1.]*(n-1), [2.]*n, [-1.]*(n-1)], [-1,0,1])\n    A = (sparse.kronsum(one_d, one_d) + sparse.eye(n*n)).tocsc()\n    A_lower = sparse.tril(A, format = \"csc\")\n    A_index = A_lower.indices\n    A_indptr = A_lower.indptr\n    A_x = A_lower.data\n    return (A_index, A_indptr, A_x, A)\n\nA_indices, A_indptr, A_x, A = make_matrix(10)\n```\n:::\n\n\n### Primitive one: $A^{-1}b$\n\nBecause I'm feeling lazy today and we don't actually need the Cholesky directly for any of this, I'm going to just use scipy. Why? Well, honestly, just because I'm lazy. But also so I can prove an\nimportant point: the implementation of the primitive _does not_ need to be JAX\ntraceable. So I'm implementing it in a way that is not now and will likely never\nbe JAX traceable^[But mostly because although I'm going to have to implement the Cholesky and triangular solves later on down the line, I'm writing this in order and I don't wanna.]. \n\nFirst off, we need to write the solve function and bind it^[The JAX docs don't use decorators for their bindings but I use decorators because I like decorators.] to JAX. \nSpecific information about what exactly some of these commands are doing\ncan be found [in the docs](https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html#primal-evaluation-rules), but the key thing is that there is _no reason_ to\ndick around whit JAX types in any of these implementation functions. They are only ever \ncalled using (essentially) numpy^[Something something duck type. They're arrays with numbers in them that work in numpy and scipy. Get off my arse.] arrays. So we can just program\nlike normal human beings.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom jax import numpy as jnp\nfrom jax import core\n\nsparse_solve_p = core.Primitive(\"sparse_solve\")\n\ndef sparse_solve(A_indices, A_indptr, A_x, b):\n  \"\"\"A JAX traceable sparse solve\"\"\"\n  return sparse_solve_p.bind(A_indices, A_indptr, A_x, b)\n\n@sparse_solve_p.def_impl\ndef sparse_solve_impl(A_indices, A_indptr, A_x, b):\n  \"\"\"The implementation of the sparse solve. This is not JAX traceable.\"\"\"\n  A_lower = sparse.csc_array((A_x, A_indices, A_indptr)) \n  \n  assert A_lower.shape[0] == A_lower.shape[1]\n  assert A_lower.shape[0] == b.shape[0]\n  \n  A = A_lower + A_lower.T - sparse.diags(A_lower.diagonal())\n  return sparse.linalg.spsolve(A, b)\n\n## Check it works\nb = jnp.ones(100)\nx = sparse_solve(A_indices, A_indptr, A_x, b)\n\nprint(f\"The error in the sparse sovle is {np.sum(np.abs(b - A @ x)): .2e}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe error in the sparse sovle is  0.00e+00\n```\n:::\n:::\n\n\nIn order to facilitate its transformations, JAX will occasionally^[This is mostly for JIT, so it's not necessary today, but to be very honest it's the only easy thing to do here and I'm not above giving myself a participation trophy.] call functions \nusing _abstract_ data types. These data types know the shape of the inputs and \ntheir data type. So our next step is to specialise the `sparse_solve` function for this case.\nWe might as well do some shape checking while we're just hanging around. But \nthe essential part of this function is just saying that the output of $A^{-1}b$\nis the same shape as $b$ (which is usually a vector, but the code is no more \ncomplex if it's a [dense] matrix).\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom jax._src import abstract_arrays\n\n@sparse_solve_p.def_abstract_eval\ndef sparse_solve_abstract_eval(A_indices, A_indptr, A_x, b):\n  assert A_indices.shape[0] == A_x.shape[0]\n  assert b.shape[0] == A_indptr.shape[0] - 1\n  return abstract_arrays.ShapedArray(b.shape, b.dtype)\n```\n:::\n\n\n## Primitive two: The triangular solve\n\nThis is very similar. We need to have a function that computes $L^{-1}b$ and $L^{-T}b$.\nThe extra wrinkle from the last time around is that we need to pass a keyword \nargument `transpose` to indicate which system should be solved.\n\nOnce again, we are going to use the appropriate `scipy` function (in this case \n`sparse.linalg.spsolve_triangular`). There's a little bit of casting between\nsparse matrix types here as `sparse.linalg.spsolve_triangular` assumes the matrix\nis in CSR format.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nsparse_triangular_solve_p = core.Primitive(\"sparse_triangular_solve\")\n\ndef sparse_triangular_solve(L_indices, L_indptr, L_x, b, *, transpose: bool = False):\n  \"\"\"A JAX traceable sparse  triangular solve\"\"\"\n  return sparse_triangular_solve_p.bind(L_indices, L_indptr, L_x, b, transpose = transpose)\n\n@sparse_triangular_solve_p.def_impl\ndef sparse_triangular_solve_impl(L_indices, L_indptr, L_x, b, *, transpose = False):\n  \"\"\"The implementation of the sparse triangular solve. This is not JAX traceable.\"\"\"\n  L = sparse.csc_array((L_x, L_indices, L_indptr)) \n  \n  assert L.shape[0] == L.shape[1]\n  assert L.shape[0] == b.shape[0]\n  \n  if transpose:\n    return sparse.linalg.spsolve_triangular(L.T, b, lower = False)\n  else:\n    return sparse.linalg.spsolve_triangular(L.tocsr(), b, lower = True)\n```\n:::\n\n\nNow we can check if it works. We can use the fact that our matrix `(A_indices, A_indptr, A_x)`\nis lower-triangular (because we only store the lower triangle) to make our test case.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n## Check if it works\nb = np.random.standard_normal(100)\nx1 = sparse_triangular_solve(A_indices, A_indptr, A_x, b)\nx2 = sparse_triangular_solve(A_indices, A_indptr, A_x, b, transpose = True)\nprint(f\"\"\"Error in trianglular solve: {np.sum(np.abs(b - sparse.tril(A) @ x1)): .2e}\nError in triangular transpose solve: {np.sum(np.abs(b - sparse.triu(A) @ x2)): .2e}\"\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in trianglular solve:  3.53e-15\nError in triangular transpose solve:  5.08e-15\n```\n:::\n:::\n\n\nAnd we can also do the abstract evaluation.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n@sparse_triangular_solve_p.def_abstract_eval\ndef sparse_triangular_solve_abstract_eval(L_indices, L_indptr, L_x, b, *, transpose = False):\n  assert L_indices.shape[0] == L_x.shape[0]\n  assert b.shape[0] == L_indptr.shape[0] - 1\n  return abstract_arrays.ShapedArray(b.shape, b.dtype)\n```\n:::\n\n\nGreat! Now on to the next one!\n\n### Primitive three: The sparse cholesky\n\nOk. This one is gonna be a pain in the arse. But we need to do it. Why?\nBecause we are going to need a JAX-traceable version further on down the track.\n\nThe issue here is that the non-zero pattern of the Cholesky decomposition is \ncomputed _on the fly_. This is absolutely not allowed in JAX. It _must_ know the \nshape of all things at the moment it is called.\n\nThis is going to make for a somewhat shitty user experience for this function.\nIt's unavoidable with JAX designed^[This is a ... fringe problem in JAX-land, so it makes sense that there is a less than beautiful solution to the problem. I think this would be less of a design problem in Stan, \nwhere it's possible to make the number of unknowns in the\nautodiff tree depend on `int` arrays is a complex way.] the way it is. \n\nThe code in `jax.experimental.sparse.bcoo.fromdense` has this exact problem.\nIn their case, they are turning a dense matrix into a sparse matrix and they\ncan't know until they see the dense matrix how many non-zeros there are. So \nthey do the sensible thing and ask the user to specify it. They do this using\nthe `nse` keyword parameter. If you're curious what `nse` stands for, it turns\nout it's not \"non-standard evaluation\" but rather \"number of specified entries\".\nMost other systems use the abbreviation `nnz` for \"number of non-zeros\", but I'm\ngoing to stick with the JAX notation.\n\nThe one little thing we need to add to this code is a guard to make sure that if\nthe `sparse_cholesky` function is called without specifying \n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nsparse_cholesky_p = core.Primitive(\"sparse_cholesky\")\n\ndef sparse_cholesky(A_indices, A_indptr, A_x, *, L_nse: int = None):\n  \"\"\"A JAX traceable sparse cholesky decomposition\"\"\"\n  if L_nse is None:\n    err_string = \"You need to pass a value to L_nse when doing fancy sparse_cholesky.\"\n    _ = core.concrete_or_error(None, A_x, err_string)\n  return sparse_cholesky_p.bind(A_indices, A_indptr, A_x, L_nse = L_nse)\n\n@sparse_cholesky_p.def_impl\ndef sparse_cholesky_impl(A_indices, A_indptr, A_x, *, L_nse = None):\n  \"\"\"The implementation of the sparse cholesky This is not JAX traceable.\"\"\"\n  \n  L_indices, L_indptr= _symbolic_factor(A_indices, A_indptr)\n  if L_nse is not None:\n    assert len(L_indices) == nse\n    \n  L_x = _structured_copy(A_indices, A_indptr, A_x, L_indices, L_indptr)\n  L_x = _sparse_cholesky_impl(L_indices, L_indptr, L_x)\n  return L_indices, L_indptr, L_x\n```\n:::\n\n\nThe rest of the code is just the sparse Cholesky code from [blog 2](https://dansblog.netlify.app/posts/2022-03-23-getting-jax-to-love-sparse-matrices/) \nand I've hidden it under the fold. (You would think I would package this up properly, \nbut I simply haven't. Why not? Who knows^[Well, me. I'm who knows. I'm still treating this like scratch code in a notepad. Although we are moving towards the point where I'm going to have to set everything out properly. Maybe that's the next post?].)\n\n<details><summary>Click here to see the implementation</summary>\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndef _symbolic_factor(A_indices, A_indptr):\n  # Assumes A_indices and A_indptr index the lower triangle of $A$ ONLY.\n  n = len(A_indptr) - 1\n  L_sym = [np.array([], dtype=int) for j in range(n)]\n  children = [np.array([], dtype=int) for j in range(n)]\n  \n  for j in range(n):\n    L_sym[j] = A_indices[A_indptr[j]:A_indptr[j + 1]]\n    for child in children[j]:\n      tmp = L_sym[child][L_sym[child] > j]\n      L_sym[j] = np.unique(np.append(L_sym[j], tmp))\n    if len(L_sym[j]) > 1:\n      p = L_sym[j][1]\n      children[p] = np.append(children[p], j)\n        \n  L_indptr = np.zeros(n+1, dtype=int)\n  L_indptr[1:] = np.cumsum([len(x) for x in L_sym])\n  L_indices = np.concatenate(L_sym)\n  \n  return L_indices, L_indptr\n\n\n\ndef _structured_copy(A_indices, A_indptr, A_x, L_indices, L_indptr):\n  n = len(A_indptr) - 1\n  L_x = np.zeros(len(L_indices))\n  \n  for j in range(0, n):\n    copy_idx = np.nonzero(np.in1d(L_indices[L_indptr[j]:L_indptr[j + 1]],\n                                  A_indices[A_indptr[j]:A_indptr[j+1]]))[0]\n    L_x[L_indptr[j] + copy_idx] = A_x[A_indptr[j]:A_indptr[j+1]]\n  return L_x\n\ndef _sparse_cholesky_impl(L_indices, L_indptr, L_x):\n  n = len(L_indptr) - 1\n  descendant = [[] for j in range(0, n)]\n  for j in range(0, n):\n    tmp = L_x[L_indptr[j]:L_indptr[j + 1]]\n    for bebe in descendant[j]:\n      k = bebe[0]\n      Ljk= L_x[bebe[1]]\n      pad = np.nonzero(                                                       \\\n          L_indices[L_indptr[k]:L_indptr[k+1]] == L_indices[L_indptr[j]])[0][0]\n      update_idx = np.nonzero(np.in1d(                                        \\\n                    L_indices[L_indptr[j]:L_indptr[j+1]],                     \\\n                    L_indices[(L_indptr[k] + pad):L_indptr[k+1]]))[0]\n      tmp[update_idx] = tmp[update_idx] -                                     \\\n                        Ljk * L_x[(L_indptr[k] + pad):L_indptr[k + 1]]\n            \n    diag = np.sqrt(tmp[0])\n    L_x[L_indptr[j]] = diag\n    L_x[(L_indptr[j] + 1):L_indptr[j + 1]] = tmp[1:] / diag\n    for idx in range(L_indptr[j] + 1, L_indptr[j + 1]):\n      descendant[L_indices[idx]].append((j, idx))\n  return L_x\n```\n:::\n\n\n</details>\n\n\nOnce again, we can check to see if this worked!\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nL_indices, L_indptr, L_x = sparse_cholesky(A_indices, A_indptr, A_x)\nL = sparse.csc_array((L_x, L_indices, L_indptr))\nprint(f\"The error in the sparse cholesky is {np.sum(np.abs((A - L @ L.T).todense())): .2e}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe error in the sparse cholesky is  1.02e-13\n```\n:::\n:::\n\n\nAnd, of course, we can do abstract evaluation. Here is where we actually need\nto use `L_nse` to work out the dimension of our output.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n@sparse_cholesky_p.def_abstract_eval\ndef sparse_cholesky_abstract_eval(A_indices, A_indptr, A_x, *, L_nse):\n  return core.ShapedArray((L_nse,), A_indices.dtype),                   \\\n         core.ShapedArray(A_indptr.shape, A_indptr.dtype),             \\\n         core.ShapedArray((L_nse,), A_x.dtype)\n```\n:::\n\n\n## Primitive four: $\\log(|A|)$\n\nAnd now we have our final primitive: the log determinant! Wow. So much binding.\nFor this one, we compute the Cholesky factorisation and note that \n\\begin{align*}\n|A| = |LL^T| = |L||L^T| = |L|^2.\n\\end{align*}\nIf we successfully remember that the determinant of a triangular matrix is the \nproduct of its diagonal entries, we have a formula we can implement.\n\nSame deal as last time.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nsparse_log_det_p = core.Primitive(\"sparse_log_det\")\n\ndef sparse_log_det(A_indices, A_indptr, A_x):\n  \"\"\"A JAX traceable sparse log-determinant\"\"\"\n  return sparse_log_det_p.bind(A_indices, A_indptr, A_x)\n\n@sparse_log_det_p.def_impl\ndef sparse_log_det_impl(A_indices, A_indptr, A_x):\n  \"\"\"The implementation of the sparse log-determinant. This is not JAX traceable.\n  \"\"\"\n  L_indices, L_indptr, L_x = sparse_cholesky_impl(A_indices, A_indptr, A_x)\n  return 2.0 * sum(np.log(L_x[L_indptr[:-1]]))\n```\n:::\n\n\nA canny reader may notice that I'm assuming that the first element in each column\nis the diagonal. This will be true as long as the diagonal elements of $L$ are non-zero,\nwhich is true as long as $A$ is symmetric positive definite.\n\nLet's test^[Full disclosure: first time out I forgot to multiply by two. This is why we test.] it out.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nld = sparse_log_det(A_indices, A_indptr, A_x)\nLU = sparse.linalg.splu(A)\nld_true = sum(np.log(LU.U.diagonal()))\nprint(f\"The error in the log-determinant is {ld - ld_true: .2e}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe error in the log-determinant is  0.00e+00\n```\n:::\n:::\n\n\nFinally, we can do the abstract evaluation.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n@sparse_log_det_p.def_abstract_eval\ndef sparse_log_det_abstract_eval(A_indices, A_indptr, A_x):\n  return core.ShapedArray((1,), A_x.dtype)\n```\n:::\n\n\n## Where are we now but nowhere?\n\nSo we are done for today. Our next step will be to implement all of the bits that \nare needed to make the derivatives work. So in the next instalment we will differentiate \nlog-determinants, Cholesky decompositions, and all kinds of other fun things.\n\nIt should be a blast. \n\n",
    "supporting": [
      "sparse4-some-primatives_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}