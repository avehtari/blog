{
  "hash": "26547e863dbdeeb2ed997884a1f08fce",
  "result": {
    "markdown": "---\ntitle: \"Sparse Matrices 2: An invitation to a sparse Cholesky factorisation\"\ndescription: |\n  Come for the details, stay for the shitty Python, leave with disappointment. Not unlike the experience of dating me.\ndate: 2022-03-31\nimage: steven.JPG\nrepository_url: https://github.com/dpsimpson/blog/tree/master/_posts/2022-03-23-getting-jax-to-love-sparse-matrices\ncategories: [Sparse matrices, Sparse Cholesky factorisation, Python]\ntwitter-card:\n  title:  \"Sparse Matrices 2: An invitation to a sparse Cholesky factorisation\"\n  creator: \"@dan_p_simpson\"\ncitation: \n  url: https://dansblog.netlify.app/2022-03-23-getting-jax-to-love-sparse-matrices\nknitr: true\n---\n\n::: {.cell}\n\n:::\n\n\n\nThis is part two of an ongoing exercise in hubris. [Part one is here.](https://dansblog.netlify.app/posts/2022-03-22-a-linear-mixed-effects-model/)\n\n# The Choleksy factorisation\n\nSo first things first: Cholesky wasn't Russian. I don't know why I always thought he was, but \nyou know. Sometime you should do a little googling first. Cholesky was French and\ndied in the First World War.\n\nBut now that's out of the way, let's talk about matrices. If $A$^[The old numerical linear algebra naming conventions: Symmetric letters are symmetric matrices, upper case is a matrix, lower case is a vector, etc etc etc. Obviously, all conventions in statistics go against this so who really cares. Burn it all down.] \nis a symmetric positive definite matrix, then there is a unique lower-triangular matrix\n$L$ such that $A = LL^T$.\n\nLike all good theorems in numerical linear algebra, the proof of the existence of the Cholesky decomposition gives a \npretty clear algorithm for constructing $L$.  To sketch^[Go girl. Give us nothing.] it, let us see what it looks\nlike if build up our Choleksy factorisation from left to right, so the first $j-1$\ncolumns have been modified and we are looking at how to build the $j$th column. \nIn order to make $L$ lower-triangular, we need the first $j-1$ elements of the $j$th\ncolumn to be zero. Let's see if we can work out what the other columns have to be.\n\nWriting this as a matrix equation, we get\n$$\n\\begin{pmatrix} A_{11} & a_{12} & A_{32}^T \\\\\na_{12}^T & a_{22} & a_{32}^T \\\\\nA_{31} & a_{32} & A_{33}\\end{pmatrix} = \n\\begin{pmatrix} L_{11}&& \\\\\nl_{12}^T & l_{22}&\\\\\nL_{31} & l_{32} & L_{33}\\end{pmatrix}\n\\begin{pmatrix}L_{11}^T  &l_{12} & L_{31}^T\\\\\n & l_{22}&l_{32}^T\\\\\n &  & L_{33}^T\\end{pmatrix},\n$$\nwhere $L_{11}$ is lower-triangular (and $A_{11} = L_{11}L_{11}^T$) and lower-case\nletters are vectors^[or scalars] and everything is of the appropriate dimension to make $A_{11}$ \nthe top-left $(j-1) \\times (j-1)$ submatrix of $A$. \n\nIf we can find equations for $l_{22}$ and $l_{32}$ \nthat don't depend on $L_{33}$ (ie we can express them in terms of things we already know), then we have found an algorithm that marches from the \nleft of the matrix to the right leaving a Choleksy factorisation in its wake!\n\nIf we do our matrix multiplications, we get the following equation for $a_{22} = A_{jj}$:\n$$\na_{22} = l_{12}^Tl_{12} + l_{22}^2.\n$$\nRearranging, we get\n$$\nl_{22}  = \\sqrt{a_{22} - l_{12}^Tl_{12}}.\n$$\nThe canny amongst you will be asking \"yes but is that a real number\". The answer\nturns out to be \"yes\" for all diagonals if and only if^[This is actually how you check if a matrix is SPD. Such a useful agorithm!] $A$ is symmetric positive definite.\n\nOk! We have expressed $l_{22}$ in terms of things we know, so we are half way there. Now to attack\nthe vector $l_{3,2}$. Looking at the (3,2) equation implied by the above block matrices, we get \n$$\na_{32} = L_{31}l_{12} + l_{32} l_{22}.\n$$\nRemembering that $l_{22}$ is a scalar (that we have already computed!), we get \n$$\nl_{32} = (a_{32} - L_{31}l_{12}) / l_{22}.\n$$\n\nSuccess!\n\nThis then gives us the^[This variant is called the left-looking Cholesky. There are 6 distinct ways to rearrange these computations that lead to algorithms that are well-adapted to different structures. The left-looking algorithm is well adapted to matrices stored column-by-column. But it is not the only one! The variant of the sparse Cholesky in Matlab and Eigen is the upward-looking Cholesky. CHOLMOD uses the left-looking Cholesky (because that's how you get supernodes). MUMPS uses the right-looking variant. Honestly this is a fucking fascinating wormhole you can fall down. A solid review of some of the possibilities is in Chapter 4 of Tim Davis' book.] Cholesky factorisation^[Here `A` is a $n\\times n$ matrix and  `u'` is the transpose of the vector `u`.]:\n\n```\nfor j in range(0,n) (using python slicing notation because why notation)\n  L[j,j] = sqrt(A[j,j] - L[j, 1:(j-1)] * L[j, 1:(j-1)]')\n  L[(j+1):n, j] = (A[(j+1):n, j] - L[(j+1):n, 1:(j-1)] * L[j, 1:(j-1)]') / L[j,j]\n```\n\nEasy as. \n\nWhen $A$ is a dense matrix, this costs $\\mathcal{O}(n^3)$ floating point operations^[You can also see that if $A$ is stored in memory by stacking the columns, this algorithm is set up to be fairly memory efficient. Of course, if you find yourself caring about what your cache is doing, you've gone astray somewhere. That is why professionals have coded this up (only a fool competes with LAPACK).].\n\nSo how can we take advantage of the observation that most of the entries of $A$ \nare zero (aka $A$ is a sparse matrix)?\nWell. That is the topic of this post. In order, we are going to look at the \nfollowing:\n\n1. Storing a sparse matrix so it works with the algorithm\n2. How sparse is a Cholesky factor?\n3. Which elements of the Cholesky factor are non-zero (aka symbolic factorisation)\n4. Computing the Cholesky factorisation\n5. ~~What about JAX? (or: fucking immutable arrays are trying to ruin my fucking life)~~ (This did not happen. Next time. The post is long enough.)\n\n## So how do we store a sparse matrix?\n\nIf we look at the Cholesky algorithm, we notice that we are scanning through the matrix column-by-column. When a computer stores a matrix, it stores it as a long 1D array with some side information. How this array is constructed from the matrix depends on the language.\n\nThere are (roughly) two options: column-major or row-major storage. Column major storage (used by Fortran^[The ultimate language of scientific computing. Do not slide into my DMs and suggest Julia is.], R, Matlab, Julia, Eigen, etc) stacks a matrix column by column. A small example: \n$$\n\\begin{pmatrix}1&3&5\\\\2&4&6 \\end{pmatrix} \\Rightarrow [1,2,3,4,5,6].\n$$\nRow-mjor ordering (C/C++ arrays, SAS, Pascal, numpy^[You may be thinking _well surely we have to use a row-major ordering_. But honey let me tell you. We are building our own damn storage method, so we can order it however we bloody want. Also, somewhere down the line I'm going to do this in Eigen, which is column major by default.]) stores things row-by-row.\n\nWhich one do we use? Well. If you look at the Cholesky algorithm, it scans through the matrix column-by-column. It is much much much more memory efficient in this case to have the whole column available in one contiguous chunk of memory. So we are going to use column-major storage.\n\nBut there's an extra wrinkle: Most of the entries in our matrix are zero. It would be very inefficient to store all of those zeros. You may be sceptical about this, but it's true. It helps to realize that even in the examples at the bottom of this post that are not trying very hard to minimise the fill in, only 3-4% of the potential elements in $L$ are non-zero.\n\nIt is far more efficient to just store the locations^[If you look at the algorithm, you'll see that we only need to store the diagonal and the entries below. This is enough (in general) because we know the matrix is symmetric!] of the non-zeros and their values. If only 4% of your matrix is non-zero, you are saving^[CPU operations are a lot less memory-limited than they used to be, but nevertheless it piles up. GPU operations still very much are, but sparse matrix operations mostly don't have the arithmetic intensity to be worth putting on a GPU.] a lot of memory!\n\nThe storage scheme we are inching towards is called _compressed sparse column (CSC)_ storage. This stores the matrix in three arrays. The first array `indices` (which has as many entries as there are non-zeros) stores the row numbers for each non-zero element. So if \n$$\nB = \\begin{pmatrix}\n1 &&5 \\\\\n2&3& \\\\\n&4&6\n\\end{pmatrix}\n$$ \nthen (using zero-based indices because I've to to make this work in Python)\n\n\n::: {.cell}\n\n```{.python .cell-code}\nB_indices = [0,1,1,2,0,3]\n```\n:::\n\n\nThe second array `indptr` is an $n+1$-dimensional array that indexes the first element of each row. The final element of `indptr` is `nnz(B)`^[(NB: zero-based indexing!) This is a superfluous entry (the information is available elsewhere), but having it in makes life just a million times easier because you don't have to treat the final column separately!.]. This leads to \n\n\n::: {.cell}\n\n```{.python .cell-code}\nB_indptr = [0,2,4,6]\n```\n:::\n\n\nThis means that the entries in column^[ZERO BASED, PYTHON SLICES] j are have row numbers\n\n\n::: {.cell}\n\n```{.python .cell-code}\nB_indices[B_indptr[j]:B_indptr[j+1]]\n```\n:::\n\n\nThe third and final array is `x`, which stores the _values_ of the non-negative entries of $A$ _column-by-column_. This gives \n\n\n::: {.cell}\n\n```{.python .cell-code}\nB_x = [1,2,3,4,5,6]\n```\n:::\n\n\nUsing these three arrays we can get access to the `j`th row of $B$ by accessing \n\n\n::: {.cell}\n\n```{.python .cell-code}\nB_x[B_indptr[j]:B_indptr[j+1]]\n```\n:::\n\n\nThis storage scheme is very efficient for what we are about to do. But it is fundamentally a static scheme: it is _extremely_ expensive to add a new non-zero element. There are other sparse matrix storage schemes that make this work better.\n\n## How sparse is a Cholesky factor of a sparse matrix?\n\nOk. So now we've got that out of the way, we need to work out the sparsity structure of a Choleksy factorisation. At this point we need to close our eyes, pray, and start thinking about graphs.\n\nWhy graphs? I promise, it is not because I love discrete^[I am not a headless torso that can't host. I differentiate.] maths. It is because symmetric sparse matrices are strongly related to graphs. \n\nTo remind people, a graph^[We only care about undirected graphs] (in a mathematical sense) $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ consists of two lists:\n\n1. A list of vertices $\\mathcal{V}$ numbered from $1$ to $n$^[Or from $0$ to $n-1$ if you have hate in your heart and darkness in your soul.]. \n2. A list of edges $\\mathcal{E}$ in the graph (aka all the pairs $(i,j)$ such that $i<j$ and there is an edge between $i$ and $j$).\n\nEvery symmetric sparse matrix $A$ has a graph naturally associated with it. The relationship is that $(i,j)$ (for $i\\neq j$) is an edge in $\\mathcal{G}$ if and only if $A_{ij} \\neq 0$.  \n\nSo, for instance, if\n$$\nA = \\begin{pmatrix}\n1&2&&8 \\\\\n2&3&& 5\\\\\n&&4&6 \\\\\n8&5&6&7\n\\end{pmatrix},\n$$\n\nthen we can plot the associated graph, $\\mathcal{G}$.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](getting-jax-to-love-sparse-matrices_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nBut why do we care about graphs? \n\nWe care because they let us answer our question for this section: _which elements of the Cholesky factor $L$ are non-zero?_\n\nIt is useful to write the algorithm out for a second time^[To get from the previous version of the algorithm to this, we unwound all of those beautiful vectorised matrix-vector products. This would be a terrible idea if we were doing a dense Cholesky, but as general rule if you are implementing your own dense Cholesky factorisation you have already committed to a terrible idea. (The same, to be honest, is true for sparse Choleskys. But nevertheless, she persisted.)], but this time closer to how we will implement it.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nL = np.tril(A)\nfor j in range(n):\n  for k in range(j-1):\n    L[j:n, j] -= L[j, k] * L[j:n, k]\n  L[j,j]= np.sqrt(L[j,j])\n  L[j+1:n, j] = L[j+1:n] / L[j, j]\n```\n:::\n\n\nIf we stare at this long enough we can work out when $L_{ij}$ is going to be potentially non-zero.\n\nAnd here is where we have to take a quick zoom out. We are _not_ interested if the numerical entry $L_{ij}$ is _actually_ non-zero. We are interested if it _could be_ non-zero. Why? Because this will allow us to set up our storage scheme for the sparse Cholesky factor. And it will tell us exactly which bits of the above loops we actually need to do!\n\nSo with that motivation in mind, can we spot the non-zeros? Well. I'll be honest with you. I struggle at this game. This is part of why I do not like thinking about graphs^[or trees or really any discrete structure.]. But with a piece of paper and a bit of time, I can convince myslef that $L_ij$ is potentially non-zero (or a _structural_ non-zero) if:\n\n- $A_{ij}$ is non-zero (because `tmp[i-j]` is non-zero!), or\n- $L_{ik} \\neq 0$ _and_ $L_{jk} \\neq 0$ for some $k < \\min\\{i, j\\}$ (because that is the only time an element of `tmp` is updated through `tmp[i] = tmp[i] - L[i, k] * L[j, k]`)\n\nIf we dig into the second condition a bit more,^[Don't kid yourself, [we look this shit up](https://epubs.siam.org/doi/10.1137/0205021).] we notice that the second case can happen if and only if there is a path in $\\mathcal{G}$^[This means that all of the pairs $(i, v_1)$, $(v_i, v_{i+1})$ and $(v_{\\ell-1}, v_j)$ are all in the edge set $\\mathcal{E}$] from node $i$ to node $j$ \n$$\ni \\rightarrow v_1 \\rightarrow v_2 \\rightarrow \\ldots \\rightarrow v_{\\ell-1} \\rightarrow j\n$$ \nwith $v_1, \\ldots v_{\\ell-1} < \\min\\{i,j\\}$. The proof is an induction on $\\min\\{i,j\\}$ that I can't be arsed typing out.\n\n(As an aside, Theorem 2.8 in [Rue and Held's book](https://www.routledge.com/Gaussian-Markov-Random-Fields-Theory-and-Applications/Rue-Held/p/book/9781584884323) gives a very clearn nice statistical proof of this result.)\n\nThis is enough to see that fill in patterns are going to be a complex thing.\n\n### A toy example\n\nConsider the following graph\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](getting-jax-to-love-sparse-matrices_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nIt's pretty clear that there is a path between $(i,j)$ for every pair $(i,j)$ (the path goes through the fully connected vertex, which is labelled `1`).\n\nAnd indeed, we can check this numerically^[The specific choices building this matrix are to make sure it's positive definite. The transpose is there because in R, `R <- chol(A)` returns an _upper_ triangular matrix that satisfies $A = R^TR$. I assume this is because C has row-major storage, but I honestly don't care enough to look it up.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Matrix)\nn <- 6\nA <- sparseMatrix(i = c(1:n, rep(1,n)), \n                  j = c(rep(1,n),1:n), \n                  x = -0.2, \n                  dims = c(n,n)) + \n      Diagonal(n)\nA != 0 #print the non-zero structrure\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n6 x 6 sparse Matrix of class \"lgCMatrix\"\n                \n[1,] | | | | | |\n[2,] | | . . . .\n[3,] | . | . . .\n[4,] | . . | . .\n[5,] | . . . | .\n[6,] | . . . . |\n```\n:::\n\n```{.r .cell-code}\nL = t(chol(as.matrix(A))) # transpose is for R reasons\nround(L, digits = 1) # Fully dense!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]  0.8  0.0  0.0  0.0  0.0    0\n[2,] -0.3  1.0  0.0  0.0  0.0    0\n[3,] -0.3 -0.1  1.0  0.0  0.0    0\n[4,] -0.3 -0.1 -0.1  1.0  0.0    0\n[5,] -0.3 -0.1 -0.1 -0.1  1.0    0\n[6,] -0.3 -0.1 -0.1 -0.1 -0.1    1\n```\n:::\n:::\n\n\nBut what if we changed the labels of our vertices? What is the fill in pattern implied by a labelling where the fully collected vertex is labelled last instead of first?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](getting-jax-to-love-sparse-matrices_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nThere are now _no paths_ from $i$ to $j$ that only go through lower-numbered vertices. So there is no fill in! We can check this numerically!^[Here the `pivot = FALSE` option is needed because the default for a sparse Cholesky decomposition in R is to re-order the vertices to try to minimise the fill-in. But that goes against the example!]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA2 <- A[n:1,n:1]\nL2 <- t(chol(A2))\nL2!=0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n6 x 6 sparse Matrix of class \"ltCMatrix\"\n                \n[1,] | . . . . .\n[2,] . | . . . .\n[3,] . . | . . .\n[4,] . . . | . .\n[5,] . . . . | .\n[6,] | | | | | |\n```\n:::\n:::\n\n\n### So what is the lesson here? \n\nThe lesson is that the sparse Cholesky algorithm cares _deeply_ about what order the rows and columns of the matrix are in. This is why, [in the previous post](https://dansblog.netlify.app/posts/2022-03-22-a-linear-mixed-effects-model/), we put the dense rows and columns of $Q_{u \\mid y, \\theta}$ at the _end_ of the matrix!\n\nLuckily, a lot of clever graph theorists got on the job a while back and found a number of good algorithms for finding decent^[Finding the minimum fill reordering is NP-hard, so everything is heuristic.] ways to reorder the vertices of a graph to minimise fill in. There are two particularly well-known reorderings: the approximate minimum degree (AMD) reordering and the nested-dissection reordering. Neither of these are easily available in Python^[scipy has the reverse Cuthill-McKee reordering---which is shit---easily available. As far as I can tell, the easiest way to get AMD out is to factorise a sparse matrix in scipy and pull the reordering out. If I were less lazy, I'd probably just bind SuiteSparse's AMD algorithm, which is permissively licensed. But nah. The standard nested-dissection implementation is in the METIS package, which used to have a shit license but is now Apache2.0. Good on you METIS!]. \n\nAMD is a bog-standard black box that is a greedy reordering that tries to label the next vertex so that graph you get after removing that vertex and adding edges between all of the nodes that connect to that vertex isn't too fucked.\n\nNested dissection tries to generalise the toy example above by finding nodes that separate the graph into two minimally connected components. The separator node is then labelled last. The process is repeated until you run out of nodes. This algorithm can be very efficient in some cases (eg if the graph is planar^[and some other cases], the sparse Cholesky algorithm using this reordering [provably costs](https://link.springer.com/article/10.1007/BF01396660) at most $\\mathcal{O}(n^{3/2})$).\n\nTypically, you compute multiple reorderings^[They are cheap to compute] and pick the one that results in the least fill in.\n\n## Which elements of the Cholesky factor are non-zero (aka symbolic factorisation)\n\nOk. So I guess we've got to work out an algorithm for computing the non-zero structure of a sparse Cholesky factor. Naively, this seems easy: just use the Cholesky algorithm and mark which elements are non-zero.\n\nBut this is slow and inefficient. You're not thinking like a programmer! Or a graph theorist. So let's talk about how to do this efficiently.\n\n### The elimination tree\n\nLet's consider the graph $\\mathcal{G}_L$ that contains the sparsity pattern of $L$. We _know_ that the non-zero structure consists of all $(i,j)$ such that $i < j$ and there is a path $in \\mathcal{G}$ from $i$ to $j$.  This means we could just compute that and make $\\mathcal{G}_L$. \n\nThe thing that you should notice immediately is that there is a lot of redundancy in this structure. Remember that if $L_{ik}$ is non-zero and $L_{jk}$ is also non-zero, then $L_{ij}$ is also non-zero. \n\nThis suggests that if we have $(i,k)$ and $(j,k)$ in the graph, we can remove the edge $(i,j)$ from $\\mathcal{G}_L$ and still be able to work out that $L_{ij}$ is non-zero. This new graph is no longer the graph associated with $L$ but, for our purposes, it contains the same information.\n\nIf we continue pruning the graph this way, we are going to end up with a^[Actually, you get a forest in general. You get a tree if $\\mathcal{G}$ has a single connected component, otherwise you get a bunch of disjoint trees. But we still call it a tree because maths is wild.] rooted tree! From this tree, which is called the _elimination tree_ of $A$^[Fun fact: it is the spanning tree of the graph of $L + L^T$. Was that fun? I don't think that was fun.] we can easily work out the non-zero structure of $L$.\n\nThe elimination tree is the fundamental structure needed to build an efficient sparse Cholesky algorithm. We are not going to use it to its full potential, but it is very cheap to compute (roughly^[This is morally but not actually true. There is a variant (slower in practice, faster asymptotically), that costs $\\mathcal{O}\\left(\\operatorname{nnz}(A)\\alpha(\\operatorname{nnz}(A), n)\\right)$, where $\\alpha(m,n)$ is the inverse Ackerman function, which is a very slowly growing function that is always equal to 4 for our purposes. The actual version that people use is technically $\\mathcal{O}(\\operatorname{nnz}(A) \\log n)$, but is faster and the $\\log n$ is never seen in practice.] $\\mathcal{O}(\\operatorname{nnz}(A))$ operations). \n\nOnce we have the elimination tree, it's cheap to compute properties of $L$ like the number of non-zeros in a column, the exact sparsity pattern of every column, which columns can be grouped together to form supernodes^[This is beyond the scope, but basically it's trying to find groups of nodes that can be eliminated as a block using dense matrix operations. This leads to a much more efficient algorithm.], and the approximate minimum degree reordering.\n\nAll of those things would be necessary for a modern, industrial-strength sparse Cholesky factorisation. But, and I cannot stress this enough, fuck that shit. \n\n### The symbolic factorisation \n\nWe are doing the easy version. Which is to say I _refuse_ to do anything here that couldn't be easily done in the early 90s. Specifically, we are going to use the version of this that[ George, Liu, and Ng](http://heath.cs.illinois.edu/courses/cs598mh/george_liu.pdf) wrote about^[There is, of course, a typo in the algorithm we're about to implement. We're using the correct version from [here](https://epubs.siam.org/doi/10.1137/0611010).] in the 90s. Understanding this is, I think, enough to see how things like supernodal factorisations work, but it's so much less to keep track of.\n\nThe nice thing about this method is that we compute the elimination tree implicitly as we go along.\n\nLet $\\mathcal{L}_j$ be the non-zero entries in the $j$th column of $L$. Then our discussion in the previous section tells us that we need to determine the _reach_ of the node i \n$$\n\\text{Reach}(j, S_j) = \\left\\{i: \\text{there is a path from } i\\text{ to }j\\text{ through }S_j\\right\\},\n$$ \nwhere $S_j = \\{1,\\ldots, j-1\\}$.\n\nIf we can compute the reach, then $\\mathcal{L}_j  = \\text{Reach}(j, S_j) \\cup\\{j\\}$!\n\nThis is where the elimination tree comes in: it is an efficient representation of these sets. Indeed, $i \\in \\text{Reach}(j, S_j)$ _if and only if_ there is a directed^[from parent to child (aka in descending node order)] path from $j$ to $i$ in the elimination tree! Now this tree is ordered^[by construction] so that if $i$ is a child of $j$ (aka directly below it in the tree), then $i < j$. This means that its column in the Cholesky factorisation has already been computed. So all of the nodes that can be reached from $j$ by going through $i$ are in $\\mathcal{L}_{i} \\cap \\{j+1, \\ldots, n\\}$. \n\nThis means that we can compute the non-zeros of the $j$th column of $L$ efficiently from the non-zeros of all of the (very few, hopefully) columns associated with the child nodes of $j$. \n\nSo all that's left is to ask \"how can we find the child?\" (as phones around the city start buzzing). Well, a little bit of thinking time should convince you that if \n$$\np = \\min\\{i : i \\in \\text{Reach}(j, S_j) \\},\n$$ \nthen $p$ is the parent of $i$. Or, the parent of column $j$ is the index of its first^[If there are no non-zeros below the diagonal, then we have a root of one of the trees in the forest!] non-zero below the diagonal.\n\nWe can put all of these observations together into the following algorithm. We\nassume that we are given the non-zero structure of `tril(A)` (aka the lower-triangle\nof $A$).\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\n\ndef _symbolic_factor_csc(A_indices, A_indptr):\n  # Assumes A_indices and A_indptr index the lower triangle of $A$ ONLY.\n  n = len(A_indptr) - 1\n  L_sym = [np.array([], dtype=int) for j in range(n)]\n  children = [np.array([], dtype=int) for j in range(n)]\n  \n  for j in range(n):\n    L_sym[j] = A_indices[A_indptr[j]:A_indptr[j + 1]]\n    for child in children[j]:\n      tmp = L_sym[child][L_sym[child] > j]\n      L_sym[j] = np.unique(np.append(L_sym[j], tmp))\n    if len(L_sym[j]) > 1:\n      p = L_sym[j][1]\n      children[p] = np.append(children[p], j)\n        \n  L_indptr = np.zeros(n+1, dtype=int)\n  L_indptr[1:] = np.cumsum([len(x) for x in L_sym])\n  L_indices = np.concatenate(L_sym)\n  \n  return L_indices, L_indptr\n  \n```\n:::\n\n\nThis was the first piece of Python I've written in about 13 years^[I did not make it prettier because a) I think it's useful to show bad code sometimes, and b) I can't be arsed. The real file has some comments in it because I am not a monster, but in some sense this whole damn blog is a code comment.], so it's a bit shit. Nevertheless, it works.  It is possible to replace the `children` structure by a linked list implemented in an n-dimensional integer array^[The George, Liu, Ng book does that in FORTRAN. Enjoy decoding it.], but why bother. This function is run once.\n\nIt's also worth noting that the `children` array expresses the elimination tree. If we were going to do something with it explicitly, we could just spit it out and reshape it into a more useful data structure. \n\nThere's one more piece of tedium before we can get to the main event: we need to do a deep copy of $A$ into the data structure of $L$. There is no^[Well, there is some avoiding this. If the amount of fill in is small, it may be more efficient to do insertions instead. But again, I am not going to bother. And anyway. If `A_x` is a JAX array, it's going to be immutable and we are not going to be able to avoid the deep copy.] avoiding this.\n\nHere is the code.\n\n::: {.cell}\n\n```{.python .cell-code}\ndef _deep_copy_csc(A_indices, A_indptr, A_x, L_indices, L_indptr):\n  n = len(A_indptr) - 1\n  L_x = np.zeros(len(L_indices))\n  \n  for j in range(0, n):\n    copy_idx = np.nonzero(np.in1d(L_indices[L_indptr[j]:L_indptr[j + 1]],\n                                  A_indices[A_indptr[j]:A_indptr[j+1]]))[0]\n    L_x[L_indptr[j] + copy_idx] = A_x[A_indptr[j]:A_indptr[j+1]]\n  return L_x\n```\n:::\n\n\n## Computing the Cholesky factorisation\n\nIt feels like we've been going for a really long time and we still don't have a \nCholesky factorisation. Mate. I feel your pain. Believe me.\n\nBut we are here now: everything is in place. We can now write down the Cholesky algorithm!\n\nThe algorithm is as it was before, with the main difference being that we now \nknow two things:\n\n1. We only need to update `tmp` with descendent of `j` in the elimination tree.\n2. That's it. That is the only thing we know.\n\nOf course, we could use the elimination tree to do this very efficiently, but, _as \nper my last email_, I do not care. So we will simply build up a copy of all of \nthe descendants. This will obviously be less efficient, but it's fine for our purposes.\nLet's face it, we're all going to die eventually.\n\nSo here it goes.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef _sparse_cholesky_csc_impl(L_indices, L_indptr, L_x):\n    n = len(L_indptr) - 1\n    descendant = [[] for j in range(0, n)]\n    for j in range(0, n):\n        tmp = L_x[L_indptr[j]:L_indptr[j + 1]]\n        for bebe in descendant[j]:\n            k = bebe[0]\n            Ljk= L_x[bebe[1]]\n            pad = np.nonzero(                                                \\\n              L_indices[L_indptr[k]:L_indptr[k+1]] == L_indices[L_indptr[j]])[0][0]\n            update_idx = np.nonzero(np.in1d(                                 \\\n              L_indices[L_indptr[j]:L_indptr[j+1]],                          \\\n              L_indices[(L_indptr[k] + pad):L_indptr[k+1]]))[0]\n            tmp[update_idx] = tmp[update_idx] -                              \\\n              Ljk * L_x[(L_indptr[k] + pad):L_indptr[k + 1]]\n            \n        diag = np.sqrt(tmp[0])\n        L_x[L_indptr[j]] = diag\n        L_x[(L_indptr[j] + 1):L_indptr[j + 1]] = tmp[1:] / diag\n        for idx in range(L_indptr[j] + 1, L_indptr[j + 1]):\n            descendant[L_indices[idx]].append((j, idx))\n    return L_x\n```\n:::\n\n\nThe one thing that you'll note in this code^[and in the deep copy code] is that \nwe are implicitly using things that we know about the sparsity structure of the\n$j$th column. In particular, we _know_ that the sparsity structure of the $j$th\ncolumn is the _union_ of the relevant parts of the sparsity structure of their dependent\ncolumns. This allows a lot of our faster indexing to work.\n\nFinally, we can put it all together.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef sparse_cholesky_csc(A_indices, A_indptr, A_x):\n    L_indices, L_indptr= _symbolic_factor_csc(A_indices, A_indptr)\n    L_x = _deep_copy_csc(A_indices, A_indptr, A_x, L_indices, L_indptr)\n    L_x = _sparse_cholesky_csc_impl(L_indices, L_indptr, L_x)\n    return L_indices, L_indptr, L_x\n```\n:::\n\n\nRight. Let's test it. We're going to work on a particular^[This is the discretisation of a 2D laplacian on a square with some specific boundary conditions] sparse matrix.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy import sparse\n\nn = 50\none_d = sparse.diags([[-1.]*(n-1), [2.]*n, [-1.]*(n-1)], [-1,0,1])\nA = sparse.kronsum(one_d, one_d) + sparse.eye(n*n)\nA_lower = sparse.tril(A, format = \"csc\")\nA_indices = A_lower.indices\nA_indptr = A_lower.indptr\nA_x = A_lower.data\n\nL_indices, L_indptr, L_x = sparse_cholesky_csc(A_indices, A_indptr, A_x)\nL = sparse.csc_array((L_x, L_indices, L_indptr), shape = (n**2, n**2))\n\nerr = np.sum(np.abs((A - L @ L.transpose()).todense()))\nprint(f\"Error in Cholesky is {err}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in Cholesky is 3.871041263071504e-12\n```\n:::\n\n```{.python .cell-code}\nnnz = len(L_x)\nprint(f\"Number of non-zeros is {nnz} (fill in of {len(L_x) - len(A_x)})\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of non-zeros is 125049 (fill in of 117649)\n```\n:::\n:::\n\n\nFinally, let's demonstrate that we can reduce the amount of fill-in with a reordering.\nObviously, the built in permutation in `scipy` is crappy, so we will not see much of a difference. But nevertheless. It's there.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nperm = sparse.csgraph.reverse_cuthill_mckee(A, symmetric_mode=True)\nprint(perm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[2499 2498 2449 ...   50    1    0]\n```\n:::\n\n```{.python .cell-code}\nA_perm = A[perm[:,None], perm]\nA_perm_lower = sparse.tril(A_perm, format = \"csc\")\nA_indices = A_perm_lower.indices\nA_indptr = A_perm_lower.indptr\nA_x = A_perm_lower.data\n\nL_indices, L_indptr, L_x = sparse_cholesky_csc(A_indices, A_indptr, A_x)\nL = sparse.csc_array((L_x, L_indices, L_indptr), shape = (n**2, n**2))\nerr = np.sum(np.abs((A_perm - L @ L.transpose()).todense()))\nprint(f\"Error in Cholesky is {err}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in Cholesky is 3.0580421951974465e-12\n```\n:::\n\n```{.python .cell-code}\nnnz_rcm = len(L_x)\nprint(f\"Number of non-zeros is {nnz_rcm} (fill in of {len(L_x) - len(A_x)}),\\nwhich is less than the unpermuted matrix, which had {nnz} non-zeros.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of non-zeros is 87025 (fill in of 79625),\nwhich is less than the unpermuted matrix, which had 125049 non-zeros.\n```\n:::\n:::\n\n\nAnd finally, let's check that we've not made some fake non-zeros. To do this we\nneed to wander back into `R` because `scipy` doesn't have a sparse Cholesky^[Cholmod, which is the natural choice, is GPL'd, which basically means it can't be used in something like Scipy. R does not have this problem.] factorisation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nind <- py$A_indices\nindptr <- py$A_indptr\nx <- as.numeric(py$A_x)\nA = sparseMatrix(i = ind + 1, p = indptr, x=x, symmetric = TRUE)\n\nL = t(chol(A))\nsum(L@i - py$L_indices)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\nsum(L@p - py$L_indptr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\nPerfect.\n\n## Ok we are done for today.\n\nI was hoping that we were going to make it to the JAX implementation, but this \nis long enough now. And I suspect that there will be some _issues_ that are going to \ncome up.\n\nIf you want some references, I recommend:\n\n- [George, Liu, and Ng's notes](http://heath.cs.illinois.edu/courses/cs598mh/george_liu.pdf) (warning: FORTRAN).\n- [Timothy Davis' book](https://epubs.siam.org/doi/book/10.1137/1.9780898718881) (warning: pure C).\n- Liu's [survey paper about elimination trees](https://epubs.siam.org/doi/10.1137/0611010) (warning: trees).\n- [Rue and Held's book](https://www.routledge.com/Gaussian-Markov-Random-Fields-Theory-and-Applications/Rue-Held/p/book/9781584884323) (Statistically motivated).\n\nObviously this is a massive area and I obviously did not do it justice in a single blog post. It's well worth looking further into. It is very cool. And obviously, _I go through all this_^[Björk voice] to get a prototype that I can play with all of the bits of. For the love of god, use Cholmod or Eigen or MUMPS or literally anything else. The only reason to write these yourself is to learn how to understand it.",
    "supporting": [
      "getting-jax-to-love-sparse-matrices_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}