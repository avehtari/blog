{
  "hash": "b8f06dae2b00bfe7d50cbed8105a9abb",
  "result": {
    "markdown": "---\ntitle: \"Sparse Matrices 4: Design is my passion\"\ndescription: |\n  Just some harmeless notes. Like the ones Judy Dench took in that movie.\ndate: 2022-05-16\nimage: scrod.JPG\ntwitter-card:\n  title: \"Sparse Matrices 4: Design is my passion\"\n  creator: \"@dan_p_simpson\"\ncitation: \n  url: https://dansblog.netlify.app/2022-05-16-design-is-my-passion-sparse-matrices-part-four\n\n---\n\nThis is the fourth post in a series where I try to squeeze autodiffable sparse \nmatrices into JAX with the aim to speed up some model classes in PyMC. So far,\nI have:\n\n- Outlined the problem [Post 1](https://dansblog.netlify.app/posts/2022-03-22-a-linear-mixed-effects-model/)\n- Worked through a basic python implementation of a sparse Cholesky decomposition [Post 2](https://dansblog.netlify.app/posts/2022-03-23-getting-jax-to-love-sparse-matrices/)\n- Failed to get JAX to transform some numpy code into efficient, JIT-compileable code [Post 3](https://dansblog.netlify.app/posts/2022-05-14-jax-ing-a-sparse-cholesky-factorisation-part-3-in-an-ongoing-journey/)\n\nI am in the process of writing a blog on building new primitives^[functions that have explicit transformations written for them (eg explicit instruction on how to JIT or how to differentiate)] into JAX, but \nas I was doing it I accidentally wrote a long section about options for exposing\nsparse matrices. It really didn't fit very well into that blog, so here it is.\n\n\n## What are we trying to do here?\n\nIf you recall from [the first blog](), we need to be able to compute the value\nand gradients of the (un-normalised) log-posterior \n$$\n\\log(p(\\theta \\mid y)) = \\frac{1}{2} \\mu_{u\\mid y, \\theta}(\\theta)^TA^TW^{-1}y + \\frac{1}{2} \\log(|Q(\\theta)|) - \\frac{1}{2}\\log(|Q_{u\\mid y, \\theta}(\\theta)|) + \\text{const},\n$$\nwhere $Q(\\theta)$ is a sparse matrix, and \n$$\n\\mu_{u\\mid y, \\theta}(\\theta) = \\frac{1}{\\sigma^2} Q_{u\\mid y,\\theta}(\\theta)^{-1} A^TW^{-1}y.\n$$\n\nOverall, our task is to design a system where this un-normalised log-posterior can be\nevaluated and differentiated efficiently. As with all design problems, there are\na lot of different ways that we can implement it. They share a bunch of similarities,\nso we will actually end up implementing the guts of all of the systems. \n\nTo that end, let's think of all of the ways we can implement our target^[I get sick of typing \"unnormalised log-posterior\"].\n\n\n## Option 1: The direct design \n\n- $A \\rightarrow \\log(|A|)$, for a sparse, symmetric positive definite matrix $A$ \n- $(A,b) \\rightarrow A^{-1}b$, for a sparse, symmetric positive definite matrix $A$ and a vector $b$\n\nThis option is, in some sense, the most straightforward. We implement primitives\nfor both of the major components of our target and combine them using existing JAX \nprimitives (like addition, scalar multiplication, and dot products).\n\nThis is a bad idea.\n\nThe problem is that both primitives require the Cholesky decomposition of $A$, so \nif we take this route we might end up computing an extra Cholesky decomposition.\nAnd you may ask yourself: _what's an extra Cholesky decomposition between friends?_\n\nWell, Jonathan, it's the most expensive operation we are doing for these models, so \nperhaps we should avoid the 1/3 increase in running time!\n\nThere are some ways around this. We might implement sparse, symmetric positive \ndefinite matrices as a class that, upon instantiation, computes the Cholesky \nfactorisation. \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nclass SPDSparse: \n  def __init__(self, A_indices, A_indptr, A_x):\n    self._perm, self._iperm = _find_perm(A_indices, A_indptr)\n    self._A_indices, self._A_indptr, self._A_x = _twist(self._perm, A_indices, A_indptr, A_x)\n    try:\n      self._L_indices, self._L_indptr, self._L_x = _compute_cholesky()\n    except SPDError:\n      print(\"Matrix is not symmetric positive definite to machine precision.\")\n  \n  def _find_perm(self, indices, indptr):\n    \"\"\"Finds the best fill-reducing permutation\"\"\"\n    raise NotImplemented(\"_find_perm\")\n  \n  def _twist(self, perm, indices, indptr, x):\n    \"\"\"Returns A[perm, perm]\"\"\"\n    raise NotImplemented(\"_twist\")\n  \n  def _compute_cholesky():\n    \"\"\"Compute the Cholesky decomposition of the permuted matrix\"\"\"\n    raise NotImplemented(\"_compute_cholesky\")\n  \n  # Not pictured: a whole forest of gets\n```\n:::\n\n\nIn contexts where we need a Cholesky decomposition of every SPD matrix we instantiate,\nthis design might be useful.  It might also  be useful to write a constructor\nthat takes a `jax.experimental.CSCMatrix`, so that we could build a differentiable\nmatrix and then just absolutely _slam_ it into our filthy little Cholesky context^[I am sorry. I have had some wine.].\n\nIn order to use this type of pattern with JAX, we would need to register it as a\nPytree class, which involves writing flatten and unflatten routines. The [CSCSparse\nclass](https://github.com/google/jax/blob/712ab66f2855acf8a3f3c3977f80edb4447e7644/jax/experimental/sparse/csr.py)\nis a good example of how to implement this type of thing. Some care would be needed\nto make sure the differentiation rules don't try to do something  stupid\nlike differentiate with respect to `self.iperm` or `self.L_x`. This is beyond the\nextra [autodiff sugar](https://github.com/google/jax/blob/712ab66f2855acf8a3f3c3977f80edb4447e7644/jax/experimental/sparse/ad.py) in the experimental sparse library.\n\nImplementing this would be quite an undertaking, but it's certainly an option.\nThe most obvious downside of this pattern (plus a fully functional sparse matrix\nclass) is that it may end up being quite delicate to have this volume of auxillary information^[Permuations, cholesky, etc] in a pytree while making everything differentiate \nproperly. This doesn't seem to be how most parts of JAX has been built.\nThere are also a couple of [sharp corners](https://jax.readthedocs.io/en/latest/pytrees.html#custom-pytrees-and-initialization) we could run into with instantiation.\n\nTo close this out, it's worth noting a variation on this pattern that comes up:\nthe optional Cholesky. The idea is that rather than compute the permutations and \nthe Cholesky factorisation on initialisation, we store a boolean flag in the class\n`is_cholesky` and, whenever we need a Cholesky factor we check `is_cholesky` and \nif it's `True` we use the computed Cholesky factor and otherwise we compute it and\nset `is_cholesky = True`.\n\nThis pattern introduces state to the object: it is no longer _set and forget_. \nThis will not work within JAX^[This also won't work in Stan, because all Stan objects\nare stateless.], where objects need to be immutable. It's also \nnot an exceptional pattern in general: it is considerably easier to debug code\nwith stateless objects.\n\n## Option 2: Implement all of the combinations of functions that we need\n\nRather than dicking around with classes, we could just implement primitives that \ncompute\n\n- $A \\rightarrow \\log(|A|)$, for a sparse, symmetric positive definite matrix $A$ \n- $(A,b, c) \\rightarrow \\log(|A|) + c^TA^{-1}b$, for a sparse, symmetric positive definite matrix $A$ and  vectors $b$ and $c$.\n\nThis is exactly what we need to do our task and nothing more. It won't result in \nany unnecessary Cholesky factors. It doesn't need us to store computed Cholesky factors.\nWe can simply eat, prey, love.\n\nThe obvious downside to this option is it's going to just massively expand the \ncodebase if there are more things that we want to do. It's also not obvious why\nwe would do this instead of just making $\\log p(\\theta \\mid y)$ a primitive^[This is actually what Stan has done for a bunch of its [GLM-type models](https://mc-stan.org/docs/2_29/functions-reference/poisson-log-glm.html). It's very efficient and fast. But with a maintainance burden.].\n\n\n## Option 3: Just compute the Cholesky\n\nOur third option is to simply compute (and differentiate) the Cholesky factor \ndirectly. We can then compute $\\log(|A|)$ and $A^{-1}b$ through a combination \nof differentiable operations on the elements of the Cholesky factor (for $\\log(|A|)$) and triangular linear solves $L^{-1}b$ and $L^{-T}c$ (for $A^{-1}b$).\n\nHence we require the following two^[or three, but you can implement both triangular solves in one function]\nJAX primitives:\n\n- $A \\rightarrow \\operatorname{chol}(A)$, where $\\operatorname{chol}(A)$ is the Cholesky factor of $A$,\n- $(L, b) \\rightarrow L^{-1} b$ and $(L, b) \\rightarrow L^{-T}b$ for lower-triangular sparse matrix $L$.\n\nThis is pretty close to how the dense version of this function would be implemented.\n\nThere are two little challenges with this pattern:\n\n1. We are adding another large-ish node $L$ to our autodiff tree. As we saw in other patterns,\nthis is unnecessary storage for our problem at hand.\n\n2. The number of non-zeros in $L$ is a function of the non-zero pattern of $A$. This\nmeans the Cholesky will need to be implemented very carefully to ensure that its traceable enough.\n\nThe second point here might actually be an issue. To be honest, I have no idea.\nI think maybe it's fine? But I need to do a close read on [the adding primitives doc](https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html#reverse-differentiation).\nEssentially, as long as the abstract traces just need shapes but not dimensions, we should be ok.\n\nFor adding this to something like Stan, however, we will likely need to do some \nextra work to make sure we know the number of parameters. \n\nThe advantage of this type of design pattern is that it gives users the flexibility\nto do whatever perverted thing they want to do with the Cholesky triangle. For\nexample, they might want to do a centring/non-centring transformation. In Option 1,\nwe would need to write explicit functions to let them do that (not difficult, but \nthere's a lot of code to write, which has the annoying tendency to increases the maintainence burden).\n\n## Option 4: Functors!\n\nA slightly wilder design pattern would be to abandon sparse matrices and just \nmake functions `A(theta, ...)` that return a sparse matrix. If that function is \ndifferentiable wrt its first argument, then we can build this whole thing up that\nway.\n\nIn reality, the only way I can think of to implement this pattern would be to\nimplement a whole differentiable sparse matrix arithmetic (make operations like `alpha * A + beta * B`, `C * D`\nwork for sparse matrices). At which point, we've basically just recreated option 1.\n\nI'm really only bringing up functors because unlike sparse matrices, it is actually\na pretty good model for implementing Gaussian Processes with general covariance functions.\nThere's a little bit of the idea in [this Stan issue](https://github.com/stan-dev/math/issues/1011) that, to my knowledge, hasn't gone anywhere. More recently, a variant has been used successfully in the \n(as yet un-merged) [Laplace approximation feature](https://github.com/stan-dev/math/tree/try-laplace_student/stan/math/laplace) in Stan.\n\n## Which one should we use?\n\nWe don't really need to make that choice yet. So we won't. \n\nBut personally, I like option 1. I expect everyone else on earth would prefer option 3. For densities that see a lot of action, it would make quite a bit of sense to consider making that density a primitive when it has a complex derivative (_à la_ option 2).\n\nBut for now, let's park this and start getting in on the implementations. \n\n",
    "supporting": [
      "design-is-my-passion-sparse-matrices-part-four_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}