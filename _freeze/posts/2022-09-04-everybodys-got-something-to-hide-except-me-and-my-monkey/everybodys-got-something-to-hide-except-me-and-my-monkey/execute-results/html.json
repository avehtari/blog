{
  "hash": "bdd9776bd51c5329e1bc14316f15247d",
  "result": {
    "markdown": "---\ntitle: \"A first look at multilevel regression; or Everybody's got something to hide except me and my macaques\"\ndescription: |\n  A small introduction to multilevel models. Why? Because I said so, that's why. And you will simply not believe what happens to residual plots.\ndate: 2022-09-06\nimage: mark.png\ncategories: [Multilevel models, Visual diagnostics, Prior distributions, fundamentals]\ntwitter-card:\n  title: \"A first look at multilevel regression; or Everybody's got something to hide except me and my macaques\"\n  description: |\n    A small introduction to multilevel models. Why? Because I said so, that's why. And you will simply not believe what happens to residual plots.\n  creator: \"@dan_p_simpson\"\ncitation: \n  url: https://dansblog.netlify.app/2022-09-04-everybodys-got-something-to-hide-except-me-and-my-monkey.html\nformat: \n  html:\n    df-print: paged\n\ndraft: false\n---\n\n\n\n[Eliza](https://www.elizablissmoreau.com) knows a little something about monkeys. This will become relevant in a moment.\n\nIn about 2016, [Almeling _et al._](https://www.cell.com/current-biology/fulltext/S0960-9822(16)30460-2) published\na paper that suggested aged Barbary macaques maintained interest in members of their\nown species while losing interest in novel non-social stimuli (eg toys or puzzles with \nfood inside). \n\n![I'd never come across the concept of a Graphical Abstract before, but here is the one for this paper. Graphic design is my passion. [Source](https://www.cell.com/current-biology/fulltext/S0960-9822(16)30460-2)](fx1.jpeg)\n\nThis is where Eliza---who knows a little something about monkeys---comes\ninto frame: this did not gel with her experiences at all.\n\nSo Eliza (and [Mark](https://scholar.google.com.au/citations?hl=en&user=yg9U_okAAAAJ)^[Mark insisted that I like to his google scholar rather than his website. He's cute that way.] ^[Mark wants me to tell you that he's not vain he's just moving. Sure Jan.], who also knows a little something about monkeys) decided to \nlook into it.\n\n## What are the stake?s (According to the papers, not according to me, who knows exactly nothing^[I know that marmosets suffer from lesbian bed death, but I'm told that a marmoset is not a macaque, which in turn is not a macaw. Ecology is fascinating.] about this type of work)\n\nA big motivation for studying macaques and other non-human\nprimates is that they're good models of humans. This means that if there was solid\nevidence of macaques becoming less interested in novel stimuli as they age \n(while maintaining interest in people), this could suggest an evolutionary \nreason from this (commonly observed) behaviour in humans.\n\nSo if this result is true, it could help us understand the psychology of humans as \nthey age (and in particular, the learned vs evolved trade off they are making).\n\n## So what did Eliza and Mark do?\n\nThere are a few things you can do when confronted with a result that contradicts\nyour experience: you can complain about it on the Internet, you can mobilize a\ndirect replication effort, or you can conduct your own experiments.  Eliza and Mark\nopted for the third option, designing a _conceptual replication_. \n\nDirect replications tell you more about the specific experiment that was conducted,\nbut not necessarily more about the phenomenon under investigation. In a study involving\naged monkeys^[A real problem in the world is that there aren't enough monkeys for animal research at the best of times. Once you need aged monkeys, it's an even smaller population. Non-human primate research is _hard_.], it's difficult to imagine how a direct replication could take place.\n\nOn the other hand, a conceptual replication has a lot more flexibility. It allows you\nto probe the question in a more targeted manner, appropriate for incremental science.\nIn this case, Eliza and Mark opted to study only the claim that the monkeys lose interest\nin novel stimuli as they age ([paper here](https://royalsocietypublishing.org/doi/full/10.1098/rsos.182237)). They did not look into the social claim. They also used\na slightly different species of macaque (_M. mulatta_ rather than _M. butterfly_). \nThis is reasonable insofar as understanding macaques as a model for human behaviour.\n\n## What does the data look like?\n\nThe experiment used 243^[Actually 244, but one of them turned out to be blind. Animal research is a journey.] monkeys aged between 4 and 30 and gave them a novel puzzle task (opening a fancy tube with food in it) for twenty minutes over two days. The puzzle was fitted with an activity tracker. Each monkey had two tries at the puzzle over two days.\nMonkeys had access to the puzzle for around^[It turns out that some of the monkeys didn't want to give up the puzzle after 20 minutes. One held out for 72 minutes before the data collection ended. Cheeky monkeys.] 20 minutes.\n\nIn order to match the original study's analysis, Eliza and Mark divided the first \ntwo minutes into 15 second intervals and counted the number of intervals where the \nmonkey interacted with the puzzle. They also measured the same thing over 20 minutes in order to see if there was a difference between short-term curiosity and more sustained exploration.\n\nFor each monkey, we have the following information:\n\n- Monkey ID\n- Age (4-30)\n- Day (one or two)\n- Number of active intervals in the first two minutes (0-8)\n- Number of active intervals in the first twenty minutes (0-80)\n\nThe data and their analysis are freely^[Did Mark make me do unspeakable, degrading, borderline immoral things to get the data? No. It's open source. Truly the first time I've been disappointed that something was open source.] available [here](https://datadryad.org/stash/dataset/doi:10.5061/dryad.1bj133v).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nacti_data <- read_csv(\"activity_data.csv\") \nactivity_2mins <- acti_data |>\n  filter(obs<9) |> group_by(subj_id, Day) |>\n  summarize(total=sum(Activity), \n            active_bins = sum(Activity > 0), \n            age = min(age)) |>\n  rename(monkey = subj_id, day = Day) |>\n  ungroup()\n\nactivity_20minms80 <- acti_data |> filter(obs<81) |>\n  group_by(subj_id, Day) |>\n  summarize(total=sum(Activity), \n            active_bins = sum(Activity > 0), \n            age = min(age)) |>\n  rename(monkey = subj_id, day = Day) |>\n  ungroup()\n```\n:::\n\n\n## Ok Mary, how are we going to analyze this data?\n\nEliza and Mark's monkey data is an example of a fairly common type of experimental data, where the same subject is measured multiple times. It is useful to break the covariates down into three types: _grouping variables_,  _group-level covariates_, and _individual-level covariates_.\n\n_Grouping variables_ indicate what _group_ each observation is in. We will see a lot of different ways of defining groups as we go on, but a core idea is that observations within a group should conceptually more similar to each other than observations in different groups. For Eliza and Mark, their grouping variable is `monkey`. This encodes the idea that different monkeys might have very different levels of curiosity, but the same monkey across two different days would probably have fairly similar levels of curiosity. \n\n_Group-level covariates_ are covariates that describe a feature of the _group_ rather than the observation. In this example, `age` is a group-level covariate, because the monkeys are the same age at each observation.\n\n_Individual-level covariates_ are covariates that describe a feature that is specific to an observation. (The nomenclature here can be a bit confusing: the \"individual\" refers to individual observations, not to individual monkeys. All good naming conventions go to shit eventually.) The individual-level covariate is experiment day. This can be a bit harder to see than the other designations, but it's a little clearer if you think of it as an indicator of whether this is the first time the monkey has seen the task or the second time. Viewed this way, it is very clearly a measurement of an property of an observation rather than of a group.\n\nEliza and Mark's monkey data is an example of a fairly general type of experimental data where subjects (our groups) are given the same task under different experimental conditions (described through individual-level covariates). As we will see, it's not uncommon to have much more complex group definitions (that involve several grouping covariates) and larger sets of both group-level and individual-level covariates.\n\nSo how do we fit a model to this data.\n\n### There are just too many monkeys; or Why can't we just analyse this with regression?\n\nThe temptation with this sort of data is to fit a linear regression to it as\na first model. In this case, we are using grouping, group-level, and individual-level covariates in the same way. Let's suck it and see.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\nfit_lm <- lm(active_bins ~ age*factor(day) + factor(monkey), data = activity_2mins)\n\ntidy(fit_lm) \n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"std.error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"(Intercept)\",\"2\":\"-6.688440e+00\",\"3\":\"5.38400465\",\"4\":\"-1.242280e+00\",\"5\":\"0.215345864\"},{\"1\":\"age\",\"2\":\"4.234598e-01\",\"3\":\"0.21287645\",\"4\":\"1.989228e+00\",\"5\":\"0.047811367\"},{\"1\":\"factor(day)2\",\"2\":\"1.879672e-03\",\"3\":\"0.39658478\",\"4\":\"4.739646e-03\",\"5\":\"0.996222261\"},{\"1\":\"factor(monkey)88\",\"2\":\"1.000000e+00\",\"3\":\"1.70008898\",\"4\":\"5.882045e-01\",\"5\":\"0.556948129\"},{\"1\":\"factor(monkey)636\",\"2\":\"-3.062500e+00\",\"3\":\"1.60442379\",\"4\":\"-1.908785e+00\",\"5\":\"0.057482707\"},{\"1\":\"factor(monkey)760\",\"2\":\"-2.937500e+00\",\"3\":\"1.81569582\",\"4\":\"-1.617837e+00\",\"5\":\"0.107011195\"},{\"1\":\"factor(monkey)1257\",\"2\":\"3.750000e-01\",\"3\":\"1.53243949\",\"4\":\"2.447079e-01\",\"5\":\"0.806891726\"},{\"1\":\"factor(monkey)1607\",\"2\":\"3.750000e-01\",\"3\":\"1.53243949\",\"4\":\"2.447079e-01\",\"5\":\"0.806891726\"},{\"1\":\"factor(monkey)1632\",\"2\":\"-2.125000e+00\",\"3\":\"1.53243949\",\"4\":\"-1.386678e+00\",\"5\":\"0.166826659\"},{\"1\":\"factor(monkey)1860\",\"2\":\"8.125000e-01\",\"3\":\"1.48757785\",\"4\":\"5.461899e-01\",\"5\":\"0.585442774\"},{\"1\":\"factor(monkey)1869\",\"2\":\"2.812500e+00\",\"3\":\"1.48757785\",\"4\":\"1.890657e+00\",\"5\":\"0.059874728\"},{\"1\":\"factor(monkey)2191\",\"2\":\"1.312500e+00\",\"3\":\"1.48757785\",\"4\":\"8.823068e-01\",\"5\":\"0.378493776\"},{\"1\":\"factor(monkey)2637\",\"2\":\"-2.500000e-01\",\"3\":\"1.47232024\",\"4\":\"-1.698000e-01\",\"5\":\"0.865310455\"},{\"1\":\"factor(monkey)2747\",\"2\":\"3.750000e+00\",\"3\":\"1.47232024\",\"4\":\"2.547000e+00\",\"5\":\"0.011490788\"},{\"1\":\"factor(monkey)2833\",\"2\":\"2.500000e-01\",\"3\":\"1.47232024\",\"4\":\"1.698000e-01\",\"5\":\"0.865310455\"},{\"1\":\"factor(monkey)2912\",\"2\":\"2.250000e+00\",\"3\":\"1.47232024\",\"4\":\"1.528200e+00\",\"5\":\"0.127779712\"},{\"1\":\"factor(monkey)3536\",\"2\":\"-3.125000e-01\",\"3\":\"1.48757785\",\"4\":\"-2.100730e-01\",\"5\":\"0.833788896\"},{\"1\":\"factor(monkey)3545\",\"2\":\"-8.125000e-01\",\"3\":\"1.48757785\",\"4\":\"-5.461899e-01\",\"5\":\"0.585442774\"},{\"1\":\"factor(monkey)3696\",\"2\":\"-3.812500e+00\",\"3\":\"1.48757785\",\"4\":\"-2.562891e+00\",\"5\":\"0.010991577\"},{\"1\":\"factor(monkey)4009\",\"2\":\"1.125000e+00\",\"3\":\"1.53243949\",\"4\":\"7.341236e-01\",\"5\":\"0.463590006\"},{\"1\":\"factor(monkey)4392\",\"2\":\"1.250000e-01\",\"3\":\"1.53243949\",\"4\":\"8.156929e-02\",\"5\":\"0.935057207\"},{\"1\":\"factor(monkey)4624\",\"2\":\"-4.375000e-01\",\"3\":\"1.60442379\",\"4\":\"-2.726836e-01\",\"5\":\"0.785330940\"},{\"1\":\"factor(monkey)4686\",\"2\":\"1.062500e+00\",\"3\":\"1.60442379\",\"4\":\"6.622315e-01\",\"5\":\"0.508458252\"},{\"1\":\"factor(monkey)4776\",\"2\":\"3.562500e+00\",\"3\":\"1.60442379\",\"4\":\"2.220423e+00\",\"5\":\"0.027324613\"},{\"1\":\"factor(monkey)4795\",\"2\":\"1.562500e+00\",\"3\":\"1.60442379\",\"4\":\"9.738699e-01\",\"5\":\"0.331101681\"},{\"1\":\"factor(monkey)4886\",\"2\":\"5.625000e-01\",\"3\":\"1.60442379\",\"4\":\"3.505932e-01\",\"5\":\"0.726201115\"},{\"1\":\"factor(monkey)4964\",\"2\":\"3.062500e+00\",\"3\":\"1.60442379\",\"4\":\"1.908785e+00\",\"5\":\"0.057482707\"},{\"1\":\"factor(monkey)5252\",\"2\":\"4.500000e+00\",\"3\":\"1.70008898\",\"4\":\"2.646920e+00\",\"5\":\"0.008660496\"},{\"1\":\"factor(monkey)5332\",\"2\":\"-5.000000e-01\",\"3\":\"1.70008898\",\"4\":\"-2.941023e-01\",\"5\":\"0.768933965\"},{\"1\":\"factor(monkey)5388\",\"2\":\"3.000000e+00\",\"3\":\"1.70008898\",\"4\":\"1.764614e+00\",\"5\":\"0.078900697\"},{\"1\":\"factor(monkey)5413\",\"2\":\"5.500000e+00\",\"3\":\"1.70008898\",\"4\":\"3.235125e+00\",\"5\":\"0.001386994\"},{\"1\":\"factor(monkey)5453\",\"2\":\"2.000000e+00\",\"3\":\"1.70008898\",\"4\":\"1.176409e+00\",\"5\":\"0.240596983\"},{\"1\":\"factor(monkey)5498\",\"2\":\"-5.000000e-01\",\"3\":\"1.70008898\",\"4\":\"-2.941023e-01\",\"5\":\"0.768933965\"},{\"1\":\"factor(monkey)5604\",\"2\":\"-2.680485e-14\",\"3\":\"1.70008898\",\"4\":\"-1.576674e-14\",\"5\":\"1.000000000\"},{\"1\":\"factor(monkey)5607\",\"2\":\"5.000000e-01\",\"3\":\"1.70008898\",\"4\":\"2.941023e-01\",\"5\":\"0.768933965\"},{\"1\":\"factor(monkey)5646\",\"2\":\"-1.747008e-14\",\"3\":\"1.70008898\",\"4\":\"-1.027598e-14\",\"5\":\"1.000000000\"},{\"1\":\"factor(monkey)5774\",\"2\":\"2.000000e+00\",\"3\":\"1.70008898\",\"4\":\"1.176409e+00\",\"5\":\"0.240596983\"},{\"1\":\"factor(monkey)5895\",\"2\":\"2.937500e+00\",\"3\":\"1.81569582\",\"4\":\"1.617837e+00\",\"5\":\"0.107011195\"},{\"1\":\"factor(monkey)5967\",\"2\":\"-6.250000e-02\",\"3\":\"1.81569582\",\"4\":\"-3.442207e-02\",\"5\":\"0.972569200\"},{\"1\":\"factor(monkey)5990\",\"2\":\"1.937500e+00\",\"3\":\"1.81569582\",\"4\":\"1.067084e+00\",\"5\":\"0.287006110\"},{\"1\":\"factor(monkey)6008\",\"2\":\"-6.250000e-02\",\"3\":\"1.81569582\",\"4\":\"-3.442207e-02\",\"5\":\"0.972569200\"},{\"1\":\"factor(monkey)6098\",\"2\":\"1.937500e+00\",\"3\":\"1.81569582\",\"4\":\"1.067084e+00\",\"5\":\"0.287006110\"},{\"1\":\"factor(monkey)6159\",\"2\":\"3.437500e+00\",\"3\":\"1.81569582\",\"4\":\"1.893214e+00\",\"5\":\"0.059532471\"},{\"1\":\"factor(monkey)6258\",\"2\":\"2.937500e+00\",\"3\":\"1.81569582\",\"4\":\"1.617837e+00\",\"5\":\"0.107011195\"},{\"1\":\"factor(monkey)6264\",\"2\":\"2.937500e+00\",\"3\":\"1.81569582\",\"4\":\"1.617837e+00\",\"5\":\"0.107011195\"},{\"1\":\"factor(monkey)6287\",\"2\":\"4.375000e-01\",\"3\":\"1.81569582\",\"4\":\"2.409545e-01\",\"5\":\"0.809796132\"},{\"1\":\"factor(monkey)6505\",\"2\":\"2.375000e+00\",\"3\":\"1.94769661\",\"4\":\"1.219389e+00\",\"5\":\"0.223893579\"},{\"1\":\"factor(monkey)6512\",\"2\":\"3.875000e+00\",\"3\":\"1.94769661\",\"4\":\"1.989530e+00\",\"5\":\"0.047777867\"},{\"1\":\"factor(monkey)6516\",\"2\":\"3.375000e+00\",\"3\":\"1.94769661\",\"4\":\"1.732816e+00\",\"5\":\"0.084412906\"},{\"1\":\"factor(monkey)6807\",\"2\":\"2.375000e+00\",\"3\":\"1.94769661\",\"4\":\"1.219389e+00\",\"5\":\"0.223893579\"},{\"1\":\"factor(monkey)6877\",\"2\":\"1.375000e+00\",\"3\":\"1.94769661\",\"4\":\"7.059621e-01\",\"5\":\"0.480896409\"},{\"1\":\"factor(monkey)6930\",\"2\":\"8.750000e-01\",\"3\":\"1.94769661\",\"4\":\"4.492486e-01\",\"5\":\"0.653657739\"},{\"1\":\"factor(monkey)7261\",\"2\":\"8.125000e-01\",\"3\":\"2.09299182\",\"4\":\"3.882003e-01\",\"5\":\"0.698211947\"},{\"1\":\"factor(monkey)7289\",\"2\":\"5.812500e+00\",\"3\":\"2.09299182\",\"4\":\"2.777125e+00\",\"5\":\"0.005917223\"},{\"1\":\"factor(monkey)7307\",\"2\":\"5.312500e+00\",\"3\":\"2.09299182\",\"4\":\"2.538233e+00\",\"5\":\"0.011774807\"},{\"1\":\"factor(monkey)7321\",\"2\":\"3.312500e+00\",\"3\":\"2.09299182\",\"4\":\"1.582663e+00\",\"5\":\"0.114815239\"},{\"1\":\"factor(monkey)7333\",\"2\":\"4.312500e+00\",\"3\":\"2.09299182\",\"4\":\"2.060448e+00\",\"5\":\"0.040433840\"},{\"1\":\"factor(monkey)7451\",\"2\":\"2.312500e+00\",\"3\":\"2.09299182\",\"4\":\"1.104878e+00\",\"5\":\"0.270319009\"},{\"1\":\"factor(monkey)7588\",\"2\":\"-6.250000e-02\",\"3\":\"1.81569582\",\"4\":\"-3.442207e-02\",\"5\":\"0.972569200\"},{\"1\":\"factor(monkey)7598\",\"2\":\"3.750000e-01\",\"3\":\"1.94769661\",\"4\":\"1.925351e-01\",\"5\":\"0.847485879\"},{\"1\":\"factor(monkey)7600\",\"2\":\"-1.625000e+00\",\"3\":\"1.94769661\",\"4\":\"-8.343189e-01\",\"5\":\"0.404930992\"},{\"1\":\"factor(monkey)7623\",\"2\":\"2.812500e+00\",\"3\":\"2.09299182\",\"4\":\"1.343770e+00\",\"5\":\"0.180291735\"},{\"1\":\"factor(monkey)7707\",\"2\":\"1.312500e+00\",\"3\":\"2.09299182\",\"4\":\"6.270928e-01\",\"5\":\"0.531194536\"},{\"1\":\"factor(monkey)7721\",\"2\":\"1.812500e+00\",\"3\":\"2.09299182\",\"4\":\"8.659852e-01\",\"5\":\"0.387363127\"},{\"1\":\"factor(monkey)7828\",\"2\":\"1.812500e+00\",\"3\":\"2.09299182\",\"4\":\"8.659852e-01\",\"5\":\"0.387363127\"},{\"1\":\"factor(monkey)7942\",\"2\":\"4.375000e+00\",\"3\":\"1.94769661\",\"4\":\"2.246243e+00\",\"5\":\"0.025598621\"},{\"1\":\"factor(monkey)7992\",\"2\":\"5.250000e+00\",\"3\":\"2.24900632\",\"4\":\"2.334364e+00\",\"5\":\"0.020402519\"},{\"1\":\"factor(monkey)8053\",\"2\":\"4.750000e+00\",\"3\":\"2.24900632\",\"4\":\"2.112044e+00\",\"5\":\"0.035716332\"},{\"1\":\"factor(monkey)8094\",\"2\":\"2.750000e+00\",\"3\":\"2.24900632\",\"4\":\"1.222762e+00\",\"5\":\"0.222618881\"},{\"1\":\"factor(monkey)8103\",\"2\":\"2.250000e+00\",\"3\":\"2.24900632\",\"4\":\"1.000442e+00\",\"5\":\"0.318104340\"},{\"1\":\"factor(monkey)8179\",\"2\":\"4.250000e+00\",\"3\":\"2.24900632\",\"4\":\"1.889723e+00\",\"5\":\"0.060000176\"},{\"1\":\"factor(monkey)8183\",\"2\":\"-2.500000e-01\",\"3\":\"2.24900632\",\"4\":\"-1.111602e-01\",\"5\":\"0.911582214\"},{\"1\":\"factor(monkey)8338\",\"2\":\"1.750000e+00\",\"3\":\"2.24900632\",\"4\":\"7.781214e-01\",\"5\":\"0.437263858\"},{\"1\":\"factor(monkey)8340\",\"2\":\"4.750000e+00\",\"3\":\"2.24900632\",\"4\":\"2.112044e+00\",\"5\":\"0.035716332\"},{\"1\":\"factor(monkey)8343\",\"2\":\"4.250000e+00\",\"3\":\"2.24900632\",\"4\":\"1.889723e+00\",\"5\":\"0.060000176\"},{\"1\":\"factor(monkey)8371\",\"2\":\"5.750000e+00\",\"3\":\"2.24900632\",\"4\":\"2.556685e+00\",\"5\":\"0.011184196\"},{\"1\":\"factor(monkey)8544\",\"2\":\"2.750000e+00\",\"3\":\"2.24900632\",\"4\":\"1.222762e+00\",\"5\":\"0.222618881\"},{\"1\":\"factor(monkey)8611\",\"2\":\"3.250000e+00\",\"3\":\"2.24900632\",\"4\":\"1.445083e+00\",\"5\":\"0.149739064\"},{\"1\":\"factor(monkey)8729\",\"2\":\"3.312500e+00\",\"3\":\"2.09299182\",\"4\":\"1.582663e+00\",\"5\":\"0.114815239\"},{\"1\":\"factor(monkey)8834\",\"2\":\"2.187500e+00\",\"3\":\"2.41366237\",\"4\":\"9.062991e-01\",\"5\":\"0.365686559\"},{\"1\":\"factor(monkey)8873\",\"2\":\"6.187500e+00\",\"3\":\"2.41366237\",\"4\":\"2.563532e+00\",\"5\":\"0.010971865\"},{\"1\":\"factor(monkey)8956\",\"2\":\"4.687500e+00\",\"3\":\"2.41366237\",\"4\":\"1.942069e+00\",\"5\":\"0.053298767\"},{\"1\":\"factor(monkey)8963\",\"2\":\"4.187500e+00\",\"3\":\"2.41366237\",\"4\":\"1.734915e+00\",\"5\":\"0.084039563\"},{\"1\":\"factor(monkey)9009\",\"2\":\"2.687500e+00\",\"3\":\"2.41366237\",\"4\":\"1.113453e+00\",\"5\":\"0.266627767\"},{\"1\":\"factor(monkey)9014\",\"2\":\"3.187500e+00\",\"3\":\"2.41366237\",\"4\":\"1.320607e+00\",\"5\":\"0.187890272\"},{\"1\":\"factor(monkey)9023\",\"2\":\"2.187500e+00\",\"3\":\"2.41366237\",\"4\":\"9.062991e-01\",\"5\":\"0.365686559\"},{\"1\":\"factor(monkey)9117\",\"2\":\"1.187500e+00\",\"3\":\"2.41366237\",\"4\":\"4.919909e-01\",\"5\":\"0.623175457\"},{\"1\":\"factor(monkey)9344\",\"2\":\"4.687500e+00\",\"3\":\"2.41366237\",\"4\":\"1.942069e+00\",\"5\":\"0.053298767\"},{\"1\":\"factor(monkey)9355\",\"2\":\"6.187500e+00\",\"3\":\"2.41366237\",\"4\":\"2.563532e+00\",\"5\":\"0.010971865\"},{\"1\":\"factor(monkey)9411\",\"2\":\"3.187500e+00\",\"3\":\"2.41366237\",\"4\":\"1.320607e+00\",\"5\":\"0.187890272\"},{\"1\":\"factor(monkey)9416\",\"2\":\"6.187500e+00\",\"3\":\"2.41366237\",\"4\":\"2.563532e+00\",\"5\":\"0.010971865\"},{\"1\":\"factor(monkey)9542\",\"2\":\"8.125000e-01\",\"3\":\"1.48757785\",\"4\":\"5.461899e-01\",\"5\":\"0.585442774\"},{\"1\":\"factor(monkey)9598\",\"2\":\"7.625000e+00\",\"3\":\"2.58530938\",\"4\":\"2.949357e+00\",\"5\":\"0.003499076\"},{\"1\":\"factor(monkey)9609\",\"2\":\"2.125000e+00\",\"3\":\"2.58530938\",\"4\":\"8.219519e-01\",\"5\":\"0.411920078\"},{\"1\":\"factor(monkey)9615\",\"2\":\"2.625000e+00\",\"3\":\"2.58530938\",\"4\":\"1.015352e+00\",\"5\":\"0.310960381\"},{\"1\":\"factor(monkey)9662\",\"2\":\"4.625000e+00\",\"3\":\"2.58530938\",\"4\":\"1.788954e+00\",\"5\":\"0.074883259\"},{\"1\":\"factor(monkey)9680\",\"2\":\"6.250000e-01\",\"3\":\"2.58530938\",\"4\":\"2.417506e-01\",\"5\":\"0.809179879\"},{\"1\":\"factor(monkey)9683\",\"2\":\"5.125000e+00\",\"3\":\"2.58530938\",\"4\":\"1.982355e+00\",\"5\":\"0.048580117\"},{\"1\":\"factor(monkey)9771\",\"2\":\"6.250000e-01\",\"3\":\"2.58530938\",\"4\":\"2.417506e-01\",\"5\":\"0.809179879\"},{\"1\":\"factor(monkey)9847\",\"2\":\"3.125000e+00\",\"3\":\"2.58530938\",\"4\":\"1.208753e+00\",\"5\":\"0.227947352\"},{\"1\":\"factor(monkey)9926\",\"2\":\"6.625000e+00\",\"3\":\"2.58530938\",\"4\":\"2.562556e+00\",\"5\":\"0.011001901\"},{\"1\":\"factor(monkey)9940\",\"2\":\"3.625000e+00\",\"3\":\"2.58530938\",\"4\":\"1.402153e+00\",\"5\":\"0.162161553\"},{\"1\":\"factor(monkey)9986\",\"2\":\"3.625000e+00\",\"3\":\"2.58530938\",\"4\":\"1.402153e+00\",\"5\":\"0.162161553\"},{\"1\":\"factor(monkey)10069\",\"2\":\"-6.250000e-02\",\"3\":\"1.81569582\",\"4\":\"-3.442207e-02\",\"5\":\"0.972569200\"},{\"1\":\"factor(monkey)10084\",\"2\":\"2.437500e+00\",\"3\":\"1.81569582\",\"4\":\"1.342461e+00\",\"5\":\"0.180715137\"},{\"1\":\"factor(monkey)10290\",\"2\":\"4.625000e+00\",\"3\":\"2.58530938\",\"4\":\"1.788954e+00\",\"5\":\"0.074883259\"},{\"1\":\"factor(monkey)10399\",\"2\":\"-3.750000e-01\",\"3\":\"1.53243949\",\"4\":\"-2.447079e-01\",\"5\":\"0.806891726\"},{\"1\":\"factor(monkey)10646\",\"2\":\"6.062500e+00\",\"3\":\"2.76264459\",\"4\":\"2.194455e+00\",\"5\":\"0.029161550\"},{\"1\":\"factor(monkey)10790\",\"2\":\"6.562500e+00\",\"3\":\"2.76264459\",\"4\":\"2.375441e+00\",\"5\":\"0.018314388\"},{\"1\":\"factor(monkey)10826\",\"2\":\"3.062500e+00\",\"3\":\"2.76264459\",\"4\":\"1.108539e+00\",\"5\":\"0.268738626\"},{\"1\":\"factor(monkey)10850\",\"2\":\"3.562500e+00\",\"3\":\"2.76264459\",\"4\":\"1.289525e+00\",\"5\":\"0.198456854\"},{\"1\":\"factor(monkey)10866\",\"2\":\"2.562500e+00\",\"3\":\"2.76264459\",\"4\":\"9.275533e-01\",\"5\":\"0.354571211\"},{\"1\":\"factor(monkey)11252\",\"2\":\"8.562500e+00\",\"3\":\"2.76264459\",\"4\":\"3.099385e+00\",\"5\":\"0.002170583\"},{\"1\":\"factor(monkey)11440\",\"2\":\"5.500000e+00\",\"3\":\"2.94464048\",\"4\":\"1.867800e+00\",\"5\":\"0.063008689\"},{\"1\":\"factor(monkey)11446\",\"2\":\"8.000000e+00\",\"3\":\"2.94464048\",\"4\":\"2.716800e+00\",\"5\":\"0.007071614\"},{\"1\":\"factor(monkey)11475\",\"2\":\"3.000000e+00\",\"3\":\"2.94464048\",\"4\":\"1.018800e+00\",\"5\":\"0.309323781\"},{\"1\":\"factor(monkey)11483\",\"2\":\"5.500000e+00\",\"3\":\"2.94464048\",\"4\":\"1.867800e+00\",\"5\":\"0.063008689\"},{\"1\":\"factor(monkey)11484\",\"2\":\"6.500000e+00\",\"3\":\"2.94464048\",\"4\":\"2.207400e+00\",\"5\":\"0.028232894\"},{\"1\":\"factor(monkey)11542\",\"2\":\"3.500000e+00\",\"3\":\"2.94464048\",\"4\":\"1.188600e+00\",\"5\":\"0.235771839\"},{\"1\":\"factor(monkey)11717\",\"2\":\"4.000000e+00\",\"3\":\"2.94464048\",\"4\":\"1.358400e+00\",\"5\":\"0.175612222\"},{\"1\":\"factor(monkey)11754\",\"2\":\"4.000000e+00\",\"3\":\"2.94464048\",\"4\":\"1.358400e+00\",\"5\":\"0.175612222\"},{\"1\":\"factor(monkey)11781\",\"2\":\"6.000000e+00\",\"3\":\"2.94464048\",\"4\":\"2.037600e+00\",\"5\":\"0.042686931\"},{\"1\":\"factor(monkey)11799\",\"2\":\"7.000000e+00\",\"3\":\"2.94464048\",\"4\":\"2.377200e+00\",\"5\":\"0.018229337\"},{\"1\":\"factor(monkey)11895\",\"2\":\"2.500000e+00\",\"3\":\"2.94464048\",\"4\":\"8.490001e-01\",\"5\":\"0.396727282\"},{\"1\":\"factor(monkey)11916\",\"2\":\"5.000000e+00\",\"3\":\"2.94464048\",\"4\":\"1.698000e+00\",\"5\":\"0.090803840\"},{\"1\":\"factor(monkey)12017\",\"2\":\"5.500000e+00\",\"3\":\"2.94464048\",\"4\":\"1.867800e+00\",\"5\":\"0.063008689\"},{\"1\":\"factor(monkey)12164\",\"2\":\"5.437500e+00\",\"3\":\"3.13048431\",\"4\":\"1.736952e+00\",\"5\":\"0.083678713\"},{\"1\":\"factor(monkey)12298\",\"2\":\"8.937500e+00\",\"3\":\"3.13048431\",\"4\":\"2.854990e+00\",\"5\":\"0.004680388\"},{\"1\":\"factor(monkey)12355\",\"2\":\"7.937500e+00\",\"3\":\"3.13048431\",\"4\":\"2.535550e+00\",\"5\":\"0.011862943\"},{\"1\":\"factor(monkey)12368\",\"2\":\"3.437500e+00\",\"3\":\"3.13048431\",\"4\":\"1.098073e+00\",\"5\":\"0.273273063\"},{\"1\":\"factor(monkey)12381\",\"2\":\"4.937500e+00\",\"3\":\"3.13048431\",\"4\":\"1.577232e+00\",\"5\":\"0.116059306\"},{\"1\":\"factor(monkey)12505\",\"2\":\"6.437500e+00\",\"3\":\"3.13048431\",\"4\":\"2.056391e+00\",\"5\":\"0.040826302\"},{\"1\":\"factor(monkey)12520\",\"2\":\"4.937500e+00\",\"3\":\"3.13048431\",\"4\":\"1.577232e+00\",\"5\":\"0.116059306\"},{\"1\":\"factor(monkey)12532\",\"2\":\"6.437500e+00\",\"3\":\"3.13048431\",\"4\":\"2.056391e+00\",\"5\":\"0.040826302\"},{\"1\":\"factor(monkey)12630\",\"2\":\"5.437500e+00\",\"3\":\"3.13048431\",\"4\":\"1.736952e+00\",\"5\":\"0.083678713\"},{\"1\":\"factor(monkey)12631\",\"2\":\"7.437500e+00\",\"3\":\"3.13048431\",\"4\":\"2.375830e+00\",\"5\":\"0.018295539\"},{\"1\":\"factor(monkey)12749\",\"2\":\"6.937500e+00\",\"3\":\"3.13048431\",\"4\":\"2.216111e+00\",\"5\":\"0.027622537\"},{\"1\":\"factor(monkey)12906\",\"2\":\"2.437500e+00\",\"3\":\"3.13048431\",\"4\":\"7.786335e-01\",\"5\":\"0.436962634\"},{\"1\":\"factor(monkey)12947\",\"2\":\"5.375000e+00\",\"3\":\"3.31952984\",\"4\":\"1.619205e+00\",\"5\":\"0.106716412\"},{\"1\":\"factor(monkey)12958\",\"2\":\"7.375000e+00\",\"3\":\"3.31952984\",\"4\":\"2.221700e+00\",\"5\":\"0.027236943\"},{\"1\":\"factor(monkey)13121\",\"2\":\"4.875000e+00\",\"3\":\"3.31952984\",\"4\":\"1.468581e+00\",\"5\":\"0.143255789\"},{\"1\":\"factor(monkey)13129\",\"2\":\"5.375000e+00\",\"3\":\"3.31952984\",\"4\":\"1.619205e+00\",\"5\":\"0.106716412\"},{\"1\":\"factor(monkey)13131\",\"2\":\"4.875000e+00\",\"3\":\"3.31952984\",\"4\":\"1.468581e+00\",\"5\":\"0.143255789\"},{\"1\":\"factor(monkey)13260\",\"2\":\"5.875000e+00\",\"3\":\"3.31952984\",\"4\":\"1.769829e+00\",\"5\":\"0.078025353\"},{\"1\":\"factor(monkey)13279\",\"2\":\"5.375000e+00\",\"3\":\"3.31952984\",\"4\":\"1.619205e+00\",\"5\":\"0.106716412\"},{\"1\":\"factor(monkey)13312\",\"2\":\"4.375000e+00\",\"3\":\"3.31952984\",\"4\":\"1.317958e+00\",\"5\":\"0.188774375\"},{\"1\":\"factor(monkey)13442\",\"2\":\"3.375000e+00\",\"3\":\"3.31952984\",\"4\":\"1.016710e+00\",\"5\":\"0.310315126\"},{\"1\":\"factor(monkey)13473\",\"2\":\"6.375000e+00\",\"3\":\"3.31952984\",\"4\":\"1.920453e+00\",\"5\":\"0.055985798\"},{\"1\":\"factor(monkey)13578\",\"2\":\"4.875000e+00\",\"3\":\"3.31952984\",\"4\":\"1.468581e+00\",\"5\":\"0.143255789\"},{\"1\":\"factor(monkey)13590\",\"2\":\"6.875000e+00\",\"3\":\"3.31952984\",\"4\":\"2.071076e+00\",\"5\":\"0.039420756\"},{\"1\":\"factor(monkey)13790\",\"2\":\"5.812500e+00\",\"3\":\"3.51125999\",\"4\":\"1.655389e+00\",\"5\":\"0.099152667\"},{\"1\":\"factor(monkey)13825\",\"2\":\"4.812500e+00\",\"3\":\"3.51125999\",\"4\":\"1.370591e+00\",\"5\":\"0.171783107\"},{\"1\":\"factor(monkey)13883\",\"2\":\"6.312500e+00\",\"3\":\"3.51125999\",\"4\":\"1.797788e+00\",\"5\":\"0.073467506\"},{\"1\":\"factor(monkey)13922\",\"2\":\"7.812500e+00\",\"3\":\"3.51125999\",\"4\":\"2.224985e+00\",\"5\":\"0.027012536\"},{\"1\":\"factor(monkey)14043\",\"2\":\"7.312500e+00\",\"3\":\"3.51125999\",\"4\":\"2.082586e+00\",\"5\":\"0.038348277\"},{\"1\":\"factor(monkey)14066\",\"2\":\"4.812500e+00\",\"3\":\"3.51125999\",\"4\":\"1.370591e+00\",\"5\":\"0.171783107\"},{\"1\":\"factor(monkey)14077\",\"2\":\"6.812500e+00\",\"3\":\"3.51125999\",\"4\":\"1.940187e+00\",\"5\":\"0.053528408\"},{\"1\":\"factor(monkey)14137\",\"2\":\"4.312500e+00\",\"3\":\"3.51125999\",\"4\":\"1.228192e+00\",\"5\":\"0.220578143\"},{\"1\":\"factor(monkey)14165\",\"2\":\"6.312500e+00\",\"3\":\"3.51125999\",\"4\":\"1.797788e+00\",\"5\":\"0.073467506\"},{\"1\":\"factor(monkey)14177\",\"2\":\"4.812500e+00\",\"3\":\"3.51125999\",\"4\":\"1.370591e+00\",\"5\":\"0.171783107\"},{\"1\":\"factor(monkey)14307\",\"2\":\"6.812500e+00\",\"3\":\"3.51125999\",\"4\":\"1.940187e+00\",\"5\":\"0.053528408\"},{\"1\":\"factor(monkey)14323\",\"2\":\"8.312500e+00\",\"3\":\"3.51125999\",\"4\":\"2.367384e+00\",\"5\":\"0.018708484\"},{\"1\":\"factor(monkey)14351\",\"2\":\"7.312500e+00\",\"3\":\"3.51125999\",\"4\":\"2.082586e+00\",\"5\":\"0.038348277\"},{\"1\":\"factor(monkey)14651\",\"2\":\"8.250000e+00\",\"3\":\"3.70525802\",\"4\":\"2.226566e+00\",\"5\":\"0.026905107\"},{\"1\":\"factor(monkey)14666\",\"2\":\"5.250000e+00\",\"3\":\"3.70525802\",\"4\":\"1.416905e+00\",\"5\":\"0.157807347\"},{\"1\":\"factor(monkey)14699\",\"2\":\"8.750000e+00\",\"3\":\"3.70525802\",\"4\":\"2.361509e+00\",\"5\":\"0.019000512\"},{\"1\":\"factor(monkey)14823\",\"2\":\"1.025000e+01\",\"3\":\"3.70525802\",\"4\":\"2.766339e+00\",\"5\":\"0.006110166\"},{\"1\":\"factor(monkey)14826\",\"2\":\"3.750000e+00\",\"3\":\"3.70525802\",\"4\":\"1.012075e+00\",\"5\":\"0.312521306\"},{\"1\":\"factor(monkey)14902\",\"2\":\"7.250000e+00\",\"3\":\"3.70525802\",\"4\":\"1.956679e+00\",\"5\":\"0.051544843\"},{\"1\":\"factor(monkey)14919\",\"2\":\"7.250000e+00\",\"3\":\"3.70525802\",\"4\":\"1.956679e+00\",\"5\":\"0.051544843\"},{\"1\":\"factor(monkey)14985\",\"2\":\"7.750000e+00\",\"3\":\"3.70525802\",\"4\":\"2.091622e+00\",\"5\":\"0.037523810\"},{\"1\":\"factor(monkey)15041\",\"2\":\"7.250000e+00\",\"3\":\"3.70525802\",\"4\":\"1.956679e+00\",\"5\":\"0.051544843\"},{\"1\":\"factor(monkey)15088\",\"2\":\"4.750000e+00\",\"3\":\"3.70525802\",\"4\":\"1.281962e+00\",\"5\":\"0.201092964\"},{\"1\":\"factor(monkey)15120\",\"2\":\"4.750000e+00\",\"3\":\"3.70525802\",\"4\":\"1.281962e+00\",\"5\":\"0.201092964\"},{\"1\":\"factor(monkey)15218\",\"2\":\"5.750000e+00\",\"3\":\"3.70525802\",\"4\":\"1.551849e+00\",\"5\":\"0.122016071\"},{\"1\":\"factor(monkey)15530\",\"2\":\"7.687500e+00\",\"3\":\"3.90118562\",\"4\":\"1.970555e+00\",\"5\":\"0.049924231\"},{\"1\":\"factor(monkey)15642\",\"2\":\"7.187500e+00\",\"3\":\"3.90118562\",\"4\":\"1.842389e+00\",\"5\":\"0.066651841\"},{\"1\":\"factor(monkey)15732\",\"2\":\"6.687500e+00\",\"3\":\"3.90118562\",\"4\":\"1.714222e+00\",\"5\":\"0.087778891\"},{\"1\":\"factor(monkey)15820\",\"2\":\"5.687500e+00\",\"3\":\"3.90118562\",\"4\":\"1.457890e+00\",\"5\":\"0.146178124\"},{\"1\":\"factor(monkey)15909\",\"2\":\"4.687500e+00\",\"3\":\"3.90118562\",\"4\":\"1.201558e+00\",\"5\":\"0.230719242\"},{\"1\":\"factor(monkey)15926\",\"2\":\"7.187500e+00\",\"3\":\"3.90118562\",\"4\":\"1.842389e+00\",\"5\":\"0.066651841\"},{\"1\":\"factor(monkey)16002\",\"2\":\"9.187500e+00\",\"3\":\"3.90118562\",\"4\":\"2.355053e+00\",\"5\":\"0.019326031\"},{\"1\":\"factor(monkey)16090\",\"2\":\"7.687500e+00\",\"3\":\"3.90118562\",\"4\":\"1.970555e+00\",\"5\":\"0.049924231\"},{\"1\":\"factor(monkey)16097\",\"2\":\"5.187500e+00\",\"3\":\"3.90118562\",\"4\":\"1.329724e+00\",\"5\":\"0.184871653\"},{\"1\":\"factor(monkey)16169\",\"2\":\"9.187500e+00\",\"3\":\"3.90118562\",\"4\":\"2.355053e+00\",\"5\":\"0.019326031\"},{\"1\":\"factor(monkey)16236\",\"2\":\"5.187500e+00\",\"3\":\"3.90118562\",\"4\":\"1.329724e+00\",\"5\":\"0.184871653\"},{\"1\":\"factor(monkey)16258\",\"2\":\"6.187500e+00\",\"3\":\"3.90118562\",\"4\":\"1.586056e+00\",\"5\":\"0.114043194\"},{\"1\":\"factor(monkey)16447\",\"2\":\"1.162500e+01\",\"3\":\"4.09876609\",\"4\":\"2.836219e+00\",\"5\":\"0.004954837\"},{\"1\":\"factor(monkey)16553\",\"2\":\"7.625000e+00\",\"3\":\"4.09876609\",\"4\":\"1.860316e+00\",\"5\":\"0.064064047\"},{\"1\":\"factor(monkey)16556\",\"2\":\"8.625000e+00\",\"3\":\"4.09876609\",\"4\":\"2.104292e+00\",\"5\":\"0.036393453\"},{\"1\":\"factor(monkey)16598\",\"2\":\"5.625000e+00\",\"3\":\"4.09876609\",\"4\":\"1.372364e+00\",\"5\":\"0.171231281\"},{\"1\":\"factor(monkey)16670\",\"2\":\"5.625000e+00\",\"3\":\"4.09876609\",\"4\":\"1.372364e+00\",\"5\":\"0.171231281\"},{\"1\":\"factor(monkey)16736\",\"2\":\"9.125000e+00\",\"3\":\"4.09876609\",\"4\":\"2.226280e+00\",\"5\":\"0.026924503\"},{\"1\":\"factor(monkey)16807\",\"2\":\"5.125000e+00\",\"3\":\"4.09876609\",\"4\":\"1.250376e+00\",\"5\":\"0.212379846\"},{\"1\":\"factor(monkey)16972\",\"2\":\"9.125000e+00\",\"3\":\"4.09876609\",\"4\":\"2.226280e+00\",\"5\":\"0.026924503\"},{\"1\":\"factor(monkey)17037\",\"2\":\"8.125000e+00\",\"3\":\"4.09876609\",\"4\":\"1.982304e+00\",\"5\":\"0.048585829\"},{\"1\":\"factor(monkey)17047\",\"2\":\"1.012500e+01\",\"3\":\"4.09876609\",\"4\":\"2.470256e+00\",\"5\":\"0.014197944\"},{\"1\":\"factor(monkey)17069\",\"2\":\"7.625000e+00\",\"3\":\"4.09876609\",\"4\":\"1.860316e+00\",\"5\":\"0.064064047\"},{\"1\":\"factor(monkey)17103\",\"2\":\"7.125000e+00\",\"3\":\"4.09876609\",\"4\":\"1.738328e+00\",\"5\":\"0.083435526\"},{\"1\":\"factor(monkey)17285\",\"2\":\"7.625000e+00\",\"3\":\"4.09876609\",\"4\":\"1.860316e+00\",\"5\":\"0.064064047\"},{\"1\":\"factor(monkey)17516\",\"2\":\"7.062500e+00\",\"3\":\"4.29777147\",\"4\":\"1.643294e+00\",\"5\":\"0.101631615\"},{\"1\":\"factor(monkey)17718\",\"2\":\"5.562500e+00\",\"3\":\"4.29777147\",\"4\":\"1.294275e+00\",\"5\":\"0.196814278\"},{\"1\":\"factor(monkey)17738\",\"2\":\"7.562500e+00\",\"3\":\"4.29777147\",\"4\":\"1.759633e+00\",\"5\":\"0.079744124\"},{\"1\":\"factor(monkey)17794\",\"2\":\"5.062500e+00\",\"3\":\"4.29777147\",\"4\":\"1.177936e+00\",\"5\":\"0.239988783\"},{\"1\":\"factor(monkey)17825\",\"2\":\"7.062500e+00\",\"3\":\"4.29777147\",\"4\":\"1.643294e+00\",\"5\":\"0.101631615\"},{\"1\":\"factor(monkey)17927\",\"2\":\"5.062500e+00\",\"3\":\"4.29777147\",\"4\":\"1.177936e+00\",\"5\":\"0.239988783\"},{\"1\":\"factor(monkey)18038\",\"2\":\"7.562500e+00\",\"3\":\"4.29777147\",\"4\":\"1.759633e+00\",\"5\":\"0.079744124\"},{\"1\":\"factor(monkey)18067\",\"2\":\"5.562500e+00\",\"3\":\"4.29777147\",\"4\":\"1.294275e+00\",\"5\":\"0.196814278\"},{\"1\":\"factor(monkey)18069\",\"2\":\"1.056250e+01\",\"3\":\"4.29777147\",\"4\":\"2.457669e+00\",\"5\":\"0.014692320\"},{\"1\":\"factor(monkey)18106\",\"2\":\"8.562500e+00\",\"3\":\"4.29777147\",\"4\":\"1.992312e+00\",\"5\":\"0.047469830\"},{\"1\":\"factor(monkey)18153\",\"2\":\"9.062500e+00\",\"3\":\"4.29777147\",\"4\":\"2.108651e+00\",\"5\":\"0.036011350\"},{\"1\":\"factor(monkey)18230\",\"2\":\"9.000000e+00\",\"3\":\"4.49801264\",\"4\":\"2.000884e+00\",\"5\":\"0.046531245\"},{\"1\":\"factor(monkey)18232\",\"2\":\"1.000000e+01\",\"3\":\"4.49801264\",\"4\":\"2.223204e+00\",\"5\":\"0.027133995\"},{\"1\":\"factor(monkey)18364\",\"2\":\"1.000000e+01\",\"3\":\"4.49801264\",\"4\":\"2.223204e+00\",\"5\":\"0.027133995\"},{\"1\":\"factor(monkey)18407\",\"2\":\"1.150000e+01\",\"3\":\"4.49801264\",\"4\":\"2.556685e+00\",\"5\":\"0.011184196\"},{\"1\":\"factor(monkey)18450\",\"2\":\"9.000000e+00\",\"3\":\"4.49801264\",\"4\":\"2.000884e+00\",\"5\":\"0.046531245\"},{\"1\":\"factor(monkey)18489\",\"2\":\"5.571141e+00\",\"3\":\"4.65807995\",\"4\":\"1.196017e+00\",\"5\":\"0.232870369\"},{\"1\":\"factor(monkey)18520\",\"2\":\"1.100000e+01\",\"3\":\"4.49801264\",\"4\":\"2.445524e+00\",\"5\":\"0.015183749\"},{\"1\":\"factor(monkey)18569\",\"2\":\"8.500000e+00\",\"3\":\"4.49801264\",\"4\":\"1.889723e+00\",\"5\":\"0.060000176\"},{\"1\":\"factor(monkey)18652\",\"2\":\"1.100000e+01\",\"3\":\"4.49801264\",\"4\":\"2.445524e+00\",\"5\":\"0.015183749\"},{\"1\":\"factor(monkey)18653\",\"2\":\"9.000000e+00\",\"3\":\"4.49801264\",\"4\":\"2.000884e+00\",\"5\":\"0.046531245\"},{\"1\":\"factor(monkey)18873\",\"2\":\"9.000000e+00\",\"3\":\"4.49801264\",\"4\":\"2.000884e+00\",\"5\":\"0.046531245\"},{\"1\":\"factor(monkey)18947\",\"2\":\"1.100000e+01\",\"3\":\"4.49801264\",\"4\":\"2.445524e+00\",\"5\":\"0.015183749\"},{\"1\":\"factor(monkey)19178\",\"2\":\"5.937500e+00\",\"3\":\"4.69933163\",\"4\":\"1.263478e+00\",\"5\":\"0.207643559\"},{\"1\":\"factor(monkey)19220\",\"2\":\"9.937500e+00\",\"3\":\"4.69933163\",\"4\":\"2.114662e+00\",\"5\":\"0.035490064\"},{\"1\":\"factor(monkey)19239\",\"2\":\"8.937500e+00\",\"3\":\"4.69933163\",\"4\":\"1.901866e+00\",\"5\":\"0.058386063\"},{\"1\":\"factor(monkey)22020\",\"2\":\"3.312500e+00\",\"3\":\"2.09299182\",\"4\":\"1.582663e+00\",\"5\":\"0.114815239\"},{\"1\":\"factor(monkey)22021\",\"2\":\"2.812500e+00\",\"3\":\"2.09299182\",\"4\":\"1.343770e+00\",\"5\":\"0.180291735\"},{\"1\":\"factor(monkey)22023\",\"2\":\"6.250000e-02\",\"3\":\"1.60442379\",\"4\":\"3.895480e-02\",\"5\":\"0.968958814\"},{\"1\":\"factor(monkey)22024\",\"2\":\"7.500000e-01\",\"3\":\"2.24900632\",\"4\":\"3.334806e-01\",\"5\":\"0.739062688\"},{\"1\":\"factor(monkey)22025\",\"2\":\"-1.437500e+00\",\"3\":\"1.60442379\",\"4\":\"-8.959603e-01\",\"5\":\"0.371171733\"},{\"1\":\"factor(monkey)22047\",\"2\":\"1.875000e+00\",\"3\":\"1.94769661\",\"4\":\"9.626756e-01\",\"5\":\"0.336679273\"},{\"1\":\"factor(monkey)22048\",\"2\":\"3.750000e-01\",\"3\":\"1.94769661\",\"4\":\"1.925351e-01\",\"5\":\"0.847485879\"},{\"1\":\"factor(monkey)22049\",\"2\":\"2.875000e+00\",\"3\":\"1.94769661\",\"4\":\"1.476103e+00\",\"5\":\"0.141227161\"},{\"1\":\"factor(monkey)22050\",\"2\":\"-1.000000e+00\",\"3\":\"1.70008898\",\"4\":\"-5.882045e-01\",\"5\":\"0.556948129\"},{\"1\":\"factor(monkey)22052\",\"2\":\"1.562500e+00\",\"3\":\"1.60442379\",\"4\":\"9.738699e-01\",\"5\":\"0.331101681\"},{\"1\":\"factor(monkey)22053\",\"2\":\"2.625000e+00\",\"3\":\"1.53243949\",\"4\":\"1.712955e+00\",\"5\":\"0.088012235\"},{\"1\":\"factor(monkey)22054\",\"2\":\"1.875000e+00\",\"3\":\"1.94769661\",\"4\":\"9.626756e-01\",\"5\":\"0.336679273\"},{\"1\":\"factor(monkey)22055\",\"2\":\"3.625000e+00\",\"3\":\"1.53243949\",\"4\":\"2.365509e+00\",\"5\":\"0.018801226\"},{\"1\":\"factor(monkey)22056\",\"2\":\"2.937500e+00\",\"3\":\"1.81569582\",\"4\":\"1.617837e+00\",\"5\":\"0.107011195\"},{\"1\":\"factor(monkey)22057\",\"2\":\"2.000000e+00\",\"3\":\"1.70008898\",\"4\":\"1.176409e+00\",\"5\":\"0.240596983\"},{\"1\":\"factor(monkey)22058\",\"2\":\"2.500000e-01\",\"3\":\"1.47232024\",\"4\":\"1.698000e-01\",\"5\":\"0.865310455\"},{\"1\":\"factor(monkey)22060\",\"2\":\"-1.250000e-01\",\"3\":\"1.94769661\",\"4\":\"-6.417837e-02\",\"5\":\"0.948881619\"},{\"1\":\"factor(monkey)22062\",\"2\":\"2.500000e+00\",\"3\":\"1.70008898\",\"4\":\"1.470511e+00\",\"5\":\"0.142733141\"},{\"1\":\"factor(monkey)22064\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"NA\",\"5\":\"NA\"},{\"1\":\"age:factor(day)2\",\"2\":\"2.808043e-02\",\"3\":\"0.02493246\",\"4\":\"1.126260e+00\",\"5\":\"0.261180467\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nSo the first thing you will notice is that that is _a lot_ of regression coefficients!\nThere are 243 monkeys and 2 days, but only 485 observations. This isn't enough data\nto reliably estimate all of these parameters. (Look at the standard errors for the monkey-related coefficients. They are huge!)\n\nSo what are we to do?\n\nThe problem is the monkeys. If we use `monkey` as a factor variable, we only have (at most) two observations of each factor level. This is  simply not enough observations per to estimate a different intercept for each monkey!\n\nThis type of model is often described as having _no pooling_, which indicates that there is no explicit dependence between the intercepts for each group (`monkey`). (There is some dependence between groups due to the group-level covariate `age`.)\n\n### If we ignore the monkeys, will they go away? or Another attempt at regression \n\nOur first attempt at a regression model didn't work particularly well, but that doesn't mean we should give up^[If statisticians abandoned linear regression we would have nothing left. We would be desiccated husks propping up the bar at 3am talking about how we used to do loads of lines in the 80s.]. A second option is that we can assume that there is, fundamentally, no difference between monkeys. If all monkeys of the same age have similar amounts of interest in new puzzles, this would be a reasonable assumption. The best case scenario is that not accounting for differences between individual monkeys would still lead to approximately normal residuals, albeit with probably a larger residual variance. \n\nThis type of modelling assumption is called _complete pooling_ as it pools the information between groups by treating them all as the same. \n\nLet's see what happens in this case!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_lm_pool <- lm(active_bins ~ age*factor(day), data = activity_2mins)\nsummary(fit_lm_pool)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = active_bins ~ age * factor(day), data = activity_2mins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5249 -1.5532  0.1415  1.6731  4.1884 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      3.789718   0.344466  11.002   <2e-16 ***\nage              0.003126   0.021696   0.144    0.885    \nfactor(day)2     0.056112   0.488818   0.115    0.909    \nage:factor(day)2 0.025170   0.030759   0.818    0.414    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.103 on 481 degrees of freedom\nMultiple R-squared:  0.01365,\tAdjusted R-squared:  0.0075 \nF-statistic: 2.219 on 3 and 481 DF,  p-value: 0.0851\n```\n:::\n:::\n\n\nOn the up side, the regression runs and doesn't have too many parameters!\n\nThe brave and the bold might even try to interpret the coefficients and say something like _there doesn't seem to be a strong effect of age_. But there's real danger in trying to interpret regression coefficients in the presence of a potential confounder (in this case, the monkey ID). And it's particularly bad form to do this without ever looking at any sort of regression diagnostics. Linear regression is not a magic eight ball. \n\n\nLet's look at the diagnostic plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\naugment(fit_lm_pool) |> \n  ggplot(aes(x = .fitted, y = active_bins - .fitted)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](everybodys-got-something-to-hide-except-me-and-my-monkey_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\naugment(fit_lm_pool) |> ggplot(aes(sample = .std.resid)) + \n  stat_qq() +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") + \n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](everybodys-got-something-to-hide-except-me-and-my-monkey_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n:::\n\n\nThere are certainly some patterns in those residuals (and some suggestion that the error need a heavier tail for this model to make sense).\n\n## What is between no pooling and complete pooling? Multilevel models, that's what\n\nWe are in a Goldilocks situation: no pooling results in a model that has too many \nindependent parameters for the amount of data that we've got, while complete\npooling has too few parameters to correctly account for the differences between the \nmonkeys. So what is our perfectly tempered porridge^[Our perfect amount of pool? I don't know how metaphors work]? \n\nThe answer is to assume that each monkey has its own intercept, but that it's intercept\ncan only be _so far_ from the overall intercept (that we would've gotten from complete\npooling). There are a bunch of ways to realize this concept, but the classical method\nis to use a normal distribution. \n\nIn particular, if the $j$th monkey has observations $y_{ij}$, $i=1,2$, then we \ncan write our model as \n$$\ny_{ij}  \\sim N(\\mu_j + \\beta_\\text{age}\\, \\text{age}_j + \\beta_\\text{day}\\, \\text{day}_{ij} + \\beta_\\text{age,day}\\, \\text{[age*day]}_{ij}, \\sigma^2).\n$$\n\nThe effects of age and day and the data standard deviation ($\\sigma$) are just like they'd be in an ordinary linear regression model. Our modification comes in how we treat the $\\mu_j$.\n\nIn a classical linear regression model, we would fit the $\\mu_j$s independently, \nperhaps with some weakly informative prior distribution. But we've already discussed \nthat that won't work.\n\nInstead we will make the $\\mu_j$ _exchangeable_ rather than independent. Exchangeability is a relaxation of the independence assumption to say instead encode that we have no idea which of the intercepts will do what. That is, if we switch around the labels of our intercepts the prior should not change. There is a long and storied history of exchangeable models in statistics, but the short version that is more than sufficient for our purposes is that they usually^[They _always_ take this form if there is a countable collection of exchangeable random variables. For a finite set there are a few more options. But no one talks about those.] take the form\n\\begin{align*}\n\\mu_j \\mid \\tau \\stackrel{\\text{iid}}{\\sim} &p(\\mu_j \\mid \\tau), \\qquad i = 1,\\ldots, J \\\\\n \\tau \\sim & p(\\tau).\n\\end{align*}\n\nIn a regression context, we typically assume that \n$$\n\\mu_j \\mid \\tau \\sim N(\\mu, \\tau^2)\n$$\nfor some $\\mu$ and $\\tau$ that will need their own priors.\n\nWe can explore this difference mathematically. The regression model, which assumes independence of the $\\mu_j$, uses\n$$\np(\\mu_1, \\ldots, \\mu_J) = \\prod_{j=1}^J N(\\mu, \\tau_\\text{fixed}^2)\n$$\nas the joint prior on $\\mu_1,\\ldots,\\mu_J$. On the other hand, the exchangeable model, which forms the basis of multilevel models, assumes the joint prior \n$$\np(\\mu_1, \\ldots, \\mu_J) = \\int_0^\\infty \\left(\\prod_{j=1}^J N(\\mu, \\tau^2)\\right)p(\\tau)\\,d\\tau,\n$$\nfor some prior on $p(\\tau)$ on $\\tau$.\n\nThis might not seem like much of a change, but it can be quite profound. In both cases, the prior is saying that each $\\mu_j$ is, with high probability, at most $3\\tau$ away from the overall mean $\\mu$. The difference is that while the classical least squares formulation uses a fixed value of $\\tau$ that needs to be specified by the modeller, while the exchangeable model lets $\\tau$ adapt to the data.\n\nThis data adaptation is really nifty! It means that if the groups have similar means, they can borrow information from the other groups (via the narrowing of $\\tau$) in order to improve their precision over an unpooled estimate. On the other hand, if there is a meaningful difference between the groups^[monkeys], this model can still represent that, unlike the unpooled model.\n\nIn our context, however, we need a tiny bit more. We have a _group-level covariate_ (specifically `age`) that we think is going to effect the group mean. So the model we want is \n\\begin{align*}\ny_{ij}  \\mid \\mu_j,\\beta, \\sigma &\\sim N(\\mu_j + \\beta_\\text{day}\\, \\text{day}_{ij} + \\beta_\\text{age,day}\\, \\text{[age*day]}_{ij} , \\sigma^2) \\\\\n\\mu_j\\mid \\tau, \\mu,\\beta &\\sim N(\\mu +  \\beta_\\text{age}\\, \\text{age}_j, \\tau^2) \\\\\n\\mu &\\sim p(\\mu)\\\\\n\\beta &\\sim p(\\beta)\\\\\n\\tau & \\sim p(\\tau) \\\\\n\\sigma &\\sim p(\\sigma).\n\\end{align*}\n\n\nIn order to fully specify the model we need to set the four prior  distributions.\n\nThis is an example of a _multilevel_^[Also known as a mixed effects or a linear mixed effects model.] _model_. The name comes from the data having multiple levels (in this case two: the observation level and the group level). Both levels have an appropriate model for their mean. \n\nThis mathematical representation does a good job in separating out the two different levels. However, there are a lot of other ways of writing multilevel models. An important example is the extended formula notation created^[There are _many_ other ways to represnt Gaussian multilevel models. My former colleague Emi Tanaka and Francis Hui wrote a [great paper](https://arxiv.org/abs/1911.08628) on this topic.] by R's `lme4` package. In their notation, we would write this model as \n\n::: {.cell}\n\n```{.r .cell-code}\nformula <- active_bins_scaled ~ age_centred*day + (1 | monkey)\n```\n:::\n\n\nThe first bit of this formula is the same as the formula used in linear regression.\nThe interesting bit is is the `(1 | monkey)`. This is the way to tell R that the intercept (aka `1` in formula notation) is going to be grouped by `monkey` and we are going to put an exchangeable normal prior on it. For more complex models there are more complex variations on this theme, but for the moment we won't go any further. \n\n\n## Reasoning out some prior distributions\n\nWe need to set priors. The canny amongst you may have noticed that I did not set priors in the previous two examples. There are two reasons for this: firstly I didn't feel like it, and secondly none but the most terrible prior distributions would have meaningfully changed the conclusions. This is, it turns out, one of the great truths when it comes to prior distributions: _they do not matter until they do_^[Some particularly bold and foolish people take this to mean that priors aren't important. They usually get their arse handed to them the moment they try to fit an even mildly complex model.].\n\nIn particular, if you have a parameter that _directly_ sees the data (eg it's in the likelihood) and there is nothing weird going on^[A non-exhaustive set of weird things: categorical regressors with a rare category, tail parameters, mixture models], then the prior distribution will usually not do much as any prior will be quickly overwhelmed by the data. \n\nThe problem is that we have one parameter in our model ($\\tau$) that does not directly see the data. Instead of directly telling us about an observation, it tells us about how different the _groups_ of observations are. There is usually less information in the data about this type of parameter and, consequently, the prior distribution will be more important. This is especially true when you have more than one grouping variable, or when a variable only has a small number of groups.\n\nSo let's pay some proper attention to the priors.\n\nTo begin with, let's set priors on $\\mu$, $\\beta$, and $\\sigma$ (aka the data-level parameters). This is a _considerably_ easier task if the data is scaled. Otherwise, you need to encode information about the usual scale^[There are situations where this is not true. For instance if you have a log or logit link function you can put reasonable bounds on your coefficients regardless of the scaling of your data. That said, the computational procedures _always_ appreciate a bit of scaling. If there's one thing that computers hate more that big numbers it's small numbers.] of the data into your priors. Sometimes this is a sensible and easy thing to do, but usually it's easier to simply scale the data. (A lot of software will simply scale your data for you, but it is _always_ better to do it yourself!)\n\nSo let's scale our data. We have three variables that need scaling: `age` (aka the covariate that isn't categorical) and `active_bins` (aka the response).\nFor age, we are going to want to measure it as either _years from the youngest monkey_ or _years from the average monkey_. I think, in this situation, the first version could make a lot of sense, but we are going with the second. This allows us to interpret $\\mu$ as the over-all mean. Otherwise, $\\mu$ would tell us about the overall average activity of 4 year old monkeys and we will use $\\beta(\\text{age}_j - 4)$ to estimate how much the activity changes, on average keeping all other aspects constant, as the monkey ages.  \n\nOn the other hand, we have no sensible baseline for activity, so deviation from the average seems like a sensible scaling. I also don't know, _a priori_, how variable activity is going to be, so I might want to scale^[Of course, we know that the there are only 8 fifteen second intervals in two minutes, so we could use this information to make a data-independent scaling. To be brutally francis with you, that's what you should probably do in this situation, but I'm trying to be pedagogical so let's at least think about scaling it by the standard deviation.] it by its standard deviation. In this case, I'm not going to do that because we have a sensible fixed^[Fixed scaling is always easier than data-dependent scaling] upper limit (8), which I can scale by.\n\nOne important thing here is that if we scale the data by data-dependent quantities (like the minimum, the mean, or the standard deviation) we _must_ keep track of this information. This is because _any_ future data we try to predict with this model will need to be transformed _the same way using the same_^[A real trick for young players is scaling new data by the mean and standard deviation of the new data rather than the old data. That's a very subtle bug that can be _very_ hard to squash.] _numbers_! This particularly has implication when you are doing things like test/training set validation or cross validation: in the first case, the test set needs to be scaled in the same way the training set was; while in the second case each cross validation training set needs to be scaled independently and that scaling needs to be used on the corresponding left-out data^[The `tidymodels` package in R is a great example of an ecosystem that does this properly. [Max and Julia's book](https://www.tmwr.org) on using `tidymodels` is very excellent and well worth a read.].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nage_centre <- mean(activity_2mins$age)\nage_scale <- diff(range(activity_2mins$age))/2\nactive_bins_centre <- 4\n\nactivity_2mins_scaled <- activity_2mins |>\n  mutate(monkey = factor(monkey),\n         day = factor(day),\n         age_centred = (age - age_centre)/age_scale,\n         active_bins_scaled = (active_bins - active_bins_centre)/4)\n```\n:::\n\n\nWith our scaling completed, we can now start thinking about prior distributions. The trick with priors is to make them wide enough to cover all plausible values of a parameter without making them so wide that they put a whole bunch of weight on essentially silly values.  \n\nWe know, for instance, that our unscaled activity will go between 0 and 8. That means that it's unlikely for the mean of the scaled process to be much bigger than 3 or 4. These considerations, along with the fact that we have centred the data so the mean should be closer to zero, suggest that a $N(0,1)$ prior should be appropriate for $\\mu$. \n\nAs we normalised our age data relative to the smallest age, we should think more carefully about the scaling of $\\beta$. Macaques live for 20-30^[Of all of the things in this post, this has been the most aggressively fact checked one] years, so we need to think about, for instance, an ordinary aged macaque that would be 15 years older than the baseline. Thanks to our scaling, the largest change that we can have is around 1, which strongly suggests that if $\\beta$ was too much larger than $1/8$ we are going to be in unreasonable territory. So let's put a $N(0,0.2^2)$ prior^[In prior width and on grindr, you should always expect that he's rounding up.] on $\\beta_\\text{age}$ and $\\beta_\\text{age,day}$. For $\\beta_\\text{day}$ we can use a $N(0,1)$ prior. \n\nSimilarly, the scaling of `activity_bins` suggests that a $N(0,1)$ prior would be sufficient for the data-level standard deviation $\\sigma$.\n\nThat just leaves us with our choice of prior for the standard deviation of the intercept^[In some places, we would call this a random effect.] $\\mu_j$, $\\tau$. Thankfully, we considered this case in detail [in the previous blog post](https://dansblog.netlify.app/posts/2022-08-29-priors4/priors4.html). There I argued that a sensible prior for $\\tau$ would be an exponential prior. To be quite honest with you, a half-normal or a half-t also would be fine. But I'm going to stick to my guns. For the scaling, again, it would be a touch surprising (given the scaling of the data) if the group means were more than 3 apart, so choosing $\\lambda=1$ in the exponential distribution should give a relatively weak prior without being so wide that we are putting prior mass on a bunch of values that we would never actually want to put prior mass on.\n\nWe can then fit the model with `brms`. In this case, I'm using the `cmdstanr` back end, because it's fast and I like it.\n\nTo specify the model, we use the `lme4`-style formula notation discussed above.\n\nTo set the priors, we will use `brms`. Now, if you are Paul you might be able to remember how to set priors in `brms` without having to look it up, but I am sadly not Paul^[He is very lovely. Many people would prefer that I was him.], so every time I need to set priors in `brms` I write the formula and use the convenient `get_prior` function\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cmdstanr)\nlibrary(brms)\nget_prior(formula, activity_2mins_scaled)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"prior\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"class\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"coef\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"group\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"resp\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"dpar\"],\"name\":[6],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"nlpar\"],\"name\":[7],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"lb\"],\"name\":[8],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"ub\"],\"name\":[9],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"source\"],\"name\":[10],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"\",\"2\":\"b\",\"3\":\"\",\"4\":\"\",\"5\":\"\",\"6\":\"\",\"7\":\"\",\"8\":\"\",\"9\":\"\",\"10\":\"default\"},{\"1\":\"\",\"2\":\"b\",\"3\":\"age_centred\",\"4\":\"\",\"5\":\"\",\"6\":\"\",\"7\":\"\",\"8\":\"\",\"9\":\"\",\"10\":\"default\"},{\"1\":\"\",\"2\":\"b\",\"3\":\"age_centred:day2\",\"4\":\"\",\"5\":\"\",\"6\":\"\",\"7\":\"\",\"8\":\"\",\"9\":\"\",\"10\":\"default\"},{\"1\":\"\",\"2\":\"b\",\"3\":\"day2\",\"4\":\"\",\"5\":\"\",\"6\":\"\",\"7\":\"\",\"8\":\"\",\"9\":\"\",\"10\":\"default\"},{\"1\":\"student_t(3, 0, 2.5)\",\"2\":\"Intercept\",\"3\":\"\",\"4\":\"\",\"5\":\"\",\"6\":\"\",\"7\":\"\",\"8\":\"\",\"9\":\"\",\"10\":\"default\"},{\"1\":\"student_t(3, 0, 2.5)\",\"2\":\"sd\",\"3\":\"\",\"4\":\"\",\"5\":\"\",\"6\":\"\",\"7\":\"\",\"8\":\"0\",\"9\":\"\",\"10\":\"default\"},{\"1\":\"\",\"2\":\"sd\",\"3\":\"\",\"4\":\"monkey\",\"5\":\"\",\"6\":\"\",\"7\":\"\",\"8\":\"\",\"9\":\"\",\"10\":\"default\"},{\"1\":\"\",\"2\":\"sd\",\"3\":\"Intercept\",\"4\":\"monkey\",\"5\":\"\",\"6\":\"\",\"7\":\"\",\"8\":\"\",\"9\":\"\",\"10\":\"default\"},{\"1\":\"student_t(3, 0, 2.5)\",\"2\":\"sigma\",\"3\":\"\",\"4\":\"\",\"5\":\"\",\"6\":\"\",\"7\":\"\",\"8\":\"0\",\"9\":\"\",\"10\":\"default\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nFrom this, we can see that the default prior on $\\beta$ is an improper flat prior, the default prior on the intercept is a Student-t with 3 degrees of freedom centred at zero with standard deviation 2.5. The same prior (restricted to positive numbers) is put on all of the standard deviation parameters. These default prior distributions are, to be honest, probably fine in this context^[It's possible the the prior on $\\tau$ might be too wide. If we were doing a logistic regression, these priors would definitely be too wide. And if we had a lot of different random terms (eg if we had lots of different species or lots of different labs) then they would also probably be too wide. But they are better than not having priors.], but it is good practice to always set your prior.\n\nWe do this as follows. (Note that `brms` uses Stan, which parameterises the normal distribution by its mean and _standard deviation_!)\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- prior(normal(0, 0.2), coef = \"age_centred\") + \n  prior(normal(0,0.2), coef = \"age_centred:day2\") +\n  prior(normal(0, 1), coef = \"day2\") +\n  prior(normal(0,1), class = \"sigma\") +\n  prior(exponential(1), class = sd) + # tau\n  prior(normal(0,1), class = \"Intercept\")\n```\n:::\n\n\n## Pre-experiment prophylaxis\n\nSo we have specified some priors using the power of _our thoughts_. But we should probably check to see if they are broadly sensible. A great thing about Bayesian modelling is that we are explicitly specifying our _a priori_ (or pre-data) assumptions about the data generating process. That means that we can do a fast validation of our priors by simulating from them and checking that they're not too wild.\n\nThere are lots of ways to do this, but the easiest^[Not the most computationally efficient, but the easiest. Also because it's the same code we will later use to fit the model, we are evaluating the priors that are actually used and not the ones that we think we're using.] way to do this is to use the `sample_prior = \"only\"` option in the `brm()` function. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_draws <- brm(formula, \n           data = activity_2mins_scaled,\n           prior = priors,\n           sample_prior = \"only\",\n           backend = \"cmdstanr\",\n           cores = 4,\n           refresh = 0)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nStart sampling\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.5 seconds.\nChain 2 finished in 0.5 seconds.\nChain 3 finished in 0.5 seconds.\nChain 4 finished in 0.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 0.6 seconds.\n```\n:::\n:::\n\n\nNow that we have samples from the prior distribution, we can assemble them to work out what our prior tells us we would, pre-data, predict for the number of active bins for a single monkey (in this a single monkey^[ It's number 88, but because our prior is exchangeable it does not matter which monkey we do this for!] that is 10 years older than the baseline).\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_data <- data.frame(age_centred = 10, day = 1, monkey = \"88\") \ntibble(pred = brms::posterior_predict(prior_draws, \n                                      newdata = pred_data )) |>\n  ggplot(aes(pred)) +\n  geom_histogram(aes(y = after_stat(density)), fill = \"lightgrey\") +\n  geom_vline(xintercept = -1, linetype = \"dashed\") + \n  geom_vline(xintercept = 1, linetype = \"dashed\") +\n  xlim(c(-20,20)) + \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](everybodys-got-something-to-hide-except-me-and-my-monkey_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThe vertical lines are (approximately) the minimum and maximum of the data. This^[I also checked different values of `age` as well as looking at the posterior mean (via `posterior_epred`) and the conclusions stay the same.] suggests that the implied priors are definitely wider than our observed data, but they are not several orders of magnitude too wide. This is a good situation to be in: it gives enough room in the priors that we might be wrong with our specification while also not allowing for truly wild values of the parameters (and implied predictive distribution). One could even go so far as to say that the prior is weakly informative.\n\n\n\nLet's compare this to the default priors on the standard deviation parameters. (The default priors on the regression parameters are improper so we can't simulate from them. So I replaced the improper prior with a much narrower $N(0,10^2)$ prior. If you make the prior on the $\\beta$ wider the prior predictive distribution also gets wider.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors_default <- prior(normal(0,10), class = \"b\")\nprior_draws_default <- brm(formula, \n           data = activity_2mins_scaled,\n           prior = priors_default,\n           sample_prior = \"only\",\n           backend = \"cmdstanr\",\n           cores = 4,\n           refresh = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.4 seconds.\nChain 2 finished in 0.4 seconds.\nChain 3 finished in 0.4 seconds.\nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.4 seconds.\nTotal execution time: 0.5 seconds.\n```\n:::\n\n```{.r .cell-code}\ntibble(pred = brms::posterior_predict(prior_draws_default, \n                                      newdata = pred_data )) |>\n  ggplot(aes(pred)) +\n  geom_histogram(aes(y = after_stat(density)), fill = \"lightgrey\") +\n  geom_vline(xintercept = -1, linetype = \"dashed\") + \n  geom_vline(xintercept = 1, linetype = \"dashed\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](everybodys-got-something-to-hide-except-me-and-my-monkey_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\nThis is considerably wider.\n\n## Fitting the data; or do my monkeys get less interesting as they age\n\nWith all of that in hand, we can now fit the data. Hooray. This is done with the same command (minus the `sample_prior` bit).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_draws <- brm(formula, \n           data = activity_2mins_scaled,\n           prior = priors,\n           backend = \"cmdstanr\",\n           cores = 4,\n           refresh = 0)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nStart sampling\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 1.2 seconds.\nChain 2 finished in 1.2 seconds.\nChain 3 finished in 1.2 seconds.\nChain 4 finished in 1.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.2 seconds.\nTotal execution time: 1.3 seconds.\n```\n:::\n\n```{.r .cell-code}\nposterior_draws\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: active_bins_scaled ~ age_centred * day + (1 | monkey) \n   Data: activity_2mins_scaled (Number of observations: 485) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~monkey (Number of levels: 243) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.31      0.03     0.24     0.37 1.00     1351     2196\n\nPopulation-Level Effects: \n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept           -0.04      0.03    -0.11     0.02 1.00     4039     2964\nage_centred          0.01      0.07    -0.12     0.14 1.00     3902     3049\nday2                 0.10      0.04     0.03     0.18 1.00     8897     2663\nage_centred:day2     0.07      0.07    -0.07     0.22 1.00     6157     2947\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.43      0.02     0.39     0.47 1.00     1997     2630\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\nThere doesn't seem to be much of an effect of age in this data.\n\nIf you're curious, this matches well^[The numbers will never be exactly equal, but they are of similar orders of magnitude.] with  the output of `lme4`, which is a \nnice sense check for simple models. Generally speaking, if they're the same\nthen they're both fine. If they are different^[Or if you get some sort of error or warning from `lme4`], then you've got to look deeper.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4)\nfit_lme4 <- lmer(formula, activity_2mins_scaled)\nfit_lme4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: active_bins_scaled ~ age_centred * day + (1 | monkey)\n   Data: activity_2mins_scaled\nREML criterion at convergence: 734.9096\nRandom effects:\n Groups   Name        Std.Dev.\n monkey   (Intercept) 0.3091  \n Residual             0.4253  \nNumber of obs: 485, groups:  monkey, 243\nFixed Effects:\n     (Intercept)       age_centred              day2  age_centred:day2  \n        -0.04114           0.01016           0.10507           0.08507  \n```\n:::\n:::\n\n\n\nWe can also compare the fit using leave-one-out cross validation. This is similar to AIC, but more directly interpretable. It is the average of \n$$\n\\log p_\\text{posterior predictive}(y_{ij} \\mid y_{-ij}) = \\log \\left(\\int_\\theta p(y_{ij} \\mid \\theta)p(\\theta \\mid y_{-ij})\\, d\\theta\\right),\n$$\nwhere $\\theta$ is a vector of all of the parameters in the model. The notation $y_{-ij}$ is the data _without_ the $ij$th observation. This average is sometimes called the _expected log predictive density_ or elpd.\n\nTo compare it with the two linear regression models, I need to fit them in `brms`. I will use a $N(0,1)$ prior for the monkey intercepts and the same priors as the previous model for the other parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors_lm <-  prior(normal(0,1), class = \"b\") +\n  prior(normal(0, 0.2), coef = \"age_centred\") + \n  prior(normal(0,0.2), coef = \"age_centred:day2\") +\n  prior(normal(0, 1), coef = \"day2\") +\n  prior(normal(0,1), class = \"Intercept\") +\n  prior(normal(0,1), class = \"sigma\")\n\nposterior_nopool <- brm(\n  active_bins_scaled ~ age_centred * day + monkey, \n  data = activity_2mins_scaled,\n  prior = priors_lm,\n  backend = \"cmdstanr\",\n  cores = 4,\n  refresh = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 2.5 seconds.\nChain 3 finished in 2.5 seconds.\nChain 2 finished in 2.5 seconds.\nChain 4 finished in 2.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 2.5 seconds.\nTotal execution time: 2.7 seconds.\n```\n:::\n\n```{.r .cell-code}\nposterior_pool <- brm(\n  active_bins_scaled ~ age_centred * day, \n  data = activity_2mins_scaled,\n  prior = priors_lm,\n  backend = \"cmdstanr\",\n  cores = 4,\n  refresh = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.3 seconds.\n```\n:::\n:::\n\n\nWe an now use the `loo_compare` function to compare the models. By default, the best model is listed first and the other models are listed below it with the difference in elpd values given. To do this, we need to tell `brms` to compute the `loo` criterion using the `add_criterion` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_draws <- add_criterion(posterior_draws, \"loo\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Found 6 observations with a pareto_k > 0.7 in model 'posterior_draws'.\nIt is recommended to set 'moment_match = TRUE' in order to perform moment\nmatching for problematic observations.\n```\n:::\n\n```{.r .cell-code}\nposterior_nopool <- add_criterion(posterior_nopool, \"loo\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Found 61 observations with a pareto_k > 0.7 in model\n'posterior_nopool'. It is recommended to set 'moment_match = TRUE' in order to\nperform moment matching for problematic observations.\n```\n:::\n\n```{.r .cell-code}\nposterior_pool <- add_criterion(posterior_pool, \"loo\")\nloo_compare(posterior_draws, posterior_nopool, posterior_pool)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 elpd_diff se_diff\nposterior_draws    0.0       0.0  \nposterior_pool   -29.2       7.4  \nposterior_nopool -54.2       9.0  \n```\n:::\n:::\n\n\nThere are some warnings there suggesting that we could recompute these using a slower method, but for the purposes of today I'm not going to do that and I shall declare that the multilevel model performs _far better_ than the other two models.\n\n## Post-experiment prophylaxis\n\nOf course, we would be fools to just assume that because we fit a model and compared it to some other models, \nthe model is a good representation of the data. To do that, we need to look at some posterior checks.\n\nThe easiest thing to look at is the predictions themselves. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted <- activity_2mins_scaled |>\n  cbind(t(posterior_predict(posterior_draws,ndraws = 200))) |>\n  pivot_longer(8:207, names_to = \"draw\", values_to = \"fitted\")\n\nday_labs <- c(\"Day 1\", \"Day 2\")\nnames(day_labs) <- c(\"1\", \"2\")\n\nviolin_plot <- fitted |> \n  ggplot(aes( x=age, y = 4*fitted + active_bins_centre, group = age)) + \n  geom_violin(colour = \"lightgrey\") +\n  geom_point(aes(y = active_bins), colour = \"red\") +\n  facet_wrap(~day, labeller = labeller(day = day_labs)) +\n  theme_bw() \nviolin_plot\n```\n\n::: {.cell-output-display}\n![](everybodys-got-something-to-hide-except-me-and-my-monkey_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nThat appears to be a reasonably good fit, although it's possible that the prediction intervals are a bit wide. We can also look at the plot of the posterior residuals vs the fitted values. Here the fitted values are the mean of the posterior predictive distribution. \n\nNext, let's check for evidence of non-linearity in `age`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_data <- activity_2mins_scaled |>\n  mutate(fitted_mean = colMeans(posterior_epred(posterior_draws,ndraws = 200)))\n\nage_plot <- plot_data |> \n  ggplot(aes(x = age, y = active_bins_scaled - fitted_mean)) +\n  geom_point() +\n  theme_bw()\nage_plot\n```\n\n::: {.cell-output-display}\n![](everybodys-got-something-to-hide-except-me-and-my-monkey_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nThere doesn't seem to be any obvious evidence of non-linearity in the residuals, which suggests the linear model for age was sufficient.\n\nWe can also check the distributional assumption^[So there's a wrinkle here. Technically, all of the residuals have different variances, which is annoying. You typically studentise them using the leverage scores, but this is a touch trickier for multilevel models. Chapter 7 of [Jim Hodges's excellent book](https://www.google.com/search?client=safari&rls=en&q=richly+parametrized+linear+models&ie=UTF-8&oe=UTF-8) contains a really good discussion.] that the residuals \n$$\nr_{ij} = y_{ij} - \\mu_j\n$$\nhave a Gaussian distribution. We can check this with a qq-plot. Here we are using the \nposterior mean to define our residuals.\n\nWe can look at the qq-plot to see how we're doing with normality.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndistribution_plot <- plot_data |> ggplot(aes(sample = (active_bins_scaled - fitted_mean)/sd(active_bins_scaled - fitted_mean))) + \n  stat_qq() +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") + \n  theme_classic()\ndistribution_plot\n```\n\n::: {.cell-output-display}\n![](everybodys-got-something-to-hide-except-me-and-my-monkey_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\nThat's not too bad. A bit of a deviation from normality in the tails but nothing that would make me weep. It could well be an artifact of how I defined and normalised the residuals.\n\nWe can also look at the so-called k-hat plot, which can be useful for finding \nhigh-leverage observations in general models. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nloo_posterior <- LOO(posterior_draws) #warnings suppressed\nloo_posterior\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nComputed from 4000 by 485 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -349.5 12.3\np_loo       117.0  5.1\nlooic       699.0 24.6\n------\nMonte Carlo SE of elpd_loo is NA.\n\nPareto k diagnostic values:\n                         Count Pct.    Min. n_eff\n(-Inf, 0.5]   (good)     417   86.0%   748       \n (0.5, 0.7]   (ok)        62   12.8%   341       \n   (0.7, 1]   (bad)        6    1.2%   275       \n   (1, Inf)   (very bad)   0    0.0%   <NA>      \nSee help('pareto-k-diagnostic') for details.\n```\n:::\n\n```{.r .cell-code}\nplot(loo_posterior)\n```\n\n::: {.cell-output-display}\n![](everybodys-got-something-to-hide-except-me-and-my-monkey_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nThis suggests that observations 10, 233, 234, 367, 394, 479 are potentially high leverage and we should check them more carefully. I won't be doing that today.\n\n\n\nFinally, let's look at the residuals vs the fitted values. This is a commonly used\ndiagnostic plot in linear regression and it can be very useful for visually detecting\nnon-linear patterns and heteroskedasticity in the residuals. So let's make the plot^[Once again, we are not studentizing the residuals. I'm sorry.].\n\n::: {.cell}\n\n```{.r .cell-code}\nproblem_plot <- plot_data |> \n  ggplot(aes(x = fitted_mean, y = active_bins_scaled - fitted_mean)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, linetype = \"dashed\", colour = \"blue\")+\n  facet_wrap(~day) +\n  theme_bw() +  theme(legend.position=\"none\") +\n  xlim(c(-1,1)) +\n  ylim(c(-1,1))\nproblem_plot\n```\n\n::: {.cell-output-display}\n![](everybodys-got-something-to-hide-except-me-and-my-monkey_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\nHmmmm. That's not _excellent_. The stripes are related to the 8 distinct values the \nresponse can take, but there is definitely a trend in the residuals. In particular, we\nare under-predicting small values and over-predicting large values. _There is something here and we will look into it_!\n\n## Understanding diagnostic plots from multilevel models\n\n\nThe thing is, multilevel models are notorious for having patterns that are essentially\na product of the data design and not of any type of statistical misspecification. In a\nreally great paper that you should all read, [Adam Loy, Heike Hofmann, and Di Cook](https://arxiv.org/pdf/1502.06988.pdf) talk extensively about the challenges\nwith interpreting diagnostic plots for linear mixed effects models^[Another name for a multilevel model with a Gaussian response]. \n\nI'm not going to fully follow their recommendations, mostly because I'm too lazy^[Also because all of my data plots are gonna be stripey as hell, and that kinda destroys the point of visual inference.] to \nwrite a for loop, but I am going to appropriate the guts of their idea.\n\nThey note that strange patterns can occur in diagnostic plots _even for correctly specified models_. Moreover, we simply do not know what these patters will be. It's too complex a function of the design, the structure, the data, and the potential misspecification. That sounds bad, but they note that _we don't need to know what pattern to expect_. Why not? Because we can simulate it!\n\nSo this is the idea: Let's simulate some fake^[They call it _null data_.] data from a correctly specified model that otherwise matches with our data. We can then compare the diagnostic plots from the fake data with diagnostic plots from the real data and see if the patterns are meaningfully different. \n\nIn order to do this, we should have a method to construct _multiple_ fake data sets. Why? Well a plot is nothing but another test statistic and we _must_ take this variability into account.\n\n(That said, do what I say, not what I do. This is a blog. I'm not going to code well enough to make this clean and straightforward, so I'm just going to do one.)\n\nThere is an entire theory of [_visual inference_](https://royalsocietypublishing.org/doi/10.1098/rsta.2009.0120) that uses these lineups of diagnostic plots, where one uses the real data and the rest use realisations of the null data, that is really quite interesting and _well_ beyond the scope of this post. But if you want to know more, read the [Low, Hoffman, and Cook](https://arxiv.org/pdf/1502.06988.pdf) paper!\n\n### Making new data\nThe first thing that we need to do is to work out how to simulate fake data from a correctly specified model with the same structure. Following the Low etc paper, I'm going to do a simple parameteric bootstrap, where I take the posterior medians of the fitted distribution and simulate data from them. \n\nThat said, there are a bunch of other options. Specifically, we have a whole bag of samples from our posterior distribution and it would be possible to use that to select values of^[Note that I am _not_ using values of $\\mu_j$! I will simulate those from the normal distribution to ensure correct model specification. For the same reason, I am not using a residual bootstrap. The aim here is not to assess uncertainty so much as it is to ] $(\\mu, \\beta, \\tau, \\sigma)$ for our simulation. \n\n\nSo let's make some fake data and fit the model to it!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmonkey_effect <- tibble(monkey = unique(activity_2mins_scaled$monkey), \n                        monkey_effect = rnorm(243,0,0.31))\ndata_fake <- activity_2mins_scaled |>\n  left_join(monkey_effect, by = \"monkey\")  |>\n  mutate(active_bins_scaled = rnorm(length(age_centred),\n            mean = -0.04 +0.01 * age_centred + \n              monkey_effect + if_else(day == \"2\", 0.1 + 0.085 *age_centred, 0.0), \n            sd = 0.43))\n                                              \nposterior_draws_fake <- brm(formula, \n           data = data_fake,\n           prior = priors,\n           backend = \"cmdstanr\",\n           cores = 4,\n           refresh = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 1.2 seconds.\nChain 2 finished in 1.2 seconds.\nChain 3 finished in 1.3 seconds.\nChain 4 finished in 1.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.2 seconds.\nTotal execution time: 1.4 seconds.\n```\n:::\n:::\n\n\n### The good plots\n\nFirst up, let's look at the violin plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cowplot)\nfitted_fake <- data_fake |>\n  cbind(t(posterior_predict(posterior_draws_fake,ndraws = 200))) |>\n  pivot_longer(8:207, names_to = \"draw\", values_to = \"fitted\")\n\nday_labs <- c(\"Day 1\", \"Day 2\")\nnames(day_labs) <- c(\"1\", \"2\")\n\nviolin_fake <- fitted_fake |> \n  ggplot(aes( x=age, y = 4*fitted + active_bins_centre, group = age)) + \n  geom_violin(colour = \"lightgrey\") +\n  geom_point(aes(y = active_bins), colour = \"red\") +\n  facet_wrap(~day, labeller = labeller(day = day_labs)) +\n  theme_bw() \n  \nplot_grid(violin_plot, violin_fake, labels = c(\"Real\", \"Fake\"))\n```\n\n::: {.cell-output-display}\n![](everybodys-got-something-to-hide-except-me-and-my-monkey_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\nThat's very similar to our data plot.\n\nNext up, we will look at the residuals ordered by age\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_data_fake <- data_fake |>\n  mutate(fitted_mean = colMeans(posterior_epred(posterior_draws_fake,ndraws = 200)))\n\nage_fake <- plot_data_fake |> \n  ggplot(aes(x = age, y = active_bins_scaled - fitted_mean)) +\n  geom_point() +\n  theme_bw()\nplot_grid(age_plot, age_fake, labels = c(\"Real\", \"Fake\"))\n```\n\n::: {.cell-output-display}\n![](everybodys-got-something-to-hide-except-me-and-my-monkey_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\nFabulous!\n\nNow let's check the distributional assumption on the residuals!\n\n::: {.cell}\n\n```{.r .cell-code}\ndistribution_fake <- plot_data_fake |>\n  ggplot(aes(sample = (active_bins_scaled - fitted_mean)/sd(active_bins_scaled - fitted_mean))) + \n  stat_qq() +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") + \n  theme_classic()\n\nplot_grid(distribution_plot, distribution_fake, labels = c(\"Real\", \"Fake\"))\n```\n\n::: {.cell-output-display}\n![](everybodys-got-something-to-hide-except-me-and-my-monkey_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\nExcellent!\n\nFinally, we can look at the k-hat plot. Because I'm lazy, I'm not going to put them side by side. You can scroll.\n\n::: {.cell}\n\n```{.r .cell-code}\nloo_fake <- LOO(posterior_draws_fake)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Found 4 observations with a pareto_k > 0.7 in model\n'posterior_draws_fake'. It is recommended to set 'moment_match = TRUE' in order\nto perform moment matching for problematic observations.\n```\n:::\n\n```{.r .cell-code}\nloo_fake\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nComputed from 4000 by 485 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -362.3 14.3\np_loo       115.1  5.8\nlooic       724.6 28.5\n------\nMonte Carlo SE of elpd_loo is NA.\n\nPareto k diagnostic values:\n                         Count Pct.    Min. n_eff\n(-Inf, 0.5]   (good)     422   87.0%   571       \n (0.5, 0.7]   (ok)        59   12.2%   205       \n   (0.7, 1]   (bad)        4    0.8%   169       \n   (1, Inf)   (very bad)   0    0.0%   <NA>      \nSee help('pareto-k-diagnostic') for details.\n```\n:::\n\n```{.r .cell-code}\nplot(loo_fake)\n```\n\n::: {.cell-output-display}\n![](everybodys-got-something-to-hide-except-me-and-my-monkey_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\nAnd look: we get some extreme values. (Depending on the run we get more or less). This suggests that while it would be useful to look at the data points flagged by the k-hat statistic, it may just be sampling variation.;\n\nAll of this suggests our model assumptions are not being grossly violated. All except for that residual vs fitted values plot...\n\n### The haunted residual vs fitted plot\n\nNow let's look at our residual vs fitted plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nproblem_fake <- plot_data_fake |> \n  ggplot(aes(x = fitted_mean, y = active_bins_scaled - fitted_mean)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, linetype = \"dashed\", colour = \"blue\")+\n  facet_wrap(~day) +\n  theme_bw() +  theme(legend.position=\"none\") +\n  xlim(c(-1,1)) +\n  ylim(c(-1,1))\nplot_grid(problem_plot, problem_fake, labels = c(\"Real\", \"Fake\"))\n```\n\n::: {.cell-output-display}\n![](everybodys-got-something-to-hide-except-me-and-my-monkey_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\nAnd what do you know! They look the same. (Well, minus the discretisation artefacts.)\n\n### So what the hell is going on? \n\nGreat question! It turns out that this is one of those cases where our intuition from linear models _does not_ transfer over to multilevel models.\n\nWe can actually reason this out by thinking about a model where we have no covariates.\n\nIf we have no pooling then the observations for every monkey are, essentially, averaged to get our estimate of $\\mu_j$. If we repeat this, we will find that our $\\mu_j$ are basically^[This is a bit more complex when you're Bayesian, but the intuition still holds. The difference is that now it is asymptotic] unbiased and the corresponding residual \n$$\nr_{ij} = y_{ij} - \\mu_j\n$$\nwill have mean zero.\n\nBut that's not what happens when we have partial pooling. When we have partial pooling we are _combining_ our naive average^[This is the average of all observations in group j. $$\n\\bar y_j = \\frac{1}{n_j} \\sum_{i=1}^{n_j} y_{ij}.\n$$] $\\bar y_j$ with the global average $\\mu$ in a way that accounts for the size of group $j$ relative to other groups as well as the within-group variability relative to the between-group variability.\n\n<details><summary> Expand for maths. Just a little</summary>\nThere is, in fact, a formula for it. Just in case you're a formula sort of person. The posterior estimate for a Gaussian multilevel model with an intercept but no covariates is \n$$\n\\frac{1}{1 +\\frac{\\sigma^2/n}{\\tau^2}}\\left(\\bar{y}_j + \\frac{\\sigma^2/n}{\\tau^2} \\mu\\right).\n$$\nWhen $\\sigma/\\sqrt{n}$ is small, which happens when the sampling standard deviation of $\\bar y_j$ is small relative to the between group variation $\\tau$, this is almost equal to $\\bar{y}_j$ and there is almost no pooling. On the other hand, when $\\sigma/\\sqrt{n}$ is large relative to $\\tau$, then the estimate of $\\mu_j$ will be very close to the overall mean $\\mu$.\n</details>\n\nThe short version is that there is some magical number $\\alpha$, which depends on $\\tau$, $\\sigma$, and $n_j$ such that \n$$\n\\hat \\mu_j = \\alpha \\bar{y}_j + (1-\\alpha) \\mu.\n$$\nBecause of this, the residuals \n$$\nr_{ij} = y_j - \\alpha \\bar{y_j} - (1-\\alpha)\\mu\n$$\nare suddenly _not_ going to have mean zero. \n\nIn fact, if we think about it a bit more, we will realise that the model will drag extreme groups to the centre, which accounts for the positive slope in the residuals vs the fitted values.\n\nThe slope in this example is quite extreme because the groups are very small (only one or two individuals). But it is a general phenomenon and it's discussed extensively in Chapter 7 of [Jim Hodges' excellent book](http://www.biostat.umn.edu/~hodges/RPLMBook/RPLMBookpage.htm).  His suggestion is that there isn't really a good, general way to remove the trend. But that doesn't mean the plot is useless. It is still able to pinpoint outliers and heteroskedasticity. You've just got to tilt your head.\n\nBut for the purposes of today we can notice that there don't seem to be any extreme outliers so everything is probably ok.\n\n## Conclusion\n\nSo what have we done? Well we've gone through the process of fitting and scruitinising a simple Bayesian multilevel model. We've talked about some of the challenges associated with graphical diagnostics for structured data. And we've all^[I mean, some of us knew this. Personally, I only remembered after I saw it and swore a bit.] learnt something about the residual-vs-fitted plot for a multilevel model.\n\nMost importantly, we've all learnt the value of using fake data simulated from the posterior model to help us understand our diagnostics.\n\nThere is more to the scientific story here. It turns out that while there is no effect over 2 minutes, there is [a slight effect over 20 minutes](https://royalsocietypublishing.org/doi/10.1098/rsos.200316). So the conceptual replication failed, but still found some interesting things.\n\nOf course, I've ignored one big elephant in the room: That data was discrete. In the end, our distributional diagnostics didn't throw up any massive red flags, but nevertheless it could be an interesting exercise to see what happens if we use a more problem-adapted likelihood.\n\nLast, and certainly not least, I barely scratched the surface^[In particular, they have an interesting discussion on assessing the distributional assumption for $\\mu_j$.] of the [Loy, Hoffman, and Cook](https://arxiv.org/pdf/1502.06988.pdf) paper. Anyone who is interested in fitting Gaussian multilevel models should definitely give it a read.\n\n",
    "supporting": [
      "everybodys-got-something-to-hide-except-me-and-my-monkey_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}